<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Engineering Math</sitename>
    <dbname>engineeringmathwiki</dbname>
    <base>https://engineeringmath.miraheze.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.34.1</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Engineering Math</namespace>
      <namespace key="5" case="first-letter">Engineering Math talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Main Page</title>
    <ns>0</ns>
    <id>1</id>
    <revision>
      <id>1</id>
      <timestamp>2018-06-16T17:32:22Z</timestamp>
      <contributor>
        <username>MediaWiki default</username>
        <id>0</id>
      </contributor>
      <comment>Create main page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1751">__NOTOC__
== Welcome to {{SITENAME}}! ==
This Main Page was automatically created by a wiki creator (a volunteer who created this wiki per a request), and it seems it hasn't been replaced yet.

=== For the bureaucrat(s) of this wiki ===
Hello, and welcome at your new wiki! Thank you for choosing Miraheze for the hosting of your wiki, and we hope you will enjoy our hosting.

You can immediately start working on your wiki, whenever you want.

Need help? No problem! We will help you with your wiki as needed. To make a start we have added a few links about working with MediaWiki:
* &lt;span class="plainlinks"&gt;[https://www.mediawiki.org/wiki/Help:Contents MediaWiki guide (e.g. navigation, editing, deleting pages, blocking users)]&lt;/span&gt;
* &lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/FAQ Miraheze FAQ]&lt;/span&gt;
*&lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/Request_features Request settings changes on your wiki. (Extensions and Logo/Favicon changes should be done through Special:ManageWiki on your wiki].&lt;/span&gt;

==== But Miraheze, I still don't understand X! ====
Well, that's no problem. Even if something isn't explained in the documentation/FAQ, we still are happy to help you. You can find us here:
* &lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/Help_center On our own Miraheze wiki]&lt;/span&gt;
* On IRC in #miraheze on irc.freenode.net ([irc://irc.freenode.net/#miraheze direct link]; [http://webchat.freenode.net?channels=%23miraheze webchat])

=== For a visitor of this wiki ===
Hello, the default Main Page of this wiki (this is the default Main Page) has not been replaced yet by the bureaucrat(s) of this wiki. The bureaucrat(s) might still be working on a Main Page, so please check this page again later!</text>
      <sha1>33gfoswy9uje2kijx14lhtadxul5ej8</sha1>
    </revision>
    <revision>
      <id>2</id>
      <parentid>1</parentid>
      <timestamp>2018-06-16T17:54:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Welcome to {{SITENAME}}! */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1766">__NOTOC__
== Welcome to {{SITENAME}}! ==
This Main Page was automatically created by a wiki creator (a volunteer who created this wiki per a request), and it seems it hasn't been replaced yet.

=== For the bureaucrat(s) of this wiki ===
Hello, and welcome at your new wiki! Thank you for choosing Miraheze for the hosting of your wiki, and we hope you will enjoy our hosting.

You can immediately start working on your wiki, whenever you want.

Need help? No problem! We will help you with your wiki as needed. To make a start we have added a few links about working with MediaWiki:
* &lt;span class="plainlinks"&gt;[https://www.mediawiki.org/wiki/Help:Contents MediaWiki guide (e.g. navigation, editing, deleting pages, blocking users)]&lt;/span&gt;
* &lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/FAQ Miraheze FAQ]&lt;/span&gt;
*&lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/Request_features Request settings changes on your wiki. (Extensions and Logo/Favicon changes should be done through Special:ManageWiki on your wiki].&lt;/span&gt;

==== But Miraheze, I still don't understand X! ====
Well, that's no problem. Even if something isn't explained in the documentation/FAQ, we still are happy to help you. You can find us here:
* &lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/Help_center On our own Miraheze wiki]&lt;/span&gt;
* On IRC in #miraheze on irc.freenode.net ([irc://irc.freenode.net/#miraheze direct link]; [http://webchat.freenode.net?channels=%23miraheze webchat])

=== For a visitor of this wiki ===
Hello, the default Main Page of this wiki (this is the default Main Page) has not been replaced yet by the bureaucrat(s) of this wiki. The bureaucrat(s) might still be working on a Main Page, so please check this page again later!
&lt;math&gt;A&lt;/math&gt;</text>
      <sha1>rly93dk947lddkiao8g81h1u9vuckao</sha1>
    </revision>
    <revision>
      <id>3</id>
      <parentid>2</parentid>
      <timestamp>2018-06-16T17:58:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1751">__NOTOC__
== Welcome to {{SITENAME}}! ==
This Main Page was automatically created by a wiki creator (a volunteer who created this wiki per a request), and it seems it hasn't been replaced yet.

=== For the bureaucrat(s) of this wiki ===
Hello, and welcome at your new wiki! Thank you for choosing Miraheze for the hosting of your wiki, and we hope you will enjoy our hosting.

You can immediately start working on your wiki, whenever you want.

Need help? No problem! We will help you with your wiki as needed. To make a start we have added a few links about working with MediaWiki:
* &lt;span class="plainlinks"&gt;[https://www.mediawiki.org/wiki/Help:Contents MediaWiki guide (e.g. navigation, editing, deleting pages, blocking users)]&lt;/span&gt;
* &lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/FAQ Miraheze FAQ]&lt;/span&gt;
*&lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/Request_features Request settings changes on your wiki. (Extensions and Logo/Favicon changes should be done through Special:ManageWiki on your wiki].&lt;/span&gt;

==== But Miraheze, I still don't understand X! ====
Well, that's no problem. Even if something isn't explained in the documentation/FAQ, we still are happy to help you. You can find us here:
* &lt;span class="plainlinks"&gt;[https://meta.miraheze.org/wiki/Help_center On our own Miraheze wiki]&lt;/span&gt;
* On IRC in #miraheze on irc.freenode.net ([irc://irc.freenode.net/#miraheze direct link]; [http://webchat.freenode.net?channels=%23miraheze webchat])

=== For a visitor of this wiki ===
Hello, the default Main Page of this wiki (this is the default Main Page) has not been replaced yet by the bureaucrat(s) of this wiki. The bureaucrat(s) might still be working on a Main Page, so please check this page again later!</text>
      <sha1>33gfoswy9uje2kijx14lhtadxul5ej8</sha1>
    </revision>
    <revision>
      <id>224</id>
      <parentid>3</parentid>
      <timestamp>2018-10-06T14:39:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="309">__NOTOC__
== Hi! ==

This is my wiki where I keep my notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of my notes are about super simple things (that I constantly forget), others are slightly more advanced stuff.</text>
      <sha1>14h3sws8jragcwh46mc6rour2t8j7rw</sha1>
    </revision>
    <revision>
      <id>225</id>
      <parentid>224</parentid>
      <timestamp>2018-10-06T18:40:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Hi! */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="733">__NOTOC__
== Welcome to my notes ==

This is my wiki where I keep my notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of my notes are about fairly simple things (that I constantly forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010</text>
      <sha1>kn5x2kefaervc2akwks76dxtnqvj9qs</sha1>
    </revision>
    <revision>
      <id>317</id>
      <parentid>225</parentid>
      <timestamp>2018-11-17T22:04:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="787">__NOTOC__
== Welcome to my notes ==

This is my wiki where I keep my notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of my notes are about fairly simple things (that I constantly forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010
* Probabilistic Graphical Models, Daphne Koller, 2009</text>
      <sha1>hzvdg6ac49ndrxzp8z7k7lgvzc7mwdl</sha1>
    </revision>
    <revision>
      <id>351</id>
      <parentid>317</parentid>
      <timestamp>2019-01-15T12:27:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="774">__NOTOC__
== Welcome to my notes ==

This a wiki with notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of those notes are about fairly simple things (that are easy to forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010
* Probabilistic Graphical Models, Daphne Koller, 2009</text>
      <sha1>83zzwdwje47ml5m5wskcdf6ersrpko2</sha1>
    </revision>
    <revision>
      <id>482</id>
      <parentid>351</parentid>
      <timestamp>2019-08-04T14:17:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Welcome to my notes */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="850">__NOTOC__
== Welcome to my notes ==

This a wiki with notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of those notes are about fairly simple things (that are easy to forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010
* Probabilistic Graphical Models, Daphne Koller, 2009
* Intuitive Probability and Random Processes Using MATLAB, Steven Kay, 2006</text>
      <sha1>lct4pql9bya93y7kdht5tyv066a90rc</sha1>
    </revision>
    <revision>
      <id>520</id>
      <parentid>482</parentid>
      <timestamp>2019-12-11T08:52:43Z</timestamp>
      <contributor>
        <username>Reuben</username>
        <id>8</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="834">__NOTOC__
==notes==

This a wiki with notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of those notes are about fairly simple things (that are easy to forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010
* Probabilistic Graphical Models, Daphne Koller, 2009
* Intuitive Probability and Random Processes Using MATLAB, Steven Kay, 2006</text>
      <sha1>rxkvj23zdpco2nctd3p9qlbqzi93j4g</sha1>
    </revision>
    <revision>
      <id>532</id>
      <parentid>520</parentid>
      <timestamp>2020-03-21T14:41:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* notes */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="834">__NOTOC__
==Notes==

This a wiki with notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of those notes are about fairly simple things (that are easy to forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010
* Probabilistic Graphical Models, Daphne Koller, 2009
* Intuitive Probability and Random Processes Using MATLAB, Steven Kay, 2006</text>
      <sha1>cdpkaggafskfxv3qid90r3ejaw5r2u4</sha1>
    </revision>
    <revision>
      <id>571</id>
      <parentid>532</parentid>
      <timestamp>2020-04-14T13:32:51Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="962">__NOTOC__
==Notes==

This a wiki with notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of those notes are about fairly simple things (that are easy to forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010
* Probabilistic Graphical Models, Daphne Koller, 2009
* Intuitive Probability and Random Processes Using MATLAB, Steven Kay, 2006

----------------------
"If you go after just aesthetics, then often very important practical things will follow." Stephen Boyd</text>
      <sha1>5ylyrs1osz2kwe4ybo99if19iung4bt</sha1>
    </revision>
    <revision>
      <id>572</id>
      <parentid>571</parentid>
      <timestamp>2020-04-14T14:42:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1062">__NOTOC__
==Notes==

This a wiki with notes about some math topics that are commonplace in engineering, such as probability, linear algebra, functional analysis and optimization. Most of those notes are about fairly simple things (that are easy to forget), others are slightly more advanced stuff.

My notes are mainly from the following sources:
* Stephen Boyd, EE263 Introduction to Dynamical Systems Class
* Stephen Boyd, EE364A Convex Optimization I
* Stephen Boyd, Convex Optimization (book), 2008
* Athanasios Papoulis, Probability, Random Variables, and Stochastic Processes
* D. Harville, Matrix Algebra from a Statistician's Perspective
* Ole Christensen, Functions, Spaces, and Expansions, 2010
* Probabilistic Graphical Models, Daphne Koller, 2009
* Intuitive Probability and Random Processes Using MATLAB, Steven Kay, 2006

----------------------
"If you go after just aesthetics, then often very important practical things will follow."  Stephen Boyd &lt;ref&gt;Stephen Boyd, Online Convex Optimization I Class, Lecture 16, ~13:30 &lt;/ref&gt;

== References ==</text>
      <sha1>nolbxean1xhabgivds98ukdueurcyrz</sha1>
    </revision>
  </page>
  <page>
    <title>Linear dynamical systems</title>
    <ns>0</ns>
    <id>2</id>
    <revision>
      <id>4</id>
      <timestamp>2018-06-23T13:44:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with " == Definitions == A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;  &lt;math&gt;\frac{dx}{dt}=A(t) x..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1287">
== Definitions ==
A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;

&lt;math&gt;\frac{dx}{dt}=A(t) x(t) + B(t)u(t)&lt;/math&gt;, &lt;math&gt;\,\,\,\,y(t) = C(t) x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{R}&lt;/math&gt; denotes time, &lt;math&gt;x(t) \in \mathbb{R}^n&lt;/math&gt; is the state (vector), &lt;math&gt;u(t) \in \mathbb{R}^m&lt;/math&gt; is the input or control and &lt;math&gt;y(t) \in \mathbb{R}^p&lt;/math&gt; is the output. Further, &lt;math&gt;A(t) \in \mathbb{R}^{n\times n}&lt;/math&gt; is the ''dynamics matrix'', &lt;math&gt;B(t) \in \mathbb{R}^{n\times m}&lt;/math&gt; is the ''input matrix'', &lt;math&gt;C(t) \in \mathbb{R}^{p\times n}&lt;/math&gt; is the ''output'' or  ''sensor'' matrix and &lt;math&gt;D(t) \in \mathbb{R}^{p\times m}&lt;/math&gt; is the ''feedthrough matrix''. The matrices &lt;math&gt;A,B,C,D&lt;/math&gt; are generally time-invariant.

For lighter appearance, the CT LDS can be written as:
&lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

A discrete-time linear dynamical system (DT LDS) has the form&lt;ref name='boyd_l1'/&gt;:
&lt;math&gt;x(t+1) = A(t)x(t) + B(t)u(t),&lt;/math&gt; &lt;math&gt;\,\,\,\, y(t) = C(t)x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{Z} = \{0,\pm 1, \pm 2, \ldots\}&lt;/math&gt; and the (vector) signals &lt;math&gt;x, u, y&lt;/math&gt; are sequences.




=== References ===</text>
      <sha1>o8he3ynfsggnc0acyu4fmrqw95lru5v</sha1>
    </revision>
    <revision>
      <id>5</id>
      <parentid>4</parentid>
      <timestamp>2018-06-23T13:46:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>Evangelos moved page [[Linear dynamical system]] to [[Linear dynamical systems]] without leaving a redirect</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1287">
== Definitions ==
A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;

&lt;math&gt;\frac{dx}{dt}=A(t) x(t) + B(t)u(t)&lt;/math&gt;, &lt;math&gt;\,\,\,\,y(t) = C(t) x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{R}&lt;/math&gt; denotes time, &lt;math&gt;x(t) \in \mathbb{R}^n&lt;/math&gt; is the state (vector), &lt;math&gt;u(t) \in \mathbb{R}^m&lt;/math&gt; is the input or control and &lt;math&gt;y(t) \in \mathbb{R}^p&lt;/math&gt; is the output. Further, &lt;math&gt;A(t) \in \mathbb{R}^{n\times n}&lt;/math&gt; is the ''dynamics matrix'', &lt;math&gt;B(t) \in \mathbb{R}^{n\times m}&lt;/math&gt; is the ''input matrix'', &lt;math&gt;C(t) \in \mathbb{R}^{p\times n}&lt;/math&gt; is the ''output'' or  ''sensor'' matrix and &lt;math&gt;D(t) \in \mathbb{R}^{p\times m}&lt;/math&gt; is the ''feedthrough matrix''. The matrices &lt;math&gt;A,B,C,D&lt;/math&gt; are generally time-invariant.

For lighter appearance, the CT LDS can be written as:
&lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

A discrete-time linear dynamical system (DT LDS) has the form&lt;ref name='boyd_l1'/&gt;:
&lt;math&gt;x(t+1) = A(t)x(t) + B(t)u(t),&lt;/math&gt; &lt;math&gt;\,\,\,\, y(t) = C(t)x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{Z} = \{0,\pm 1, \pm 2, \ldots\}&lt;/math&gt; and the (vector) signals &lt;math&gt;x, u, y&lt;/math&gt; are sequences.




=== References ===</text>
      <sha1>o8he3ynfsggnc0acyu4fmrqw95lru5v</sha1>
    </revision>
    <revision>
      <id>6</id>
      <parentid>5</parentid>
      <timestamp>2018-06-23T13:55:50Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2123">
== Definitions ==
A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;

&lt;math&gt;\frac{dx}{dt}=A(t) x(t) + B(t)u(t)&lt;/math&gt;, &lt;math&gt;\,\,\,\,y(t) = C(t) x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{R}&lt;/math&gt; denotes time, &lt;math&gt;x(t) \in \mathbb{R}^n&lt;/math&gt; is the state (vector), &lt;math&gt;u(t) \in \mathbb{R}^m&lt;/math&gt; is the input or control and &lt;math&gt;y(t) \in \mathbb{R}^p&lt;/math&gt; is the output. Further, &lt;math&gt;A(t) \in \mathbb{R}^{n\times n}&lt;/math&gt; is the ''dynamics matrix'', &lt;math&gt;B(t) \in \mathbb{R}^{n\times m}&lt;/math&gt; is the ''input matrix'', &lt;math&gt;C(t) \in \mathbb{R}^{p\times n}&lt;/math&gt; is the ''output'' or  ''sensor'' matrix and &lt;math&gt;D(t) \in \mathbb{R}^{p\times m}&lt;/math&gt; is the ''feedthrough matrix''. The matrices &lt;math&gt;A,B,C,D&lt;/math&gt; are generally time-invariant.

For lighter appearance, the CT LDS can be written as:
&lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

A discrete-time linear dynamical system (DT LDS) has the form&lt;ref name='boyd_l1'/&gt;:
&lt;math&gt;x(t+1) = A(t)x(t) + B(t)u(t),&lt;/math&gt; &lt;math&gt;\,\,\,\, y(t) = C(t)x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{Z} = \{0,\pm 1, \pm 2, \ldots\}&lt;/math&gt; and the (vector) signals &lt;math&gt;x, u, y&lt;/math&gt; are sequences.


===What's the point?===
There are lots of reasons to study linear dynamical systems (LDSs). One can model a lot of engineering problems as an LDS; if one succeeds in modelling an engineering problem as an LDS, then solving it becomes very easy. The art is taking the real-life problem and transforming it into the form of &lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

====Categories of LDS problems===
#1: Estimation or inversion. This happens when we can observe &lt;math&gt;y&lt;/math&gt; and want to infer &lt;math&gt;x&lt;/math&gt; (e.g., your weight on the scale: we can measure the output of a strain gauge and we want to infer the weight from it).
#2: Control or design: We can control &lt;math&gt;x&lt;/math&gt; and we want to achieve a desired &lt;math&gt;y&lt;/math&gt;.
#3: Transformation: We want to find a mapping between &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;.





=== References ===</text>
      <sha1>34ayt238atq6dress1ugyu1eh7h34my</sha1>
    </revision>
    <revision>
      <id>7</id>
      <parentid>6</parentid>
      <timestamp>2018-06-23T14:08:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2545">
== Definitions ==
A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;

&lt;math&gt;\frac{dx}{dt}=A(t) x(t) + B(t)u(t)&lt;/math&gt;, &lt;math&gt;\,\,\,\,y(t) = C(t) x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{R}&lt;/math&gt; denotes time, &lt;math&gt;x(t) \in \mathbb{R}^n&lt;/math&gt; is the state (vector), &lt;math&gt;u(t) \in \mathbb{R}^m&lt;/math&gt; is the input or control and &lt;math&gt;y(t) \in \mathbb{R}^p&lt;/math&gt; is the output. Further, &lt;math&gt;A(t) \in \mathbb{R}^{n\times n}&lt;/math&gt; is the ''dynamics matrix'', &lt;math&gt;B(t) \in \mathbb{R}^{n\times m}&lt;/math&gt; is the ''input matrix'', &lt;math&gt;C(t) \in \mathbb{R}^{p\times n}&lt;/math&gt; is the ''output'' or  ''sensor'' matrix and &lt;math&gt;D(t) \in \mathbb{R}^{p\times m}&lt;/math&gt; is the ''feedthrough matrix''. The matrices &lt;math&gt;A,B,C,D&lt;/math&gt; are generally time-invariant.

For lighter appearance, the CT LDS can be written as:
&lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

A discrete-time linear dynamical system (DT LDS) has the form&lt;ref name='boyd_l1'/&gt;:
&lt;math&gt;x(t+1) = A(t)x(t) + B(t)u(t),&lt;/math&gt; &lt;math&gt;\,\,\,\, y(t) = C(t)x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{Z} = \{0,\pm 1, \pm 2, \ldots\}&lt;/math&gt; and the (vector) signals &lt;math&gt;x, u, y&lt;/math&gt; are sequences.


===What's the point?===
There are lots of reasons to study linear dynamical systems (LDSs). One can model a lot of engineering problems as an LDS; if one succeeds in modelling an engineering problem as an LDS, then solving it becomes very easy. The art is taking the real-life problem and transforming it into the form of &lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

====Categories of LDS problems====
#1: Estimation or inversion. This happens when we can observe &lt;math&gt;y&lt;/math&gt; and want to infer &lt;math&gt;x&lt;/math&gt; (e.g., your weight on the scale: we can measure the output of a strain gauge and we want to infer the weight from it).
#2: Control or design: We can control &lt;math&gt;x&lt;/math&gt; and we want to achieve a desired &lt;math&gt;y&lt;/math&gt; (e.g., you control the input of the CD drive's motor to bring the drive's head to a desired position).
#3: Transformation: We want to find a mapping between &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;.

Those have lots of applications, such as modelling the reaction of a building to wind or earthquake, modelling the current in circuit, predicting the temperature on various points of a circuit, predicting production cost of a certain product from various production elements, controlling the illumination on a surface... 

=== References ===</text>
      <sha1>8o86d47snde019c4lamy8ugym7mc4h3</sha1>
    </revision>
    <revision>
      <id>8</id>
      <parentid>7</parentid>
      <timestamp>2018-06-23T14:08:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Definitions */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2544">
== Definitions ==
A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;

&lt;math&gt;\frac{dx}{dt}=A(t) x(t) + B(t)u(t)&lt;/math&gt;, &lt;math&gt;\,\,\,\,y(t) = C(t) x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{R}&lt;/math&gt; denotes time, &lt;math&gt;x(t) \in \mathbb{R}^n&lt;/math&gt; is the state (vector), &lt;math&gt;u(t) \in \mathbb{R}^m&lt;/math&gt; is the input or control and &lt;math&gt;y(t) \in \mathbb{R}^p&lt;/math&gt; is the output. Further, &lt;math&gt;A(t) \in \mathbb{R}^{n\times n}&lt;/math&gt; is the ''dynamics matrix'', &lt;math&gt;B(t) \in \mathbb{R}^{n\times m}&lt;/math&gt; is the ''input matrix'', &lt;math&gt;C(t) \in \mathbb{R}^{p\times n}&lt;/math&gt; is the ''output'' or  ''sensor'' matrix and &lt;math&gt;D(t) \in \mathbb{R}^{p\times m}&lt;/math&gt; is the ''feedthrough matrix''. The matrices &lt;math&gt;A,B,C,D&lt;/math&gt; are generally time-invariant.

For lighter appearance, the CT LDS can be written as:
&lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

A discrete-time linear dynamical system (DT LDS) has the form&lt;ref name='boyd_l1'/&gt;:
&lt;math&gt;x(t+1) = A(t)x(t) + B(t)u(t),&lt;/math&gt; &lt;math&gt;\,\,\,\, y(t) = C(t)x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{Z} = \{0,\pm 1, \pm 2, \ldots\}&lt;/math&gt; and the (vector) signals &lt;math&gt;x, u, y&lt;/math&gt; are sequences.

===What's the point?===
There are lots of reasons to study linear dynamical systems (LDSs). One can model a lot of engineering problems as an LDS; if one succeeds in modelling an engineering problem as an LDS, then solving it becomes very easy. The art is taking the real-life problem and transforming it into the form of &lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

====Categories of LDS problems====
#1: Estimation or inversion. This happens when we can observe &lt;math&gt;y&lt;/math&gt; and want to infer &lt;math&gt;x&lt;/math&gt; (e.g., your weight on the scale: we can measure the output of a strain gauge and we want to infer the weight from it).
#2: Control or design: We can control &lt;math&gt;x&lt;/math&gt; and we want to achieve a desired &lt;math&gt;y&lt;/math&gt; (e.g., you control the input of the CD drive's motor to bring the drive's head to a desired position).
#3: Transformation: We want to find a mapping between &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;.

Those have lots of applications, such as modelling the reaction of a building to wind or earthquake, modelling the current in circuit, predicting the temperature on various points of a circuit, predicting production cost of a certain product from various production elements, controlling the illumination on a surface... 

=== References ===</text>
      <sha1>gqrqsloaumqt8geebfm7kejmymrs0q9</sha1>
    </revision>
    <revision>
      <id>19</id>
      <parentid>8</parentid>
      <timestamp>2018-07-06T23:40:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2582">
== Definitions ==
A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;

&lt;math&gt;\frac{dx}{dt}=A(t) x(t) + B(t)u(t)&lt;/math&gt;, &lt;math&gt;\,\,\,\,y(t) = C(t) x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{R}&lt;/math&gt; denotes time, &lt;math&gt;x(t) \in \mathbb{R}^n&lt;/math&gt; is the state (vector), &lt;math&gt;u(t) \in \mathbb{R}^m&lt;/math&gt; is the input or control and &lt;math&gt;y(t) \in \mathbb{R}^p&lt;/math&gt; is the output. Further, &lt;math&gt;A(t) \in \mathbb{R}^{n\times n}&lt;/math&gt; is the ''dynamics matrix'', &lt;math&gt;B(t) \in \mathbb{R}^{n\times m}&lt;/math&gt; is the ''input matrix'', &lt;math&gt;C(t) \in \mathbb{R}^{p\times n}&lt;/math&gt; is the ''output'' or  ''sensor'' matrix and &lt;math&gt;D(t) \in \mathbb{R}^{p\times m}&lt;/math&gt; is the ''feedthrough matrix''. The matrices &lt;math&gt;A,B,C,D&lt;/math&gt; are generally time-invariant.

For lighter appearance, the CT LDS can be written as:
&lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

A discrete-time linear dynamical system (DT LDS) has the form&lt;ref name='boyd_l1'/&gt;:
&lt;math&gt;x(t+1) = A(t)x(t) + B(t)u(t),&lt;/math&gt; &lt;math&gt;\,\,\,\, y(t) = C(t)x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{Z} = \{0,\pm 1, \pm 2, \ldots\}&lt;/math&gt; and the (vector) signals &lt;math&gt;x, u, y&lt;/math&gt; are sequences.

===What's the point?===
There are lots of reasons to study linear dynamical systems (LDSs). One can model a lot of engineering problems as an LDS; if one succeeds in modelling an engineering problem as an LDS, then solving it becomes very easy. The art is taking the real-life problem and transforming it into the form of &lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

====Categories of LDS problems====
#1: Estimation or inversion. This happens when we can observe &lt;math&gt;y&lt;/math&gt; and want to infer &lt;math&gt;x&lt;/math&gt; (e.g., your weight on the scale: we can measure the output of a strain gauge and we want to infer the weight from it).
#2: Control or design: We can control &lt;math&gt;x&lt;/math&gt; and we want to achieve a desired &lt;math&gt;y&lt;/math&gt; (e.g., you control the input of the CD drive's motor to bring the drive's head to a desired position).
#3: Transformation: We want to find a mapping between &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; (e.g. discrete cosine transformation).

Those have lots of applications, such as modelling the reaction of a building to wind or earthquake, modelling the current in circuit, predicting the temperature on various points of a circuit, predicting production cost of a certain product from various production elements, controlling the illumination on a surface... 

=== References ===</text>
      <sha1>d8fea5mckldefhunw4mjebzkkkghw9a</sha1>
    </revision>
    <revision>
      <id>138</id>
      <parentid>19</parentid>
      <timestamp>2018-07-21T14:05:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Categories of LDS problems */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2769">
== Definitions ==
A ''continuous-time'' linear dynamical system (CT LDS) has the form&lt;ref name='boyd_l1'&gt;Stephen Boyd, EE263 Lecture 1 notes&lt;/ref&gt;

&lt;math&gt;\frac{dx}{dt}=A(t) x(t) + B(t)u(t)&lt;/math&gt;, &lt;math&gt;\,\,\,\,y(t) = C(t) x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{R}&lt;/math&gt; denotes time, &lt;math&gt;x(t) \in \mathbb{R}^n&lt;/math&gt; is the state (vector), &lt;math&gt;u(t) \in \mathbb{R}^m&lt;/math&gt; is the input or control and &lt;math&gt;y(t) \in \mathbb{R}^p&lt;/math&gt; is the output. Further, &lt;math&gt;A(t) \in \mathbb{R}^{n\times n}&lt;/math&gt; is the ''dynamics matrix'', &lt;math&gt;B(t) \in \mathbb{R}^{n\times m}&lt;/math&gt; is the ''input matrix'', &lt;math&gt;C(t) \in \mathbb{R}^{p\times n}&lt;/math&gt; is the ''output'' or  ''sensor'' matrix and &lt;math&gt;D(t) \in \mathbb{R}^{p\times m}&lt;/math&gt; is the ''feedthrough matrix''. The matrices &lt;math&gt;A,B,C,D&lt;/math&gt; are generally time-invariant.

For lighter appearance, the CT LDS can be written as:
&lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

A discrete-time linear dynamical system (DT LDS) has the form&lt;ref name='boyd_l1'/&gt;:
&lt;math&gt;x(t+1) = A(t)x(t) + B(t)u(t),&lt;/math&gt; &lt;math&gt;\,\,\,\, y(t) = C(t)x(t) + D(t)u(t)&lt;/math&gt;

where &lt;math&gt;t \in \mathbb{Z} = \{0,\pm 1, \pm 2, \ldots\}&lt;/math&gt; and the (vector) signals &lt;math&gt;x, u, y&lt;/math&gt; are sequences.

===What's the point?===
There are lots of reasons to study linear dynamical systems (LDSs). One can model a lot of engineering problems as an LDS; if one succeeds in modelling an engineering problem as an LDS, then solving it becomes very easy. The art is taking the real-life problem and transforming it into the form of &lt;math&gt;\dot{x} = Ax + Bu&lt;/math&gt;, &lt;math&gt;y = Cx+Du&lt;/math&gt;.

====Categories of LDS problems====
#1: Estimation or inversion. This happens when we can observe &lt;math&gt;y&lt;/math&gt; and want to infer &lt;math&gt;x&lt;/math&gt; (e.g., your weight on the scale: we can measure the output of a strain gauge and we want to infer the weight from it).
#2: Control or design: We can control &lt;math&gt;x&lt;/math&gt; and we want to achieve a desired &lt;math&gt;y&lt;/math&gt; (e.g., you control the input of the CD drive's motor to bring the drive's head to a desired position).
#3: Transformation: We want to find a mapping between &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; (e.g. discrete cosine transformation).

Those have lots of applications, such as modelling the reaction of a building to wind or earthquake, modelling the current in circuit, predicting the temperature on various points of a circuit, predicting production cost of a certain product from various production elements, controlling the illumination on a surface... One can also perform smoothing, interpolation of signals, encode/decode signals. More specific applications would be, e.g., estimating the runtime of a computer program from measurements.

=== References ===</text>
      <sha1>bwacjchvmf5plzrygr3gu3h6rp86gm7</sha1>
    </revision>
  </page>
  <page>
    <title>Affine function</title>
    <ns>0</ns>
    <id>3</id>
    <revision>
      <id>9</id>
      <timestamp>2018-06-23T14:16:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A function &lt;math&gt;f: \mathbb{R}^{n} \to \mathbb{R}^m&lt;/math&gt; is called affine if it is composed of a linear function plus a constant offset, i.e. if it adheres to the form  &lt;mat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="797">A function &lt;math&gt;f: \mathbb{R}^{n} \to \mathbb{R}^m&lt;/math&gt; is called affine if it is composed of a linear function plus a constant offset, i.e. if it adheres to the form

&lt;math&gt;f(x) = Ax+B&lt;/math&gt;

where &lt;math&gt;A \in \mathbb{R}^{m\times n}, B\in \mathbb{R}^{m}&lt;/math&gt;. If &lt;math&gt;f&lt;/math&gt; is an affine function, then for any &lt;math&gt;x,y \in \mathbb{R^n}&lt;/math&gt; and any &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt; we have&lt;ref&gt;Stephen Boyd, EE264 homework 1 exercises&lt;/ref&gt;

&lt;math&gt;f(\alpha x+ \beta y) = \alpha f(x) + \beta f(y).&lt;/math&gt;

In a geometric interpretation, affine functions map straight lines to straight lines&lt;ref&gt;Gallini, Alberto. "Affine Function." From MathWorld--A Wolfram Web Resource, created by Eric W. Weisstein. http://mathworld.wolfram.com/AffineFunction.html&lt;/ref&gt;.

===References===</text>
      <sha1>muxmkdwhwaa0n8jhnhskib9jpd65rbr</sha1>
    </revision>
  </page>
  <page>
    <title>Linear function</title>
    <ns>0</ns>
    <id>4</id>
    <revision>
      <id>10</id>
      <timestamp>2018-06-23T14:29:51Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "While being one of the simplest and most ubiquitous function category, the definition of linear function is unfortunately not what high schools or most calculus courses teach..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1318">While being one of the simplest and most ubiquitous function category, the definition of linear function is unfortunately not what high schools or most calculus courses teach as ... linear functions. In high schools, a function &lt;math&gt;f(x) = mx + b&lt;/math&gt; is linear. This contradicts the definition in linear algebra, which is as follows.

A function &lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; is called linear if, for any &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt; and for any &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt;, it satisfies the following two properties: 
* &lt;math&gt;f(\alpha x) =  \alpha f(x)&lt;/math&gt;
* &lt;math&gt;f(x + y) = f(x) + f(y).&lt;/math&gt;
The two properties can be represented more succinctly as 
* &lt;math&gt;f(\alpha x + \beta y) = \alpha f(x) + \beta f(y).&lt;/math&gt;

Clearly, the function &lt;math&gt;mx+b&lt;/math&gt; does not satisfy those properties in general. In fact, the proper term for functions in the form of &lt;math&gt;mx+b&lt;/math&gt; (or, more generally, &lt;math&gt;Ax+B&lt;/math&gt;), is [[Affine function]].

It can be shown that for any linear function &lt;math&gt;f: \mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; can be represented in a linear form, i.e., there exists &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; such that &lt;math&gt;f(x) = Ax&lt;/math&gt;&lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, Springer, 1997&lt;/ref&gt;.

===References===</text>
      <sha1>4ulyngk0ikorsys0sippf9kj8ehmfnh</sha1>
    </revision>
    <revision>
      <id>12</id>
      <parentid>10</parentid>
      <timestamp>2018-06-23T15:03:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1766">While being one of the simplest and most ubiquitous function category, the definition of linear function is unfortunately not what high schools or most calculus courses teach as ... linear functions. In high schools, a function &lt;math&gt;f(x) = mx + b&lt;/math&gt; is linear. This contradicts the definition in linear algebra, which is as follows.

A function &lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; is called linear if, for any &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt; and for any &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt;, it satisfies the following two properties: 
* &lt;math&gt;f(\alpha x) =  \alpha f(x)&lt;/math&gt;
* &lt;math&gt;f(x + y) = f(x) + f(y).&lt;/math&gt;
The two properties can be represented more succinctly as 
* &lt;math&gt;f(\alpha x + \beta y) = \alpha f(x) + \beta f(y).&lt;/math&gt;
The properties above imply that superposition holds for linear functions, as illustrated below&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;. That is, one may first obtain the sum &lt;math&gt;x+y&lt;/math&gt; and then evaluate &lt;math&gt;f(\cdot)&lt;/math&gt; on it, or evaluate &lt;math&gt;f(x)&lt;/math&gt; and &lt;math&gt;f(y)&lt;/math&gt; separately and then add them up; the result will be the same.
&lt;gallery&gt;
Vector_superposition.png|For linear functions, superposition holds.
&lt;/gallery&gt;

Clearly, the function &lt;math&gt;mx+b&lt;/math&gt; does not satisfy those properties in general. In fact, the proper term for functions in the form of &lt;math&gt;mx+b&lt;/math&gt; (or, more generally, &lt;math&gt;Ax+B&lt;/math&gt;), is [[Affine function]]. 

It can be shown that for any linear function &lt;math&gt;f: \mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; can be represented in a linear form, i.e., there exists &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; such that &lt;math&gt;f(x) = Ax&lt;/math&gt;&lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, Springer, 1997&lt;/ref&gt;.

===References===</text>
      <sha1>1f1tp404vp8avxguo7ln66mioyrpn63</sha1>
    </revision>
    <revision>
      <id>13</id>
      <parentid>12</parentid>
      <timestamp>2018-06-23T15:05:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1711">While being one of the simplest and most ubiquitous function category, the definition of linear function is unfortunately not what high schools or most calculus courses teach as ... linear functions. In high schools, a function &lt;math&gt;f(x) = mx + b&lt;/math&gt; is linear. This contradicts the definition in linear algebra, which is as follows.

A function &lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; is called linear if, for any &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt; and for any &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt;, it satisfies the following two properties: 
* &lt;math&gt;f(\alpha x) =  \alpha f(x)&lt;/math&gt;
* &lt;math&gt;f(x + y) = f(x) + f(y).&lt;/math&gt;
The two properties can be represented more succinctly as 
* &lt;math&gt;f(\alpha x + \beta y) = \alpha f(x) + \beta f(y).&lt;/math&gt;
The properties above imply that superposition holds for linear functions, as illustrated below&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;. That is, one may first obtain the sum &lt;math&gt;x+y&lt;/math&gt; and then evaluate &lt;math&gt;f(\cdot)&lt;/math&gt; on it, or evaluate &lt;math&gt;f(x)&lt;/math&gt; and &lt;math&gt;f(y)&lt;/math&gt; separately and then add them up; the result will be the same.
[[File:Vector_superposition.png]]

Clearly, the function &lt;math&gt;mx+b&lt;/math&gt; does not satisfy those properties in general. In fact, the proper term for functions in the form of &lt;math&gt;mx+b&lt;/math&gt; (or, more generally, &lt;math&gt;Ax+B&lt;/math&gt;), is [[Affine function]]. 

It can be shown that for any linear function &lt;math&gt;f: \mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; can be represented in a linear form, i.e., there exists &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; such that &lt;math&gt;f(x) = Ax&lt;/math&gt;&lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, Springer, 1997&lt;/ref&gt;.

===References===</text>
      <sha1>bdstl0huvu56xzqoltiwzvotaaipsdv</sha1>
    </revision>
    <revision>
      <id>16</id>
      <parentid>13</parentid>
      <timestamp>2018-07-06T23:18:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1712">While being one of the simplest and most ubiquitous function category, the definition of linear function is unfortunately not what high schools or most calculus courses teach as ... linear functions. In high schools, a function &lt;math&gt;f(x) = mx + b&lt;/math&gt; is linear. This contradicts the definition in linear algebra, which is as follows.

A function &lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; is called linear if, for any &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt; and for any &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt;, it satisfies the following two properties: 
* &lt;math&gt;f(\alpha x) =  \alpha f(x)&lt;/math&gt;
* &lt;math&gt;f(x + y) = f(x) + f(y).&lt;/math&gt;
The two properties can be represented more succinctly as 
* &lt;math&gt;f(\alpha x + \beta y) = \alpha f(x) + \beta f(y).&lt;/math&gt;
The properties above imply that superposition holds for linear functions, as illustrated below&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;. That is, one may first obtain the sum &lt;math&gt;x+y&lt;/math&gt; and then evaluate &lt;math&gt;f(\cdot)&lt;/math&gt; on it, or evaluate &lt;math&gt;f(x)&lt;/math&gt; and &lt;math&gt;f(y)&lt;/math&gt; separately and then add them up; the result will be the same.

[[File:Vector_superposition.png]]

Clearly, the function &lt;math&gt;mx+b&lt;/math&gt; does not satisfy those properties in general. In fact, the proper term for functions in the form of &lt;math&gt;mx+b&lt;/math&gt; (or, more generally, &lt;math&gt;Ax+B&lt;/math&gt;), is [[Affine function]]. 

It can be shown that for any linear function &lt;math&gt;f: \mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; can be represented in a linear form, i.e., there exists &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; such that &lt;math&gt;f(x) = Ax&lt;/math&gt;&lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, Springer, 1997&lt;/ref&gt;.

===References===</text>
      <sha1>k1qsf550pe62ueau35de9sv1u6ags66</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix multiplication</title>
    <ns>0</ns>
    <id>6</id>
    <revision>
      <id>14</id>
      <timestamp>2018-06-23T15:35:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "There are various ways to interpret the matrix multiplication, each of which can be useful in different contexts. For &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and &lt;math&gt;B \in..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1179">There are various ways to interpret the matrix multiplication, each of which can be useful in different contexts. For &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and &lt;math&gt;B \in \mathbb{R}^{n\times p}&lt;/math&gt;, the matrix product &lt;math&gt;C=AB&lt;/math&gt; can be interpreted as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;:

* &lt;math&gt;C = [c_1, \ldots, c_p] = AB = [Ab_1, \ldots, Ab_p]&lt;/math&gt;.

It can also be interpreted the &lt;math&gt;i&lt;/math&gt;th row of &lt;math&gt;A&lt;/math&gt; acting (on left) on &lt;math&gt;B&lt;/math&gt;:
* &lt;math&gt;
C = \begin{bmatrix}\tilde{c}^T_1\\ \vdots \\ \tilde{c}^T_m\end{bmatrix} = AB = \begin{bmatrix}\tilde{a}^T_1 B\\ \vdots \\ \tilde{a}^T_m B\end{bmatrix}
&lt;/math&gt;

It can also be interpreted in terms of inner product, i.e., the &lt;math&gt;ij&lt;/math&gt;th element of product, &lt;math&gt;c_{ij}&lt;/math&gt;, can be written as 
* &lt;math&gt;c_{ij} = \tilde{a}_i^T b_j&lt;/math&gt;

Another interesting interpretation is in terms of summation of rank-one matrices obtained by multiplying the columns of &lt;math&gt;A&lt;/math&gt; with rows of &lt;math&gt;B&lt;/math&gt; (see formula 2.9 in &lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, 1997&lt;/ref&gt;):
* &lt;math&gt;C = \sum_{i=1}^n a_i \tilde{b}_i^T&lt;/math&gt;

===References===</text>
      <sha1>bb2g74ag23xddl1cpnuxy7ms9o14m80</sha1>
    </revision>
    <revision>
      <id>15</id>
      <parentid>14</parentid>
      <timestamp>2018-06-23T15:54:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1792">===Matrix-to-Matrix multiplication===

There are various ways to interpret the matrix multiplication, each of which can be useful in different contexts. For &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and &lt;math&gt;B \in \mathbb{R}^{n\times p}&lt;/math&gt;, the matrix product &lt;math&gt;C=AB&lt;/math&gt; can be interpreted as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;:

* &lt;math&gt;C = [c_1, \ldots, c_p] = AB = [Ab_1, \ldots, Ab_p]&lt;/math&gt;.

It can also be interpreted the &lt;math&gt;i&lt;/math&gt;th row of &lt;math&gt;A&lt;/math&gt; acting (on left) on &lt;math&gt;B&lt;/math&gt;:
* &lt;math&gt;
C = \begin{bmatrix}\tilde{c}^T_1\\ \vdots \\ \tilde{c}^T_m\end{bmatrix} = AB = \begin{bmatrix}\tilde{a}^T_1 B\\ \vdots \\ \tilde{a}^T_m B\end{bmatrix}
&lt;/math&gt;

It can also be interpreted in terms of inner product, i.e., the &lt;math&gt;ij&lt;/math&gt;th element of product, &lt;math&gt;c_{ij}&lt;/math&gt;, can be written as 
* &lt;math&gt;c_{ij} = \tilde{a}_i^T b_j&lt;/math&gt;

Another interesting interpretation is in terms of summation of rank-one matrices obtained by multiplying the columns of &lt;math&gt;A&lt;/math&gt; with rows of &lt;math&gt;B&lt;/math&gt; (see formula 2.9 in &lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, 1997&lt;/ref&gt;):
* &lt;math&gt;C = \sum_{i=1}^n a_i \tilde{b}_i^T&lt;/math&gt;

===Matrix-vector multiplication===

The vector &lt;math&gt;y=Ax&lt;/math&gt; can be interpreted as having rows obtained via inner product:
* &lt;math&gt;y=Ax=\begin{bmatrix}\tilde{a}^T_1 x\\ \vdots \\ \tilde{a}^T_m x\end{bmatrix}&lt;/math&gt;

The &lt;math&gt;x&lt;/math&gt;'s that satisfy each of those inner products, &lt;math&gt;\tilde{a}_i^T x&lt;/math&gt;, can be interpreted as a hyperplane in &lt;math&gt;\mathbb{R}^n&lt;/math&gt; with a normal vector \tilde{a}_i^T and an offset of &lt;math&gt;y_i =  \tilde{a}^T_i x = \alpha&lt;/math&gt;. (See EE263 lecture notes of Stephen Boyd, slide 2-32, for a very helpful visual representation).

===References===</text>
      <sha1>ajxnyl1n84tu4cbx188gn18mikhlbxy</sha1>
    </revision>
    <revision>
      <id>207</id>
      <parentid>15</parentid>
      <timestamp>2018-09-12T19:06:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Matrix-vector multiplication */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1805">===Matrix-to-Matrix multiplication===

There are various ways to interpret the matrix multiplication, each of which can be useful in different contexts. For &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and &lt;math&gt;B \in \mathbb{R}^{n\times p}&lt;/math&gt;, the matrix product &lt;math&gt;C=AB&lt;/math&gt; can be interpreted as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;:

* &lt;math&gt;C = [c_1, \ldots, c_p] = AB = [Ab_1, \ldots, Ab_p]&lt;/math&gt;.

It can also be interpreted the &lt;math&gt;i&lt;/math&gt;th row of &lt;math&gt;A&lt;/math&gt; acting (on left) on &lt;math&gt;B&lt;/math&gt;:
* &lt;math&gt;
C = \begin{bmatrix}\tilde{c}^T_1\\ \vdots \\ \tilde{c}^T_m\end{bmatrix} = AB = \begin{bmatrix}\tilde{a}^T_1 B\\ \vdots \\ \tilde{a}^T_m B\end{bmatrix}
&lt;/math&gt;

It can also be interpreted in terms of inner product, i.e., the &lt;math&gt;ij&lt;/math&gt;th element of product, &lt;math&gt;c_{ij}&lt;/math&gt;, can be written as 
* &lt;math&gt;c_{ij} = \tilde{a}_i^T b_j&lt;/math&gt;

Another interesting interpretation is in terms of summation of rank-one matrices obtained by multiplying the columns of &lt;math&gt;A&lt;/math&gt; with rows of &lt;math&gt;B&lt;/math&gt; (see formula 2.9 in &lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, 1997&lt;/ref&gt;):
* &lt;math&gt;C = \sum_{i=1}^n a_i \tilde{b}_i^T&lt;/math&gt;

===Matrix-vector multiplication===

The vector &lt;math&gt;y=Ax&lt;/math&gt; can be interpreted as having rows obtained via inner product:
* &lt;math&gt;y=Ax=\begin{bmatrix}\tilde{a}^T_1 x\\ \vdots \\ \tilde{a}^T_m x\end{bmatrix}&lt;/math&gt;

The &lt;math&gt;x&lt;/math&gt;'s that satisfy each of those inner products, &lt;math&gt;\tilde{a}_i^T x&lt;/math&gt;, can be interpreted as a hyperplane in &lt;math&gt;\mathbb{R}^n&lt;/math&gt; with a normal vector &lt;math&gt;\tilde{a}_i^T&lt;/math&lt; and an offset of &lt;math&gt;y_i =  \tilde{a}^T_i x = \alpha&lt;/math&gt;. (See EE263 lecture notes of Stephen Boyd, slide 2-32, for a very helpful visual representation).

===References===</text>
      <sha1>8gknnovq1yyz5ci9417wvb3qmm6uq57</sha1>
    </revision>
    <revision>
      <id>208</id>
      <parentid>207</parentid>
      <timestamp>2018-09-12T19:06:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Matrix-vector multiplication */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1805">===Matrix-to-Matrix multiplication===

There are various ways to interpret the matrix multiplication, each of which can be useful in different contexts. For &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and &lt;math&gt;B \in \mathbb{R}^{n\times p}&lt;/math&gt;, the matrix product &lt;math&gt;C=AB&lt;/math&gt; can be interpreted as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;:

* &lt;math&gt;C = [c_1, \ldots, c_p] = AB = [Ab_1, \ldots, Ab_p]&lt;/math&gt;.

It can also be interpreted the &lt;math&gt;i&lt;/math&gt;th row of &lt;math&gt;A&lt;/math&gt; acting (on left) on &lt;math&gt;B&lt;/math&gt;:
* &lt;math&gt;
C = \begin{bmatrix}\tilde{c}^T_1\\ \vdots \\ \tilde{c}^T_m\end{bmatrix} = AB = \begin{bmatrix}\tilde{a}^T_1 B\\ \vdots \\ \tilde{a}^T_m B\end{bmatrix}
&lt;/math&gt;

It can also be interpreted in terms of inner product, i.e., the &lt;math&gt;ij&lt;/math&gt;th element of product, &lt;math&gt;c_{ij}&lt;/math&gt;, can be written as 
* &lt;math&gt;c_{ij} = \tilde{a}_i^T b_j&lt;/math&gt;

Another interesting interpretation is in terms of summation of rank-one matrices obtained by multiplying the columns of &lt;math&gt;A&lt;/math&gt; with rows of &lt;math&gt;B&lt;/math&gt; (see formula 2.9 in &lt;ref&gt;David A. Harville, Matrix Algebra from a Statistician's Perspective, 1997&lt;/ref&gt;):
* &lt;math&gt;C = \sum_{i=1}^n a_i \tilde{b}_i^T&lt;/math&gt;

===Matrix-vector multiplication===

The vector &lt;math&gt;y=Ax&lt;/math&gt; can be interpreted as having rows obtained via inner product:
* &lt;math&gt;y=Ax=\begin{bmatrix}\tilde{a}^T_1 x\\ \vdots \\ \tilde{a}^T_m x\end{bmatrix}&lt;/math&gt;

The &lt;math&gt;x&lt;/math&gt;'s that satisfy each of those inner products, &lt;math&gt;\tilde{a}_i^T x&lt;/math&gt;, can be interpreted as a hyperplane in &lt;math&gt;\mathbb{R}^n&lt;/math&gt; with a normal vector &lt;math&gt;\tilde{a}_i^T&lt;/math&gt; and an offset of &lt;math&gt;y_i =  \tilde{a}^T_i x = \alpha&lt;/math&gt;. (See EE263 lecture notes of Stephen Boyd, slide 2-32, for a very helpful visual representation).

===References===</text>
      <sha1>5li61wurd64juonifiv79egemtbqkyf</sha1>
    </revision>
  </page>
  <page>
    <title>Linearization</title>
    <ns>0</ns>
    <id>7</id>
    <revision>
      <id>17</id>
      <timestamp>2018-07-06T23:33:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "This page considers linearization via first-order Taylor expansion. While this is a rather simple linearization, it is probably also the most common one.  Assuming &lt;math&gt;f:\ma..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="939">This page considers linearization via first-order Taylor expansion. While this is a rather simple linearization, it is probably also the most common one.

Assuming &lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; is differentiable, the first-order linear approximation of &lt;math&gt;f(x)&lt;/math&gt; around &lt;math&gt;x_0&lt;/math&gt; is&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;:

&lt;math&gt;f(x)\approx f(x_0) +  Df(x_0)(x-x_0) &lt;/math&gt;

where &lt;math&gt;Df(x_0)_{ij} = \left.\frac{\partial f_i}{\partial x_j}\right|_{x_0}&lt;/math&gt; is the derivative (Jacobian) matrix. A mnemonic to remember the dimension of &lt;math&gt;Df(x_0)_{ij}&lt;/math&gt; is to realize that for the dimension of the right hand side of the equation above to be consistent, &lt;math&gt;Df(x_0)(x-x_0)&lt;/math&gt; needs to be the same size as &lt;math&gt;f(x)&lt;/math&gt;, i.e. an &lt;math&gt;Df(x_0)(x-x_0) \in \mathbb{R}^m&lt;/math&gt;. Since &lt;math&gt;(x-x_0) \in \mathbb{R}^n&lt;/math&gt;,  &lt;math&gt;Df(x_0)&lt;/math&gt; must be of size  &lt;math&gt;m\times n&lt;/math&gt;.</text>
      <sha1>ai8sqjbgh6aeot2ne6fg1onln6h5vdm</sha1>
    </revision>
    <revision>
      <id>18</id>
      <parentid>17</parentid>
      <timestamp>2018-07-06T23:36:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1188">This page considers linearization via first-order Taylor expansion. While this is a rather simple linearization, it is probably also the most common one.

Assuming &lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; is differentiable, the first-order linear approximation of &lt;math&gt;f(x)&lt;/math&gt; around &lt;math&gt;x_0&lt;/math&gt; is&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;:

&lt;math&gt;f(x)\approx f(x_0) +  Df(x_0)(x-x_0) &lt;/math&gt;

where &lt;math&gt;Df(x_0)_{ij} = \left.\frac{\partial f_i}{\partial x_j}\right|_{x_0}&lt;/math&gt; is the derivative (Jacobian) matrix. More technically, approximation means that:

&lt;math&gt;x \text{ near } x_0 \implies f(x) \text{ very near } f(x_0) +  Df(x_0)(x-x_0). &lt;/math&gt;
The term "very near" means that the error on the RHS above is at the order of square of the error on the LHS.

A mnemonic to remember the dimension of &lt;math&gt;Df(x_0)_{ij}&lt;/math&gt; is to realize that for the dimension of the right hand side of the equation above to be consistent, &lt;math&gt;Df(x_0)(x-x_0)&lt;/math&gt; needs to be the same size as &lt;math&gt;f(x)&lt;/math&gt;, i.e. an &lt;math&gt;Df(x_0)(x-x_0) \in \mathbb{R}^m&lt;/math&gt;. Since &lt;math&gt;(x-x_0) \in \mathbb{R}^n&lt;/math&gt;,  &lt;math&gt;Df(x_0)&lt;/math&gt; must be of size  &lt;math&gt;m\times n&lt;/math&gt;.</text>
      <sha1>dmqv1qwygizhuezd9fd9qwf53gkjg32</sha1>
    </revision>
    <revision>
      <id>132</id>
      <parentid>18</parentid>
      <timestamp>2018-07-20T16:17:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1206">This page considers linearization via first-order Taylor expansion. While this is a rather simple linearization, it is probably also the most common one.

Assuming &lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt; is differentiable, the first-order linear approximation of &lt;math&gt;f(x)&lt;/math&gt; around &lt;math&gt;x_0&lt;/math&gt; is&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 2&lt;/ref&gt;:

&lt;math&gt;f(x)\approx f(x_0) +  Df(x_0)(x-x_0) &lt;/math&gt;

where &lt;math&gt;Df(x_0)_{ij} = \left.\frac{\partial f_i}{\partial x_j}\right|_{x_0}&lt;/math&gt; is the derivative (Jacobian) matrix. More technically, approximation means that:

&lt;math&gt;x \text{ near } x_0 \implies f(x) \text{ very near } f(x_0) +  Df(x_0)(x-x_0). &lt;/math&gt;
The term "very near" means that the error on the RHS above is at the order of square of the error on the LHS.

A mnemonic to remember the dimension of &lt;math&gt;Df(x_0)_{ij}&lt;/math&gt; is to realize that for the dimension of the right hand side of the equation above to be consistent, &lt;math&gt;Df(x_0)(x-x_0)&lt;/math&gt; needs to be the same size as &lt;math&gt;f(x)&lt;/math&gt;, i.e. an &lt;math&gt;Df(x_0)(x-x_0) \in \mathbb{R}^m&lt;/math&gt;. Since &lt;math&gt;(x-x_0) \in \mathbb{R}^n&lt;/math&gt;,  &lt;math&gt;Df(x_0)&lt;/math&gt; must be of size  &lt;math&gt;m\times n&lt;/math&gt;.

== References ==</text>
      <sha1>pk847i1ggyapweet37vns41zmx6ifxa</sha1>
    </revision>
  </page>
  <page>
    <title>Vector Space</title>
    <ns>0</ns>
    <id>8</id>
    <revision>
      <id>20</id>
      <timestamp>2018-07-07T00:16:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A *vector space* or a *linear space* consists of a set  &lt;math&gt;V&lt;/math&gt; equipped with operations of addition and scalar multiplication. Assume that the following rules are sati..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1636">A *vector space* or a *linear space* consists of a set  &lt;math&gt;V&lt;/math&gt; equipped with operations of addition and scalar multiplication. Assume that the following rules are satisfied:
# &lt;math&gt;\forall \mathbf{v,w} \in V&lt;/math&gt;, we have that &lt;math&gt;\mathbf{v+w = w+v}&lt;/math&gt; (commutative)
# &lt;math&gt;\forall \mathbf{v,w,u} \in V&lt;/math&gt;, we have that &lt;math&gt;\mathbf{(v+w)+u = w+(v+u)}&lt;/math&gt; (associative)
# There exists an element called 0 in &lt;math&gt;V&lt;/math&gt;, such that &lt;math&gt;\forall \mathbf{v} \in V&lt;/math&gt;, we have that &lt;math&gt;\mathbf{(v+0) = v}&lt;/math&gt; (0 is additive identity)
# For each &lt;math&gt;\mathbf{v} \in V&lt;/math&gt;, there exists an element, called &lt;math&gt;-\mathbf{v}&lt;/math&gt;, such that \mathbf{v+(-v)} = 0 (existence of additive inverse)
# For all scalars &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt;, &lt;math&gt;\alpha(\beta \mathbf{v}) = (\alpha\beta) \mathbf{v}&lt;/math&gt; (scalar multiplication is associative)
# For all &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt; and all &lt;math&gt;\mathbf{v} \in V&lt;/math&gt;, we have that &lt;math&gt;(\alpha+\beta) \mathbf{v} = \alpha \mathbf{v} + \beta \mathbf{v}&lt;/math&gt; (left distribution rule)
# For all &lt;math&gt;\alpha \in \mathbb{R}&lt;/math&gt; and all &lt;math&gt;\mathbf{v,w} \in V&lt;/math&gt;, we have that &lt;math&gt;\alpha (\mathbf{v}+\mathbf{w}) = \alpha \mathbf{v} + \alpha \mathbf{w}&lt;/math&gt; (right distribution rule)
# For all &lt;math&gt;\mathbf{v} \in V&lt;/math&gt;, we have that &lt;math&gt;1 \mathbf{v} = \mathbf{v}&lt;/math&gt;

Then, the space &lt;math&gt;V&lt;/math&gt;, equipped with the operations of addition and scalar multiplication, forms a vector space
&lt;ref&gt;Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering&lt;/ref&gt;.</text>
      <sha1>9970iu7ph3jrkgzj69tip5pv09xshyo</sha1>
    </revision>
    <revision>
      <id>21</id>
      <parentid>20</parentid>
      <timestamp>2018-07-07T00:19:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1743">A *vector space* or a *linear space* consists of a set  &lt;math&gt;V&lt;/math&gt; equipped with operations of addition and scalar multiplication. Assume that the following rules are satisfied:
# &lt;math&gt;\forall \mathbf{v,w} \in V&lt;/math&gt;, we have that &lt;math&gt;\mathbf{v+w = w+v}&lt;/math&gt; (commutative)
# &lt;math&gt;\forall \mathbf{v,w,u} \in V&lt;/math&gt;, we have that &lt;math&gt;\mathbf{(v+w)+u = w+(v+u)}&lt;/math&gt; (associative)
# There exists an element called 0 in &lt;math&gt;V&lt;/math&gt;, such that &lt;math&gt;\forall \mathbf{v} \in V&lt;/math&gt;, we have that &lt;math&gt;\mathbf{(v+0) = v}&lt;/math&gt; (0 is additive identity)
# For each &lt;math&gt;\mathbf{v} \in V&lt;/math&gt;, there exists an element, called &lt;math&gt;-\mathbf{v}&lt;/math&gt;, such that \mathbf{v+(-v)} = 0 (existence of additive inverse)
# For all scalars &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt;, &lt;math&gt;\alpha(\beta \mathbf{v}) = (\alpha\beta) \mathbf{v}&lt;/math&gt; (scalar multiplication is associative)
# For all &lt;math&gt;\alpha, \beta \in \mathbb{R}&lt;/math&gt; and all &lt;math&gt;\mathbf{v} \in V&lt;/math&gt;, we have that &lt;math&gt;(\alpha+\beta) \mathbf{v} = \alpha \mathbf{v} + \beta \mathbf{v}&lt;/math&gt; (left distribution rule)
# For all &lt;math&gt;\alpha \in \mathbb{R}&lt;/math&gt; and all &lt;math&gt;\mathbf{v,w} \in V&lt;/math&gt;, we have that &lt;math&gt;\alpha (\mathbf{v}+\mathbf{w}) = \alpha \mathbf{v} + \alpha \mathbf{w}&lt;/math&gt; (right distribution rule)
# For all &lt;math&gt;\mathbf{v} \in V&lt;/math&gt;, we have that &lt;math&gt;1 \mathbf{v} = \mathbf{v}&lt;/math&gt;

Then, the space &lt;math&gt;V&lt;/math&gt;, equipped with the operations of addition and scalar multiplication, forms a vector space
&lt;ref&gt;Ole Christensen, Functions, Spaces, and Expansions: Mathematical Tools in Physics and Engineering&lt;/ref&gt;.

Examples to vector spaces are &lt;math&gt;\mathbb{R}^n, \mathbb{C}^n, \text{span}(v_1,\dots,v_k), \{0\}.&lt;/math&gt;</text>
      <sha1>4fp0yasnbw7uhntdxfgbc7kdyzs13tl</sha1>
    </revision>
  </page>
  <page>
    <title>Subspace</title>
    <ns>0</ns>
    <id>9</id>
    <revision>
      <id>22</id>
      <timestamp>2018-07-07T00:22:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A subspace &lt;math&gt;W&lt;/math&gt; is a subset of a [[Vector Space|vector space]] &lt;math&gt;V&lt;/math&gt; which itself is a vector space."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="119">A subspace &lt;math&gt;W&lt;/math&gt; is a subset of a [[Vector Space|vector space]] &lt;math&gt;V&lt;/math&gt; which itself is a vector space.</text>
      <sha1>hh5pya7jcno63laiz31knndqg7pn74t</sha1>
    </revision>
  </page>
  <page>
    <title>Nullspace</title>
    <ns>0</ns>
    <id>10</id>
    <revision>
      <id>23</id>
      <timestamp>2018-07-07T00:44:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The nullspace of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is defined as  &lt;math&gt;\mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}.&lt;/math&gt;  &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; is the se..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="569">The nullspace of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is defined as

&lt;math&gt;\mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}.&lt;/math&gt;

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; is the set of vectors orthogonal to all rows of &lt;math&gt;A&lt;/math&gt;.

&lt;math&gt;\mathcal{N}(A)&lt;\math&gt; gives the *ambiguity* in &lt;math&gt;x&lt;/math&gt; given &lt;math&gt;y=Ax&lt;/math&gt;:
* If &lt;math&gt;y =  Ax&lt;/math&gt; and &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;, then &lt;math&gt;y = A(x+z) = Ax&lt;/math&gt;
* Conversely, if &lt;math&gt;y = Ax&lt;/math&gt; and &lt;math&gt;y = A\tilde{x}&lt;/math&gt;, then &lt;math&gt;\tilde{x} = x+z&lt;/math&gt; for some &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;</text>
      <sha1>h1wx26ckp6exd3ic85z90oy3jloph1z</sha1>
    </revision>
    <revision>
      <id>24</id>
      <parentid>23</parentid>
      <timestamp>2018-07-07T00:45:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="569">The nullspace of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is defined as

&lt;math&gt;\mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}.&lt;/math&gt;

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; is the set of vectors orthogonal to all rows of &lt;math&gt;A&lt;/math&gt;.

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; gives the *ambiguity* in &lt;math&gt;x&lt;/math&gt; given &lt;math&gt;y=Ax&lt;/math&gt;:
* If &lt;math&gt;y =  Ax&lt;/math&gt; and &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;, then &lt;math&gt;y = A(x+z) = Ax&lt;/math&gt;
* Conversely, if &lt;math&gt;y = Ax&lt;/math&gt; and &lt;math&gt;y = A\tilde{x}&lt;/math&gt;, then &lt;math&gt;\tilde{x} = x+z&lt;/math&gt; for some &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;</text>
      <sha1>ff98zm43h0keo3p40fmt70ejgregw82</sha1>
    </revision>
    <revision>
      <id>27</id>
      <parentid>24</parentid>
      <timestamp>2018-07-07T13:10:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="763">The nullspace of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is defined as

&lt;math&gt;\mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}.&lt;/math&gt;

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; is the set of vectors orthogonal to all rows of &lt;math&gt;A&lt;/math&gt;.

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; gives the *ambiguity* in &lt;math&gt;x&lt;/math&gt; given &lt;math&gt;y=Ax&lt;/math&gt;:
* If &lt;math&gt;y =  Ax&lt;/math&gt; and &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;, then &lt;math&gt;y = A(x+z) = Ax&lt;/math&gt;
* Conversely, if &lt;math&gt;y = Ax&lt;/math&gt; and &lt;math&gt;y = A\tilde{x}&lt;/math&gt;, then &lt;math&gt;\tilde{x} = x+z&lt;/math&gt; for some &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;

== Notes ==

* If the nullspace of &lt;math&gt;A&lt;/math&gt; contains only &lt;math&gt;0&lt;/math&gt;, i.e., &lt;math&gt;\mathcal{N}(A)=\{0\}&lt;/math&gt;, then &lt;math&gt;A&lt;/math&gt; is [[one-to-one]], which has multiple implications.</text>
      <sha1>qpr40y9fqh7uj9o7kvz6jfyy0sxs640</sha1>
    </revision>
    <revision>
      <id>32</id>
      <parentid>27</parentid>
      <timestamp>2018-07-07T13:40:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="819">The nullspace of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is defined as

&lt;math&gt;\mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}.&lt;/math&gt;

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; is the set of vectors orthogonal to all rows of &lt;math&gt;A&lt;/math&gt;.

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; gives the *ambiguity* in &lt;math&gt;x&lt;/math&gt; given &lt;math&gt;y=Ax&lt;/math&gt;:
* If &lt;math&gt;y =  Ax&lt;/math&gt; and &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;, then &lt;math&gt;y = A(x+z) = Ax&lt;/math&gt;
* Conversely, if &lt;math&gt;y = Ax&lt;/math&gt; and &lt;math&gt;y = A\tilde{x}&lt;/math&gt;, then &lt;math&gt;\tilde{x} = x+z&lt;/math&gt; for some &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;

== Notes ==

* If the nullspace of &lt;math&gt;A&lt;/math&gt; contains only &lt;math&gt;0&lt;/math&gt;, i.e., &lt;math&gt;\mathcal{N}(A)=\{0\}&lt;/math&gt;, then &lt;math&gt;A&lt;/math&gt; is [[one-to-one]], which has multiple implications.
* &lt;math&gt;\dim \mathcal{N}(A) + \text{rank}(A) = n&lt;/math&gt;</text>
      <sha1>bsple6ered3ujtvj0wi3atmq4itj6qr</sha1>
    </revision>
    <revision>
      <id>38</id>
      <parentid>32</parentid>
      <timestamp>2018-07-07T14:10:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="867">The nullspace of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is defined as

&lt;math&gt;\mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}.&lt;/math&gt;

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; is the set of vectors orthogonal to all rows of &lt;math&gt;A&lt;/math&gt;.

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; gives the *ambiguity* in &lt;math&gt;x&lt;/math&gt; given &lt;math&gt;y=Ax&lt;/math&gt;:
* If &lt;math&gt;y =  Ax&lt;/math&gt; and &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;, then &lt;math&gt;y = A(x+z) = Ax&lt;/math&gt;
* Conversely, if &lt;math&gt;y = Ax&lt;/math&gt; and &lt;math&gt;y = A\tilde{x}&lt;/math&gt;, then &lt;math&gt;\tilde{x} = x+z&lt;/math&gt; for some &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;

== Notes ==

* If the nullspace of &lt;math&gt;A&lt;/math&gt; contains only &lt;math&gt;0&lt;/math&gt;, i.e., &lt;math&gt;\mathcal{N}(A)=\{0\}&lt;/math&gt;, then &lt;math&gt;A&lt;/math&gt; is [[one-to-one]], which has multiple implications.
* &lt;math&gt;\dim \mathcal{N}(A) + \text{rank}(A) = n&lt;/math&gt; (see )[[Fundamental Theorem of Linear Algebra]]</text>
      <sha1>iwi4qnrwxyetl42gx6yyvd65auxjq2o</sha1>
    </revision>
    <revision>
      <id>146</id>
      <parentid>38</parentid>
      <timestamp>2018-07-22T23:57:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="867">The nullspace of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is defined as

&lt;math&gt;\mathcal{N}(A) = \{x \in \mathbb{R}^n : Ax = 0\}.&lt;/math&gt;

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; is the set of vectors orthogonal to all rows of &lt;math&gt;A&lt;/math&gt;.

&lt;math&gt;\mathcal{N}(A)&lt;/math&gt; gives the *ambiguity* in &lt;math&gt;x&lt;/math&gt; given &lt;math&gt;y=Ax&lt;/math&gt;:
* If &lt;math&gt;y =  Ax&lt;/math&gt; and &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;, then &lt;math&gt;y = A(x+z) = Ax&lt;/math&gt;
* Conversely, if &lt;math&gt;y = Ax&lt;/math&gt; and &lt;math&gt;y = A\tilde{x}&lt;/math&gt;, then &lt;math&gt;\tilde{x} = x+z&lt;/math&gt; for some &lt;math&gt;z \in \mathcal{N}(A)&lt;/math&gt;

== Notes ==

* If the nullspace of &lt;math&gt;A&lt;/math&gt; contains only &lt;math&gt;0&lt;/math&gt;, i.e., &lt;math&gt;\mathcal{N}(A)=\{0\}&lt;/math&gt;, then &lt;math&gt;A&lt;/math&gt; is [[one-to-one]], which has multiple implications.
* &lt;math&gt;\dim \mathcal{N}(A) + \text{rank}(A) = n&lt;/math&gt; (see [[Fundamental Theorem of Linear Algebra]])</text>
      <sha1>pvm7xedian4e7if6n7q1fo1ff4bo95q</sha1>
    </revision>
  </page>
  <page>
    <title>One-to-one</title>
    <ns>0</ns>
    <id>11</id>
    <revision>
      <id>25</id>
      <timestamp>2018-07-07T00:50:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A matrix &lt;math&gt;A&lt;/math&gt; is called ''one-to-one'' &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt; if 0 is the only element of its [[Nullspace|nullspace]]: &lt;math&gt;\mathcal{N}(A) =..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="569">A matrix &lt;math&gt;A&lt;/math&gt; is called ''one-to-one'' &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt; if 0 is the only element of its [[Nullspace|nullspace]]: &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt; , which is possible if and only if:
* &lt;math&gt;x&lt;/math&gt; can always be uniquely determined from &lt;math&gt;y=Ax&lt;/math&gt;
* mapping from &lt;math&gt;x&lt;/math&gt; to &lt;math&gt;Ax&lt;/math&gt; is one-to-one: different &lt;math&gt;x&lt;/math&gt;'s map to different &lt;math&gt;y&lt;/math&gt;'s
* Columns of &lt;math&gt;A&lt;/math&gt; are independent (hence, a basis for their span)
* &lt;math&gt;A&lt;/math&gt; has a left inverse
* &lt;math&gt;\det(A^T A) \neq 0&lt;/math&gt;</text>
      <sha1>naaghkrg1ltpzi2fpbivcuwjfd6887f</sha1>
    </revision>
    <revision>
      <id>314</id>
      <parentid>25</parentid>
      <timestamp>2018-10-20T18:54:54Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="626">A (skinny or square) matrix &lt;math&gt;A&lt;/math&gt; is called ''one-to-one'' &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt; if 0 is the only element of its [[Nullspace|nullspace]]: &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt; , which is possible if and only if:
* &lt;math&gt;x&lt;/math&gt; can always be uniquely determined from &lt;math&gt;y=Ax&lt;/math&gt;
* mapping from &lt;math&gt;x&lt;/math&gt; to &lt;math&gt;Ax&lt;/math&gt; is one-to-one: different &lt;math&gt;x&lt;/math&gt;'s map to different &lt;math&gt;y&lt;/math&gt;'s
* Columns of &lt;math&gt;A&lt;/math&gt; are independent (hence, a basis for their span)
* &lt;math&gt;A&lt;/math&gt; has a left inverse
* &lt;math&gt;A&lt;/math&gt; has full column rank
* &lt;math&gt;\det(A^T A) \neq 0&lt;/math&gt;</text>
      <sha1>3b83crbjq8kcpi8tk1o0xwy14jqq12w</sha1>
    </revision>
    <revision>
      <id>392</id>
      <parentid>314</parentid>
      <timestamp>2019-06-22T14:24:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="664">A (skinny or square) matrix &lt;math&gt;A&lt;/math&gt; is called ''one-to-one'' &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt; if 0 is the only element of its [[Nullspace|nullspace]]: &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt; , which is possible if and only if:
* &lt;math&gt;x&lt;/math&gt; can always be uniquely determined from &lt;math&gt;y=Ax&lt;/math&gt;
* mapping from &lt;math&gt;x&lt;/math&gt; to &lt;math&gt;Ax&lt;/math&gt; is one-to-one: different &lt;math&gt;x&lt;/math&gt;'s map to different &lt;math&gt;y&lt;/math&gt;'s
* &lt;math&gt;A&lt;/math&gt; has full column rank
* Columns of &lt;math&gt;A&lt;/math&gt; are independent (hence, a basis for their span)
* &lt;math&gt;A&lt;/math&gt; has a left inverse
* &lt;math&gt;A&lt;/math&gt; has full column rank
* &lt;math&gt;\det(A^T A) \neq 0&lt;/math&gt;</text>
      <sha1>9ma3mnhkgksbo5qit64pu1n7jzqfwni</sha1>
    </revision>
    <revision>
      <id>397</id>
      <parentid>392</parentid>
      <timestamp>2019-06-22T14:36:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="699">A (skinny or square) matrix &lt;math&gt;A&lt;/math&gt; is called ''one-to-one'' &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt; if 0 is the only element of its [[Nullspace|nullspace]]: &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt; , which is possible if and only if:
* &lt;math&gt;x&lt;/math&gt; can always be uniquely determined from &lt;math&gt;y=Ax&lt;/math&gt;
* mapping from &lt;math&gt;x&lt;/math&gt; to &lt;math&gt;Ax&lt;/math&gt; is one-to-one: different &lt;math&gt;x&lt;/math&gt;'s map to different &lt;math&gt;y&lt;/math&gt;'s
* &lt;math&gt;A&lt;/math&gt; has full column rank
* Columns of &lt;math&gt;A&lt;/math&gt; are independent (hence, a basis for their span)
* &lt;math&gt;A&lt;/math&gt; has a left inverse
* &lt;math&gt;A&lt;/math&gt; has full column rank
* &lt;math&gt;\det(A^T A) \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; is [[injective]].</text>
      <sha1>c2ncv5sbwxm71kbjghe9lilp0b44p66</sha1>
    </revision>
  </page>
  <page>
    <title>Left Inverse</title>
    <ns>0</ns>
    <id>12</id>
    <revision>
      <id>26</id>
      <timestamp>2018-07-07T00:55:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "This part needs to be filled, but first things that I may forget later.  If &lt;math&gt;BA = I&lt;/math&gt;, then the matrix &lt;math&gt;B&lt;/math&gt; is the left inverse of &lt;math&gt;A&lt;/math&gt;. Clearly,..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="511">This part needs to be filled, but first things that I may forget later.

If &lt;math&gt;BA = I&lt;/math&gt;, then the matrix &lt;math&gt;B&lt;/math&gt; is the left inverse of &lt;math&gt;A&lt;/math&gt;. Clearly, if &lt;math&gt;y=Ax&lt;/math&gt;, then &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the context of encoding, &lt;math&gt;B&lt;/math&gt; is called the ''perfect decoder''. In the context of communication system, &lt;math&gt;B&lt;/math&gt; is the perfect equalizer. In the context of a measurement system, &lt;math&gt;B&lt;/math&gt; is the matrix that gives the parameters that we wanted to measure.</text>
      <sha1>jehixnigpbihgd74clds4sj7ggvohbm</sha1>
    </revision>
  </page>
  <page>
    <title>Onto matrices</title>
    <ns>0</ns>
    <id>13</id>
    <revision>
      <id>28</id>
      <timestamp>2018-07-07T13:27:31Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lect..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="561">A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;:
* &lt;math&gt;Ax=y&lt;/math&gt; can be solved in &lt;math&gt;x&lt;/math&gt; for any &lt;math&gt;y&lt;/math&gt;
* columns of &lt;math&gt;A&lt;/math&gt; span &lt;math&gt;\mathbb{R}^m&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has a ''right inverse'', i.e. &lt;math&gt;\exist B\in \mathbb{R}^{n\times m}&lt;/math&gt; s.t. &lt;math&gt;AB = I&lt;/math&gt;
* rows of &lt;math&gt;A&lt;/math&gt; are independent
* &lt;math&gt;\mathcal{N}(A^T) = \{0\}&lt;/math&gt;
* &lt;math&gt;det(A A^T) \neq 0&lt;/math&gt;</text>
      <sha1>f5lfinvhg6sh2l19sm6iw0i5jtzuuzv</sha1>
    </revision>
    <revision>
      <id>29</id>
      <parentid>28</parentid>
      <timestamp>2018-07-07T13:28:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="579">A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;:
* &lt;math&gt;Ax=y&lt;/math&gt; can be solved in &lt;math&gt;x&lt;/math&gt; for any &lt;math&gt;y&lt;/math&gt;
* columns of &lt;math&gt;A&lt;/math&gt; span &lt;math&gt;\mathbb{R}^m&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has a ''right inverse'', i.e. &lt;math&gt;\exist B\in \mathbb{R}^{n\times m}&lt;/math&gt; s.t. &lt;math&gt;AB = I&lt;/math&gt;
* rows of &lt;math&gt;A&lt;/math&gt; are independent
* &lt;math&gt;\mathcal{N}(A^T) = \{0\}&lt;/math&gt;
* &lt;math&gt;det(A A^T) \neq 0&lt;/math&gt;

== References ==</text>
      <sha1>ffix1xr3cllnbrlgndatdxsu2zcteuz</sha1>
    </revision>
    <revision>
      <id>391</id>
      <parentid>29</parentid>
      <timestamp>2019-06-22T14:23:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="621">A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;:
* &lt;math&gt;Ax=y&lt;/math&gt; can be solved in &lt;math&gt;x&lt;/math&gt; for any &lt;math&gt;y&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has full row rank
* columns of &lt;math&gt;A&lt;/math&gt; span &lt;math&gt;\mathbb{R}^m&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has a ''right inverse'', i.e. &lt;math&gt;\exist B\in \mathbb{R}^{n\times m}&lt;/math&gt; s.t. &lt;math&gt;AB = I&lt;/math&gt;
* rows of &lt;math&gt;A&lt;/math&gt; are independent
* &lt;math&gt;\mathcal{N}(A^T) = \{0\}&lt;/math&gt;
* &lt;math&gt;\text{det}(A A^T) \neq 0&lt;/math&gt;

== References ==</text>
      <sha1>92fxo57j1yod5a5kn1024bat9wt2rbo</sha1>
    </revision>
    <revision>
      <id>396</id>
      <parentid>391</parentid>
      <timestamp>2019-06-22T14:36:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="653">A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;:
* &lt;math&gt;Ax=y&lt;/math&gt; can be solved in &lt;math&gt;x&lt;/math&gt; for any &lt;math&gt;y&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has full row rank
* columns of &lt;math&gt;A&lt;/math&gt; span &lt;math&gt;\mathbb{R}^m&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has a ''right inverse'', i.e. &lt;math&gt;\exist B\in \mathbb{R}^{n\times m}&lt;/math&gt; s.t. &lt;math&gt;AB = I&lt;/math&gt;
* rows of &lt;math&gt;A&lt;/math&gt; are independent
* &lt;math&gt;\mathcal{N}(A^T) = \{0\}&lt;/math&gt;
* &lt;math&gt;\text{det}(A A^T) \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; is surjective.

== References ==</text>
      <sha1>2qnvco3qlho7uf4k2jelmfc6dv0lcyx</sha1>
    </revision>
    <revision>
      <id>398</id>
      <parentid>396</parentid>
      <timestamp>2019-06-22T14:37:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="657">A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;:
* &lt;math&gt;Ax=y&lt;/math&gt; can be solved in &lt;math&gt;x&lt;/math&gt; for any &lt;math&gt;y&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has full row rank
* columns of &lt;math&gt;A&lt;/math&gt; span &lt;math&gt;\mathbb{R}^m&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has a ''right inverse'', i.e. &lt;math&gt;\exist B\in \mathbb{R}^{n\times m}&lt;/math&gt; s.t. &lt;math&gt;AB = I&lt;/math&gt;
* rows of &lt;math&gt;A&lt;/math&gt; are independent
* &lt;math&gt;\mathcal{N}(A^T) = \{0\}&lt;/math&gt;
* &lt;math&gt;\text{det}(A A^T) \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; is [[surjective]].

== References ==</text>
      <sha1>2aqk2cvghkir68ujqzldg6y9bcqaacu</sha1>
    </revision>
    <revision>
      <id>511</id>
      <parentid>398</parentid>
      <timestamp>2019-11-15T17:01:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="660">A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;:
* &lt;math&gt;Ax=y&lt;/math&gt; can be solved in &lt;math&gt;x&lt;/math&gt; for any &lt;math&gt;y&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has full column rank
* columns of &lt;math&gt;A&lt;/math&gt; span &lt;math&gt;\mathbb{R}^m&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has a ''right inverse'', i.e. &lt;math&gt;\exist B\in \mathbb{R}^{n\times m}&lt;/math&gt; s.t. &lt;math&gt;AB = I&lt;/math&gt;
* rows of &lt;math&gt;A&lt;/math&gt; are independent
* &lt;math&gt;\mathcal{N}(A^T) = \{0\}&lt;/math&gt;
* &lt;math&gt;\text{det}(A A^T) \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; is [[surjective]].

== References ==</text>
      <sha1>raljaoqcynzac7mk7u7syocyc9e48o4</sha1>
    </revision>
    <revision>
      <id>512</id>
      <parentid>511</parentid>
      <timestamp>2019-11-15T17:03:02Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="657">A matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is called ''onto'' if &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^m&lt;/math&gt;, which happens if and only if &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;:
* &lt;math&gt;Ax=y&lt;/math&gt; can be solved in &lt;math&gt;x&lt;/math&gt; for any &lt;math&gt;y&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has full row rank
* columns of &lt;math&gt;A&lt;/math&gt; span &lt;math&gt;\mathbb{R}^m&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; has a ''right inverse'', i.e. &lt;math&gt;\exist B\in \mathbb{R}^{n\times m}&lt;/math&gt; s.t. &lt;math&gt;AB = I&lt;/math&gt;
* rows of &lt;math&gt;A&lt;/math&gt; are independent
* &lt;math&gt;\mathcal{N}(A^T) = \{0\}&lt;/math&gt;
* &lt;math&gt;\text{det}(A A^T) \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; is [[surjective]].

== References ==</text>
      <sha1>2aqk2cvghkir68ujqzldg6y9bcqaacu</sha1>
    </revision>
  </page>
  <page>
    <title>Inverse matrices</title>
    <ns>0</ns>
    <id>14</id>
    <revision>
      <id>30</id>
      <timestamp>2018-07-07T13:38:24Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions: * Columns of &lt;ma..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="635">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* \mathcal{N}(A) = \{0\}
* \mathcal{R}(A) = \mathbb{R}^n
* &lt;math&gt;\detA^TA=\det AA^T \neq 0&lt;/math&gt;</text>
      <sha1>h7u347ocdfn08uqg29rwrqk722xsa23</sha1>
    </revision>
    <revision>
      <id>31</id>
      <parentid>30</parentid>
      <timestamp>2018-07-07T13:38:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="636">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* \mathcal{N}(A) = \{0\}
* \mathcal{R}(A) = \mathbb{R}^n
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;</text>
      <sha1>bc49xqnxhpzg5r25lisqkhg1int9j4i</sha1>
    </revision>
    <revision>
      <id>59</id>
      <parentid>31</parentid>
      <timestamp>2018-07-08T15:55:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="916">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* \mathcal{N}(A) = \{0\}
* \mathcal{R}(A) = \mathbb{R}^n
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;
===Some identities for matrix inversion===
* ''Rank one update formula'': Iterative inversions of the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt; can be done efficiently via:

&lt;math&gt;(P+aa^T)^{-1} = P^{-1} - \frac{1}{1+a^T P^{-1}a} (P^{-1}a)(P^{-1}a)^T&lt;/math&gt;</text>
      <sha1>7gp0zyknk29fjkbo61gigj13vtevolx</sha1>
    </revision>
    <revision>
      <id>73</id>
      <parentid>59</parentid>
      <timestamp>2018-07-15T21:28:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1048">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt;
* &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;
===Some identities for matrix inversion===
* ''Rank one update formula'': Iterative inversions of the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt; can be done efficiently via:

&lt;math&gt;(P+aa^T)^{-1} = P^{-1} - \frac{1}{1+a^T P^{-1}a} (P^{-1}a)(P^{-1}a)^T&lt;/math&gt;
* (I-C)^{-1} = I+C+C^2+C^3+\dots (if the series converges&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;)</text>
      <sha1>lx06za5s2jo298lkm837jdv7oft81by</sha1>
    </revision>
    <revision>
      <id>74</id>
      <parentid>73</parentid>
      <timestamp>2018-07-15T21:28:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Some identities for matrix inversion */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1061">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt;
* &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;
===Some identities for matrix inversion===
* ''Rank one update formula'': Iterative inversions of the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt; can be done efficiently via:

&lt;math&gt;(P+aa^T)^{-1} = P^{-1} - \frac{1}{1+a^T P^{-1}a} (P^{-1}a)(P^{-1}a)^T&lt;/math&gt;
* &lt;math&gt;(I-C)^{-1} = I+C+C^2+C^3+\dots&lt;/math&gt; (if the series converges&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;)</text>
      <sha1>kjbgsbhvy1w5wo2xajou3eagxomibna</sha1>
    </revision>
    <revision>
      <id>77</id>
      <parentid>74</parentid>
      <timestamp>2018-07-15T22:38:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1077">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt;
* &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;
===Some identities for matrix inversion===
* ''Rank one update formula'': Iterative inversions of the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt; can be done efficiently via:

&lt;math&gt;(P+aa^T)^{-1} = P^{-1} - \frac{1}{1+a^T P^{-1}a} (P^{-1}a)(P^{-1}a)^T&lt;/math&gt;
* &lt;math&gt;(I-C)^{-1} = I+C+C^2+C^3+\dots&lt;/math&gt; (if the series converges&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;)

==References==</text>
      <sha1>71rz7p7j3u5oilzmrunwmr9ea9lwli9</sha1>
    </revision>
    <revision>
      <id>141</id>
      <parentid>77</parentid>
      <timestamp>2018-07-21T14:18:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1159">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt;
* &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; does not have a zero [[Eigenvectors and Eigenvalues|eigenvalue]]
===Some identities for matrix inversion===
* ''Rank one update formula'': Iterative inversions of the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt; can be done efficiently via:

&lt;math&gt;(P+aa^T)^{-1} = P^{-1} - \frac{1}{1+a^T P^{-1}a} (P^{-1}a)(P^{-1}a)^T&lt;/math&gt;
* &lt;math&gt;(I-C)^{-1} = I+C+C^2+C^3+\dots&lt;/math&gt; (if the series converges&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;)

==References==</text>
      <sha1>it215usnq7ochpoctn20qfu5tgjqbs4</sha1>
    </revision>
    <revision>
      <id>302</id>
      <parentid>141</parentid>
      <timestamp>2018-10-20T14:42:34Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1392">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt;
* &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; does not have a zero [[Eigenvectors and Eigenvalues|eigenvalue]]
===Some identities for matrix inversion===
* ''Rank one update formula'': Iterative inversions of the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt; can be done efficiently via:

&lt;math&gt;(P+aa^T)^{-1} = P^{-1} - \frac{1}{1+a^T P^{-1}a} (P^{-1}a)(P^{-1}a)^T&lt;/math&gt;
* &lt;math&gt;(I-C)^{-1} = I+C+C^2+C^3+\dots&lt;/math&gt; (if the series converges&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;)
* Matrix inversion lemma: &lt;math&gt;(A+BC)^{-1} = A^{-1}-A^{-1} B (I+C A^{-1}B)^{-1}CA^{-1}&lt;/math&gt;, which is useful when &lt;math&gt;A&lt;/math&gt; is easy to invert and &lt;math&gt;BC&lt;/math&gt; is a low-rank matrix&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;

==References==</text>
      <sha1>htxirahhxonme24um5xvxs42raucwkt</sha1>
    </revision>
    <revision>
      <id>517</id>
      <parentid>302</parentid>
      <timestamp>2019-12-02T19:21:22Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>further clarification</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1464">&lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is ''invertible'' or ''nonsingular'' if &lt;math&gt;\det A \neq 0&lt;/math&gt;, which is equivalent to the following conditions:
* Columns of &lt;math&gt;A&lt;/math&gt; are a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* Rows of &lt;math&gt;A&lt;/math&gt; are also a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;y=Ax&lt;/math&gt; has a '''unique''' solution &lt;math&gt;x&lt;/math&gt; for every &lt;math&gt;y \in \mathbb{R}^n&lt;/math&gt; 
* A has a (left and right) inverse denoted &lt;math&gt;A^{-1} \in \mathbb{R}^{n\times n}&lt;/math&gt; with &lt;math&gt;AA^{-1} = A^{-1}A = I&lt;/math&gt; 
* &lt;math&gt;\mathcal{N}(A) = \{0\}&lt;/math&gt;
* &lt;math&gt;\mathcal{R}(A) = \mathbb{R}^n&lt;/math&gt;
* &lt;math&gt;\det A^TA=\det AA^T \neq 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; does not have a zero [[Eigenvectors and Eigenvalues|eigenvalue]]
===Some identities for matrix inversion===
* ''Rank one update formula'' (known as Sherman-Morrison formula): Iterative inversions of the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt; can be done efficiently via:

&lt;math&gt;(P+aa^T)^{-1} = P^{-1} - \frac{1}{1+a^T P^{-1}a} (P^{-1}a)(P^{-1}a)^T&lt;/math&gt;
* &lt;math&gt;(I-C)^{-1} = I+C+C^2+C^3+\dots&lt;/math&gt; (if the series converges&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;)
* Matrix inversion lemma (known as Woodbury matrix identity): &lt;math&gt;(A+BC)^{-1} = A^{-1}-A^{-1} B (I+C A^{-1}B)^{-1}CA^{-1}&lt;/math&gt;, which is useful when &lt;math&gt;A&lt;/math&gt; is easy to invert and &lt;math&gt;BC&lt;/math&gt; is a low-rank matrix&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;

==References==</text>
      <sha1>lrluvhjvvyza7yzyodp7mmxect1afn3</sha1>
    </revision>
  </page>
  <page>
    <title>Fundamental Theorem of Linear Algebra</title>
    <ns>0</ns>
    <id>15</id>
    <revision>
      <id>33</id>
      <timestamp>2018-07-07T13:48:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "This page needs to be filled, but let's start it:  For any &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt;, it holds that * &lt;math&gt;\text{rank}(A)+\dim\mathcal N (A) = n&lt;/math&gt;  This..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="568">This page needs to be filled, but let's start it:

For any &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt;, it holds that
* &lt;math&gt;\text{rank}(A)+\dim\mathcal N (A) = n&lt;/math&gt;

This can be interpreted as ''conversation of dimension''&lt;ref&gt;Stephen Boyd, EE253 Lecture notes 4&lt;/ref&gt;. That is, &lt;math&gt;\text{rank}(A)&lt;/math&gt; is the dimension of set 'hit' by the mapping &lt;math&gt;y=Ax&lt;/math&gt;; &lt;math&gt;\dim\mathcal N(A)&lt;/math&gt; is the dimension of the set of &lt;math&gt;x&lt;/math&gt; 'crushed' to zero by &lt;math&gt;y=Ax&lt;/math&gt;. Those two dimensions must always add up to &lt;math&gt;0&lt;/math&gt;.

== References ==</text>
      <sha1>d2yajqyd3nxxwkv12ra8qlx2n2st6o8</sha1>
    </revision>
    <revision>
      <id>145</id>
      <parentid>33</parentid>
      <timestamp>2018-07-22T23:57:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="573">This page needs to be filled, but let's start it:

For any &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt;, it holds that
* &lt;math&gt;\text{rank}(A)+\dim\mathcal N (A) = n&lt;/math&gt;

This can be interpreted as ''conversation of dimension''&lt;ref&gt;Stephen Boyd, EE253 Lecture notes 4&lt;/ref&gt;. That is, &lt;math&gt;\text{rank}(A)&lt;/math&gt; is the dimension of set 'hit' by the mapping &lt;math&gt;y=Ax&lt;/math&gt;; &lt;math&gt;\dim\mathcal N(A)&lt;/math&gt; is the dimension of the set of &lt;math&gt;x&lt;/math&gt; 'crushed' to zero by &lt;math&gt;y=Ax&lt;/math&gt;. Those two dimensions must always add up to &lt;math&gt;0&lt;/math&gt;.

TBC

== References ==</text>
      <sha1>cqvd2tyo1ye5f8esomvk2rp5w8zduea</sha1>
    </revision>
    <revision>
      <id>363</id>
      <parentid>145</parentid>
      <timestamp>2019-05-12T22:17:16Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Fixed typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="573">This page needs to be filled, but let's start it:

For any &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt;, it holds that
* &lt;math&gt;\text{rank}(A)+\dim\mathcal N (A) = n&lt;/math&gt;

This can be interpreted as ''conversation of dimension''&lt;ref&gt;Stephen Boyd, EE253 Lecture notes 4&lt;/ref&gt;. That is, &lt;math&gt;\text{rank}(A)&lt;/math&gt; is the dimension of set 'hit' by the mapping &lt;math&gt;y=Ax&lt;/math&gt;; &lt;math&gt;\dim\mathcal N(A)&lt;/math&gt; is the dimension of the set of &lt;math&gt;x&lt;/math&gt; 'crushed' to zero by &lt;math&gt;y=Ax&lt;/math&gt;. Those two dimensions must always add up to &lt;math&gt;n&lt;/math&gt;.

TBC

== References ==</text>
      <sha1>s02ekc2cdga60akz22dc3vsjznwzvkp</sha1>
    </revision>
  </page>
  <page>
    <title>Rank</title>
    <ns>0</ns>
    <id>16</id>
    <revision>
      <id>34</id>
      <timestamp>2018-07-07T13:54:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "== Identities about rank == * &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt; * If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{m\times r}&lt;/math&gt; and &lt;ma..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="607">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.


== References ==</text>
      <sha1>5dizvh4bpx2n908gbp6d4jang18j5hc</sha1>
    </revision>
    <revision>
      <id>35</id>
      <parentid>34</parentid>
      <timestamp>2018-07-07T13:54:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="703">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim A = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

== References ==</text>
      <sha1>n34f1n79sxbhfk1dl76p0o0xejg286h</sha1>
    </revision>
    <revision>
      <id>36</id>
      <parentid>35</parentid>
      <timestamp>2018-07-07T14:00:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="711">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim A = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

== References ==</text>
      <sha1>60pnrhyux36yh63ng40cknwvqeeqhm9</sha1>
    </revision>
    <revision>
      <id>166</id>
      <parentid>36</parentid>
      <timestamp>2018-07-28T14:21:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2579">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim A = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter.

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.



== References ==</text>
      <sha1>egdekfxli6qtj9qe3luzmdkouimjkaf</sha1>
    </revision>
    <revision>
      <id>167</id>
      <parentid>166</parentid>
      <timestamp>2018-07-28T14:21:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Rank in real applications */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2633">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim A = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>8a7u4gh03ddhhztq7s1izwd2fdkbv5q</sha1>
    </revision>
    <revision>
      <id>168</id>
      <parentid>167</parentid>
      <timestamp>2018-07-28T14:25:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Why can the rank be of little use in real life? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2721">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim A = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>eg4ali9ae1xjm7ddg0c5xdqc7906qut</sha1>
    </revision>
    <revision>
      <id>169</id>
      <parentid>168</parentid>
      <timestamp>2018-07-28T14:27:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Rank in real applications */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2818">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim A = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>2heewo6dmbp8zs4r5a17fgrwiiwigh3</sha1>
    </revision>
    <revision>
      <id>357</id>
      <parentid>169</parentid>
      <timestamp>2019-04-28T17:37:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Identities about rank */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2831">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>mc7kz4boz2bj1pgk82pvcl2r3u9fmz2</sha1>
    </revision>
    <revision>
      <id>359</id>
      <parentid>357</parentid>
      <timestamp>2019-04-28T17:49:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Identities about rank */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4542">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\rowsp{T}&lt;/math&gt; and &lt;math&gt;\rowsp{V}&lt;/math&gt; are essentially disjoint and &lt;math&gt;\colsp{T}&lt;/math&gt; and &lt;math&gt;\colsp{V}&lt;/math&gt; are also essentially disjoint.

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>0bf848txzs02dykj1ajl5etlso0ltzb</sha1>
    </revision>
    <revision>
      <id>360</id>
      <parentid>359</parentid>
      <timestamp>2019-04-28T18:00:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Ranks of Partitioned Matrices */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4735">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>o9cogdp4ox5nb7azu3lwmzojgjrdiu1</sha1>
    </revision>
    <revision>
      <id>361</id>
      <parentid>360</parentid>
      <timestamp>2019-04-28T18:01:31Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Ranks of Partitioned Matrices */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5251">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A,B}) = \text{rank} (\mathbf A)+\text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{AB}) \ge \text{rank} (\mathbf A) + \text{rank}(\mathbf B) - n&lt;/math&gt; with equality holding iff &lt;math&gt;\mathbf{(I-BB^-)(I-A^-A)=0}&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf {AB}) = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf B)\cap \mathcal N(\mathbf A)] = \text{rank}(\mathbf A)-\dim[\mathcal{C}( \mathbf B')\cap \mathcal N(\mathbf A')]&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>mpzgse5b2q1ocmec5bncaev3b2yholc</sha1>
    </revision>
    <revision>
      <id>362</id>
      <parentid>361</parentid>
      <timestamp>2019-04-28T18:03:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Ranks of Partitioned Matrices */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5454">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices and multiplication of matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;. They mostly use the concept of 
[[Essentially disjoint|essential disjointness]] to sharpen inequalities regarding the ranks of partitioned matrices or multiplied matrices.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A,B}) = \text{rank} (\mathbf A)+\text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{AB}) \ge \text{rank} (\mathbf A) + \text{rank}(\mathbf B) - n&lt;/math&gt; with equality holding iff &lt;math&gt;\mathbf{(I-BB^-)(I-A^-A)=0}&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf {AB}) = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf B)\cap \mathcal N(\mathbf A)] = \text{rank}(\mathbf A)-\dim[\mathcal{C}( \mathbf B')\cap \mathcal N(\mathbf A')]&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>mfqtr24fr6z3wuvdhnm38twrf6qylxq</sha1>
    </revision>
    <revision>
      <id>369</id>
      <parentid>362</parentid>
      <timestamp>2019-06-03T11:24:17Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Added some more inequalities for rank</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6251">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices and multiplication of matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;. They mostly use the concept of 
[[Essentially disjoint|essential disjointness]] to sharpen inequalities regarding the ranks of partitioned matrices or multiplied matrices.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf B)&lt;/math&gt; (see C4.5.9&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A,B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf{B})&lt;/math&gt;. The leftmost inequality holds iff &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T17.4.2&lt;ref name=harville/&gt;). The rightmost inequality holds iff &lt;math&gt;\mathcal{R}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf B)&lt;/math&gt; as well as &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T18.5.7&lt;ref name=harville/&gt;).

* &lt;math&gt;\text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf A)&lt;/math&gt; (see C7.4.5&lt;ref name=harville &gt;)
* &lt;math&gt;\text{rank}(\mathbf{A,B}) = \text{rank} (\mathbf A)+\text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{AB}) \ge \text{rank} (\mathbf A) + \text{rank}(\mathbf B) - n&lt;/math&gt; with equality holding iff &lt;math&gt;\mathbf{(I-BB^-)(I-A^-A)=0}&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf {AB}) = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf B)\cap \mathcal N(\mathbf A)] = \text{rank}(\mathbf A)-\dim[\mathcal{C}( \mathbf B')\cap \mathcal N(\mathbf A')]&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>jrpgpp0o7di353imyo4qz7mx74rp27d</sha1>
    </revision>
    <revision>
      <id>370</id>
      <parentid>369</parentid>
      <timestamp>2019-06-03T11:24:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6251">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices and multiplication of matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;. They mostly use the concept of 
[[Essentially disjoint|essential disjointness]] to sharpen inequalities regarding the ranks of partitioned matrices or multiplied matrices.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf B)&lt;/math&gt; (see C4.5.9&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A,B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf{B})&lt;/math&gt;. The leftmost inequality holds iff &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T17.4.2&lt;ref name=harville/&gt;). The rightmost inequality holds iff &lt;math&gt;\mathcal{R}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf B)&lt;/math&gt; as well as &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T18.5.7&lt;ref name=harville/&gt;).

* &lt;math&gt;\text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf A)&lt;/math&gt; (see C7.4.5&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A,B}) = \text{rank} (\mathbf A)+\text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{AB}) \ge \text{rank} (\mathbf A) + \text{rank}(\mathbf B) - n&lt;/math&gt; with equality holding iff &lt;math&gt;\mathbf{(I-BB^-)(I-A^-A)=0}&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf {AB}) = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf B)\cap \mathcal N(\mathbf A)] = \text{rank}(\mathbf A)-\dim[\mathcal{C}( \mathbf B')\cap \mathcal N(\mathbf A')]&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>8apdhomd0jtlfp4wtiojjv0kpo9kez9</sha1>
    </revision>
    <revision>
      <id>371</id>
      <parentid>370</parentid>
      <timestamp>2019-06-16T16:01:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6284">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices and multiplication of matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;. They mostly use the concept of 
[[Essentially disjoint|essential disjointness]] to sharpen inequalities regarding the ranks of partitioned matrices or multiplied matrices.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf B)&lt;/math&gt; (see C4.5.9&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A,B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf{B})&lt;/math&gt;. The leftmost inequality holds iff &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T17.4.2&lt;ref name=harville/&gt;). The rightmost inequality holds iff &lt;math&gt;\mathcal{R}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf B)&lt;/math&gt; as well as &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T18.5.7&lt;ref name=harville/&gt;).

* &lt;math&gt;\text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf A)&lt;/math&gt; (see C7.4.5&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A,B}) = \text{rank} (\mathbf A)+\text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{AB}) \ge \text{rank} (\mathbf A) + \text{rank}(\mathbf B) - n&lt;/math&gt; with equality holding iff &lt;math&gt;\mathbf{(I-BB^-)(I-A^-A)=0}&lt;/math&gt; (see T17.5.4&lt;ref name=harville).
* &lt;math&gt;\text{rank}(\mathbf {AB}) = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf B)\cap \mathcal N(\mathbf A)] = \text{rank}(\mathbf A)-\dim[\mathcal{C}( \mathbf B')\cap \mathcal N(\mathbf A')]&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>068b2pl50rhikybsy3bc1z69dv4r07r</sha1>
    </revision>
    <revision>
      <id>374</id>
      <parentid>371</parentid>
      <timestamp>2019-06-16T16:08:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6381">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices and multiplication of matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;. They mostly use the concept of 
[[Essentially disjoint|essential disjointness]] to sharpen inequalities regarding the ranks of partitioned matrices or multiplied matrices.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf B)&lt;/math&gt; (see C4.5.9&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A,B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf{B})&lt;/math&gt;. The leftmost inequality holds iff &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T17.4.2&lt;ref name=harville/&gt;). The rightmost inequality holds iff &lt;math&gt;\mathcal{R}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf B)&lt;/math&gt; as well as &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T18.5.7&lt;ref name=harville/&gt;).

* &lt;math&gt;\text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf A)&lt;/math&gt; (see C7.4.5&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A,B}) = \text{rank} (\mathbf A)+\text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{AB}) \ge \text{rank} (\mathbf A) + \text{rank}(\mathbf B) - n&lt;/math&gt; with equality holding iff &lt;math&gt;\mathbf{(I-BB^-)(I-A^-A)=0}&lt;/math&gt; (see T17.5.4&lt;ref name=harville/&gt;).
* &lt;math&gt;\text{rank}(\mathbf {AB}) = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf B)\cap \mathcal N(\mathbf A)] = \text{rank}(\mathbf A)-\dim[\mathcal{C}( \mathbf B')\cap \mathcal N(\mathbf A')]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \ge |\text{rank}(\mathbf{A})-\text{rank}(\mathbf{B})|&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>6twikmdrh2h6igug9ik0hberc0hnzvf</sha1>
    </revision>
    <revision>
      <id>516</id>
      <parentid>374</parentid>
      <timestamp>2019-11-27T18:18:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6409">== Identities about rank ==
* &lt;math&gt;\text{rank}(BC) \le \min\{\text{rank}(B), \text{rank}(C)\}&lt;/math&gt;
* If &lt;math&gt;A=BC&lt;/math&gt; with &lt;math&gt;B \in \mathbb{R}^{m\times r}&lt;/math&gt; and &lt;math&gt;C \in \mathbb{R}^{r\times n}&lt;/math&gt;, then  &lt;math&gt;\text{rank}(A) \le r&lt;/math&gt;. This property is very useful for the application of fast matrix-vector multiplication. I.e., to obtain the product &lt;math&gt;Ax&lt;/math&gt;, we can instead do &lt;math&gt;BCx&lt;/math&gt; and if &lt;math&gt;r&lt;/math&gt; is very small compared to &lt;math&gt;m&lt;/math&gt; or &lt;math&gt;n&lt;/math&gt;, then we'd speed up the computation a lot&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.
* &lt;math&gt;\text{rank}(A) + \dim \mathcal{N}(A) = n&lt;/math&gt; (see also [[Fundamental Theorem of Linear Algebra]]).

=== Ranks of Partitioned Matrices and multiplication of matrices ===
The following results are from Sections 17.2 and 17.4 of Harville&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;. They mostly use the concept of 
[[Essentially disjoint|essential disjointness]] to sharpen inequalities regarding the ranks of partitioned matrices or multiplied matrices.
* If &lt;math&gt;\mathcal C (\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}(\mathbf B)&lt;/math&gt; are essentially disjoint, then  &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)=\text{rank}(\mathbf A)+ \text{rank}(\mathbf B).&lt;/math&gt; If they are not, then &lt;math&gt;\text{rank}(\mathbf A, \mathbf B)&lt;\text{rank}(\mathbf A)+ \text{rank}(\mathbf B)&lt;/math&gt;.
* For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} = \text{rank}(\mathbf V) + \text{rank}[\mathbf U, \mathbf{T(I-V^-V)}].&lt;/math&gt;
* For any &lt;math&gt;\mathbf{T,U,V}&lt;/math&gt; &lt;math&gt;\text{rank}\begin{bmatrix} \mathbf T &amp; \mathbf U \\ \mathbf V &amp; \mathbf 0 \end{bmatrix} \le \text{rank}(\mathbf V) + \text{rank} (\mathbf U) + \text{rank} (\mathbf T)&lt;/math&gt; with eq. holding iff &lt;math&gt;\mathcal{R}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf V)&lt;/math&gt; are essentially disjoint and &lt;math&gt;\mathcal{C}( \mathbf T)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf V)&lt;/math&gt; are also essentially disjoint.
* &lt;math&gt;\text{rank}[(\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf B)&lt;/math&gt; (see C4.5.9&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \le \text{rank}(\mathbf{A,B}) \le \text{rank}(\mathbf{A})+\text{rank}(\mathbf{B})&lt;/math&gt;. The leftmost inequality holds with equality iff &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T17.4.2&lt;ref name=harville/&gt;). The rightmost inequality holds with equality iff &lt;math&gt;\mathcal{R}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{R}( \mathbf B)&lt;/math&gt; as well as &lt;math&gt;\mathcal{C}( \mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}( \mathbf B)&lt;/math&gt; are essentially disjoint (see T18.5.7&lt;ref name=harville/&gt;).

* &lt;math&gt;\text{rank}(\mathbf{A'A}) = \text{rank}(\mathbf A)&lt;/math&gt; (see C7.4.5&lt;ref name=harville/&gt;)
* &lt;math&gt;\text{rank}(\mathbf{A,B}) = \text{rank} (\mathbf A)+\text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf A)\cap\mathcal{C}( \mathbf B)]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{AB}) \ge \text{rank} (\mathbf A) + \text{rank}(\mathbf B) - n&lt;/math&gt; with equality holding iff &lt;math&gt;\mathbf{(I-BB^-)(I-A^-A)=0}&lt;/math&gt; (see T17.5.4&lt;ref name=harville/&gt;).
* &lt;math&gt;\text{rank}(\mathbf {AB}) = \text{rank}(\mathbf B)-\dim[\mathcal{C}( \mathbf B)\cap \mathcal N(\mathbf A)] = \text{rank}(\mathbf A)-\dim[\mathcal{C}( \mathbf B')\cap \mathcal N(\mathbf A')]&lt;/math&gt;
* &lt;math&gt;\text{rank}(\mathbf{A+B}) \ge |\text{rank}(\mathbf{A})-\text{rank}(\mathbf{B})|&lt;/math&gt;

== Rank in real applications ==

According to how they were obtained, matrices can be classified at least into two: Those that are derived analytically (e.g., a DCT basis) and those that are derived from data. For the former, the concept of rank makes perfect sense, however, it may make little sense for the latter. The [[Approximate rank|approximate rank]] of a matrix can be much more useful for such matrices.

=== Why can the rank be of little use in real life?===

To understand why, consider the problem of estimating &lt;math&gt;x&lt;/math&gt; from observations &lt;math&gt;y&lt;/math&gt;, i.e.,  &lt;math&gt;y = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is square (it's easy to think about non-square matrices as well). More realistically this problem can be formulated with some noise &lt;math&gt;v&lt;/math&gt;:

&lt;math&gt;y=Ax+v&lt;/math&gt;

If we had a rank-deficient &lt;math&gt;A&lt;/math&gt;, then it would be easy to say that the problem is insoluble. However, we can add tiny perturbations to the elements of &lt;math&gt;A&lt;/math&gt; and make it nonsingular (apparently with probability 1&lt;ref&gt;Stephen Boyd, EE263 Lecture Video 17: ~1:09:00&lt;/ref&gt;). Would this "magic", which suddenly made &lt;math&gt;A&lt;/math&gt; invertible, really work? The answer is no, and an [[SVD]] analysis of the matrix would reveal that it wouldn't as the [[Approximate rank|approximate rank]] of the matrix would probably be very small. The reason that it wouldn't work is that when we apply this invertible &lt;math&gt;A&lt;/math&gt; to estimate &lt;math&gt;x&lt;/math&gt; we would get

&lt;math&gt;A^{-1}y = A^{-1}Ax+A^{-1}v \implies x = A^{-1}y - A^{-1}v&lt;/math&gt;

Now when &lt;math&gt;A&lt;/math&gt; is made artifically invertible with small perturbations, &lt;math&gt;A^{-1}&lt;/math&gt; would be a huge matrix (i.e., would have large norm) and the equation above suggests that we can have a huge noise amplification in the estimation.

The problem with matrices &lt;math&gt;A&lt;/math&gt; collected from real data is that we perhaps wouldn't even suspect that there is a problem because with real data there is always some noise therefore &lt;math&gt;A&lt;/math&gt; could appear as full rank, although it could have tiny [[SVD|singular]] values that would make its inverse huge.

== References ==</text>
      <sha1>a5tgztj8vfq3xb9lgjholfqlkuuomxu</sha1>
    </revision>
  </page>
  <page>
    <title>Similarity Transformation</title>
    <ns>0</ns>
    <id>17</id>
    <revision>
      <id>37</id>
      <timestamp>2018-07-07T14:05:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;T&lt;/math&gt; be an orthogonal matrix. Then, the matrix &lt;math&gt;T^{-1}AT&lt;/math&gt; is called similarity transformation."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="119">Let &lt;math&gt;T&lt;/math&gt; be an orthogonal matrix. Then, the matrix &lt;math&gt;T^{-1}AT&lt;/math&gt; is called similarity transformation.</text>
      <sha1>h47wwzx6hyu251e696x308x4vp09k26</sha1>
    </revision>
    <revision>
      <id>379</id>
      <parentid>37</parentid>
      <timestamp>2019-06-16T18:23:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="202">Let &lt;math&gt;T&lt;/math&gt; be an orthogonal matrix. Then, the matrix &lt;math&gt;T^{-1}AT&lt;/math&gt; is called similarity transformation. The matrices &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;T^{-1}AT&lt;/math&gt; are called similar matrices.</text>
      <sha1>jn7dcz9th8jqssqihcw44k1nljf23sw</sha1>
    </revision>
    <revision>
      <id>382</id>
      <parentid>379</parentid>
      <timestamp>2019-06-16T18:28:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="489">Let &lt;math&gt;T&lt;/math&gt; be an orthogonal matrix. Then, the matrix &lt;math&gt;T^{-1}AT&lt;/math&gt; is called similarity transformation. The matrices &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;T^{-1}AT&lt;/math&gt; are called similar matrices.

=== What's the point of similarity transformation? ===

Similarity transformation is particularly useful if the resulting matrix &lt;math&gt;T^{-1}AT&lt;/math&gt; is diagonal or triangular etc. Then, computing eigenvalues, determinant, trace etc. become much more simpler. See [[Diagonalization]].</text>
      <sha1>ekzzoltfkx7o2lu5c5sttlxx3iowzgk</sha1>
    </revision>
  </page>
  <page>
    <title>Angle</title>
    <ns>0</ns>
    <id>18</id>
    <revision>
      <id>39</id>
      <timestamp>2018-07-07T14:11:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The (unsigned) angle &lt;math&gt;\theta&lt;/math&gt; between two vectors &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt; is defined as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt; &lt;math&gt;\theta = \angl..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="245">The (unsigned) angle &lt;math&gt;\theta&lt;/math&gt; between two vectors &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt; is defined as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;
&lt;math&gt;\theta = \angle(x,y) = \cos^{-1} \frac{x^T y}{||x||\,||y||}&lt;/math&gt;

== References ==</text>
      <sha1>awl8ohg9srnwyfc8690uv8liccftzm2</sha1>
    </revision>
    <revision>
      <id>41</id>
      <parentid>39</parentid>
      <timestamp>2018-07-07T14:21:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1112">The (unsigned) angle &lt;math&gt;\theta&lt;/math&gt; between two vectors &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt; is defined as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 3&lt;/ref&gt;
&lt;math&gt;\theta = \angle(x,y) = \cos^{-1} \frac{x^T y}{||x||\,||y||}&lt;/math&gt;
=== Special cases ===
* &lt;math&gt;x, y&lt;/math&gt; are ''aligned'' (&lt;math&gt;\theta = 0&lt;/math&gt;) &lt;math&gt;\implies x^T y = ||x||\,||y||&lt;/math&gt;. If &lt;math&gt;x \neq 0&lt;/math&gt;, then it must be that &lt;math&gt;y=\alpha x&lt;/math&gt; for some &lt;math&gt;\alpha \ge 0&lt;/math&gt;.
* &lt;math&gt;x, y&lt;/math&gt; are ''opposed'' (&lt;math&gt;\theta = \pi&lt;/math&gt;) &lt;math&gt;\implies x^T y = -||x||\,||y||&lt;/math&gt;. If &lt;math&gt;x \neq 0&lt;/math&gt;, then it must be that &lt;math&gt;y=-\alpha x&lt;/math&gt; for some &lt;math&gt;\alpha \ge 0&lt;/math&gt;.
* &lt;math&gt;x, y&lt;/math&gt; are ''orthogonal'' (&lt;math&gt;\theta = \pi/2 or \theta = -\pi/2&lt;/math&gt;) &lt;math&gt;\implies x^T y = 0&lt;/math&gt;. This is denoted as &lt;math&gt;x \perp y&lt;/math&gt;.
=== Further interpretations ===
* &lt;math&gt;x^T y &gt; 0&lt;/math&gt; means that &lt;math&gt;\angle(x,y)&lt;/math&gt; is ''acute'' (smaller than &lt;math&gt;\pi/2&lt;/math&gt;).
* &lt;math&gt;x^T y &lt; 0&lt;/math&gt; means that &lt;math&gt;\angle(x,y)&lt;/math&gt; is ''obtuse'' (larger than &lt;math&gt;\pi/2&lt;/math&gt;).


== References ==</text>
      <sha1>tlrqjekjjaex7rv2zgzpemf9n3cjqif</sha1>
    </revision>
  </page>
  <page>
    <title>Cauchy-Schwartz Inequality</title>
    <ns>0</ns>
    <id>19</id>
    <revision>
      <id>40</id>
      <timestamp>2018-07-07T14:12:48Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "For any &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt;, it holds that &lt;math&gt;|x^T y| \le ||x||\,||y||&lt;/math&gt;."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="95">For any &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt;, it holds that
&lt;math&gt;|x^T y| \le ||x||\,||y||&lt;/math&gt;.</text>
      <sha1>js602gphbmp9umilxsiyz3qhobr5zwm</sha1>
    </revision>
    <revision>
      <id>203</id>
      <parentid>40</parentid>
      <timestamp>2018-09-12T12:06:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="461">For any &lt;math&gt;x,y \in \mathbb{R}^n&lt;/math&gt;, it holds that
&lt;math&gt;|x^T y| \le ||x||\,||y||&lt;/math&gt;.

An easy way to remember this is to relate it to the [[angle]] between two vectors. For the angle &lt;math&gt;\theta = \angle(x,y) = \cos^{-1} \frac{x^T y}{||x||\,||y||}&lt;/math&gt; to be real, the argument of the &lt;math&gt;\cos^{-1}&lt;/math&gt; at the right should be in the range &lt;math&gt;[-1,1]&lt;/math&gt;. This is only possible if &lt;math&gt;- ||x||\,||y|| \le |x^T y| \le ||x||\,||y||&lt;/math&gt;.</text>
      <sha1>mfsyoycrsxy0i7na3oi2amdgpvbcgp4</sha1>
    </revision>
  </page>
  <page>
    <title>Orthonormality</title>
    <ns>0</ns>
    <id>20</id>
    <revision>
      <id>42</id>
      <timestamp>2018-07-07T14:27:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A set of vectors &lt;math&gt;\{u_1,u_2,\dots,u_k\}&lt;/math&gt; are called orthonormal if &lt;math&gt;u_i^T u_i = 1&lt;/math&gt; and for &lt;math&gt;j\neq i&lt;/math&gt;, &lt;math&gt;u_j^Tu_i = 0&lt;/math&gt;"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="160">A set of vectors &lt;math&gt;\{u_1,u_2,\dots,u_k\}&lt;/math&gt; are called orthonormal if &lt;math&gt;u_i^T u_i = 1&lt;/math&gt; and for &lt;math&gt;j\neq i&lt;/math&gt;, &lt;math&gt;u_j^Tu_i = 0&lt;/math&gt;</text>
      <sha1>gm5n8sdq058xl6dy0h46ehqh2yce29j</sha1>
    </revision>
    <revision>
      <id>43</id>
      <parentid>42</parentid>
      <timestamp>2018-07-07T14:35:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="348">A set of vectors &lt;math&gt;\{u_1,u_2,\dots,u_k\}&lt;/math&gt; are called orthonormal if &lt;math&gt;u_i^T u_i = 1&lt;/math&gt; and for &lt;math&gt;j\neq i&lt;/math&gt;, &lt;math&gt;u_j^Tu_i = 0&lt;/math&gt;.

Suppose columns of &lt;math&gt;U=[u_1 \dots u_k]&lt;/math&gt; are orthonormal. Then, multiplication by &lt;math&gt;U&lt;/math&gt; does not change the norm, i.e. it's ''isometric'': 
&lt;math&gt;||Uz|| = ||z||&lt;/math&gt;</text>
      <sha1>hqqved5uojqetuwohwobz855a69uurd</sha1>
    </revision>
    <revision>
      <id>44</id>
      <parentid>43</parentid>
      <timestamp>2018-07-07T14:42:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="455">A set of vectors &lt;math&gt;\{u_1,u_2,\dots,u_k\}&lt;/math&gt; are called orthonormal if &lt;math&gt;u_i^T u_i = 1&lt;/math&gt; and for &lt;math&gt;j\neq i&lt;/math&gt;, &lt;math&gt;u_j^Tu_i = 0&lt;/math&gt;.

Suppose columns of &lt;math&gt;U=[u_1 \dots u_k]&lt;/math&gt; are orthonormal. Then, 
* multiplication by &lt;math&gt;U&lt;/math&gt; does not change the norm, i.e. &lt;math&gt;||Uz|| = ||z||&lt;/math&gt;
* mapping &lt;math&gt;w=Uz&lt;/math&gt; is ''isometric'', i.e., it preserves distances: &lt;math&gt;||Uz-U\tilde{z}|| = ||z-\tilde{z}||&lt;/math&gt;</text>
      <sha1>cyv4t1uz5pp8pre3v2s9vsniqwneggw</sha1>
    </revision>
    <revision>
      <id>45</id>
      <parentid>44</parentid>
      <timestamp>2018-07-07T14:45:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="690">A set of vectors &lt;math&gt;\{u_1,u_2,\dots,u_k\}&lt;/math&gt; are called orthonormal if &lt;math&gt;u_i^T u_i = 1&lt;/math&gt; and for &lt;math&gt;j\neq i&lt;/math&gt;, &lt;math&gt;u_j^Tu_i = 0&lt;/math&gt;.

Suppose columns of &lt;math&gt;U=[u_1 \dots u_k]&lt;/math&gt; are orthonormal. Then, 
* multiplication by &lt;math&gt;U&lt;/math&gt; does not change the norm, i.e. &lt;math&gt;||Uz|| = ||z||&lt;/math&gt;
* mapping &lt;math&gt;w=Uz&lt;/math&gt; is ''isometric'', i.e., it preserves distances: &lt;math&gt;||Uz-U\tilde{z}|| = ||z-\tilde{z}||&lt;/math&gt;
* inner products are also preserved: &lt;math&gt;\langle Uz, U\tilde{z} \rangle = \langle z, \tilde{z} \rangle&lt;/math&gt;
* since norms and inner products are preserved, ''angles'' are also preserved: \angle(Uz,U\tilde{z}) = \angle(z,\tilde{z})</text>
      <sha1>fpyuy0igv5z747hn4bxm1pfp9wvop44</sha1>
    </revision>
  </page>
  <page>
    <title>Outer product</title>
    <ns>0</ns>
    <id>21</id>
    <revision>
      <id>46</id>
      <timestamp>2018-07-07T14:48:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;u&lt;/math&gt; be a column vector. Then, &lt;math&gt;u u^T&lt;/math&gt; is called ''outer product''. It is also called ''dyad'' or ''(symmetric) rank-one matrix''."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="155">Let &lt;math&gt;u&lt;/math&gt; be a column vector. Then, &lt;math&gt;u u^T&lt;/math&gt; is called ''outer product''. It is also called ''dyad'' or ''(symmetric) rank-one matrix''.</text>
      <sha1>d7covhy0dpw9btxmygrxw3nf857wtft</sha1>
    </revision>
    <revision>
      <id>129</id>
      <parentid>46</parentid>
      <timestamp>2018-07-20T14:47:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="686">Let &lt;math&gt;u&lt;/math&gt; be a column vector. Then, &lt;math&gt;u u^T&lt;/math&gt; is called ''outer product''. It is also called ''dyad'' or ''(symmetric) rank-one matrix''.

If the columns of a matrix &lt;math&gt;U = u_1, \dots, u_k&lt;/math&gt; form a set of orthogonal vectors, then the &lt;math&gt;U U^T&lt;/math&gt; is a projection matrix &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 5&lt;/ref&gt;. &lt;math&gt;U U^T&lt;/math&gt; can be written as (see [[Matrix multiplication]])

&lt;math&gt;U U^T = \sum_{i=1}^n u_i u_i^T&lt;/math&gt;

In this interpretation, each component &lt;math&gt;q_i q_i^T&lt;/math&gt; is an outer product and also a 1-dimensional projection matrix. This interpretation is illustrated with an example in [[Symmetric matrices]].

== References ==</text>
      <sha1>pz2bvx3fxsb8rlcn3vna2s4tlwm60lb</sha1>
    </revision>
  </page>
  <page>
    <title>Orthogonal matrix</title>
    <ns>0</ns>
    <id>22</id>
    <revision>
      <id>47</id>
      <timestamp>2018-07-07T15:06:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;u_1, \dots, u_n \in \mathbb{R}^n&lt;/math&gt; be an orthonormal set of vectors. Then, it's a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt; and the matrix &lt;math&gt;U = [u_1 \dots u_n]&lt;/..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1659">Let &lt;math&gt;u_1, \dots, u_n \in \mathbb{R}^n&lt;/math&gt; be an orthonormal set of vectors. Then, it's a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt; and the matrix &lt;math&gt;U = [u_1 \dots u_n]&lt;/math&gt; is called '''orthogonal''' and it satisfies &lt;math&gt;U^TU = I&lt;/math&gt;. It follows that, &lt;math&gt;U^{-1} = U^T&lt;/math&gt; and also &lt;math&gt;UU^T = I&lt;/math&gt;. This can also be written as the sum of ''rank-one matrices''&lt;ref name=r1&gt; Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;:

&lt;math&gt;\sum_{i=1}^n u_i u_i^T = I&lt;/math&gt;.

Expansion with orthogonal matrices is very simple. Since &lt;math&gt;x = UU^Tx&lt;/math&gt;, we have that 

&lt;math&gt;x = \sum_{i=1}^n(i_i^T x) u_i&lt;/math&gt;. 

In other words, to represent the vector as a linear combination of &lt;math&gt;u_i&lt;/math&gt;'s, all we need to do is compute the linear combination coefficient for the &lt;math&gt;i&lt;/math&gt;th component as &lt;math&gt;u^T_i x&lt;/math&gt;.

Some nomenclature:
* u_i^T is called the ''component'' or the ''coefficient'' of &lt;math&gt;x&lt;/math&gt; in the direction of &lt;math&gt;u_i&lt;/math&gt;.
* a = U^Tx resolves &lt;math&gt;U^T x&lt;/math&gt; ''resolves'' x into the vectors of its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua = \sum_{i=1}^n a_i u_i&lt;/math&gt; is called the ''expansion'' of &lt;math&gt;x&lt;/math&gt;.

== Geometric interpretation ==
Recall that left-multiplying &lt;math&gt;x&lt;/math&gt; with a matrix  with orthonormal columns [[Orthonormality|preserves norm and angles]], i.e. &lt;math&gt;||Uz|| = ||z||&lt;/math&gt; and &lt;math&gt;\angle (Uz,U\tilde{z}) = \angle (z,\tilde{z})&lt;/math&gt;. Therefore, ''any'' orthogonal matrices do either &lt;ref name=r1/&gt;
* A rotation about an axis
* A reflection about an axis

== References ==</text>
      <sha1>39rnqr826ap19p101qyeqxt836zzni7</sha1>
    </revision>
    <revision>
      <id>161</id>
      <parentid>47</parentid>
      <timestamp>2018-07-27T14:53:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1672">Let &lt;math&gt;u_1, \dots, u_n \in \mathbb{R}^n&lt;/math&gt; be an orthonormal set of vectors. Then, it's a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt; and the matrix &lt;math&gt;U = [u_1 \dots u_n]&lt;/math&gt; is called '''orthogonal''' and it satisfies &lt;math&gt;U^TU = I&lt;/math&gt;. It follows that, &lt;math&gt;U^{-1} = U^T&lt;/math&gt; and also &lt;math&gt;UU^T = I&lt;/math&gt;. This can also be written as the sum of ''rank-one matrices''&lt;ref name=r1&gt; Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;:

&lt;math&gt;\sum_{i=1}^n u_i u_i^T = I&lt;/math&gt;.

Expansion with orthogonal matrices is very simple. Since &lt;math&gt;x = UU^Tx&lt;/math&gt;, we have that 

&lt;math&gt;x = \sum_{i=1}^n(i_i^T x) u_i&lt;/math&gt;. 

In other words, to represent the vector as a linear combination of &lt;math&gt;u_i&lt;/math&gt;'s, all we need to do is compute the linear combination coefficient for the &lt;math&gt;i&lt;/math&gt;th component as &lt;math&gt;u^T_i x&lt;/math&gt;.

Some nomenclature:
* u_i^T is called the ''component'' or the ''coefficient'' of &lt;math&gt;x&lt;/math&gt; in the direction of &lt;math&gt;u_i&lt;/math&gt;.
* &lt;math&gt;a = U^Tx&lt;/math&gt; resolves &lt;math&gt;U^T x&lt;/math&gt; ''resolves'' x into the vectors of its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua = \sum_{i=1}^n a_i u_i&lt;/math&gt; is called the ''expansion'' of &lt;math&gt;x&lt;/math&gt;.

== Geometric interpretation ==
Recall that left-multiplying &lt;math&gt;x&lt;/math&gt; with a matrix  with orthonormal columns [[Orthonormality|preserves norm and angles]], i.e. &lt;math&gt;||Uz|| = ||z||&lt;/math&gt; and &lt;math&gt;\angle (Uz,U\tilde{z}) = \angle (z,\tilde{z})&lt;/math&gt;. Therefore, ''any'' orthogonal matrices do either &lt;ref name=r1/&gt;
* A rotation about an axis
* A reflection about an axis

== References ==</text>
      <sha1>afb5ycce34xy9zzh9755rzkjq9fciyj</sha1>
    </revision>
    <revision>
      <id>187</id>
      <parentid>161</parentid>
      <timestamp>2018-07-29T00:54:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1970">Let &lt;math&gt;u_1, \dots, u_n \in \mathbb{R}^n&lt;/math&gt; be an orthonormal set of vectors. Then, it's a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt; and the matrix &lt;math&gt;U = [u_1 \dots u_n]&lt;/math&gt; is called '''orthogonal''' and it satisfies &lt;math&gt;U^TU = I&lt;/math&gt;. It follows that, &lt;math&gt;U^{-1} = U^T&lt;/math&gt; and also &lt;math&gt;UU^T = I&lt;/math&gt;. This can also be written as the sum of ''rank-one matrices''&lt;ref name=r1&gt; Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;:

&lt;math&gt;\sum_{i=1}^n u_i u_i^T = I&lt;/math&gt;.

Expansion with orthogonal matrices is very simple. Since &lt;math&gt;x = UU^Tx&lt;/math&gt;, we have that 

&lt;math&gt;x = \sum_{i=1}^n(i_i^T x) u_i&lt;/math&gt;. 

In other words, to represent the vector as a linear combination of &lt;math&gt;u_i&lt;/math&gt;'s, all we need to do is compute the linear combination coefficient for the &lt;math&gt;i&lt;/math&gt;th component as &lt;math&gt;u^T_i x&lt;/math&gt;.

Some nomenclature:
* &lt;math&gt;u_i^T&lt;/math&gt; is called the ''component'' or the ''coefficient'' of &lt;math&gt;x&lt;/math&gt; in the direction of &lt;math&gt;u_i&lt;/math&gt;.
* &lt;math&gt;a = U^Tx&lt;/math&gt; resolves &lt;math&gt;U^T x&lt;/math&gt; ''resolves'' x into the vectors of its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua = \sum_{i=1}^n a_i u_i&lt;/math&gt; is called the ''expansion'' of &lt;math&gt;x&lt;/math&gt;.

=== Some facts about orthogonal matrices ===
* Orthogonal matrices have a [[Condition number|condition number]] of &lt;math&gt;1&lt;/math&gt; (which makes them a very good candidate for encoding/decoding etc.) as their largest and smallest [[Singular Value Decomposition|singular values]] are 1.

== Geometric interpretation ==
Recall that left-multiplying &lt;math&gt;x&lt;/math&gt; with a matrix  with orthonormal columns [[Orthonormality|preserves norm and angles]], i.e. &lt;math&gt;||Uz|| = ||z||&lt;/math&gt; and &lt;math&gt;\angle (Uz,U\tilde{z}) = \angle (z,\tilde{z})&lt;/math&gt;. Therefore, ''any'' orthogonal matrices do either &lt;ref name=r1/&gt;
* A rotation about an axis
* A reflection about an axis

== References ==</text>
      <sha1>ss9i8ge3zz5smvjqztpwd206qrpfrjf</sha1>
    </revision>
    <revision>
      <id>353</id>
      <parentid>187</parentid>
      <timestamp>2019-02-04T23:59:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1970">Let &lt;math&gt;u_1, \dots, u_n \in \mathbb{R}^n&lt;/math&gt; be an orthonormal set of vectors. Then, it's a basis for &lt;math&gt;\mathbb{R}^n&lt;/math&gt; and the matrix &lt;math&gt;U = [u_1 \dots u_n]&lt;/math&gt; is called '''orthogonal''' and it satisfies &lt;math&gt;U^TU = I&lt;/math&gt;. It follows that, &lt;math&gt;U^{-1} = U^T&lt;/math&gt; and also &lt;math&gt;UU^T = I&lt;/math&gt;. This can also be written as the sum of ''rank-one matrices''&lt;ref name=r1&gt; Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;:

&lt;math&gt;\sum_{i=1}^n u_i u_i^T = I&lt;/math&gt;.

Expansion with orthogonal matrices is very simple. Since &lt;math&gt;x = UU^Tx&lt;/math&gt;, we have that 

&lt;math&gt;x = \sum_{i=1}^n(u_i^T x) u_i&lt;/math&gt;. 

In other words, to represent the vector as a linear combination of &lt;math&gt;u_i&lt;/math&gt;'s, all we need to do is compute the linear combination coefficient for the &lt;math&gt;i&lt;/math&gt;th component as &lt;math&gt;u^T_i x&lt;/math&gt;.

Some nomenclature:
* &lt;math&gt;u_i^T&lt;/math&gt; is called the ''component'' or the ''coefficient'' of &lt;math&gt;x&lt;/math&gt; in the direction of &lt;math&gt;u_i&lt;/math&gt;.
* &lt;math&gt;a = U^Tx&lt;/math&gt; resolves &lt;math&gt;U^T x&lt;/math&gt; ''resolves'' x into the vectors of its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components.
* &lt;math&gt;x = Ua = \sum_{i=1}^n a_i u_i&lt;/math&gt; is called the ''expansion'' of &lt;math&gt;x&lt;/math&gt;.

=== Some facts about orthogonal matrices ===
* Orthogonal matrices have a [[Condition number|condition number]] of &lt;math&gt;1&lt;/math&gt; (which makes them a very good candidate for encoding/decoding etc.) as their largest and smallest [[Singular Value Decomposition|singular values]] are 1.

== Geometric interpretation ==
Recall that left-multiplying &lt;math&gt;x&lt;/math&gt; with a matrix  with orthonormal columns [[Orthonormality|preserves norm and angles]], i.e. &lt;math&gt;||Uz|| = ||z||&lt;/math&gt; and &lt;math&gt;\angle (Uz,U\tilde{z}) = \angle (z,\tilde{z})&lt;/math&gt;. Therefore, ''any'' orthogonal matrices do either &lt;ref name=r1/&gt;
* A rotation about an axis
* A reflection about an axis

== References ==</text>
      <sha1>a6lf6v3c7742dxd62qsfftdvqa8kfyk</sha1>
    </revision>
  </page>
  <page>
    <title>Complementary Subspaces</title>
    <ns>0</ns>
    <id>23</id>
    <revision>
      <id>48</id>
      <timestamp>2018-07-07T16:43:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; be subspaces of &lt;math&gt;U&lt;/math&gt;, i.e., &lt;math&gt;V,W \subseteq U&lt;/math&gt;. &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; are called complementary subspaces..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="452">Let &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; be subspaces of &lt;math&gt;U&lt;/math&gt;, i.e., &lt;math&gt;V,W \subseteq U&lt;/math&gt;. &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; are called complementary subspaces if
# Any &lt;math&gt;u \in U&lt;/math&gt; can be expressed as &lt;math&gt;u=v+w&lt;/math&gt; for some &lt;math&gt;v \in 
V&lt;/math&gt; and &lt;math&gt;w \in W&lt;/math&gt;, and
# The intersection of &lt;math&gt;V \text{ and } W&lt;/math&gt; contains only 0 (which is the requirement to be a subspace), i.e. &lt;math&gt;V \cap W = \{0\}&lt;/math&gt;</text>
      <sha1>hs32kgorxi9igwdy9omyv1vlx3zzxc8</sha1>
    </revision>
    <revision>
      <id>50</id>
      <parentid>48</parentid>
      <timestamp>2018-07-07T16:56:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="949">Let &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; be subspaces of &lt;math&gt;U&lt;/math&gt;, i.e., &lt;math&gt;V,W \subseteq U&lt;/math&gt;. &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; are called complementary subspaces if
# Any &lt;math&gt;u \in U&lt;/math&gt; can be expressed as &lt;math&gt;u=v+w&lt;/math&gt; for some &lt;math&gt;v \in 
V&lt;/math&gt; and &lt;math&gt;w \in W&lt;/math&gt;, and
# The intersection of &lt;math&gt;V \text{ and } W&lt;/math&gt; contains only 0 (which is the requirement to be a subspace), i.e. &lt;math&gt;V \cap W = \{0\}&lt;/math&gt;

A quintessential example to complementary subspace comes from the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]]:

&lt;math&gt;\mathcal R(A)&lt;/math&gt; and &lt;math&gt;\mathcal N(A^T)&lt;/math&gt; are complementary subspaces for any matrix &lt;math&gt;A \in \mathbb{R}^{n\times k}&lt;/math&gt;, which can be written as:
* &lt;math&gt;\mathcal R(A) + \mathcal N (A^T) = \mathbb{R}^{n}&lt;/math&gt; and &lt;math&gt;\mathcal{R}(A)\perp \mathcal N (A^T)&lt;/math&gt;
* or &lt;math&gt;\mathcal R (A)^\perp = \mathcal N (A^T)&lt;/math&gt;</text>
      <sha1>t6tcchlawecobpj8nu5f06m2t9rmbnt</sha1>
    </revision>
    <revision>
      <id>51</id>
      <parentid>50</parentid>
      <timestamp>2018-07-07T17:04:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1185">Let &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; be subspaces of &lt;math&gt;U&lt;/math&gt;, i.e., &lt;math&gt;V,W \subseteq U&lt;/math&gt;. &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt; are called complementary subspaces if
# Any &lt;math&gt;u \in U&lt;/math&gt; can be expressed as &lt;math&gt;u=v+w&lt;/math&gt; for some &lt;math&gt;v \in 
V&lt;/math&gt; and &lt;math&gt;w \in W&lt;/math&gt;, and
# The intersection of &lt;math&gt;V \text{ and } W&lt;/math&gt; contains only 0 (which is the requirement to be a subspace), i.e. &lt;math&gt;V \cap W = \{0\}&lt;/math&gt;

A quintessential example to complementary subspace comes from the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]]:

&lt;math&gt;\mathcal R(A)&lt;/math&gt; and &lt;math&gt;\mathcal N(A^T)&lt;/math&gt; are complementary subspaces for any matrix &lt;math&gt;A \in \mathbb{R}^{n\times k}&lt;/math&gt;, which can be written as:
* &lt;math&gt;\mathcal R(A) + \mathcal N (A^T) = \mathbb{R}^{n}&lt;/math&gt; and &lt;math&gt;\mathcal{R}(A)\perp \mathcal N (A^T)&lt;/math&gt;
* or &lt;math&gt;\mathcal R (A)^\perp = \mathcal N (A^T)&lt;/math&gt;

Clearly, the same works for the &lt;math&gt;A^T&lt;/math&gt; as well:
* &lt;math&gt;\mathcal R(A^T) + \mathcal N (A) = \mathbb{R}^{n}&lt;/math&gt; and &lt;math&gt;\mathcal{R}(A^T)\perp \mathcal N (A)&lt;/math&gt;
* or &lt;math&gt;\mathcal R (A^T)^\perp = \mathcal N (A)&lt;/math&gt;</text>
      <sha1>mh4ku0wmwel13rwn4m23glx5y267dr3</sha1>
    </revision>
  </page>
  <page>
    <title>QR Factorization</title>
    <ns>0</ns>
    <id>24</id>
    <revision>
      <id>49</id>
      <timestamp>2018-07-07T16:52:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "==== General Gram-Schmidt procedure ==== The standard Gram-Schmidt procedure assumes that the input vectors &lt;math&gt;a_1,\dots,a_k \mathbb{R}^n&lt;/math&gt; are independent. If we remo..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2325">==== General Gram-Schmidt procedure ====
The standard Gram-Schmidt procedure assumes that the input vectors &lt;math&gt;a_1,\dots,a_k \mathbb{R}^n&lt;/math&gt; are independent. If we remove the requirement of independence, what we have is the ''general Gram-Schmidt procedure''. Clearly, the &lt;math&gt;R&lt;/math&gt; in the factorization &lt;math&gt;A=QR&lt;/math&gt; is not an upper triangular vector anymore, but it has the ''upper staircase'' form. Of course, we can always permute the columns via permutation matrices &lt;math&gt;S, P&lt;/math&gt; such that the &lt;math&gt;\tilde{R}&lt;/math&gt; in &lt;math&gt;A=Q[\tilde{R}\,S]P&lt;/math&gt; becomes an upper triangular matrix concatenated to the right with some other matrix. 

=== Full QR Factorization ===
Assume that the &lt;math&gt;QR&lt;/math&gt; factorization of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is &lt;math&gt;A=Q_1R_1&lt;/math&gt;. Note that the set columns of &lt;math&gt;Q_1&lt;/math&gt; have to be orthonormal, but &lt;math&gt;Q_1&lt;/math&gt; does not have to be an orthogonal matrix. Sometimes we want to "complete" it to an orthogonal matrix &lt;math&gt;Q = [Q_1 \,\, Q_2]&lt;/math&gt;; this leads to the  ''full QR factorization'':

&lt;math&gt;A = [Q_1 \,\, Q_2] \begin{bmatrix} R_1 \\ 0\end{bmatrix}&lt;/math&gt;.

This is pretty simple to achieve. Find any matrix &lt;math&gt;\tilde{A}&lt;/math&gt; such that &lt;math&gt;[A\,\,\tilde{A}]&lt;/math&gt; is full rank (e.g. &lt;math&gt;\tilde{A} = I&lt;/math&gt;) and apply general Gram-Schmidt to &lt;math&gt;[A\,\,\tilde{A}]&lt;/math&gt;. This operation would be called ''extending'' the original input set of vectors to an orthonormal basis.

&lt;math&gt;Q_1&lt;/math&gt; and &lt;math&gt;Q_2&lt;/math&gt; are very interesting: The ranges &lt;math&gt;\mathcal{R} (Q_1)&lt;/math&gt; and &lt;math&gt;\mathcal R (Q_2)&lt;/math&gt; are [[Complementary Subspaces|complementary subspaces]], since
* They are orthogonal, i.e. with an overloaded notation of &lt;math&gt;\perp&lt;/math&gt;, we can write &lt;math&gt;\mathcal R (Q_1) \perp \mathcal R (Q_2)&lt;/math&gt;
* Their sum is &lt;math&gt;\mathbb{R}^n&lt;/math&gt;, i.e. with the over-loaded notation of &lt;math&gt;+&lt;/math&gt;, &lt;math&gt;\mathcal R (Q_1) + \mathcal R(Q_2) = \mathbb{R}^n&lt;/math&gt;.

This can also be written as 

&lt;math&gt;\mathcal R (Q_2) = \mathcal R (Q_1)^\perp &lt;/math&gt; and &lt;math&gt;\mathcal R (Q_1) = \mathcal R(Q_2)^\perp&lt;/math&gt;.

Clearly, &lt;math&gt;\mathcal R(Q_1) = \mathcal R(A)&lt;/math&gt; and, less clearly, &lt;math&gt;\mathcal R(Q_2) = \mathcal N (A^T)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.

====References====</text>
      <sha1>hmacxc0dporeehjrrzetj6ju36g5h2y</sha1>
    </revision>
    <revision>
      <id>57</id>
      <parentid>49</parentid>
      <timestamp>2018-07-07T22:11:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2478">=== General Gram-Schmidt procedure ===
The standard Gram-Schmidt procedure assumes that the input vectors &lt;math&gt;a_1,\dots,a_k \mathbb{R}^n&lt;/math&gt; are independent. If we remove the requirement of independence, what we have is the ''general Gram-Schmidt procedure''. Clearly, the &lt;math&gt;R&lt;/math&gt; in the factorization &lt;math&gt;A=QR&lt;/math&gt; is not an upper triangular vector anymore, but it has the ''upper staircase'' form. Of course, we can always permute the columns via permutation matrices &lt;math&gt;S, P&lt;/math&gt; such that the &lt;math&gt;\tilde{R}&lt;/math&gt; in &lt;math&gt;A=Q[\tilde{R}\,S]P&lt;/math&gt; becomes an upper triangular matrix concatenated to the right with some other matrix. 

=== Full QR Factorization ===
Assume that the &lt;math&gt;QR&lt;/math&gt; factorization of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is &lt;math&gt;A=Q_1R_1&lt;/math&gt;. Note that the set columns of &lt;math&gt;Q_1&lt;/math&gt; have to be orthonormal, but &lt;math&gt;Q_1&lt;/math&gt; does not have to be an orthogonal matrix. Sometimes we want to "complete" it to an orthogonal matrix &lt;math&gt;Q = [Q_1 \,\, Q_2]&lt;/math&gt;; this leads to the  ''full QR factorization'':

&lt;math&gt;A = [Q_1 \,\, Q_2] \begin{bmatrix} R_1 \\ 0\end{bmatrix}&lt;/math&gt;.

This is pretty simple to achieve. Find any matrix &lt;math&gt;\tilde{A}&lt;/math&gt; such that &lt;math&gt;[A\,\,\tilde{A}]&lt;/math&gt; is full rank (e.g. &lt;math&gt;\tilde{A} = I&lt;/math&gt;) and apply general Gram-Schmidt to &lt;math&gt;[A\,\,\tilde{A}]&lt;/math&gt;. This operation would be called ''extending'' the original input set of vectors to an orthonormal basis.

&lt;math&gt;Q_1&lt;/math&gt; and &lt;math&gt;Q_2&lt;/math&gt; are very interesting: The ranges &lt;math&gt;\mathcal{R} (Q_1)&lt;/math&gt; and &lt;math&gt;\mathcal R (Q_2)&lt;/math&gt; are [[Complementary Subspaces|complementary subspaces]], since
* They are orthogonal, i.e. with an overloaded notation of &lt;math&gt;\perp&lt;/math&gt;, we can write &lt;math&gt;\mathcal R (Q_1) \perp \mathcal R (Q_2)&lt;/math&gt;
* Their sum is &lt;math&gt;\mathbb{R}^n&lt;/math&gt;, i.e. with the over-loaded notation of &lt;math&gt;+&lt;/math&gt;, &lt;math&gt;\mathcal R (Q_1) + \mathcal R(Q_2) = \mathbb{R}^n&lt;/math&gt;.

This can also be written as 

&lt;math&gt;\mathcal R (Q_2) = \mathcal R (Q_1)^\perp &lt;/math&gt; and &lt;math&gt;\mathcal R (Q_1) = \mathcal R(Q_2)^\perp&lt;/math&gt;.

Clearly, &lt;math&gt;\mathcal R(Q_1) = \mathcal R(A)&lt;/math&gt; and, less clearly, &lt;math&gt;\mathcal R(Q_2) = \mathcal N (A^T)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.

===Connection to Least-squares and projection===
The matrix &lt;math&gt;QQ^T&lt;/math&gt; is the projection matrix in the context of [[Least squares|least-squares]].

====References====</text>
      <sha1>s4g5f3q9w5yun4p2p8obeqwfhrpzfo1</sha1>
    </revision>
    <revision>
      <id>518</id>
      <parentid>57</parentid>
      <timestamp>2019-12-05T22:59:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>notation correction</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2482">=== General Gram-Schmidt procedure ===
The standard Gram-Schmidt procedure assumes that the input vectors &lt;math&gt;a_1,\dots,a_k \in \mathbb{R}^n&lt;/math&gt; are independent. If we remove the requirement of independence, what we have is the ''general Gram-Schmidt procedure''. Clearly, the &lt;math&gt;R&lt;/math&gt; in the factorization &lt;math&gt;A=QR&lt;/math&gt; is not an upper triangular vector anymore, but it has the ''upper staircase'' form. Of course, we can always permute the columns via permutation matrices &lt;math&gt;S, P&lt;/math&gt; such that the &lt;math&gt;\tilde{R}&lt;/math&gt; in &lt;math&gt;A=Q[\tilde{R}\,S]P&lt;/math&gt; becomes an upper triangular matrix concatenated to the right with some other matrix. 

=== Full QR Factorization ===
Assume that the &lt;math&gt;QR&lt;/math&gt; factorization of &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is &lt;math&gt;A=Q_1R_1&lt;/math&gt;. Note that the set columns of &lt;math&gt;Q_1&lt;/math&gt; have to be orthonormal, but &lt;math&gt;Q_1&lt;/math&gt; does not have to be an orthogonal matrix. Sometimes we want to "complete" it to an orthogonal matrix &lt;math&gt;Q = [Q_1 \,\, Q_2]&lt;/math&gt;; this leads to the  ''full QR factorization'':

&lt;math&gt;A = [Q_1 \,\, Q_2] \begin{bmatrix} R_1 \\ 0\end{bmatrix}&lt;/math&gt;.

This is pretty simple to achieve. Find any matrix &lt;math&gt;\tilde{A}&lt;/math&gt; such that &lt;math&gt;[A\,\,\tilde{A}]&lt;/math&gt; is full rank (e.g. &lt;math&gt;\tilde{A} = I&lt;/math&gt;) and apply general Gram-Schmidt to &lt;math&gt;[A\,\,\tilde{A}]&lt;/math&gt;. This operation would be called ''extending'' the original input set of vectors to an orthonormal basis.

&lt;math&gt;Q_1&lt;/math&gt; and &lt;math&gt;Q_2&lt;/math&gt; are very interesting: The ranges &lt;math&gt;\mathcal{R} (Q_1)&lt;/math&gt; and &lt;math&gt;\mathcal R (Q_2)&lt;/math&gt; are [[Complementary Subspaces|complementary subspaces]], since
* They are orthogonal, i.e. with an overloaded notation of &lt;math&gt;\perp&lt;/math&gt;, we can write &lt;math&gt;\mathcal R (Q_1) \perp \mathcal R (Q_2)&lt;/math&gt;
* Their sum is &lt;math&gt;\mathbb{R}^n&lt;/math&gt;, i.e. with the over-loaded notation of &lt;math&gt;+&lt;/math&gt;, &lt;math&gt;\mathcal R (Q_1) + \mathcal R(Q_2) = \mathbb{R}^n&lt;/math&gt;.

This can also be written as 

&lt;math&gt;\mathcal R (Q_2) = \mathcal R (Q_1)^\perp &lt;/math&gt; and &lt;math&gt;\mathcal R (Q_1) = \mathcal R(Q_2)^\perp&lt;/math&gt;.

Clearly, &lt;math&gt;\mathcal R(Q_1) = \mathcal R(A)&lt;/math&gt; and, less clearly, &lt;math&gt;\mathcal R(Q_2) = \mathcal N (A^T)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 4&lt;/ref&gt;.

===Connection to Least-squares and projection===
The matrix &lt;math&gt;QQ^T&lt;/math&gt; is the projection matrix in the context of [[Least squares|least-squares]].

====References====</text>
      <sha1>4pkagwldvt0kq1ut92jpi50822ncyfx</sha1>
    </revision>
  </page>
  <page>
    <title>Pseudo-inverse</title>
    <ns>0</ns>
    <id>25</id>
    <revision>
      <id>52</id>
      <timestamp>2018-07-07T21:50:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "When talking about pseudo-inverse of a matrix &lt;math&gt;A&lt;/math&gt;, we usually refer to a matrix denoted as &lt;math&gt;A^\dagger&lt;/math&gt; and is a specific generalized inverse of &lt;math&gt;A&lt;/..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="437">When talking about pseudo-inverse of a matrix &lt;math&gt;A&lt;/math&gt;, we usually refer to a matrix denoted as &lt;math&gt;A^\dagger&lt;/math&gt; and is a specific generalized inverse of &lt;math&gt;A&lt;/math&gt;. The form of this specific matrix depends on the type of matrix. 

If &lt;math&gt;A&lt;/math&gt; is '''skinny and full rank''', we generally refer to:
&lt;math&gt;A^\dagger = (A^T A)^{-1} A_T&lt;/math&gt; 

TBC

The pseudo-inverse is also referred to as the Penrose-Moore inverse.</text>
      <sha1>flvo95pcvro6x1j15gvv82hqf890kec</sha1>
    </revision>
    <revision>
      <id>53</id>
      <parentid>52</parentid>
      <timestamp>2018-07-07T21:53:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="498">When talking about pseudo-inverse of a matrix &lt;math&gt;A&lt;/math&gt;, we usually refer to a matrix denoted as &lt;math&gt;A^\dagger&lt;/math&gt; and is a specific generalized inverse of &lt;math&gt;A&lt;/math&gt;. The form of this specific matrix depends on the type of matrix. 

If &lt;math&gt;A&lt;/math&gt; is '''skinny and full rank''', we generally refer to:
&lt;math&gt;A^\dagger = (A^T A)^{-1} A^T&lt;/math&gt;.
In this case, &lt;math&gt;A^\dagger&lt;/math&gt; is a ''left inverse''.


TBC

The pseudo-inverse is also referred to as the Penrose-Moore inverse.</text>
      <sha1>p9vg8g30nciavmuab3cfesdxtskj30g</sha1>
    </revision>
    <revision>
      <id>63</id>
      <parentid>53</parentid>
      <timestamp>2018-07-10T17:36:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="688">When talking about pseudo-inverse of a matrix &lt;math&gt;A&lt;/math&gt;, we usually refer to a matrix denoted as &lt;math&gt;A^\dagger&lt;/math&gt; and is a specific generalized inverse of &lt;math&gt;A&lt;/math&gt;. The form of this specific matrix depends on the type of matrix. 

* If &lt;math&gt;A&lt;/math&gt; is '''skinny and full rank''', we generally refer to:
&lt;math&gt;A^\dagger = (A^T A)^{-1} A^T&lt;/math&gt;.
In this case, &lt;math&gt;A^\dagger&lt;/math&gt; is a ''left inverse''.

* If &lt;math&gt;A&lt;/math&gt; is '''fat and full rank''', then we refer to the pseudo-inverse:
&lt;math&gt;A^\dagger = A^T (AA^T)^{-1}&lt;/math&gt;.
In this case, &lt;math&gt;A^\dagger&lt;/math&gt; is a ''right inverse''.

TBC

The pseudo-inverse is also referred to as the Penrose-Moore inverse.</text>
      <sha1>rik24r3mg9trfiab3rks3ifuuarb1gs</sha1>
    </revision>
    <revision>
      <id>188</id>
      <parentid>63</parentid>
      <timestamp>2018-07-29T00:58:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="882">When talking about pseudo-inverse of a matrix &lt;math&gt;A&lt;/math&gt;, we usually refer to a matrix denoted as &lt;math&gt;A^\dagger&lt;/math&gt; and is a specific generalized inverse of &lt;math&gt;A&lt;/math&gt;. The form of this specific matrix depends on the type of matrix. 

* If &lt;math&gt;A&lt;/math&gt; is '''skinny and full rank''', we generally refer to:
&lt;math&gt;A^\dagger = (A^T A)^{-1} A^T&lt;/math&gt;.
In this case, &lt;math&gt;A^\dagger&lt;/math&gt; is a ''left inverse''.


* If &lt;math&gt;A&lt;/math&gt; is '''fat and full rank''', then we refer to the pseudo-inverse:
&lt;math&gt;A^\dagger = A^T (AA^T)^{-1}&lt;/math&gt;.
In this case, &lt;math&gt;A^\dagger&lt;/math&gt; is a ''right inverse''.


* If &lt;math&gt;A\neq 0 &lt;/math&gt; (i.e., the most general condition), the typical pseudo-inverse is:
&lt;math&gt;A^\dagger = V \Sigma^{-1} U^T&lt;/math&gt;
where &lt;math&gt;V, \Sigma, U&lt;/math&gt; are obtained from [[SVD]].

The pseudo-inverse is also referred to as the Penrose-Moore inverse.</text>
      <sha1>dvq23idh8l1edkmfz7kbkpkedr0ywpr</sha1>
    </revision>
    <revision>
      <id>387</id>
      <parentid>188</parentid>
      <timestamp>2019-06-16T18:41:54Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1035">When talking about pseudo-inverse of a matrix &lt;math&gt;A&lt;/math&gt;, we usually refer to a matrix denoted as &lt;math&gt;A^\dagger&lt;/math&gt; and is a specific generalized inverse of &lt;math&gt;A&lt;/math&gt;. The form of this specific matrix depends on the type of matrix. 

* If &lt;math&gt;A&lt;/math&gt; is '''skinny and full rank''', we generally refer to:
&lt;math&gt;A^\dagger = (A^T A)^{-1} A^T&lt;/math&gt;.
In this case, &lt;math&gt;A^\dagger&lt;/math&gt; is a ''left inverse''.


* If &lt;math&gt;A&lt;/math&gt; is '''fat and full rank''', then we refer to the pseudo-inverse:
&lt;math&gt;A^\dagger = A^T (AA^T)^{-1}&lt;/math&gt;.
In this case, &lt;math&gt;A^\dagger&lt;/math&gt; is a ''right inverse''.


* If &lt;math&gt;A\neq 0 &lt;/math&gt; (i.e., the most general condition), the typical pseudo-inverse is:
&lt;math&gt;A^\dagger = V \Sigma^{-1} U^T&lt;/math&gt;
where &lt;math&gt;V, \Sigma, U&lt;/math&gt; are obtained from [[SVD]].

Equivalently, the pseudo-inverse in this most general condition can be represented as:

&lt;math&gt;A^\dagger = A'(AA')^-A(A'A)^-A'.&lt;/math&gt;


The pseudo-inverse is also referred to as the Penrose-Moore or Moore-Penrose inverse.</text>
      <sha1>f2jfqo1yylzcy5ap5lk3jgbx82y5xqn</sha1>
    </revision>
  </page>
  <page>
    <title>Projection</title>
    <ns>0</ns>
    <id>26</id>
    <revision>
      <id>54</id>
      <timestamp>2018-07-07T21:56:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "If &lt;math&gt;A&lt;/math&gt; is skinny and full rank, then the projection operation is &lt;math&gt;\mathcal P_{\mathcal R(A)}(y) = A(A^TA)^{-1}A^T y&lt;/math&gt;  TBC"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="143">If &lt;math&gt;A&lt;/math&gt; is skinny and full rank, then the projection operation is &lt;math&gt;\mathcal P_{\mathcal R(A)}(y) = A(A^TA)^{-1}A^T y&lt;/math&gt;

TBC</text>
      <sha1>ai03lot02p0n0ew8ux449ic3oh3r4c6</sha1>
    </revision>
    <revision>
      <id>140</id>
      <parentid>54</parentid>
      <timestamp>2018-07-21T14:13:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="276">If &lt;math&gt;A&lt;/math&gt; is skinny and full rank, then the projection operation is &lt;math&gt;\mathcal P_{\mathcal R(A)}(y) = A(A^TA)^{-1}A^T y&lt;/math&gt;

===Properties of projection matrix &lt;math&gt;P&lt;/math&gt;===
* If &lt;math&gt;P&lt;/math&gt; is a projection matrix, then so is &lt;math&gt;I-P&lt;/math&gt;.
* TBC

TBC</text>
      <sha1>bxq76tr4h49eoag9p7qojvoy2jfm7o9</sha1>
    </revision>
    <revision>
      <id>507</id>
      <parentid>140</parentid>
      <timestamp>2019-11-14T20:44:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="387">If &lt;math&gt;A&lt;/math&gt; is skinny and full rank, then the projection operation is &lt;math&gt;\mathcal P_{\mathcal R(A)}(y) = A(A^TA)^{-1}A^T y&lt;/math&gt;

===Properties of projection matrix &lt;math&gt;P&lt;/math&gt;===
* If &lt;math&gt;P&lt;/math&gt; is a projection matrix, then so is &lt;math&gt;I-P&lt;/math&gt;.
* The norm of a projection matrix is either 1 or 0 [http://mathworld.wolfram.com/ProjectionMatrix.html either 1 or 0]
TBC</text>
      <sha1>p6gxrt8h19teupli83vr8mw4vlmzkr4</sha1>
    </revision>
    <revision>
      <id>508</id>
      <parentid>507</parentid>
      <timestamp>2019-11-14T20:47:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="835">If &lt;math&gt;A&lt;/math&gt; is skinny and full rank, then the projection operation is &lt;math&gt;\mathcal P_{\mathcal R(A)}(y) = A(A^TA)^{-1}A^T y&lt;/math&gt;

===Properties of projection matrix &lt;math&gt;P&lt;/math&gt;===
* If &lt;math&gt;P&lt;/math&gt; is a projection matrix, then so is &lt;math&gt;I-P&lt;/math&gt;.
* The norm of a projection matrix is either 1 or 0 [http://mathworld.wolfram.com/ProjectionMatrix.html either 1 or 0]
* Assume that &lt;math&gt;\mathbf{P}&lt;/math&gt; is a matrix that projects a given vector to the column space of a matrix &lt;math&gt;\mathbf{A}&lt;/math&gt;. The projected version &lt;math&gt;\mathbf{Pv}&lt;/math&gt; of a vector &lt;math&gt;v&lt;/math&gt; satisfies &lt;math&gt;||\mathbf{Pv-v}||=||\mathbf{v}||&lt;/math&gt; iff &lt;math&gt;\mathbf{v}\in \mathcal{N}(\mathbf A^T)&lt;/math&gt; (i.e., &lt;math&gt;v&lt;/math&gt; must be orthogonal to every column of &lt;math&gt;A^T&lt;/math&gt; -- easy to verify by drawing an example vector).
TBC</text>
      <sha1>8n9ro700p9cgmoks7008inxngpkovyz</sha1>
    </revision>
    <revision>
      <id>509</id>
      <parentid>508</parentid>
      <timestamp>2019-11-14T21:03:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Properties of projection matrix P */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="843">If &lt;math&gt;A&lt;/math&gt; is skinny and full rank, then the projection operation is &lt;math&gt;\mathcal P_{\mathcal R(A)}(y) = A(A^TA)^{-1}A^T y&lt;/math&gt;

===Properties of projection matrix &lt;math&gt;P&lt;/math&gt;===
* If &lt;math&gt;P&lt;/math&gt; is a projection matrix, then so is &lt;math&gt;I-P&lt;/math&gt;.
* The norm of a projection matrix is either 1 or 0 [http://mathworld.wolfram.com/ProjectionMatrix.html either 1 or 0]
* Assume that &lt;math&gt;\mathbf{P}&lt;/math&gt; is a matrix that projects a given vector to the column space of a matrix &lt;math&gt;\mathbf{A}&lt;/math&gt;. The projected version &lt;math&gt;\mathbf{Pv}&lt;/math&gt; of a vector &lt;math&gt;v&lt;/math&gt; satisfies &lt;math&gt;||\mathbf{Pv-v}||=||\mathbf{v}||&lt;/math&gt; iff &lt;math&gt;\mathbf{v}\in \mathcal{N}(\mathbf A^T)&lt;/math&gt; (i.e., &lt;math&gt;v&lt;/math&gt; must be orthogonal to every column of &lt;math&gt;\mathbf A^T&lt;/math&gt; -- easy to verify by drawing an example vector).
TBC</text>
      <sha1>6q45ah6p6rhl33scflf4h5kruo0edkq</sha1>
    </revision>
    <revision>
      <id>510</id>
      <parentid>509</parentid>
      <timestamp>2019-11-14T22:20:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="851">If &lt;math&gt;A&lt;/math&gt; is skinny and full rank, then the projection operation is &lt;math&gt;\mathcal P_{\mathcal R(A)}(y) = A(A^TA)^{-1}A^T y&lt;/math&gt;

===Properties of projection matrix &lt;math&gt;P&lt;/math&gt;===
* If &lt;math&gt;P&lt;/math&gt; is a projection matrix, then so is &lt;math&gt;I-P&lt;/math&gt;.
* The norm of a projection matrix is either 1 or 0 [http://mathworld.wolfram.com/ProjectionMatrix.html either 1 or 0]
* Assume that &lt;math&gt;\mathbf{P}&lt;/math&gt; is a matrix that projects a given vector to the column space of a matrix &lt;math&gt;\mathbf{A}&lt;/math&gt;. The projected version &lt;math&gt;\mathbf{Pv}&lt;/math&gt; of a vector &lt;math&gt;\mathbf v&lt;/math&gt; satisfies &lt;math&gt;||\mathbf{Pv-v}||=||\mathbf{v}||&lt;/math&gt; iff &lt;math&gt;\mathbf{v}\in \mathcal{N}(\mathbf A^T)&lt;/math&gt; (i.e., &lt;math&gt;v&lt;/math&gt; must be orthogonal to every column of &lt;math&gt;\mathbf A^T&lt;/math&gt; -- easy to verify by drawing an example vector).
TBC</text>
      <sha1>r5o3zrj3gno3g1ybog23zoht4vay9mj</sha1>
    </revision>
  </page>
  <page>
    <title>Least squares</title>
    <ns>0</ns>
    <id>27</id>
    <revision>
      <id>55</id>
      <timestamp>2018-07-07T21:58:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' s..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1614">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse.

=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.


== References ==</text>
      <sha1>d55g4knp24k8rj203lu2ob2mb0p3997</sha1>
    </revision>
    <revision>
      <id>56</id>
      <parentid>55</parentid>
      <timestamp>2018-07-07T22:11:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1991">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse.


=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.


=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.


== References ==</text>
      <sha1>n1w6ijrgzx9296cg0aiox1of4povv2o</sha1>
    </revision>
    <revision>
      <id>58</id>
      <parentid>56</parentid>
      <timestamp>2018-07-08T15:28:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3254">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.
=== BLUE property ===
The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' lef inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

== References ==</text>
      <sha1>3vtm8s8h2gpgsmca69w0eer6iul92df</sha1>
    </revision>
    <revision>
      <id>60</id>
      <parentid>58</parentid>
      <timestamp>2018-07-08T15:56:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4463">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.
=== BLUE property ===
The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' lef inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.

== References ==</text>
      <sha1>7qhglfkq2o1f92q6084v3bm4mij9p3g</sha1>
    </revision>
    <revision>
      <id>61</id>
      <parentid>60</parentid>
      <timestamp>2018-07-08T17:30:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5094">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.
=== BLUE property ===
The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' lef inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.


=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this in

&lt;math&gt;\left|\left|as\right\right|&lt;/math&gt;

The clue that we are actually close to a good trade-off point (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.


== References ==</text>
      <sha1>endqj0028jmyk55tb410bs89ggeywyx</sha1>
    </revision>
    <revision>
      <id>68</id>
      <parentid>61</parentid>
      <timestamp>2018-07-10T18:07:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5969">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.
=== BLUE property ===
The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' lef inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.


=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.


== References ==</text>
      <sha1>crfaf4b8lxy0axrmqh5i6o0glkuga8v</sha1>
    </revision>
    <revision>
      <id>69</id>
      <parentid>68</parentid>
      <timestamp>2018-07-10T18:08:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6082">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.
=== BLUE property ===
The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' lef inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.


=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].


== References ==</text>
      <sha1>8n9crdyty3zdmfj42lzd8wkd9lk6f51</sha1>
    </revision>
    <revision>
      <id>191</id>
      <parentid>69</parentid>
      <timestamp>2018-07-29T01:14:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7495">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_ls = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.

=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].
== Optimality of Least-Squares ==
=== BLUE property ===
Least-squares is the ''best linear unbiased estimator'' (BLUE). The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' left inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Smallest uncertainty ellipse ===
Least-squares is also optimal in that it gives the smallest ''uncertainty ellipsoid'' &lt;ref name=l16&gt;Stephen Boyd, EE263, Lecture notes 16&lt;/ref&gt;. This property applies when we have a ''norm-bound'' model of noise; that is, we assume that &lt;math&gt;||v||\le \alpha&lt;/math&gt; but otherwise know nothing about &lt;math&gt;v&lt;/math&gt;.

Consider any estimator &lt;math&gt;B&lt;/math&gt; of  &lt;math&gt;x&lt;/math&gt;, i.e., &lt;math&gt;\hat{x} = By&lt;/math&gt;, where &lt;math&gt;B&lt;/math&gt; is unbiased (i.e., &lt;math&gt;BA  = I&lt;/math&gt;). The estimation error is &lt;math&gt;\tilde x = \hat x - x = Bv&lt;/math&gt;. Due to norm-bound noise model, we have that

&lt;math&gt;\tilde x \in \mathcal{E}_{\text{unc}} = \{ Bv | ||v|| \le \alpha\}&lt;/math&gt;/

A good estimator should have a small ''uncertainty [[Ellipsoid|ellipsoid]]'' &lt;math&gt;\mathcal E_{\text{unc}}&lt;/math&gt;. It can be shown that&lt;ref name=l16/&gt; least-squares has the smallest possible uncertainty ellipsoid -- smallest in the sense of [[Matrix inequality|matrix comparison]]; that is, if &lt;math&gt;B_{\text{ls}}&lt;/math&gt; is the least-squares estimator and &lt;math&gt;\mathcal E_{\text{ls}}&lt;/math&gt; is its uncertainty ellipsoid, for any estimator &lt;math&gt;B&lt;/math&gt; with ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; we have that

&lt;math&gt;B \ge B_{\text{ls}}&lt;/math&gt; and (see [[Ellipsoid]])
&lt;math&gt;\mathcal E_{ls} \subseteq \mathcal E&lt;/math&gt;.

== References ==</text>
      <sha1>sby0v2dwq8vqxbvy4t9jcmlr4ybgdt0</sha1>
    </revision>
    <revision>
      <id>204</id>
      <parentid>191</parentid>
      <timestamp>2018-09-12T16:12:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7497">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_{ls} = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.

=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].
== Optimality of Least-Squares ==
=== BLUE property ===
Least-squares is the ''best linear unbiased estimator'' (BLUE). The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' left inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Smallest uncertainty ellipse ===
Least-squares is also optimal in that it gives the smallest ''uncertainty ellipsoid'' &lt;ref name=l16&gt;Stephen Boyd, EE263, Lecture notes 16&lt;/ref&gt;. This property applies when we have a ''norm-bound'' model of noise; that is, we assume that &lt;math&gt;||v||\le \alpha&lt;/math&gt; but otherwise know nothing about &lt;math&gt;v&lt;/math&gt;.

Consider any estimator &lt;math&gt;B&lt;/math&gt; of  &lt;math&gt;x&lt;/math&gt;, i.e., &lt;math&gt;\hat{x} = By&lt;/math&gt;, where &lt;math&gt;B&lt;/math&gt; is unbiased (i.e., &lt;math&gt;BA  = I&lt;/math&gt;). The estimation error is &lt;math&gt;\tilde x = \hat x - x = Bv&lt;/math&gt;. Due to norm-bound noise model, we have that

&lt;math&gt;\tilde x \in \mathcal{E}_{\text{unc}} = \{ Bv | ||v|| \le \alpha\}&lt;/math&gt;/

A good estimator should have a small ''uncertainty [[Ellipsoid|ellipsoid]]'' &lt;math&gt;\mathcal E_{\text{unc}}&lt;/math&gt;. It can be shown that&lt;ref name=l16/&gt; least-squares has the smallest possible uncertainty ellipsoid -- smallest in the sense of [[Matrix inequality|matrix comparison]]; that is, if &lt;math&gt;B_{\text{ls}}&lt;/math&gt; is the least-squares estimator and &lt;math&gt;\mathcal E_{\text{ls}}&lt;/math&gt; is its uncertainty ellipsoid, for any estimator &lt;math&gt;B&lt;/math&gt; with ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; we have that

&lt;math&gt;B \ge B_{\text{ls}}&lt;/math&gt; and (see [[Ellipsoid]])
&lt;math&gt;\mathcal E_{ls} \subseteq \mathcal E&lt;/math&gt;.

== References ==</text>
      <sha1>kffgukvwy2qxdvbpfkgn5dwthogi0fc</sha1>
    </revision>
    <revision>
      <id>214</id>
      <parentid>204</parentid>
      <timestamp>2018-09-12T20:40:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7681">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_{ls} = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_ls&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.

=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].
== Optimality of Least-Squares ==
=== BLUE property ===
Least-squares is the ''best linear unbiased estimator'' (BLUE). The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' left inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Smallest uncertainty ellipse ===
Least-squares is also optimal in that it gives the smallest ''uncertainty ellipsoid'' &lt;ref name=l16&gt;Stephen Boyd, EE263, Lecture notes 16&lt;/ref&gt;. This property applies when we have a ''norm-bound'' model of noise; that is, we assume that &lt;math&gt;||v||\le \alpha&lt;/math&gt; but otherwise know nothing about &lt;math&gt;v&lt;/math&gt;.

Consider any estimator &lt;math&gt;B&lt;/math&gt; of  &lt;math&gt;x&lt;/math&gt;, i.e., &lt;math&gt;\hat{x} = By&lt;/math&gt;, where &lt;math&gt;B&lt;/math&gt; is unbiased (i.e., &lt;math&gt;BA  = I&lt;/math&gt;). The estimation error is &lt;math&gt;\tilde x = \hat x - x = Bv&lt;/math&gt;. Due to norm-bound noise model, we have that

&lt;math&gt;\tilde x \in \mathcal{E}_{\text{unc}} = \{ Bv | ||v|| \le \alpha\}&lt;/math&gt;/

A good estimator should have a small ''uncertainty [[Ellipsoid|ellipsoid]]'' &lt;math&gt;\mathcal E_{\text{unc}}&lt;/math&gt;. It can be shown that&lt;ref name=l16/&gt; least-squares has the smallest possible uncertainty ellipsoid -- smallest in the sense of [[Matrix inequality|matrix comparison]]; that is, if &lt;math&gt;B_{\text{ls}}&lt;/math&gt; is the least-squares estimator and &lt;math&gt;\mathcal E_{\text{ls}}&lt;/math&gt; is its uncertainty ellipsoid, for any estimator &lt;math&gt;B&lt;/math&gt; with ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; we have that

&lt;math&gt;B \ge B_{\text{ls}}&lt;/math&gt; and (see [[Ellipsoid]])
&lt;math&gt;\mathcal E_{ls} \subseteq \mathcal E&lt;/math&gt;.

== What's the point of Least Squares? ==
Least squares has numerous applications such as system identification, interpolation, extrapolation, smoothing of data &lt;ref name=l6/&gt; ... TBC

== References ==</text>
      <sha1>mn6sl74zb0f2qtuauzalhrlttxxtebh</sha1>
    </revision>
    <revision>
      <id>246</id>
      <parentid>214</parentid>
      <timestamp>2018-10-10T17:58:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Geometric Interpretation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7683">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because A^T A is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_{ls} = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_{ls}&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.

=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].
== Optimality of Least-Squares ==
=== BLUE property ===
Least-squares is the ''best linear unbiased estimator'' (BLUE). The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' left inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Smallest uncertainty ellipse ===
Least-squares is also optimal in that it gives the smallest ''uncertainty ellipsoid'' &lt;ref name=l16&gt;Stephen Boyd, EE263, Lecture notes 16&lt;/ref&gt;. This property applies when we have a ''norm-bound'' model of noise; that is, we assume that &lt;math&gt;||v||\le \alpha&lt;/math&gt; but otherwise know nothing about &lt;math&gt;v&lt;/math&gt;.

Consider any estimator &lt;math&gt;B&lt;/math&gt; of  &lt;math&gt;x&lt;/math&gt;, i.e., &lt;math&gt;\hat{x} = By&lt;/math&gt;, where &lt;math&gt;B&lt;/math&gt; is unbiased (i.e., &lt;math&gt;BA  = I&lt;/math&gt;). The estimation error is &lt;math&gt;\tilde x = \hat x - x = Bv&lt;/math&gt;. Due to norm-bound noise model, we have that

&lt;math&gt;\tilde x \in \mathcal{E}_{\text{unc}} = \{ Bv | ||v|| \le \alpha\}&lt;/math&gt;/

A good estimator should have a small ''uncertainty [[Ellipsoid|ellipsoid]]'' &lt;math&gt;\mathcal E_{\text{unc}}&lt;/math&gt;. It can be shown that&lt;ref name=l16/&gt; least-squares has the smallest possible uncertainty ellipsoid -- smallest in the sense of [[Matrix inequality|matrix comparison]]; that is, if &lt;math&gt;B_{\text{ls}}&lt;/math&gt; is the least-squares estimator and &lt;math&gt;\mathcal E_{\text{ls}}&lt;/math&gt; is its uncertainty ellipsoid, for any estimator &lt;math&gt;B&lt;/math&gt; with ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; we have that

&lt;math&gt;B \ge B_{\text{ls}}&lt;/math&gt; and (see [[Ellipsoid]])
&lt;math&gt;\mathcal E_{ls} \subseteq \mathcal E&lt;/math&gt;.

== What's the point of Least Squares? ==
Least squares has numerous applications such as system identification, interpolation, extrapolation, smoothing of data &lt;ref name=l6/&gt; ... TBC

== References ==</text>
      <sha1>qj62b7fc3hg0tvx60p1402400h3p9yd</sha1>
    </revision>
    <revision>
      <id>316</id>
      <parentid>246</parentid>
      <timestamp>2018-10-29T19:12:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7696">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because &lt;math&gt;A^T A&lt;/math&gt; is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_{ls} = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_{ls}&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.

=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].
== Optimality of Least-Squares ==
=== BLUE property ===
Least-squares is the ''best linear unbiased estimator'' (BLUE). The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' left inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Smallest uncertainty ellipse ===
Least-squares is also optimal in that it gives the smallest ''uncertainty ellipsoid'' &lt;ref name=l16&gt;Stephen Boyd, EE263, Lecture notes 16&lt;/ref&gt;. This property applies when we have a ''norm-bound'' model of noise; that is, we assume that &lt;math&gt;||v||\le \alpha&lt;/math&gt; but otherwise know nothing about &lt;math&gt;v&lt;/math&gt;.

Consider any estimator &lt;math&gt;B&lt;/math&gt; of  &lt;math&gt;x&lt;/math&gt;, i.e., &lt;math&gt;\hat{x} = By&lt;/math&gt;, where &lt;math&gt;B&lt;/math&gt; is unbiased (i.e., &lt;math&gt;BA  = I&lt;/math&gt;). The estimation error is &lt;math&gt;\tilde x = \hat x - x = Bv&lt;/math&gt;. Due to norm-bound noise model, we have that

&lt;math&gt;\tilde x \in \mathcal{E}_{\text{unc}} = \{ Bv | ||v|| \le \alpha\}&lt;/math&gt;/

A good estimator should have a small ''uncertainty [[Ellipsoid|ellipsoid]]'' &lt;math&gt;\mathcal E_{\text{unc}}&lt;/math&gt;. It can be shown that&lt;ref name=l16/&gt; least-squares has the smallest possible uncertainty ellipsoid -- smallest in the sense of [[Matrix inequality|matrix comparison]]; that is, if &lt;math&gt;B_{\text{ls}}&lt;/math&gt; is the least-squares estimator and &lt;math&gt;\mathcal E_{\text{ls}}&lt;/math&gt; is its uncertainty ellipsoid, for any estimator &lt;math&gt;B&lt;/math&gt; with ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; we have that

&lt;math&gt;B \ge B_{\text{ls}}&lt;/math&gt; and (see [[Ellipsoid]])
&lt;math&gt;\mathcal E_{ls} \subseteq \mathcal E&lt;/math&gt;.

== What's the point of Least Squares? ==
Least squares has numerous applications such as system identification, interpolation, extrapolation, smoothing of data &lt;ref name=l6/&gt; ... TBC

== References ==</text>
      <sha1>r0ja5ail5tt7u29cyxfisv58svfeuh3</sha1>
    </revision>
    <revision>
      <id>338</id>
      <parentid>316</parentid>
      <timestamp>2018-11-19T22:00:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Multi-objective Least Squares */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7698">Consider &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; is (strictly) skinny (i.e., &lt;math&gt;m&gt;n&lt;/math&gt;); that is, &lt;math&gt;y=Ax&lt;/math&gt; is an ''overdetermined'' set of linear equations.

For most &lt;math&gt;y&lt;/math&gt;, we cannot solve for &lt;math&gt;x&lt;/math&gt;&lt;ref name=l6&gt;Stephen Boyd, EE263 Lecture Notes 6&lt;/ref&gt;. This makes sense: even if &lt;math&gt;A&lt;/math&gt; is full rank (i.e., &lt;math&gt;\text{rank}(A)=n&lt;/math&gt;)its rank will still be smaller compared to the dimensionality of &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;m&lt;/math&gt;.

The least squares approach is about ''approximately'' solving &lt;math&gt;y=Ax&lt;/math&gt;: We define the residual &lt;math&gt;r = y-Ax&lt;/math&gt; and find the least-squares (approximate) solution, &lt;math&gt;x=x_{\text{ls}}&lt;/math&gt;, that minimizes &lt;math&gt;||r||&lt;/math&gt;.

Taking the partial derivative of &lt;math&gt;||r||{}^2 = x^T A^T A x - 2y^T Ax + y^T y&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; and equating it to zero, we get:

&lt;math&gt;\nabla_x ||r||{}^2 = 2A^T Ax - 2A^T y = 0 \implies A^T A x = A^T y&lt;/math&gt;. The latter is called the ''normal equations'' and it has a unique solution (because &lt;math&gt;A^T A&lt;/math&gt; is square and full rank when &lt;math&gt;A&lt;/math&gt; is skinny and full rank), famous as the least square (approximate) solution:

&lt;math&gt;x_{ls} = (A^T A)^{-1} A^T y := A^\dagger x&lt;/math&gt;.

The matrix above, &lt;math&gt;A^\dagger&lt;/math&gt;, is called the [[pseudo-inverse]] of &lt;math&gt;A&lt;/math&gt; and it's a generalized inverse. It is also a left inverse.
=== Least-squares via QR factorization ===
One way to obtain the least-squares approximate solution is to use [[QR Factorization]], because:

&lt;math&gt;(A^T A)^{-1}A^T = (R^T Q^T Q QR)^{-1} R^T Q^T = R^{-1} Q^T&lt;/math&gt;, 

so &lt;math&gt;x_{\text{ls}} = R^{-1}Q^T y&lt;/math&gt;.

In this case, the [[projection]] matrix can be obtained as:

&lt;math&gt;A(A^TA)^{-1}A^T = AR^{-1}Q^T = QQ^T&lt;/math&gt;.
=== Geometric Interpretation ===

The vector &lt;math&gt;A x_{ls}&lt;/math&gt; is the ''[[projection]]'' of &lt;math&gt;y&lt;/math&gt; on &lt;math&gt;\mathcal R(A)&lt;/math&gt;. In this case, the residual &lt;math&gt;r=Ax-y&lt;/math&gt; is orthogonal to &lt;math&gt;\mathcal R(A)&lt;/math&gt;.

=== Recursive Least Squares ===

Another way to write the least square solution is

&lt;math&gt;x_{ls} = \left( \sum_{i=1}^m a_i a_i^T\right)^{-1} \sum_{i=1}^m y_i a_i&lt;/math&gt;

where &lt;math&gt;a_i&lt;/math&gt; are the columns of &lt;math&gt;A&lt;/math&gt;. This formulation is particularly useful for ''on-line'' applications of least squares, i.e., when measurements keep coming and we want do update our estimation incrementally. An algorithm to do this would be&lt;ref name=l6/&gt;:
# Initialize &lt;math&gt;P(0) = 0 \in \mathbb{R}^{n\times n}, q(0) = 0 \in \mathbb{R}^n&lt;/math&gt;
# For &lt;math&gt;m = 0,1,\dots&lt;/math&gt;: &lt;math&gt;P(m+1) = P(m) + a_{m+1} a_{m+1}^T,\,\,\,\, q(m+1) = q(m) + y_{m+1} a_{m+1}&lt;/math&gt;
# if &lt;math&gt;P(m)&lt;/math&gt; is invertible, we have &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;

As can be seen above, all we need to keep is the most recent versions of &lt;math&gt;P(m), q(m)&lt;/math&gt; and give an estimation whenever we are asked for one with one matrix inversion and matrix-vector calculation, &lt;math&gt;x_{ls}(m) = P(m)^{-1}q(m)&lt;/math&gt;. In fact, the matrix inversion can be done quite efficiently via the [[Inverse matrices|rank one update formula]] because it adheres to the form &lt;math&gt;P(m+1)^{-1} = \left(P(m) + a_{m+1}\right)^{-1}&lt;/math&gt;.

=== Multi-objective Least Squares ===
When we have two (generally) competing objectives &lt;math&gt;J_1, J_2&lt;/math&gt;, what we do is minimizing &lt;math&gt;J_1 + \mu J_2&lt;/math&gt; for various values of &lt;math&gt;\mu&lt;/math&gt;, which controls how much we care about each objective. Let &lt;math&gt;J_1, J_2&lt;/math&gt; be defined as:

&lt;math&gt;J_1 = ||Ax-y||&lt;/math&gt;

&lt;math&gt;J_2 = ||Fx-g||&lt;/math&gt;

We can write this as a minimization of a single objective &lt;math&gt;||Ax-y||{}^2 + \mu ||Fx-g||{}^2&lt;/math&gt;. This can actually be written as a single-objective minimization &lt;math&gt;||\tilde{A}x-\tilde{b}||&lt;/math&gt; where &lt;math&gt;\tilde{A} = \begin{bmatrix}A\\ \sqrt{\mu} F\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\tilde{b} =  \begin{bmatrix}y \\ \sqrt{\mu} g\end{bmatrix}&lt;/math&gt;. We can now solve this (assuming  &lt;math&gt;\tilde{A}&lt;/math&gt; is full rank) using the least squares solution listed above. However, we need to solve for various &lt;math&gt;\mu&lt;/math&gt; values (possibly a logarithmic range). Of course, we must define what a good value of &lt;math&gt;\mu &lt;/math&gt; would be. This will depend on how much we care about each of the two objectives &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt;. In some cases there may be a point that satisfies both with relatively little compromise from each. The clue that we are actually close to a good trade-off point is that both &lt;math&gt;J_1&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; vary little with &lt;math&gt;\mu&lt;/math&gt;. (16:30+ of Lecture 08&lt;ref&gt;Stephen Boyd, EE268 Lecture notes&lt;/ref&gt;). In other words, when our objectives becomes insensitive to &lt;math&gt;\mu&lt;/math&gt;.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].

== Optimality of Least-Squares ==
=== BLUE property ===
Least-squares is the ''best linear unbiased estimator'' (BLUE). The BLUE property is understood better when we assume that there is some noise in our measurement, i.e., &lt;math&gt;y=Ax+v&lt;/math&gt;. The least squares approx. solution is nothing but multiplying &lt;math&gt;y&lt;/math&gt; with a left inverse, as seen above. Least squares involves a ''specific'' left inverse; the matrix &lt;math&gt;A&lt;/math&gt;, however, may have more (in fact, infinite) left inverses. In this case, all of them would yield a solution. Indeed, assume that &lt;math&gt;B&lt;/math&gt; is a left inverse of &lt;math&gt;A&lt;/math&gt;; in the noiseless case, if &lt;math&gt;y=Ax&lt;/math&gt;, we have that &lt;math&gt;By = BAx = x&lt;/math&gt;.

In the case with noise (which is much more realistic), we have that &lt;math&gt; \hat{x} = By = BAx + Bv = x + Bv \implies x - \hat{x} = -Bv&lt;/math&gt;. Since we don't know the noise and as far as we are concerned it can be random, we cannot expect to fully recover &lt;math&gt;x&lt;/math&gt;. However, we can aim to have a left inverse &lt;math&gt;B&lt;/math&gt; that minimizes the effect of the noise, that is, &lt;math&gt;-Bv&lt;/math&gt;. In this case, we'd like &lt;math&gt;B&lt;/math&gt; to be 'small'. The pseudo-inverse &lt;math&gt;A^\dagger&lt;/math&gt; is the ''smallest'' left inverse of &lt;math&gt;A&lt;/math&gt;. This property is called makes least-squares the ''best linear unbiased estimator'' (BLUE).

=== Smallest uncertainty ellipse ===
Least-squares is also optimal in that it gives the smallest ''uncertainty ellipsoid'' &lt;ref name=l16&gt;Stephen Boyd, EE263, Lecture notes 16&lt;/ref&gt;. This property applies when we have a ''norm-bound'' model of noise; that is, we assume that &lt;math&gt;||v||\le \alpha&lt;/math&gt; but otherwise know nothing about &lt;math&gt;v&lt;/math&gt;.

Consider any estimator &lt;math&gt;B&lt;/math&gt; of  &lt;math&gt;x&lt;/math&gt;, i.e., &lt;math&gt;\hat{x} = By&lt;/math&gt;, where &lt;math&gt;B&lt;/math&gt; is unbiased (i.e., &lt;math&gt;BA  = I&lt;/math&gt;). The estimation error is &lt;math&gt;\tilde x = \hat x - x = Bv&lt;/math&gt;. Due to norm-bound noise model, we have that

&lt;math&gt;\tilde x \in \mathcal{E}_{\text{unc}} = \{ Bv | ||v|| \le \alpha\}&lt;/math&gt;/

A good estimator should have a small ''uncertainty [[Ellipsoid|ellipsoid]]'' &lt;math&gt;\mathcal E_{\text{unc}}&lt;/math&gt;. It can be shown that&lt;ref name=l16/&gt; least-squares has the smallest possible uncertainty ellipsoid -- smallest in the sense of [[Matrix inequality|matrix comparison]]; that is, if &lt;math&gt;B_{\text{ls}}&lt;/math&gt; is the least-squares estimator and &lt;math&gt;\mathcal E_{\text{ls}}&lt;/math&gt; is its uncertainty ellipsoid, for any estimator &lt;math&gt;B&lt;/math&gt; with ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; we have that

&lt;math&gt;B \ge B_{\text{ls}}&lt;/math&gt; and (see [[Ellipsoid]])
&lt;math&gt;\mathcal E_{ls} \subseteq \mathcal E&lt;/math&gt;.

== What's the point of Least Squares? ==
Least squares has numerous applications such as system identification, interpolation, extrapolation, smoothing of data &lt;ref name=l6/&gt; ... TBC

== References ==</text>
      <sha1>sxanbajuf662ygymoor8svf1rgysr9w</sha1>
    </revision>
  </page>
  <page>
    <title>Least-norm solution</title>
    <ns>0</ns>
    <id>28</id>
    <revision>
      <id>62</id>
      <timestamp>2018-07-10T17:34:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, th..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1266">When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, this can happen only if &lt;math&gt;A\in \mathbb{R}^{m\times n}&lt;/math&gt; is strictly fat. In other words, &lt;math&gt;x&lt;/math&gt; is underspecified -- many choices of &lt;math&gt;x&lt;/math&gt; lead to the same &lt;math&gt;y&lt;/math&gt;. If we assume that the &lt;math&gt;A&lt;/math&gt; has full rank, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], &lt;math&gt;\text{dim} \mathcal N (A) = n-m&lt;/math&gt;.

The least-norm solution  &lt;math&gt;x_{ln}&lt;/math&gt; is then&lt;ref name=ln8&gt;Stephen Boyd, EE263 Lecture Notes 8&lt;/ref&gt;:

&lt;math&gt;x_{ln} = A^T (AA^T)^{-1}y&lt;/math&gt;

This can be shown by first showing that &lt;math&gt;(x-x_{ln}) \perp x_{ln}&lt;/math&gt;, and using this to expand  &lt;math&gt;||x||{}^2&lt;/math&gt; as &lt;math&gt;||x||{}^2 = ||x_{ln} + x - x_{ln}||{}^2 = ||x_{ln}||{}^2 + ||x-x_{ln}||{}^2 \ge ||x_{ln}||{}^2&lt;/math&gt;

===Geomteric Interprtation===
The orthogonality condition leads to the fact that &lt;math&gt;x_{ln} \perp \mathcal N(A)&lt;/math&gt;. The ''projection interpretation'' of the least-norm solution is that &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on the solution set &lt;math&gt;{x | Ax= y}&lt;/math&gt;.


==References==</text>
      <sha1>1sh6noc6jo8hhbvuv2pk5tuwco5wj78</sha1>
    </revision>
    <revision>
      <id>64</id>
      <parentid>62</parentid>
      <timestamp>2018-07-10T17:48:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2146">When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, this can happen only if &lt;math&gt;A\in \mathbb{R}^{m\times n}&lt;/math&gt; is strictly fat. In other words, &lt;math&gt;x&lt;/math&gt; is underspecified -- many choices of &lt;math&gt;x&lt;/math&gt; lead to the same &lt;math&gt;y&lt;/math&gt;. If we assume that the &lt;math&gt;A&lt;/math&gt; has full rank, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], &lt;math&gt;\text{dim} \mathcal N (A) = n-m&lt;/math&gt;.

The least-norm solution  &lt;math&gt;x_{ln}&lt;/math&gt; is then&lt;ref name=ln8&gt;Stephen Boyd, EE263 Lecture Notes 8&lt;/ref&gt;:

&lt;math&gt;x_{ln} = A^T (AA^T)^{-1}y&lt;/math&gt;

The matrix &lt;math&gt;A^T (AA^T)^{-1}&lt;/math&gt; is the [[pseudo-inverse]] -- it is a right inverse. The matrix &lt;math&gt;I-A^T (AA^T)^{-1}A&lt;/math&gt; gives the projection onto &lt;math&gt;\mathcal N(A)&lt;/math&gt;

This can be shown by first showing that &lt;math&gt;(x-x_{ln}) \perp x_{ln}&lt;/math&gt;, and using this to expand  &lt;math&gt;||x||{}^2&lt;/math&gt; as &lt;math&gt;||x||{}^2 = ||x_{ln} + x - x_{ln}||{}^2 = ||x_{ln}||{}^2 + ||x-x_{ln}||{}^2 \ge ||x_{ln}||{}^2&lt;/math&gt;

===Geomteric Interprtation===
The orthogonality condition leads to the fact that &lt;math&gt;x_{ln} \perp \mathcal N(A)&lt;/math&gt;. The ''projection interpretation'' of the least-norm solution is that &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on the solution set &lt;math&gt;\{x : Ax= y\}&lt;/math&gt;.

===Relation to regularized least-squares===
There is a relationship between a multi-objective [[least squares]] and the least-norm solution. If we define first objective as &lt;math&gt;J_1 = ||Ax-y||{}^2&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; as &lt;math&gt;J_2 = ||x||{}^2&lt;/math&gt;, as &lt;math&gt;\mu\to 0&lt;/math&gt;, &lt;math&gt;x_{ls}\to x_{ln}&lt;/math&gt; because in the latter case we have that

&lt;math&gt;(A^TA + \mu I)^{-1} \to A^T (A A^T)^{-1}&lt;/math&gt;.

Note that &lt;math&gt;\mu&lt;/math&gt; cannot be zero in this approach because &lt;math&gt;A&lt;/math&gt; is assumed to be (strictly) fat and when &lt;math&gt;\mu =0&lt;/math&gt;, the matrix &lt;math&gt;(A^TA + \mu I)^{-1} &lt;/math&gt; is not invertible.

Both least-norm and least-squares are special cases of general norm

==References==</text>
      <sha1>ovwk2he46o5747y4n7u9sqw5y6f7he2</sha1>
    </revision>
    <revision>
      <id>67</id>
      <parentid>64</parentid>
      <timestamp>2018-07-10T17:58:43Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2190">When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, this can happen only if &lt;math&gt;A\in \mathbb{R}^{m\times n}&lt;/math&gt; is strictly fat. In other words, &lt;math&gt;x&lt;/math&gt; is underspecified -- many choices of &lt;math&gt;x&lt;/math&gt; lead to the same &lt;math&gt;y&lt;/math&gt;. If we assume that the &lt;math&gt;A&lt;/math&gt; has full rank, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], &lt;math&gt;\text{dim} \mathcal N (A) = n-m&lt;/math&gt;.

The least-norm solution  &lt;math&gt;x_{ln}&lt;/math&gt; is then&lt;ref name=ln8&gt;Stephen Boyd, EE263 Lecture Notes 8&lt;/ref&gt;:

&lt;math&gt;x_{ln} = A^T (AA^T)^{-1}y&lt;/math&gt;

The matrix &lt;math&gt;A^T (AA^T)^{-1}&lt;/math&gt; is the [[pseudo-inverse]] -- it is a right inverse. The matrix &lt;math&gt;I-A^T (AA^T)^{-1}A&lt;/math&gt; gives the projection onto &lt;math&gt;\mathcal N(A)&lt;/math&gt;

This can be shown by first showing that &lt;math&gt;(x-x_{ln}) \perp x_{ln}&lt;/math&gt;, and using this to expand  &lt;math&gt;||x||{}^2&lt;/math&gt; as &lt;math&gt;||x||{}^2 = ||x_{ln} + x - x_{ln}||{}^2 = ||x_{ln}||{}^2 + ||x-x_{ln}||{}^2 \ge ||x_{ln}||{}^2&lt;/math&gt;

===Geomteric Interprtation===
The orthogonality condition leads to the fact that &lt;math&gt;x_{ln} \perp \mathcal N(A)&lt;/math&gt;. The ''projection interpretation'' of the least-norm solution is that &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on the solution set &lt;math&gt;\{x : Ax= y\}&lt;/math&gt;.

===Relation to regularized least-squares===
There is a relationship between a multi-objective [[least squares]] and the least-norm solution. If we define first objective as &lt;math&gt;J_1 = ||Ax-y||{}^2&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; as &lt;math&gt;J_2 = ||x||{}^2&lt;/math&gt;, as &lt;math&gt;\mu\to 0&lt;/math&gt;, &lt;math&gt;x_{ls}\to x_{ln}&lt;/math&gt; because in the latter case we have that

&lt;math&gt;(A^TA + \mu I)^{-1} \to A^T (A A^T)^{-1}&lt;/math&gt;.

Note that &lt;math&gt;\mu&lt;/math&gt; cannot be zero in this approach because &lt;math&gt;A&lt;/math&gt; is assumed to be (strictly) fat and when &lt;math&gt;\mu =0&lt;/math&gt;, the matrix &lt;math&gt;(A^TA + \mu I)^{-1} &lt;/math&gt; is not invertible.

Both least-norm and least-squares are special cases of general [[Norm minimization with equality constraints]].

==References==</text>
      <sha1>5eb2acnjxadk4852un6lsrvki07gmv2</sha1>
    </revision>
    <revision>
      <id>70</id>
      <parentid>67</parentid>
      <timestamp>2018-07-10T18:18:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2190">When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, this can happen only if &lt;math&gt;A\in \mathbb{R}^{m\times n}&lt;/math&gt; is strictly fat. In other words, &lt;math&gt;x&lt;/math&gt; is underspecified -- many choices of &lt;math&gt;x&lt;/math&gt; lead to the same &lt;math&gt;y&lt;/math&gt;. If we assume that the &lt;math&gt;A&lt;/math&gt; has full rank, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], &lt;math&gt;\text{dim} \mathcal N (A) = n-m&lt;/math&gt;.

The least-norm solution  &lt;math&gt;x_{ln}&lt;/math&gt; is then&lt;ref name=ln8&gt;Stephen Boyd, EE263 Lecture Notes 8&lt;/ref&gt;:

&lt;math&gt;x_{ln} = A^T (AA^T)^{-1}y&lt;/math&gt;

The matrix &lt;math&gt;A^T (AA^T)^{-1}&lt;/math&gt; is the [[pseudo-inverse]] -- it is a right inverse. The matrix &lt;math&gt;I-A^T (AA^T)^{-1}A&lt;/math&gt; gives the projection onto &lt;math&gt;\mathcal N(A)&lt;/math&gt;

This can be shown by first showing that &lt;math&gt;(x-x_{ln}) \perp x_{ln}&lt;/math&gt;, and using this to expand  &lt;math&gt;||x||{}^2&lt;/math&gt; as &lt;math&gt;||x||{}^2 = ||x_{ln} + x - x_{ln}||{}^2 = ||x_{ln}||{}^2 + ||x-x_{ln}||{}^2 \ge ||x_{ln}||{}^2&lt;/math&gt;

===Geomteric Interprtation===
The orthogonality condition leads to the fact that &lt;math&gt;x_{ln} \perp \mathcal N(A)&lt;/math&gt;. The ''projection interpretation'' of the least-norm solution is that &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on the solution set &lt;math&gt;\{x : Ax= y\}&lt;/math&gt;.

===Relation to regularized least-squares===
There is a relationship between a multi-objective [[least squares]] and the least-norm solution. If we define first objective as &lt;math&gt;J_1 = ||Ax-y||{}^2&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; as &lt;math&gt;J_2 = ||x||{}^2&lt;/math&gt;, as &lt;math&gt;\mu\to 0&lt;/math&gt;, &lt;math&gt;x_{ls}\to x_{ln}&lt;/math&gt; because in the latter case we have that

&lt;math&gt;(A^TA + \mu I)^{-1} \to A^T (A A^T)^{-1}&lt;/math&gt;.

Note that &lt;math&gt;\mu&lt;/math&gt; cannot be zero in this approach because &lt;math&gt;A&lt;/math&gt; is assumed to be (strictly) fat and when &lt;math&gt;\mu =0&lt;/math&gt;, the matrix &lt;math&gt;(A^TA + \mu I)^{-1} &lt;/math&gt; is not invertible.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].

==References==</text>
      <sha1>ckpo9ueri6lydmpasbs8rrgdbnwpous</sha1>
    </revision>
    <revision>
      <id>223</id>
      <parentid>70</parentid>
      <timestamp>2018-09-25T19:58:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2191">When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, this can happen only if &lt;math&gt;A\in \mathbb{R}^{m\times n}&lt;/math&gt; is strictly fat. In other words, &lt;math&gt;x&lt;/math&gt; is underspecified -- many choices of &lt;math&gt;x&lt;/math&gt; lead to the same &lt;math&gt;y&lt;/math&gt;. If we assume that the &lt;math&gt;A&lt;/math&gt; has full rank, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], &lt;math&gt;\text{dim} \mathcal N (A) = n-m&lt;/math&gt;.

The least-norm solution  &lt;math&gt;x_{ln}&lt;/math&gt; is then&lt;ref name=ln8&gt;Stephen Boyd, EE263 Lecture Notes 8&lt;/ref&gt;:

&lt;math&gt;x_{ln} = A^T (AA^T)^{-1}y&lt;/math&gt;

The matrix &lt;math&gt;A^T (AA^T)^{-1}&lt;/math&gt; is the [[pseudo-inverse]] -- it is a right inverse. The matrix &lt;math&gt;I-A^T (AA^T)^{-1}A&lt;/math&gt; gives the projection onto &lt;math&gt;\mathcal N(A)&lt;/math&gt;

This can be shown by first showing that &lt;math&gt;(x-x_{ln}) \perp x_{ln}&lt;/math&gt;, and using this to expand  &lt;math&gt;||x||{}^2&lt;/math&gt; as &lt;math&gt;||x||{}^2 = ||x_{ln} + x - x_{ln}||{}^2 = ||x_{ln}||{}^2 + ||x-x_{ln}||{}^2 \ge ||x_{ln}||{}^2&lt;/math&gt;.

===Geomteric Interprtation===
The orthogonality condition leads to the fact that &lt;math&gt;x_{ln} \perp \mathcal N(A)&lt;/math&gt;. The ''projection interpretation'' of the least-norm solution is that &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on the solution set &lt;math&gt;\{x : Ax= y\}&lt;/math&gt;.

===Relation to regularized least-squares===
There is a relationship between a multi-objective [[least squares]] and the least-norm solution. If we define first objective as &lt;math&gt;J_1 = ||Ax-y||{}^2&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; as &lt;math&gt;J_2 = ||x||{}^2&lt;/math&gt;, as &lt;math&gt;\mu\to 0&lt;/math&gt;, &lt;math&gt;x_{ls}\to x_{ln}&lt;/math&gt; because in the latter case we have that

&lt;math&gt;(A^TA + \mu I)^{-1} \to A^T (A A^T)^{-1}&lt;/math&gt;.

Note that &lt;math&gt;\mu&lt;/math&gt; cannot be zero in this approach because &lt;math&gt;A&lt;/math&gt; is assumed to be (strictly) fat and when &lt;math&gt;\mu =0&lt;/math&gt;, the matrix &lt;math&gt;(A^TA + \mu I)^{-1} &lt;/math&gt; is not invertible.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].

==References==</text>
      <sha1>gb75tu1q5enfah9w0mo1z8v7o6tminy</sha1>
    </revision>
    <revision>
      <id>226</id>
      <parentid>223</parentid>
      <timestamp>2018-10-06T18:49:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2192">When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, this can happen only if &lt;math&gt;A\in \mathbb{R}^{m\times n}&lt;/math&gt; is strictly fat. In other words, &lt;math&gt;x&lt;/math&gt; is underspecified -- many choices of &lt;math&gt;x&lt;/math&gt; lead to the same &lt;math&gt;y&lt;/math&gt;. If we assume that the &lt;math&gt;A&lt;/math&gt; has full rank, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], &lt;math&gt;\text{dim} \mathcal N (A) = n-m&lt;/math&gt;.

The least-norm solution  &lt;math&gt;x_{ln}&lt;/math&gt; is then&lt;ref name=ln8&gt;Stephen Boyd, EE263 Lecture Notes 8&lt;/ref&gt;:

&lt;math&gt;x_{ln} = A^T (AA^T)^{-1}y&lt;/math&gt;

The matrix &lt;math&gt;A^T (AA^T)^{-1}&lt;/math&gt; is the [[pseudo-inverse]] -- it is a right inverse. The matrix &lt;math&gt;I-A^T (AA^T)^{-1}A&lt;/math&gt; gives the projection onto &lt;math&gt;\mathcal N(A)&lt;/math&gt;.

This can be shown by first showing that &lt;math&gt;(x-x_{ln}) \perp x_{ln}&lt;/math&gt;, and using this to expand  &lt;math&gt;||x||{}^2&lt;/math&gt; as &lt;math&gt;||x||{}^2 = ||x_{ln} + x - x_{ln}||{}^2 = ||x_{ln}||{}^2 + ||x-x_{ln}||{}^2 \ge ||x_{ln}||{}^2&lt;/math&gt;.

===Geomteric Interprtation===
The orthogonality condition leads to the fact that &lt;math&gt;x_{ln} \perp \mathcal N(A)&lt;/math&gt;. The ''projection interpretation'' of the least-norm solution is that &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on the solution set &lt;math&gt;\{x : Ax= y\}&lt;/math&gt;.

===Relation to regularized least-squares===
There is a relationship between a multi-objective [[least squares]] and the least-norm solution. If we define first objective as &lt;math&gt;J_1 = ||Ax-y||{}^2&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; as &lt;math&gt;J_2 = ||x||{}^2&lt;/math&gt;, as &lt;math&gt;\mu\to 0&lt;/math&gt;, &lt;math&gt;x_{ls}\to x_{ln}&lt;/math&gt; because in the latter case we have that

&lt;math&gt;(A^TA + \mu I)^{-1} \to A^T (A A^T)^{-1}&lt;/math&gt;.

Note that &lt;math&gt;\mu&lt;/math&gt; cannot be zero in this approach because &lt;math&gt;A&lt;/math&gt; is assumed to be (strictly) fat and when &lt;math&gt;\mu =0&lt;/math&gt;, the matrix &lt;math&gt;(A^TA + \mu I)^{-1} &lt;/math&gt; is not invertible.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].

==References==</text>
      <sha1>d4f1mpm1a1mha9f1kpmoppq55pjtgfo</sha1>
    </revision>
    <revision>
      <id>364</id>
      <parentid>226</parentid>
      <timestamp>2019-05-12T22:20:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Geometric Interprtation */Typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2192">When a linear system &lt;math&gt;y=Ax&lt;/math&gt; has more than one solution, we may be interested in getting the solution that has the least L2 norm, &lt;math&gt;||\cdot||&lt;/math&gt;. Clearly, this can happen only if &lt;math&gt;A\in \mathbb{R}^{m\times n}&lt;/math&gt; is strictly fat. In other words, &lt;math&gt;x&lt;/math&gt; is underspecified -- many choices of &lt;math&gt;x&lt;/math&gt; lead to the same &lt;math&gt;y&lt;/math&gt;. If we assume that the &lt;math&gt;A&lt;/math&gt; has full rank, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], &lt;math&gt;\text{dim} \mathcal N (A) = n-m&lt;/math&gt;.

The least-norm solution  &lt;math&gt;x_{ln}&lt;/math&gt; is then&lt;ref name=ln8&gt;Stephen Boyd, EE263 Lecture Notes 8&lt;/ref&gt;:

&lt;math&gt;x_{ln} = A^T (AA^T)^{-1}y&lt;/math&gt;

The matrix &lt;math&gt;A^T (AA^T)^{-1}&lt;/math&gt; is the [[pseudo-inverse]] -- it is a right inverse. The matrix &lt;math&gt;I-A^T (AA^T)^{-1}A&lt;/math&gt; gives the projection onto &lt;math&gt;\mathcal N(A)&lt;/math&gt;.

This can be shown by first showing that &lt;math&gt;(x-x_{ln}) \perp x_{ln}&lt;/math&gt;, and using this to expand  &lt;math&gt;||x||{}^2&lt;/math&gt; as &lt;math&gt;||x||{}^2 = ||x_{ln} + x - x_{ln}||{}^2 = ||x_{ln}||{}^2 + ||x-x_{ln}||{}^2 \ge ||x_{ln}||{}^2&lt;/math&gt;.

===Geometric Interprtation===
The orthogonality condition leads to the fact that &lt;math&gt;x_{ln} \perp \mathcal N(A)&lt;/math&gt;. The ''projection interpretation'' of the least-norm solution is that &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on the solution set &lt;math&gt;\{x : Ax= y\}&lt;/math&gt;.

===Relation to regularized least-squares===
There is a relationship between a multi-objective [[least squares]] and the least-norm solution. If we define first objective as &lt;math&gt;J_1 = ||Ax-y||{}^2&lt;/math&gt; and &lt;math&gt;J_2&lt;/math&gt; as &lt;math&gt;J_2 = ||x||{}^2&lt;/math&gt;, as &lt;math&gt;\mu\to 0&lt;/math&gt;, &lt;math&gt;x_{ls}\to x_{ln}&lt;/math&gt; because in the latter case we have that

&lt;math&gt;(A^TA + \mu I)^{-1} \to A^T (A A^T)^{-1}&lt;/math&gt;.

Note that &lt;math&gt;\mu&lt;/math&gt; cannot be zero in this approach because &lt;math&gt;A&lt;/math&gt; is assumed to be (strictly) fat and when &lt;math&gt;\mu =0&lt;/math&gt;, the matrix &lt;math&gt;(A^TA + \mu I)^{-1} &lt;/math&gt; is not invertible.

Both least-norm and least-squares are special cases of general [[norm minimization with equality constraints]].

==References==</text>
      <sha1>g5qp30ghs1vk0iz22af4d2vowowq2g5</sha1>
    </revision>
  </page>
  <page>
    <title>Norm minimization with equality constraints</title>
    <ns>0</ns>
    <id>29</id>
    <revision>
      <id>65</id>
      <timestamp>2018-07-10T17:51:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The general problem of norm minimization with equality constraints can be formulated as:  minimize &lt;math&gt;||Ax-b||&lt;/math&gt;  subject to &lt;math&gt;Cx=d&lt;/math&gt;"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="150">The general problem of norm minimization with equality constraints can be formulated as:

minimize &lt;math&gt;||Ax-b||&lt;/math&gt;

subject to &lt;math&gt;Cx=d&lt;/math&gt;</text>
      <sha1>3yn49h9f6sso3kf4pefwum9tq2hmgci</sha1>
    </revision>
    <revision>
      <id>66</id>
      <parentid>65</parentid>
      <timestamp>2018-07-10T17:58:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="974">The general problem of norm minimization with equality constraints can be formulated as:

&lt;math&gt;\,\,\,\,\,\,\,&lt;/math&gt;minimize &lt;math&gt;||Ax-b||&lt;/math&gt;

&lt;math&gt;\,\,\,\,\,\,\,&lt;/math&gt;subject to &lt;math&gt;Cx=d&lt;/math&gt;

If we set &lt;math&gt;C&lt;/math&gt; as a null matrix and &lt;math&gt;d&lt;/math&gt; as a null vector, then we recover the least-squares problem. If we set &lt;math&gt;A=I&lt;/math&gt; and &lt;math&gt;b=0&lt;/math&gt;, we recover the least-norm problem. 

This problem can be solved via the Lagrangian equation: 

&lt;math&gt;L(x,\lambda) = (1/2) ||Ax-b||{}^2 + \lambda^T (Cx-d)&lt;/math&gt;.
The optimality conditions of &lt;math&gt;L(x,y)&lt;/math&gt; are having zero partial derivatives w.r.t. &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt;. After expanding the norm in the &lt;math&gt;L(x,y)&lt;/math&gt;, we can write the solutions of &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; in the block-matrix form as:

&lt;math&gt;\begin{bmatrix}x\\ \lambda\end{bmatrix} = \begin{bmatrix} A^T A &amp; C^T \\ C &amp; 0 \end{bmatrix}^{-1} \begin{bmatrix}A^T b \\ d\end{bmatrix}&lt;/math&gt;</text>
      <sha1>jfli9zowdt4c6xt7utvm4rm6ou28e6a</sha1>
    </revision>
  </page>
  <page>
    <title>Exponential</title>
    <ns>0</ns>
    <id>30</id>
    <revision>
      <id>71</id>
      <timestamp>2018-07-15T20:02:43Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The expansion of the exponential of a number &lt;math&gt;a&lt;/math&gt;, &lt;math&gt;e^a&lt;/math&gt;, is the infinite series:   &lt;math&gt;e^a = 1+a+\frac{a^2}{2!}+\dots&lt;/math&gt;  TBC"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="153">The expansion of the exponential of a number &lt;math&gt;a&lt;/math&gt;, &lt;math&gt;e^a&lt;/math&gt;, is the infinite series: 

&lt;math&gt;e^a = 1+a+\frac{a^2}{2!}+\dots&lt;/math&gt;

TBC</text>
      <sha1>md92lgfygfrrnk00rphmvhmoai68eo8</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix exponential</title>
    <ns>0</ns>
    <id>31</id>
    <revision>
      <id>72</id>
      <timestamp>2018-07-15T20:38:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The exponential of a ''square'' matrix &lt;math&gt;A&lt;/math&gt; is the matrix denoted with &lt;math&gt;e^{A}&lt;/math&gt; and is by defintion:  &lt;math&gt;e^A = I + A + \frac{A^2}{2!}+ \dots&lt;/math&gt;  The..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2507">The exponential of a ''square'' matrix &lt;math&gt;A&lt;/math&gt; is the matrix denoted with &lt;math&gt;e^{A}&lt;/math&gt; and is by defintion:

&lt;math&gt;e^A = I + A + \frac{A^2}{2!}+ \dots&lt;/math&gt;

The matrix exponential is defined to mirror certain properties of the regular exponential (i.e., exponential of a number), &lt;math&gt;e^a&lt;/math&gt;. The major similarity is that the expansions of &lt;math&gt;e^A&lt;/math&gt; and &lt;math&gt;e^a&lt;/math&gt; (see [[Exponential|exponential]]).

===Properties of the Matrix Exponential===
Some, but not all, properties of the matrix exponential are similar to the scalar exponential.

* If &lt;math&gt;AB = BA&lt;/math&gt;, then &lt;math&gt;e^{A+B}=e^A e^B&lt;/math&gt;

* &lt;math&gt;(e^{tA})^{-1} = e^{-tA}&lt;/math&gt;


===Solution to Linear Dynamical Systems===
Another aspect of the generalization is being the solution of a linear dynamical system (or, of a first-order differential equation). Just like &lt;math&gt;x(t) = e^{at}x(0)&lt;/math&gt; is the solution of &lt;math&gt;\dot{x} = a x&lt;/math&gt;, where &lt;math&gt;a&lt;/math&gt; is some scalar, so is &lt;math&gt;x(t) = e^{tA} x(0)&lt;/math&gt; the solution of the dynamical system &lt;math&gt;\dot{x} = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is a square matrix&lt;ref name=l10&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;.

In fact, &lt;math&gt;e^{tA}&lt;/math&gt; is the ''state transition matrix'' (denoted also as &lt;math&gt;\Phi(t)&lt;/math&gt;), as it propagates the initial state to the state at a given time &lt;math&gt;t&lt;/math&gt;:

&lt;math&gt;x(t) = \Phi(t) x(0) = e^{tA} x(0)&lt;/math&gt;

In fact, it is easy to show that the transfer need not take place from the initial condition, as it can take place from a certain state &lt;math&gt;x(\tau)&lt;/math&gt;, that is:

&lt;math&gt;x(t+\tau) = e^{tA} x(\tau)&lt;/math&gt;

====Solution to discretized LDS====
The exponential matrix gives the solutions also for uniformly sampled (or discretized) versions of continuous [[Linear dynamical systems|LDS]]s sampled with time steps of &lt;math&gt;h&lt;/math&gt; (i.e., &lt;math&gt;t_{k+1}-t_k = h&lt;/math&gt;): 

&lt;math&gt;z(k+1)=e^{hA} z(k)&lt;/math&gt;

==== Relation to System Stability====
The matrix exponential is also used to test the stability of an LDS. Specifically, let &lt;math&gt;\dot{x} = Ax&lt;/math&gt; be an (autonomous) LDS. This LDS is stable if &lt;math&gt;e^{tA}\to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;&lt;ref name=l10/&gt;. The specific meaning of stability here is that &lt;math&gt;x \to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;. It can be shown that this condition to stability is satisfied iff all the eigenvalue of &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;
have negative real part &lt;ref name=l10&gt;:

&lt;math&gt;\Re \lambda_i &lt;0 &lt;/math&gt;, &lt;math&gt;i=1,\dots,n&lt;/math&gt;</text>
      <sha1>1tvdulwy8njh60ad7r1fmrrpr4r2pq9</sha1>
    </revision>
    <revision>
      <id>85</id>
      <parentid>72</parentid>
      <timestamp>2018-07-16T21:10:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2522">The exponential of a ''square'' matrix &lt;math&gt;A&lt;/math&gt; is the matrix denoted with &lt;math&gt;e^{A}&lt;/math&gt; and is by defintion:

&lt;math&gt;e^A = I + A + \frac{A^2}{2!}+ \dots&lt;/math&gt;

The matrix exponential is defined to mirror certain properties of the regular exponential (i.e., exponential of a number), &lt;math&gt;e^a&lt;/math&gt;. The major similarity is that the expansions of &lt;math&gt;e^A&lt;/math&gt; and &lt;math&gt;e^a&lt;/math&gt; (see [[Exponential|exponential]]).

===Properties of the Matrix Exponential===
Some, but not all, properties of the matrix exponential are similar to the scalar exponential.

* If &lt;math&gt;AB = BA&lt;/math&gt;, then &lt;math&gt;e^{A+B}=e^A e^B&lt;/math&gt;

* &lt;math&gt;(e^{tA})^{-1} = e^{-tA}&lt;/math&gt;

===Solution to Linear Dynamical Systems===
Another aspect of the generalization is being the solution of a linear dynamical system (or, of a first-order differential equation). Just like &lt;math&gt;x(t) = e^{at}x(0)&lt;/math&gt; is the solution of &lt;math&gt;\dot{x} = a x&lt;/math&gt;, where &lt;math&gt;a&lt;/math&gt; is some scalar, so is &lt;math&gt;x(t) = e^{tA} x(0)&lt;/math&gt; the solution of the dynamical system &lt;math&gt;\dot{x} = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is a square matrix&lt;ref name=l10&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;.

In fact, &lt;math&gt;e^{tA}&lt;/math&gt; is the ''state transition matrix'' (denoted also as &lt;math&gt;\Phi(t)&lt;/math&gt;), as it propagates the initial state to the state at a given time &lt;math&gt;t&lt;/math&gt;:

&lt;math&gt;x(t) = \Phi(t) x(0) = e^{tA} x(0)&lt;/math&gt;

In fact, it is easy to show that the transfer need not take place from the initial condition, as it can take place from a certain state &lt;math&gt;x(\tau)&lt;/math&gt;, that is:

&lt;math&gt;x(t+\tau) = e^{tA} x(\tau)&lt;/math&gt;

====Solution to discretized LDS====
The exponential matrix gives the solutions also for uniformly sampled (or discretized) versions of continuous [[Linear dynamical systems|LDS]]s sampled with time steps of &lt;math&gt;h&lt;/math&gt; (i.e., &lt;math&gt;t_{k+1}-t_k = h&lt;/math&gt;): 

&lt;math&gt;z(k+1)=e^{hA} z(k)&lt;/math&gt;

==== Relation to System Stability====
The matrix exponential is also used to test the stability of an LDS. Specifically, let &lt;math&gt;\dot{x} = Ax&lt;/math&gt; be an (autonomous) LDS. This LDS is stable if &lt;math&gt;e^{tA}\to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;&lt;ref name=l10/&gt;. The specific meaning of stability here is that &lt;math&gt;x \to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;. It can be shown that this condition to stability is satisfied iff all the eigenvalue of &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;
have negative real part &lt;ref name=l10&gt;:

&lt;math&gt;\Re \lambda_i &lt;0 &lt;/math&gt;, &lt;math&gt;i=1,\dots,n&lt;/math&gt;

==References==</text>
      <sha1>2smv2p8zx6f4badqf2snqk7pvuorw5j</sha1>
    </revision>
    <revision>
      <id>97</id>
      <parentid>85</parentid>
      <timestamp>2018-07-17T23:50:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2757">The exponential of a ''square'' matrix &lt;math&gt;A&lt;/math&gt; is the matrix denoted with &lt;math&gt;e^{A}&lt;/math&gt; and is by defintion:

&lt;math&gt;e^A = I + A + \frac{A^2}{2!}+ \dots&lt;/math&gt;

The matrix exponential is defined to mirror certain properties of the regular exponential (i.e., exponential of a number), &lt;math&gt;e^a&lt;/math&gt;. The major similarity is that the expansions of &lt;math&gt;e^A&lt;/math&gt; and &lt;math&gt;e^a&lt;/math&gt; (see [[Exponential|exponential]]).

===Properties of the Matrix Exponential===
Some, but not all, properties of the matrix exponential are similar to the scalar exponential&lt;ref&gt;B.C. Kuo, Automatic Control Systems, 7th edition&lt;/ref&gt;.

* If &lt;math&gt;AB = BA&lt;/math&gt;, then &lt;math&gt;e^{A+B}=e^A e^B&lt;/math&gt;
* &lt;math&gt;(e^{tA})^{-1} = e^{-tA}&lt;/math&gt;
* &lt;math&gt;e^{0A} = I&lt;/math&gt;
* &lt;math&gt;[e^{tA}]^k = e^{ktA}&lt;/math&gt; for &lt;math&gt;k&lt;/math&gt; positive integer
* &lt;math&gt;e^{(t_2-t_1)A}e^{(t_1-t_0)A}&lt;/math&gt; for any &lt;math&gt;t_0, t_1, t_2&lt;/math&gt;

===Solution to Linear Dynamical Systems===
Another aspect of the generalization is being the solution of a linear dynamical system (or, of a first-order differential equation). Just like &lt;math&gt;x(t) = e^{at}x(0)&lt;/math&gt; is the solution of &lt;math&gt;\dot{x} = a x&lt;/math&gt;, where &lt;math&gt;a&lt;/math&gt; is some scalar, so is &lt;math&gt;x(t) = e^{tA} x(0)&lt;/math&gt; the solution of the dynamical system &lt;math&gt;\dot{x} = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is a square matrix&lt;ref name=l10&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;.

In fact, &lt;math&gt;e^{tA}&lt;/math&gt; is the ''state transition matrix'' (denoted also as &lt;math&gt;\Phi(t)&lt;/math&gt;), as it propagates the initial state to the state at a given time &lt;math&gt;t&lt;/math&gt;:

&lt;math&gt;x(t) = \Phi(t) x(0) = e^{tA} x(0)&lt;/math&gt;

In fact, it is easy to show that the transfer need not take place from the initial condition, as it can take place from a certain state &lt;math&gt;x(\tau)&lt;/math&gt;, that is:

&lt;math&gt;x(t+\tau) = e^{tA} x(\tau)&lt;/math&gt;

====Solution to discretized LDS====
The exponential matrix gives the solutions also for uniformly sampled (or discretized) versions of continuous [[Linear dynamical systems|LDS]]s sampled with time steps of &lt;math&gt;h&lt;/math&gt; (i.e., &lt;math&gt;t_{k+1}-t_k = h&lt;/math&gt;): 

&lt;math&gt;z(k+1)=e^{hA} z(k)&lt;/math&gt;

==== Relation to System Stability====
The matrix exponential is also used to test the stability of an LDS. Specifically, let &lt;math&gt;\dot{x} = Ax&lt;/math&gt; be an (autonomous) LDS. This LDS is stable if &lt;math&gt;e^{tA}\to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;&lt;ref name=l10/&gt;. The specific meaning of stability here is that &lt;math&gt;x \to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;. It can be shown that this condition to stability is satisfied iff all the eigenvalue of &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;
have negative real part &lt;ref name=l10&gt;:

&lt;math&gt;\Re \lambda_i &lt;0 &lt;/math&gt;, &lt;math&gt;i=1,\dots,n&lt;/math&gt;

==References==</text>
      <sha1>2xzs26am6z9wyo4zid433f2tvl4yumw</sha1>
    </revision>
    <revision>
      <id>485</id>
      <parentid>97</parentid>
      <timestamp>2019-08-04T14:27:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Relation to System Stability */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2758">The exponential of a ''square'' matrix &lt;math&gt;A&lt;/math&gt; is the matrix denoted with &lt;math&gt;e^{A}&lt;/math&gt; and is by defintion:

&lt;math&gt;e^A = I + A + \frac{A^2}{2!}+ \dots&lt;/math&gt;

The matrix exponential is defined to mirror certain properties of the regular exponential (i.e., exponential of a number), &lt;math&gt;e^a&lt;/math&gt;. The major similarity is that the expansions of &lt;math&gt;e^A&lt;/math&gt; and &lt;math&gt;e^a&lt;/math&gt; (see [[Exponential|exponential]]).

===Properties of the Matrix Exponential===
Some, but not all, properties of the matrix exponential are similar to the scalar exponential&lt;ref&gt;B.C. Kuo, Automatic Control Systems, 7th edition&lt;/ref&gt;.

* If &lt;math&gt;AB = BA&lt;/math&gt;, then &lt;math&gt;e^{A+B}=e^A e^B&lt;/math&gt;
* &lt;math&gt;(e^{tA})^{-1} = e^{-tA}&lt;/math&gt;
* &lt;math&gt;e^{0A} = I&lt;/math&gt;
* &lt;math&gt;[e^{tA}]^k = e^{ktA}&lt;/math&gt; for &lt;math&gt;k&lt;/math&gt; positive integer
* &lt;math&gt;e^{(t_2-t_1)A}e^{(t_1-t_0)A}&lt;/math&gt; for any &lt;math&gt;t_0, t_1, t_2&lt;/math&gt;

===Solution to Linear Dynamical Systems===
Another aspect of the generalization is being the solution of a linear dynamical system (or, of a first-order differential equation). Just like &lt;math&gt;x(t) = e^{at}x(0)&lt;/math&gt; is the solution of &lt;math&gt;\dot{x} = a x&lt;/math&gt;, where &lt;math&gt;a&lt;/math&gt; is some scalar, so is &lt;math&gt;x(t) = e^{tA} x(0)&lt;/math&gt; the solution of the dynamical system &lt;math&gt;\dot{x} = Ax&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; is a square matrix&lt;ref name=l10&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;.

In fact, &lt;math&gt;e^{tA}&lt;/math&gt; is the ''state transition matrix'' (denoted also as &lt;math&gt;\Phi(t)&lt;/math&gt;), as it propagates the initial state to the state at a given time &lt;math&gt;t&lt;/math&gt;:

&lt;math&gt;x(t) = \Phi(t) x(0) = e^{tA} x(0)&lt;/math&gt;

In fact, it is easy to show that the transfer need not take place from the initial condition, as it can take place from a certain state &lt;math&gt;x(\tau)&lt;/math&gt;, that is:

&lt;math&gt;x(t+\tau) = e^{tA} x(\tau)&lt;/math&gt;

====Solution to discretized LDS====
The exponential matrix gives the solutions also for uniformly sampled (or discretized) versions of continuous [[Linear dynamical systems|LDS]]s sampled with time steps of &lt;math&gt;h&lt;/math&gt; (i.e., &lt;math&gt;t_{k+1}-t_k = h&lt;/math&gt;): 

&lt;math&gt;z(k+1)=e^{hA} z(k)&lt;/math&gt;

==== Relation to System Stability====
The matrix exponential is also used to test the stability of an LDS. Specifically, let &lt;math&gt;\dot{x} = Ax&lt;/math&gt; be an (autonomous) LDS. This LDS is stable if &lt;math&gt;e^{tA}\to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;&lt;ref name=l10/&gt;. The specific meaning of stability here is that &lt;math&gt;x \to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;. It can be shown that this condition to stability is satisfied iff all the eigenvalue of &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;
have negative real part &lt;ref name=l10/&gt;:

&lt;math&gt;\Re \lambda_i &lt;0 &lt;/math&gt;, &lt;math&gt;i=1,\dots,n&lt;/math&gt;

==References==</text>
      <sha1>18nl4ekfj2rqknwi625mw7nthsl1aei</sha1>
    </revision>
  </page>
  <page>
    <title>Laplace Transform</title>
    <ns>0</ns>
    <id>32</id>
    <revision>
      <id>75</id>
      <timestamp>2018-07-15T21:37:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The Laplace transform of a ''matrix-valued'' function &lt;math&gt;z:\mathbb{R}_{+} \to \mathbb{R}^{p\times q}&lt;/math&gt; (e.g., a function that takes as input time) is denoted with &lt;mat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1906">The Laplace transform of a ''matrix-valued'' function &lt;math&gt;z:\mathbb{R}_{+} \to \mathbb{R}^{p\times q}&lt;/math&gt; (e.g., a function that takes as input time) is denoted with &lt;math&gt;Z(s)&lt;/math&gt; and defined as:

&lt;math&gt;Z(s) = \int\limits_{0}^{\infty} e^{-st}z(t) dt&lt;/math&gt;. The integral of this matrix is done term-by-term (example below).

What makes the Laplace transform so useful in the context of LDSs is it's derivative property, that is:

&lt;math&gt;L(\dot{z}) = sZ(s) - z(0)&lt;/math&gt;.

Using this property, it can be shown that the LDS &lt;math&gt;\dot{x} = A x(t)&lt;/math&gt; can be represented in the Laplace domain as:

&lt;math&gt;X(s) = (sI-A)^{-1}x(0)&lt;/math&gt;.

We can now solve the LDS via the inverse laplace transform as: 
&lt;math&gt;x(t)=\mathcal{L}^{-1}((sI-A)^{-1})x(0)&lt;/math&gt;

The matrix &lt;math&gt;(sI-A)^{-1}&lt;/math&gt; is called the ''resolvent matrix''. The matrix &lt;math&gt;\Phi(t):=\mathcal{L}^{-1}((sI-A)^{-1})&lt;/math&gt; is called the ''state-transition matrix'' as it maps the initial state to the state at time &lt;math&gt;t&lt;/math&gt;:

&lt;math&gt;x(t) = \Phi(t) x(0)&lt;/math&gt;.

Using the matrix inversion via series expansion (see &lt;math&gt;(I-C)^{-1}&lt;/math&gt; in [[Inverse matrices|inverse matrices]]) and the linearity of the Laplace transform, it can be shown that the matrix &lt;math&gt;\Phi(t)&lt;/math&gt; can be computed as:

&lt;math&gt;\Phi(t) = \mathcal{L}^{-1}((sI-A)^{-1}) = I+tA+\frac{(tA)^{2}}{2!}&lt;/math&gt;.

In fact, the RHS above corresponds to the [[Matrix exponential|matrix exponential]] of &lt;math&gt;tA&lt;/math&gt;.

====Example of Laplace transform====
Let &lt;math&gt;A&lt;/math&gt; be &lt;math&gt;A = \begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0  \end{bmatrix}&lt;/math&gt;.

Then, &lt;math&gt;sI-A = \begin{bmatrix} s &amp; -1 \\ 0 &amp; s \end{bmatrix}&lt;/math&gt; so the resolvent is:

&lt;math&gt;(sI-A)^{-1} = \begin{bmatrix}1/s &amp; 1/s^2 \\ 0 &amp; 1/s\end{bmatrix}&lt;/math&gt; and the state transition matrix is:

&lt;math&gt;\Phi(t) = \mathcal{L}^{-1}\left (sI-A)^{-1} \right) = \begin{bmatrix}1 &amp; t \\ 0  &amp; 1\end{bmatrix}&lt;/math&gt;</text>
      <sha1>m76338ceycua3s4kogsx063dy9yt6c6</sha1>
    </revision>
    <revision>
      <id>87</id>
      <parentid>75</parentid>
      <timestamp>2018-07-16T21:20:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2167">The Laplace transform of a ''matrix-valued'' function &lt;math&gt;z:\mathbb{R}_{+} \to \mathbb{R}^{p\times q}&lt;/math&gt; (e.g., a function that takes as input time) is denoted with &lt;math&gt;Z(s)&lt;/math&gt; and defined as:

&lt;math&gt;Z(s) = \int\limits_{0}^{\infty} e^{-st}z(t) dt&lt;/math&gt;. The integral of this matrix is done term-by-term (example below).

What makes the Laplace transform so useful in the context of LDSs is it's derivative property, that is:

&lt;math&gt;L(\dot{z}) = sZ(s) - z(0)&lt;/math&gt;.

Using this property, it can be shown that the LDS &lt;math&gt;\dot{x} = A x(t)&lt;/math&gt; can be represented in the Laplace domain as:

&lt;math&gt;X(s) = (sI-A)^{-1}x(0)&lt;/math&gt;.

We can now solve the LDS via the inverse laplace transform as: 
&lt;math&gt;x(t)=\mathcal{L}^{-1}((sI-A)^{-1})x(0)&lt;/math&gt;

The matrix &lt;math&gt;(sI-A)^{-1}&lt;/math&gt; is called the ''resolvent matrix''. The matrix &lt;math&gt;\Phi(t):=\mathcal{L}^{-1}((sI-A)^{-1})&lt;/math&gt; is called the ''state-transition matrix'' as it maps the initial state to the state at time &lt;math&gt;t&lt;/math&gt;:

&lt;math&gt;x(t) = \Phi(t) x(0)&lt;/math&gt;.

Using the matrix inversion via series expansion (see &lt;math&gt;(I-C)^{-1}&lt;/math&gt; in [[Inverse matrices|inverse matrices]]) and the linearity of the Laplace transform, it can be shown that the matrix &lt;math&gt;\Phi(t)&lt;/math&gt; can be computed as:

&lt;math&gt;\Phi(t) = \mathcal{L}^{-1}((sI-A)^{-1}) = I+tA+\frac{(tA)^{2}}{2!}&lt;/math&gt;.

In fact, the RHS above corresponds to the [[Matrix exponential|matrix exponential]] of &lt;math&gt;tA&lt;/math&gt;.

====Example of Laplace transform====
Let &lt;math&gt;A&lt;/math&gt; be &lt;math&gt;A = \begin{bmatrix}0 &amp; 1 \\ 0 &amp; 0  \end{bmatrix}&lt;/math&gt;.

Then, &lt;math&gt;sI-A = \begin{bmatrix} s &amp; -1 \\ 0 &amp; s \end{bmatrix}&lt;/math&gt; so the resolvent is:

&lt;math&gt;(sI-A)^{-1} = \begin{bmatrix}1/s &amp; 1/s^2 \\ 0 &amp; 1/s\end{bmatrix}&lt;/math&gt; and the state transition matrix is:

&lt;math&gt;\Phi(t) = \mathcal{L}^{-1}\left (sI-A)^{-1} \right) = \begin{bmatrix}1 &amp; t \\ 0  &amp; 1\end{bmatrix}&lt;/math&gt;

==What's the point of the Laplace transform?==
The main use of the Laplace transform in the context of LDSs is that it facilitates the expression of the state-transform matrix. Operations that involve differentiation become much simpler in the Laplace domain.</text>
      <sha1>dwwel4cy9ztmdu1yykbvxv15qszsvgi</sha1>
    </revision>
  </page>
  <page>
    <title>Characteristic Polynomial</title>
    <ns>0</ns>
    <id>33</id>
    <revision>
      <id>76</id>
      <timestamp>2018-07-15T22:29:51Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "&lt;math&gt;\mathcal{X}(s) = \det(sI-A)&lt;/math&gt; is called the ''characteristic polynomial'' of &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="571">&lt;math&gt;\mathcal{X}(s) = \det(sI-A)&lt;/math&gt; is called the ''characteristic polynomial'' of &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;. 
* &lt;math&gt;\mathcal X(s)&lt;/math&gt; is a polynomial of degree &lt;math&gt;n&lt;/math&gt; with leading coefficient 1. 
* The roots of &lt;math&gt;\mathcal X(s)&lt;/math&gt; are the '''eigenvalues''' of &lt;math&gt;A&lt;/math&gt;.
* &lt;math&gt;\mathcal X&lt;/math&gt; has real coefficients, so eigenvalues are either real or occur in conjugate pairs.
* There are &lt;math&gt;n&lt;/math&gt; eigenvalues but not necessarily &lt;math&gt;n&lt;/math&gt; ''unique'' eigenvalues.</text>
      <sha1>hmr6k5k6wstl6uy3v4lxlqvui9nf1sd</sha1>
    </revision>
    <revision>
      <id>133</id>
      <parentid>76</parentid>
      <timestamp>2018-07-20T16:18:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="589">&lt;math&gt;\mathcal{X}(s) = \det(sI-A)&lt;/math&gt; is called the ''characteristic polynomial'' of &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 10&lt;/ref&gt;. 
* &lt;math&gt;\mathcal X(s)&lt;/math&gt; is a polynomial of degree &lt;math&gt;n&lt;/math&gt; with leading coefficient 1. 
* The roots of &lt;math&gt;\mathcal X(s)&lt;/math&gt; are the '''eigenvalues''' of &lt;math&gt;A&lt;/math&gt;.
* &lt;math&gt;\mathcal X&lt;/math&gt; has real coefficients, so eigenvalues are either real or occur in conjugate pairs.
* There are &lt;math&gt;n&lt;/math&gt; eigenvalues but not necessarily &lt;math&gt;n&lt;/math&gt; ''unique'' eigenvalues.

== References ==</text>
      <sha1>afbaxc1ezydd0ej2x97x897g8lprbxi</sha1>
    </revision>
  </page>
  <page>
    <title>Eigenvectors and Eigenvalues</title>
    <ns>0</ns>
    <id>34</id>
    <revision>
      <id>78</id>
      <timestamp>2018-07-16T19:57:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;  &lt;math&gt;\mat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1905">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

== References ==</text>
      <sha1>rw7v0ck57qhbbieho21pbpb9y9kbi85</sha1>
    </revision>
    <revision>
      <id>86</id>
      <parentid>78</parentid>
      <timestamp>2018-07-16T21:16:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2386">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...

* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).

TBC

== References ==</text>
      <sha1>fsfy8m79tejhvrp0hpshfxadju1o17n</sha1>
    </revision>
    <revision>
      <id>90</id>
      <parentid>86</parentid>
      <timestamp>2018-07-16T21:30:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of Eigenvalues or Eigenvectors? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2618">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...

* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is an eigenvector, then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 11&lt;/ref&gt;

TBC

== References ==</text>
      <sha1>b2xsyicntjijqepzwtpk75aaceyk2mj</sha1>
    </revision>
    <revision>
      <id>91</id>
      <parentid>90</parentid>
      <timestamp>2018-07-16T21:32:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2596">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...

* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is an eigenvector, then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.

TBC

== References ==</text>
      <sha1>1ibcr1s9vwwfsk5bvqfd5olclyj3gp3</sha1>
    </revision>
    <revision>
      <id>92</id>
      <parentid>91</parentid>
      <timestamp>2018-07-16T23:32:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2937">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

== Eigenvectors in an LDS ==
Eigenvectors have are useful entities for understanding and analyzing LDSs.


==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...

* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)
TBC

== References ==</text>
      <sha1>lb0dj7rytdd5uywt19iprk1nj10bese</sha1>
    </revision>
    <revision>
      <id>95</id>
      <parentid>92</parentid>
      <timestamp>2018-07-17T14:57:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Properties of Eigenvectors and Eigenvalues */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2938">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

== Eigenvectors in an LDS ==
Eigenvectors have are useful entities for understanding and analyzing LDSs.


==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...

* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)
TBC

== References ==</text>
      <sha1>q7uvr0bkrx7mi70vq9kbszqdqoo6zw4</sha1>
    </revision>
    <revision>
      <id>115</id>
      <parentid>95</parentid>
      <timestamp>2018-07-18T13:43:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of Eigenvalues or Eigenvectors? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3286">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

== Eigenvectors in an LDS ==
Eigenvectors have are useful entities for understanding and analyzing LDSs.


==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).
TBC

== References ==</text>
      <sha1>9sl4rbe9wvvdp7v5qy5dp70zt5xt4x7</sha1>
    </revision>
    <revision>
      <id>119</id>
      <parentid>115</parentid>
      <timestamp>2018-07-20T13:12:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* In the context of LDSs */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3442">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.

== Eigenvectors in an LDS ==
Eigenvectors have are useful entities for understanding and analyzing LDSs.


==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).
TBC

== References ==</text>
      <sha1>6s4ak5165oy1zgeimm1ixmjp90uvl5q</sha1>
    </revision>
    <revision>
      <id>120</id>
      <parentid>119</parentid>
      <timestamp>2018-07-20T13:16:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3398">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real. 

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).
TBC

== References ==</text>
      <sha1>tii0ilzi4zju4bj7nj0vfq58ymf0pl7</sha1>
    </revision>
    <revision>
      <id>123</id>
      <parentid>120</parentid>
      <timestamp>2018-07-20T13:34:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of Eigenvalues or Eigenvectors? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3629">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real. 

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]])
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>1ginxdicy6mc4f1wyk8b2wjiijb687w</sha1>
    </revision>
    <revision>
      <id>124</id>
      <parentid>123</parentid>
      <timestamp>2018-07-20T13:35:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Matrix norm */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3782">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real. 

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>8ra790pv0iytrkljwikycc4bh4jki9l</sha1>
    </revision>
    <revision>
      <id>142</id>
      <parentid>124</parentid>
      <timestamp>2018-07-22T22:58:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3895">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[[[Characteristic Polynomial|characteristic polynomial]]]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvecotr of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real. 

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>gj7650qy5waun9qnmntlazcqeov7chl</sha1>
    </revision>
    <revision>
      <id>143</id>
      <parentid>142</parentid>
      <timestamp>2018-07-22T23:06:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Properties of Eigenvectors and Eigenvalues */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3894">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[[[Characteristic Polynomial|characteristic polynomial]]]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>4kj05d0e3r1z6rrholiiizmtrw8fm7e</sha1>
    </revision>
    <revision>
      <id>144</id>
      <parentid>143</parentid>
      <timestamp>2018-07-22T23:32:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3890">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>1miuwv42ggs7v178xwjmdtnmgcr2z1t</sha1>
    </revision>
    <revision>
      <id>156</id>
      <parentid>144</parentid>
      <timestamp>2018-07-27T13:28:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Properties of Eigenvectors and Eigenvalues */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4222">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;A^T&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; \---- it's the number &lt;math&gt;v^T u&lt;/math&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>rzy3r0frorhgq3zr45wgg036ah9j53a</sha1>
    </revision>
    <revision>
      <id>157</id>
      <parentid>156</parentid>
      <timestamp>2018-07-27T13:30:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Properties of Eigenvectors and Eigenvalues */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4371">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;A^T&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; -- it's the number &lt;math&gt;v^T u&lt;/math&gt;. This should also be the property [TBC] that is used to compute the [[PCA|PCA]] of matrices that are very large in one dimension [I need to check this].

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>bu0wks03l4izrz5e6r1g216afs2xgoh</sha1>
    </revision>
    <revision>
      <id>158</id>
      <parentid>157</parentid>
      <timestamp>2018-07-27T13:57:50Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Properties of Eigenvectors and Eigenvalues */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4538">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A B&lt;/math&gt; and &lt;math&gt;B A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; -- it's the number &lt;math&gt;v^T u&lt;/math&gt;. This should also be the property [TBC] that is used to compute the [[PCA|PCA]] of matrices that are very large in one dimension [I need to check this]. However, the analogous property does not apply for the ''eigenvectors'' of &lt;math&gt;AB&lt;/math&gt; and &lt;math&gt;BA&lt;/math&gt; &lt;ref&gt;Stephen Boyd, Homework 7 of EE263, Question 12.1 &lt;/ref&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>dc9tx7ca0k85et1y2w951pbduxj6x16</sha1>
    </revision>
    <revision>
      <id>340</id>
      <parentid>158</parentid>
      <timestamp>2018-12-15T17:05:48Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4539">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.)
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A B&lt;/math&gt; and &lt;math&gt;B A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; -- it's the number &lt;math&gt;v^T u&lt;/math&gt;. This should also be the property [TBC] that is used to compute the [[PCA|PCA]] of matrices that are very large in one dimension [I need to check this]. However, the analogous property does not apply for the ''eigenvectors'' of &lt;math&gt;AB&lt;/math&gt; and &lt;math&gt;BA&lt;/math&gt; &lt;ref&gt;Stephen Boyd, Homework 7 of EE263, Question 12.1 &lt;/ref&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>ns58f08ljvi43y7kowzr3tj3zlrna1j</sha1>
    </revision>
    <revision>
      <id>377</id>
      <parentid>340</parentid>
      <timestamp>2019-06-16T18:19:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5097">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.
Any such &lt;math&gt;w&lt;/math&gt;

===Geometric interpretation===
The eigenvectors of a matrix have a noteworthy geometrical property. Let &lt;math&gt;\lambda&lt;/math&gt; represwent any nonzero eigenvalue of &lt;math&gt;A&lt;/math&gt;, let &lt;math&gt;v&lt;/math&gt; represent any eigenvector that corresponds to &lt;math&gt;\lambda&lt;/math&gt;, and take the usual inner product. Then, the direction of the vector &lt;math&gt;Av&lt;/math&gt; is either the same as &lt;math&gt;v&lt;/math&gt; or its opposite, depending on whether &lt;math&gt;\lambda&lt;/math&gt; is positive or negative. The length of &lt;math&gt;Av&lt;/math&gt; is &lt;math&gt;|\lambda|&lt;/math&gt; times that of &lt;math&gt;v&lt;/math&gt;.


===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.)
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A B&lt;/math&gt; and &lt;math&gt;B A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; -- it's the number &lt;math&gt;v^T u&lt;/math&gt;. This should also be the property [TBC] that is used to compute the [[PCA|PCA]] of matrices that are very large in one dimension [I need to check this]. However, the analogous property does not apply for the ''eigenvectors'' of &lt;math&gt;AB&lt;/math&gt; and &lt;math&gt;BA&lt;/math&gt; &lt;ref&gt;Stephen Boyd, Homework 7 of EE263, Question 12.1 &lt;/ref&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>bm5gipsg3i2gnph0xjswyu5vgygik1f</sha1>
    </revision>
    <revision>
      <id>378</id>
      <parentid>377</parentid>
      <timestamp>2019-06-16T18:22:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5593">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.

The eigenvalues of a matrix depend on the elements of the matrix in a very complicated way, and in general their computation requires the use of rather sophisticated and time consuming numerical procedures. However, for certain types of matrices such as block-triangular or block-diagonal matrices, the eigenvalues can be reexpressed in terms of the eigenvalues of matrices of smaller dimensions or in other relatively simple ways.

===Geometric interpretation===
The eigenvectors of a matrix have a noteworthy geometrical property (page 520 &lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;). Let &lt;math&gt;\lambda&lt;/math&gt; represwent any nonzero eigenvalue of &lt;math&gt;A&lt;/math&gt;, let &lt;math&gt;v&lt;/math&gt; represent any eigenvector that corresponds to &lt;math&gt;\lambda&lt;/math&gt;, and take the usual inner product. Then, the direction of the vector &lt;math&gt;Av&lt;/math&gt; is either the same as &lt;math&gt;v&lt;/math&gt; or its opposite, depending on whether &lt;math&gt;\lambda&lt;/math&gt; is positive or negative. The length of &lt;math&gt;Av&lt;/math&gt; is &lt;math&gt;|\lambda|&lt;/math&gt; times that of &lt;math&gt;v&lt;/math&gt;.


===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.)
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A B&lt;/math&gt; and &lt;math&gt;B A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; -- it's the number &lt;math&gt;v^T u&lt;/math&gt;. This should also be the property [TBC] that is used to compute the [[PCA|PCA]] of matrices that are very large in one dimension [I need to check this]. However, the analogous property does not apply for the ''eigenvectors'' of &lt;math&gt;AB&lt;/math&gt; and &lt;math&gt;BA&lt;/math&gt; &lt;ref&gt;Stephen Boyd, Homework 7 of EE263, Question 12.1 &lt;/ref&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>atdoxqo0h489pj5eosevpxtskbyg0gv</sha1>
    </revision>
    <revision>
      <id>388</id>
      <parentid>378</parentid>
      <timestamp>2019-06-16T18:44:54Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5994">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.

The eigenvalues of a matrix depend on the elements of the matrix in a very complicated way, and in general their computation requires the use of rather sophisticated and time consuming numerical procedures. However, for certain types of matrices such as block-triangular or block-diagonal matrices, the eigenvalues can be reexpressed in terms of the eigenvalues of matrices of smaller dimensions or in other relatively simple ways.

===Geometric interpretation===
The eigenvectors of a matrix have a noteworthy geometrical property (page 520 &lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;). Let &lt;math&gt;\lambda&lt;/math&gt; represwent any nonzero eigenvalue of &lt;math&gt;A&lt;/math&gt;, let &lt;math&gt;v&lt;/math&gt; represent any eigenvector that corresponds to &lt;math&gt;\lambda&lt;/math&gt;, and take the usual inner product. Then, the direction of the vector &lt;math&gt;Av&lt;/math&gt; is either the same as &lt;math&gt;v&lt;/math&gt; or its opposite, depending on whether &lt;math&gt;\lambda&lt;/math&gt; is positive or negative. The length of &lt;math&gt;Av&lt;/math&gt; is &lt;math&gt;|\lambda|&lt;/math&gt; times that of &lt;math&gt;v&lt;/math&gt;.

===Eigenvalues of some special matrices===
* Moore-penrose inverse: Let &lt;math&gt;\lambda&lt;/math&gt; be an eigenvalue of &lt;math&gt;A&lt;/math&gt;. Then &lt;math&gt;\lambda^\dagger&lt;/math&gt; is the eigenvalue of the Moore-Penrose inverse &lt;math&gt;A^\dagger&lt;/math&gt;, where &lt;math&gt;\lambda^\dagger&lt;/math&gt; is defined as
&lt;math&gt;\lambda^\dagger = \begin{cases}1/\lambda &amp; \text{if } \lambda =0 \\ 0 &amp; \text{if } \lambda=0\end{cases}&lt;/math&gt;


===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.)
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A B&lt;/math&gt; and &lt;math&gt;B A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; -- it's the number &lt;math&gt;v^T u&lt;/math&gt;. This should also be the property [TBC] that is used to compute the [[PCA|PCA]] of matrices that are very large in one dimension [I need to check this]. However, the analogous property does not apply for the ''eigenvectors'' of &lt;math&gt;AB&lt;/math&gt; and &lt;math&gt;BA&lt;/math&gt; &lt;ref&gt;Stephen Boyd, Homework 7 of EE263, Question 12.1 &lt;/ref&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>6uiab3d7q76otbqyzi3ngeoxzkifq3o</sha1>
    </revision>
    <revision>
      <id>389</id>
      <parentid>388</parentid>
      <timestamp>2019-06-16T18:49:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6296">By definition, &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt; is an eigenvalue of &lt;math&gt;A \in \mathbb{C}^{n\times n}&lt;/math&gt; if the [[Characteristic Polynomial|characteristic polynomial]] becomes zero for &lt;math&gt;\lambda&lt;/math&gt;, that is &lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture notes 11&lt;/ref&gt;

&lt;math&gt;\mathcal{X}(\lambda) = \det(\lambda I - A)=0&lt;/math&gt;.

This is equivalent to the following two things:

1) There exists a ''(right) eigenvector'': There exists non-zero &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; such that &lt;math&gt;(\lambda I -A) v = 0&lt;/math&gt;, i.e., 

&lt;math&gt;Av = \lambda v&lt;/math&gt;
any such &lt;math&gt;v&lt;/math&gt; is called an eigenvector of &lt;math&gt;A&lt;/math&gt; associated with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;.

2) There exists a ''left eigenvector'': There exists non-zero &lt;math&gt;w \in \mathbb{C}^N&lt;/math&gt; s.t. &lt;math&gt;w^T(\lambda I - A) = 0&lt;/math&gt;, i.e., 

&lt;math&gt;w^T A = \lambda w^T&lt;/math&gt;.

The eigenvalues of a matrix depend on the elements of the matrix in a very complicated way, and in general their computation requires the use of rather sophisticated and time consuming numerical procedures. However, for certain types of matrices such as block-triangular or block-diagonal matrices, the eigenvalues can be reexpressed in terms of the eigenvalues of matrices of smaller dimensions or in other relatively simple ways.

===Geometric interpretation===
The eigenvectors of a matrix have a noteworthy geometrical property (page 520 &lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;). Let &lt;math&gt;\lambda&lt;/math&gt; represwent any nonzero eigenvalue of &lt;math&gt;A&lt;/math&gt;, let &lt;math&gt;v&lt;/math&gt; represent any eigenvector that corresponds to &lt;math&gt;\lambda&lt;/math&gt;, and take the usual inner product. Then, the direction of the vector &lt;math&gt;Av&lt;/math&gt; is either the same as &lt;math&gt;v&lt;/math&gt; or its opposite, depending on whether &lt;math&gt;\lambda&lt;/math&gt; is positive or negative. The length of &lt;math&gt;Av&lt;/math&gt; is &lt;math&gt;|\lambda|&lt;/math&gt; times that of &lt;math&gt;v&lt;/math&gt;.

===Eigenvalues of some special matrices===
* [[Pseudo-inverse|Moore-Penrose inverse]]: Let &lt;math&gt;\lambda&lt;/math&gt; be an eigenvalue of &lt;math&gt;A&lt;/math&gt;. Then &lt;math&gt;\lambda^\dagger&lt;/math&gt; is the eigenvalue of the Moore-Penrose inverse &lt;math&gt;A^\dagger&lt;/math&gt;, where &lt;math&gt;\lambda^\dagger&lt;/math&gt; is defined as
&lt;math&gt;\lambda^\dagger = \begin{cases}1/\lambda &amp; \text{if } \lambda =0 \\ 0 &amp; \text{if } \lambda=0\end{cases}&lt;/math&gt; (L21.7.1&lt;ref name=harville/&gt;)
* Orthogonal matrices: The eigenvalue of an orthogonal matrix is either &lt;math&gt;1&lt;/math&gt; or &lt;math&gt;-1&lt;/math&gt;  (T21.81.&lt;ref name=harville/&gt;)
* Idempotent: An idempotent matrix has no eigenvalues other than 0 or 1 (T21.8.1&lt;ref name=harville/&gt;).

===Properties of Eigenvectors and Eigenvalues===
* If &lt;math&gt;v&lt;/math&gt; is an eigenvector of A with eigenvalue &lt;math&gt;\lambda&lt;/math&gt;, then so is &lt;math&gt;av&lt;/math&gt;, for any &lt;math&gt;\alpha \in \mathbb{C}&lt;/math&gt;, &lt;math&gt;a\neq 0&lt;/math&gt;
* Even when &lt;math&gt;A&lt;/math&gt; is real, &lt;math&gt;\lambda&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; can be complex
* When &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;\lambda&lt;/math&gt; are real, we can always find a real eigenvector associated with &lt;math&gt;\lambda&lt;/math&gt;: if &lt;math&gt;Av = \lambda v&lt;/math&gt; with &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt;, &lt;math&gt;\lambda \in \mathbb{R}&lt;/math&gt; and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt;, then &lt;math&gt;A \Re v = \lambda \Re v&lt;/math&gt; and &lt;math&gt;A \Im v = \lambda \Im v&lt;/math&gt;. In other words, &lt;math&gt;\Re v&lt;/math&gt; and &lt;math&gt;\Im v&lt;/math&gt; are real eigenvectors, if they are non-zero (and at least one is).
* ''Conjugate symmetry'': if &lt;math&gt;A&lt;/math&gt; is real and &lt;math&gt;v \in \mathbb{C}^n&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\lambda \in \mathbb{C}&lt;/math&gt;, then &lt;math&gt;\bar{v}&lt;/math&gt; is an eigenvector associated with &lt;math&gt;\bar{\lambda}&lt;/math&gt;. (Can be shown by taking conjugate of &lt;math&gt;Av=\lambda v&lt;/math&gt;.)
* When &lt;math&gt;A&lt;/math&gt; is symmetric, its eigenvalues are real.
* The non-zero eigenvalues of &lt;math&gt;A B&lt;/math&gt; and &lt;math&gt;B A&lt;/math&gt; are the same even when the sizes of &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are different. This way, it's very easy to compute, for example, the non-zero eigenvalue of an [[Outer product|outer product]] &lt;math&gt;u v^T&lt;/math&gt; -- it's the number &lt;math&gt;v^T u&lt;/math&gt;. This should also be the property [TBC] that is used to compute the [[PCA|PCA]] of matrices that are very large in one dimension [I need to check this]. However, the analogous property does not apply for the ''eigenvectors'' of &lt;math&gt;AB&lt;/math&gt; and &lt;math&gt;BA&lt;/math&gt; &lt;ref&gt;Stephen Boyd, Homework 7 of EE263, Question 12.1 &lt;/ref&gt;.

==What's the point of Eigenvalues or Eigenvectors?==

Eigenvalues or eigenvectors are ubiquitous, and any list that will be compiled will be largely incomplete. But here are some purposes and uses of eigenvalues/eigenvectors...
===In the context of LDSs===
* The poles of an autonomous LDS are the eigenvalues. They are also the roots of the [[Characteristic Polynomial|characteristic polynomial]] of the system.
* The conditions for the [[Stability|stability]] of a continuous-time LDS can be expressed via left or right eigenvectors.
* The conditions for the [[Stability|stability]] of a discrete-time LDS can be expressed eigenvalues (see [[Modal Form]]).
* Understanding behavior of LDSs: If the initial condition of an LDS is a '''right eigenvector''', then we know that the system trajectory will remain within the line spanned by that eigenvector &lt;ref name=l11/&gt;.
* Understanding the behavior of an LDS #2: Independently of the initial conditions, the '''left eigenvectors''' give linear functions of the state that are simple (e.g., decaying exponentials) (see 11-11 in &lt;ref name=l11/&gt;)

===For quadratic forms (and positive definiteness etc.)===
* If a matrix is symmetric, then it's eigenvalues can be used to test if it's positive/negative (semi)definite (see [[Positive definite matrix]])
* For symmetric matrices, eigenvalues define upper and lower bounds on a quadratic form (see [[Quadratic form]]).

===Matrix norm and gain of a matrix===
* Eigenvalues determine how ''big'' a matrix is (what's the [[Matrix gain in a direction|largest gain of a matrix]]) in a given direction.
* The ratio of the largest and smallest eigenvalues determine the ''eccentricity'' of an [[Ellipse|Ellipse]].
* Therefore, the largest eigenvalue is arguably the most standard [[Matrix norm|matrix norm]].
TBC

== References ==</text>
      <sha1>ampunw4e5uonk1bc1qykpi8v2mmpljp</sha1>
    </revision>
  </page>
  <page>
    <title>Diagonalization</title>
    <ns>0</ns>
    <id>35</id>
    <revision>
      <id>79</id>
      <timestamp>2018-07-16T20:29:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:  &lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;mat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1276">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternative, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)</text>
      <sha1>f7ju41h7o5fp38d94l3ch6c2olfmnca</sha1>
    </revision>
    <revision>
      <id>80</id>
      <parentid>79</parentid>
      <timestamp>2018-07-16T20:34:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1718">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write&lt;ref&gt;Stephen Boyd, EE263 Lecutre Notes 11&lt;/ref&gt;:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternative, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)

===Left Eigenvectors===
If &lt;math&gt;A&lt;/math&gt; is diagonalizable, then we can also write &lt;math&gt;T^{-1}A = \Lambda T^{-1}&lt;/math&gt;. Let &lt;math&gt;w_i^T&lt;/math&gt; be the rows of &lt;math&gt;T^{-1}&lt;/math&gt;. Then, we can write:

&lt;math&gt;w_i^T A = \lambda_i w_i^T&lt;/math&gt;.

In other words, &lt;math&gt;w_i&lt;/math&gt;s are (linearly independent) [[Eigenvectors and Eigenvalues|left eigenvectors]] of &lt;math&gt;A&lt;/math&gt;.




==References==</text>
      <sha1>427zhg9agzkjvvqgv1lfax3lkxvx1cq</sha1>
    </revision>
    <revision>
      <id>83</id>
      <parentid>80</parentid>
      <timestamp>2018-07-16T21:09:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1715">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write&lt;ref&gt;Stephen Boyd, EE263 Lecutre Notes 11&lt;/ref&gt;:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternative, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)

===Left Eigenvectors===
If &lt;math&gt;A&lt;/math&gt; is diagonalizable, then we can also write &lt;math&gt;T^{-1}A = \Lambda T^{-1}&lt;/math&gt;. Let &lt;math&gt;w_i^T&lt;/math&gt; be the rows of &lt;math&gt;T^{-1}&lt;/math&gt;. Then, we can write:

&lt;math&gt;w_i^T A = \lambda_i w_i^T&lt;/math&gt;.

In other words, &lt;math&gt;w_i&lt;/math&gt;s are (linearly independent) [[Eigenvectors and Eigenvalues|left eigenvectors]] of &lt;math&gt;A&lt;/math&gt;.

==References==</text>
      <sha1>393tnk1fqedwuvio7pwyba8z91nkmon</sha1>
    </revision>
    <revision>
      <id>94</id>
      <parentid>83</parentid>
      <timestamp>2018-07-16T23:40:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2283">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write&lt;ref&gt;Stephen Boyd, EE263 Lecutre Notes 11&lt;/ref&gt;:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternative, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)

===Left Eigenvectors===
If &lt;math&gt;A&lt;/math&gt; is diagonalizable, then we can also write &lt;math&gt;T^{-1}A = \Lambda T^{-1}&lt;/math&gt;. Let &lt;math&gt;w_i^T&lt;/math&gt; be the rows of &lt;math&gt;T^{-1}&lt;/math&gt;. Then, we can write:

&lt;math&gt;w_i^T A = \lambda_i w_i^T&lt;/math&gt;.

In other words, &lt;math&gt;w_i&lt;/math&gt;s are (linearly independent) [[Eigenvectors and Eigenvalues|left eigenvectors]] of &lt;math&gt;A&lt;/math&gt;.

==What's the point of Diagonalization?==
Diagonalization leads to [[Modal Form|modal form]], which is very useful for analyzing LDSs. Specifically, modal forms allow us to find the conditions for system [[Stability|stability]]. Also, the modal form (or one can attribute it to diagonalization itself) allows us to obtain simple expressions of the resolvent of a system &lt;math&gt;(sI-A)^{-1}&lt;/math&gt;, the matrix power &lt;math&gt;A^k&lt;/math&gt; and the matrix exponential &lt;math&gt;e^A&lt;/math&gt;; the latter give the solutions to discrete- and continuous-time systems (see [[Modal Form]]).

==References==</text>
      <sha1>a8vz8rh2rp6yfd1956zb79f38ww93ku</sha1>
    </revision>
    <revision>
      <id>96</id>
      <parentid>94</parentid>
      <timestamp>2018-07-17T15:18:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* When is A diagonalizable? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2285">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write&lt;ref&gt;Stephen Boyd, EE263 Lecutre Notes 11&lt;/ref&gt;:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternatively, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)

===Left Eigenvectors===
If &lt;math&gt;A&lt;/math&gt; is diagonalizable, then we can also write &lt;math&gt;T^{-1}A = \Lambda T^{-1}&lt;/math&gt;. Let &lt;math&gt;w_i^T&lt;/math&gt; be the rows of &lt;math&gt;T^{-1}&lt;/math&gt;. Then, we can write:

&lt;math&gt;w_i^T A = \lambda_i w_i^T&lt;/math&gt;.

In other words, &lt;math&gt;w_i&lt;/math&gt;s are (linearly independent) [[Eigenvectors and Eigenvalues|left eigenvectors]] of &lt;math&gt;A&lt;/math&gt;.

==What's the point of Diagonalization?==
Diagonalization leads to [[Modal Form|modal form]], which is very useful for analyzing LDSs. Specifically, modal forms allow us to find the conditions for system [[Stability|stability]]. Also, the modal form (or one can attribute it to diagonalization itself) allows us to obtain simple expressions of the resolvent of a system &lt;math&gt;(sI-A)^{-1}&lt;/math&gt;, the matrix power &lt;math&gt;A^k&lt;/math&gt; and the matrix exponential &lt;math&gt;e^A&lt;/math&gt;; the latter give the solutions to discrete- and continuous-time systems (see [[Modal Form]]).

==References==</text>
      <sha1>lscd6519djgsyj8s3e7l519924n3o53</sha1>
    </revision>
    <revision>
      <id>149</id>
      <parentid>96</parentid>
      <timestamp>2018-07-27T13:12:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2494">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write&lt;ref&gt;Stephen Boyd, EE263 Lecutre Notes 11&lt;/ref&gt;:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===Eigendecomposition===
Clearly, one can decompose &lt;math&gt;A&lt;/math&gt; as &lt;math&gt;A = T A T^{-1} &lt;/math&gt;. This is called ''eigendecomposition'' as it is a decomposition that involves eigenvectors anda eigenvalues.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternatively, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)

===Left Eigenvectors===
If &lt;math&gt;A&lt;/math&gt; is diagonalizable, then we can also write &lt;math&gt;T^{-1}A = \Lambda T^{-1}&lt;/math&gt;. Let &lt;math&gt;w_i^T&lt;/math&gt; be the rows of &lt;math&gt;T^{-1}&lt;/math&gt;. Then, we can write:

&lt;math&gt;w_i^T A = \lambda_i w_i^T&lt;/math&gt;.

In other words, &lt;math&gt;w_i&lt;/math&gt;s are (linearly independent) [[Eigenvectors and Eigenvalues|left eigenvectors]] of &lt;math&gt;A&lt;/math&gt;.

==What's the point of Diagonalization?==
Diagonalization leads to [[Modal Form|modal form]], which is very useful for analyzing LDSs. Specifically, modal forms allow us to find the conditions for system [[Stability|stability]]. Also, the modal form (or one can attribute it to diagonalization itself) allows us to obtain simple expressions of the resolvent of a system &lt;math&gt;(sI-A)^{-1}&lt;/math&gt;, the matrix power &lt;math&gt;A^k&lt;/math&gt; and the matrix exponential &lt;math&gt;e^A&lt;/math&gt;; the latter give the solutions to discrete- and continuous-time systems (see [[Modal Form]]).

==References==</text>
      <sha1>mdq03x79hh3crh5c9kvhxk9wm6qdx9v</sha1>
    </revision>
    <revision>
      <id>198</id>
      <parentid>149</parentid>
      <timestamp>2018-09-05T12:49:51Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Eigendecomposition */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2493">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write&lt;ref&gt;Stephen Boyd, EE263 Lecutre Notes 11&lt;/ref&gt;:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===Eigendecomposition===
Clearly, one can decompose &lt;math&gt;A&lt;/math&gt; as &lt;math&gt;A = T A T^{-1} &lt;/math&gt;. This is called ''eigendecomposition'' as it is a decomposition that involves eigenvectors and eigenvalues.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternatively, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)

===Left Eigenvectors===
If &lt;math&gt;A&lt;/math&gt; is diagonalizable, then we can also write &lt;math&gt;T^{-1}A = \Lambda T^{-1}&lt;/math&gt;. Let &lt;math&gt;w_i^T&lt;/math&gt; be the rows of &lt;math&gt;T^{-1}&lt;/math&gt;. Then, we can write:

&lt;math&gt;w_i^T A = \lambda_i w_i^T&lt;/math&gt;.

In other words, &lt;math&gt;w_i&lt;/math&gt;s are (linearly independent) [[Eigenvectors and Eigenvalues|left eigenvectors]] of &lt;math&gt;A&lt;/math&gt;.

==What's the point of Diagonalization?==
Diagonalization leads to [[Modal Form|modal form]], which is very useful for analyzing LDSs. Specifically, modal forms allow us to find the conditions for system [[Stability|stability]]. Also, the modal form (or one can attribute it to diagonalization itself) allows us to obtain simple expressions of the resolvent of a system &lt;math&gt;(sI-A)^{-1}&lt;/math&gt;, the matrix power &lt;math&gt;A^k&lt;/math&gt; and the matrix exponential &lt;math&gt;e^A&lt;/math&gt;; the latter give the solutions to discrete- and continuous-time systems (see [[Modal Form]]).

==References==</text>
      <sha1>0168lz0runnkhgqg0secgyitdhmt1hy</sha1>
    </revision>
    <revision>
      <id>366</id>
      <parentid>198</parentid>
      <timestamp>2019-05-14T12:49:12Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>Typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2499">Suppose that &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; has a linearly independent set of eigenvectors &lt;math&gt;v_1, \dots, v_n&lt;/math&gt;:

&lt;math&gt;Av_i = \lambda_i v_i&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;.

We can define &lt;math&gt;T:=[v_1 \dots v_n]&lt;/math&gt; and &lt;math&gt;\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)&lt;/math&gt;. Using the eigenvalue identities listed above, we can write&lt;ref&gt;Stephen Boyd, EE263 Lecutre Notes 11&lt;/ref&gt;:

&lt;math&gt;AT = T\Lambda&lt;/math&gt;

and finally (since &lt;math&gt;T&lt;/math&gt; has a set of linearly independent eigenvectors):

&lt;math&gt;T^{-1}A T = \Lambda&lt;/math&gt;.

In other words, [[Similarity Transformation|similarity transformation]] by &lt;math&gt;T&lt;/math&gt; ''diagonalizes'' &lt;math&gt;A&lt;/math&gt;. The converse holds too: If there is &lt;math&gt;T=[v_1 \dots v_n]&lt;/math&gt; such that &lt;math&gt;T^{-1}A T = \Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)&lt;/math&gt;, then &lt;math&gt;AT = T\Lambda&lt;/math&gt;, which implies that &lt;math&gt;v_1,\dots, v_n&lt;/math&gt; form a set of linearly independent eigenvectors.

===Eigendecomposition===
Clearly, one can decompose &lt;math&gt;A&lt;/math&gt; as &lt;math&gt;A = T \Lambda T^{-1} &lt;/math&gt;. This is called ''eigendecomposition'' as it is a decomposition that involves eigenvectors and eigenvalues.

===When is &lt;math&gt;A&lt;/math&gt; diagonalizable?===
&lt;math&gt;A&lt;/math&gt; is diagonalizable if 
* there exists &lt;math&gt;T&lt;/math&gt; s.t. &lt;math&gt;T^{-1}A T  = \Lambda&lt;/math&gt; is diagonal
* &lt;math&gt;A&lt;/math&gt; has a set of linearly independent eigenvectors

Alternatively, &lt;math&gt;A&lt;/math&gt; is diagonizable also if it has distinct eigenvalues. (The converse does not necessarily hold.)

===Left Eigenvectors===
If &lt;math&gt;A&lt;/math&gt; is diagonalizable, then we can also write &lt;math&gt;T^{-1}A = \Lambda T^{-1}&lt;/math&gt;. Let &lt;math&gt;w_i^T&lt;/math&gt; be the rows of &lt;math&gt;T^{-1}&lt;/math&gt;. Then, we can write:

&lt;math&gt;w_i^T A = \lambda_i w_i^T&lt;/math&gt;.

In other words, &lt;math&gt;w_i&lt;/math&gt;s are (linearly independent) [[Eigenvectors and Eigenvalues|left eigenvectors]] of &lt;math&gt;A&lt;/math&gt;.

==What's the point of Diagonalization?==
Diagonalization leads to [[Modal Form|modal form]], which is very useful for analyzing LDSs. Specifically, modal forms allow us to find the conditions for system [[Stability|stability]]. Also, the modal form (or one can attribute it to diagonalization itself) allows us to obtain simple expressions of the resolvent of a system &lt;math&gt;(sI-A)^{-1}&lt;/math&gt;, the matrix power &lt;math&gt;A^k&lt;/math&gt; and the matrix exponential &lt;math&gt;e^A&lt;/math&gt;; the latter give the solutions to discrete- and continuous-time systems (see [[Modal Form]]).

==References==</text>
      <sha1>rlgu5h8360e8zbr4dbm7dsuy3pd24nn</sha1>
    </revision>
  </page>
  <page>
    <title>Modal Form</title>
    <ns>0</ns>
    <id>36</id>
    <revision>
      <id>81</id>
      <timestamp>2018-07-16T20:40:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Suppose that an (autonomous) LDS is [[diagonalization|diagonalizable]] by a matrix &lt;math&gt;T&lt;/math&gt;. Then, we can define new coordinates for states by &lt;math&gt;x = T\tilde{x}&lt;/math..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="827">Suppose that an (autonomous) LDS is [[diagonalization|diagonalizable]] by a matrix &lt;math&gt;T&lt;/math&gt;. Then, we can define new coordinates for states by &lt;math&gt;x = T\tilde{x}&lt;/math&gt;, so&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 11&lt;/ref&gt;:

&lt;math&gt;T\dot{\tilde{x}}=AT\tilde{x}&lt;/math&gt;

Since &lt;math&gt;T&lt;/math&gt; is invertible (by assumption of diagonalizability), we have that:

&lt;math&gt;\dot{\tilde{x}} = T^{-1} A T \tilde{x} = \Lambda \tilde{x}&lt;/math&gt;

where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues, &lt;math&gt;\Lambda = \text{diag}(\lambda_1, \lambda_2,\dots, \lambda_n)&lt;/math&gt;.

Note that in this new coordinate system, the system is decoupled; that is, it consists of &lt;math&gt;n&lt;/math&gt; independent modes so that:

&lt;math&gt;\tilde{x}_i(t) = e^{\lambda_i t} \tilde{x}_i(0)&lt;/math&gt;. This is why it is called ''model form''.

==References==</text>
      <sha1>c8sv309mts78wcnkmxpe36i9pdrk64o</sha1>
    </revision>
    <revision>
      <id>84</id>
      <parentid>81</parentid>
      <timestamp>2018-07-16T21:10:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3425">Suppose that an (autonomous) LDS is [[diagonalization|diagonalizable]] by a matrix &lt;math&gt;T&lt;/math&gt;. Then, we can define new coordinates for states by &lt;math&gt;x = T\tilde{x}&lt;/math&gt;, so&lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture Notes 11&lt;/ref&gt;:

&lt;math&gt;T\dot{\tilde{x}}=AT\tilde{x}&lt;/math&gt;

Since &lt;math&gt;T&lt;/math&gt; is invertible (by assumption of diagonalizability), we have that:

&lt;math&gt;\dot{\tilde{x}} = T^{-1} A T \tilde{x} = \Lambda \tilde{x}&lt;/math&gt;

where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues, &lt;math&gt;\Lambda = \text{diag}(\lambda_1, \lambda_2,\dots, \lambda_n)&lt;/math&gt;.

Note that in this new coordinate system, the system is decoupled; that is, it consists of &lt;math&gt;n&lt;/math&gt; independent modes so that:

&lt;math&gt;\tilde{x}_i(t) = e^{\lambda_i t} \tilde{x}_i(0)&lt;/math&gt;. This is why it is called ''model form''.

===What's the point of a Modal Form?===
Diagonalization is a powerful conceptual tool. That is, it's not necessarily useful for doing calculations, but it's very useful for (i) showing things (see also below) and for analyzing systems. The system &lt;math&gt;\dot{x} = A x&lt;/math&gt; is easier to analyze in the new coordinate system &lt;math&gt;\dot{\tilde{x}} = \Lambda \tilde{x}&lt;/math&gt;. 
====Simplifying expressions====
The following are examples to how modal form simplifies experssions:

1) The '''resolvent''' &lt;math&gt;(sI-A)^{-1}&lt;/math&gt; is very easy to compute on this new system; it's not difficult to show that&lt;ref name=l11/&gt;: 

&lt;math&gt;(sI-A)^{-1} =  T \text{diag}\left( \frac{1}{s-\lambda_1}, \dots, \frac{1}{s-\lambda_n} \right)&lt;/math&gt;.

2) Computing the matrix powers (and hence the '''discrete-time solution''') is also very easy; it can be easily shown that:
&lt;math&gt;A^k = T\text{diag}(\lambda_1^k, \dots, \lambda_{n}^k )T^{-1}&lt;/math&gt;

3) Computing the exponential (and hence the '''continuous-time solution''') is also very simple:
&lt;math&gt;e^A = T\text{diag}(e^{\lambda_1}, \dots, e^{\lambda_n})T^{-1}&lt;/math&gt;

====Interpreting and analyzing system====
The simplifications in expressions allow us to interpret systems and understand the conditions under which they are [[Stability|stable]]. Examples are:

1) Since the '''continuous-time solution''' can be written as a linear combination of the modes, we can interpret the solution (in the original coordinate system) as&lt;ref name=l11/&gt;:

&lt;math&gt;x(t) = e^{tA}x(0)&lt;/math&gt;  = \sum_{i=1}^n e^{\lambda_i t}(w_i^T x(0)) v_i

This leads to the following interpretations: (i) left eigenvectors decompose the initial state &lt;math&gt;x(0)&lt;/math&gt; into modal components &lt;math&gt;w^T_i x(0)&lt;/math&gt; --- as can be seen by evaluating the expression above for &lt;math&gt;t=0&lt;/math&gt;. (ii) we can reconstruct state as a linear combination of (right) eigenvectors.

2) '''System stability (continuous)''': The interpretations above allow us to find which initial conditions &lt;math&gt;x(0)&lt;/math&gt; lead to a [[Stability|stable]] system. It can be shown that &lt;ref name=l11&gt; an autonomous system is stable, only if &lt;math&gt;x(0) \in \text{span}\{v_1,\dots,v_s\}&lt;/math&gt;, where &lt;math&gt;v_1, \dots, v_s&lt;/math&gt; are the eigenvectors of &lt;math&gt;A&lt;/math&gt; associated with eigenvalues that have a negative real part.

3) '''System stability (discrete)''': The modal form also shows as that a discrete-time system is [[Stability|stable]] (for all &lt;math&gt;x(0)&lt;/math&gt;) iff the magnitude of all the eigenvalues of &lt;math&gt;A&lt;/math&gt; is smaller than 1. (I.e, the spectral norm of &lt;math&gt;A&lt;/math&gt; is smaller than 1).
==References==</text>
      <sha1>ha9g8ik285qbuczzqt2hkrvo2ba6mrz</sha1>
    </revision>
    <revision>
      <id>93</id>
      <parentid>84</parentid>
      <timestamp>2018-07-16T23:38:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Interpreting and analyzing system */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3426">Suppose that an (autonomous) LDS is [[diagonalization|diagonalizable]] by a matrix &lt;math&gt;T&lt;/math&gt;. Then, we can define new coordinates for states by &lt;math&gt;x = T\tilde{x}&lt;/math&gt;, so&lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture Notes 11&lt;/ref&gt;:

&lt;math&gt;T\dot{\tilde{x}}=AT\tilde{x}&lt;/math&gt;

Since &lt;math&gt;T&lt;/math&gt; is invertible (by assumption of diagonalizability), we have that:

&lt;math&gt;\dot{\tilde{x}} = T^{-1} A T \tilde{x} = \Lambda \tilde{x}&lt;/math&gt;

where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues, &lt;math&gt;\Lambda = \text{diag}(\lambda_1, \lambda_2,\dots, \lambda_n)&lt;/math&gt;.

Note that in this new coordinate system, the system is decoupled; that is, it consists of &lt;math&gt;n&lt;/math&gt; independent modes so that:

&lt;math&gt;\tilde{x}_i(t) = e^{\lambda_i t} \tilde{x}_i(0)&lt;/math&gt;. This is why it is called ''model form''.

===What's the point of a Modal Form?===
Diagonalization is a powerful conceptual tool. That is, it's not necessarily useful for doing calculations, but it's very useful for (i) showing things (see also below) and for analyzing systems. The system &lt;math&gt;\dot{x} = A x&lt;/math&gt; is easier to analyze in the new coordinate system &lt;math&gt;\dot{\tilde{x}} = \Lambda \tilde{x}&lt;/math&gt;. 
====Simplifying expressions====
The following are examples to how modal form simplifies experssions:

1) The '''resolvent''' &lt;math&gt;(sI-A)^{-1}&lt;/math&gt; is very easy to compute on this new system; it's not difficult to show that&lt;ref name=l11/&gt;: 

&lt;math&gt;(sI-A)^{-1} =  T \text{diag}\left( \frac{1}{s-\lambda_1}, \dots, \frac{1}{s-\lambda_n} \right)&lt;/math&gt;.

2) Computing the matrix powers (and hence the '''discrete-time solution''') is also very easy; it can be easily shown that:
&lt;math&gt;A^k = T\text{diag}(\lambda_1^k, \dots, \lambda_{n}^k )T^{-1}&lt;/math&gt;

3) Computing the exponential (and hence the '''continuous-time solution''') is also very simple:
&lt;math&gt;e^A = T\text{diag}(e^{\lambda_1}, \dots, e^{\lambda_n})T^{-1}&lt;/math&gt;

====Interpreting and analyzing system====
The simplifications in expressions allow us to interpret systems and understand the conditions under which they are [[Stability|stable]]. Examples are:

1) Since the '''continuous-time solution''' can be written as a linear combination of the modes, we can interpret the solution (in the original coordinate system) as&lt;ref name=l11/&gt;:

&lt;math&gt;x(t) = e^{tA}x(0)  = \sum_{i=1}^n e^{\lambda_i t}(w_i^T x(0)) v_i&lt;/math&gt;

This leads to the following interpretations: (i) left eigenvectors decompose the initial state &lt;math&gt;x(0)&lt;/math&gt; into modal components &lt;math&gt;w^T_i x(0)&lt;/math&gt; --- as can be seen by evaluating the expression above for &lt;math&gt;t=0&lt;/math&gt;. (ii) we can reconstruct state as a linear combination of (right) eigenvectors.

2) '''System stability (continuous)''': The interpretations above allow us to find which initial conditions &lt;math&gt;x(0)&lt;/math&gt; lead to a [[Stability|stable]] system. It can be shown that &lt;ref name=l11&gt; an autonomous system is stable, only if &lt;math&gt;x(0) \in \text{span}\{v_1,\dots,v_s\}&lt;/math&gt;, where &lt;math&gt;v_1, \dots, v_s&lt;/math&gt; are the eigenvectors of &lt;math&gt;A&lt;/math&gt; associated with eigenvalues that have a negative real part.

3) '''System stability (discrete)''': The modal form also shows as that a discrete-time system is [[Stability|stable]] (for all &lt;math&gt;x(0)&lt;/math&gt;) iff the magnitude of all the eigenvalues of &lt;math&gt;A&lt;/math&gt; is smaller than 1. (I.e, the spectral norm of &lt;math&gt;A&lt;/math&gt; is smaller than 1).

==References==</text>
      <sha1>5pj34mrtyeqlosq6oj5vgq2ozwwo3tv</sha1>
    </revision>
    <revision>
      <id>367</id>
      <parentid>93</parentid>
      <timestamp>2019-05-14T12:57:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3426">Suppose that an (autonomous) LDS is [[diagonalization|diagonalizable]] by a matrix &lt;math&gt;T&lt;/math&gt;. Then, we can define new coordinates for states by &lt;math&gt;x = T\tilde{x}&lt;/math&gt;, so&lt;ref name=l11&gt;Stephen Boyd, EE263 Lecture Notes 11&lt;/ref&gt;:

&lt;math&gt;T\dot{\tilde{x}}=AT\tilde{x}&lt;/math&gt;

Since &lt;math&gt;T&lt;/math&gt; is invertible (by assumption of diagonalizability), we have that:

&lt;math&gt;\dot{\tilde{x}} = T^{-1} A T \tilde{x} = \Lambda \tilde{x}&lt;/math&gt;

where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues, &lt;math&gt;\Lambda = \text{diag}(\lambda_1, \lambda_2,\dots, \lambda_n)&lt;/math&gt;.

Note that in this new coordinate system, the system is decoupled; that is, it consists of &lt;math&gt;n&lt;/math&gt; independent modes so that:

&lt;math&gt;\tilde{x}_i(t) = e^{\lambda_i t} \tilde{x}_i(0)&lt;/math&gt;. This is why it is called ''modal form''.

===What's the point of a Modal Form?===
Diagonalization is a powerful conceptual tool. That is, it's not necessarily useful for doing calculations, but it's very useful for (i) showing things (see also below) and for analyzing systems. The system &lt;math&gt;\dot{x} = A x&lt;/math&gt; is easier to analyze in the new coordinate system &lt;math&gt;\dot{\tilde{x}} = \Lambda \tilde{x}&lt;/math&gt;. 
====Simplifying expressions====
The following are examples to how modal form simplifies experssions:

1) The '''resolvent''' &lt;math&gt;(sI-A)^{-1}&lt;/math&gt; is very easy to compute on this new system; it's not difficult to show that&lt;ref name=l11/&gt;: 

&lt;math&gt;(sI-A)^{-1} =  T \text{diag}\left( \frac{1}{s-\lambda_1}, \dots, \frac{1}{s-\lambda_n} \right)&lt;/math&gt;.

2) Computing the matrix powers (and hence the '''discrete-time solution''') is also very easy; it can be easily shown that:
&lt;math&gt;A^k = T\text{diag}(\lambda_1^k, \dots, \lambda_{n}^k )T^{-1}&lt;/math&gt;

3) Computing the exponential (and hence the '''continuous-time solution''') is also very simple:
&lt;math&gt;e^A = T\text{diag}(e^{\lambda_1}, \dots, e^{\lambda_n})T^{-1}&lt;/math&gt;

====Interpreting and analyzing system====
The simplifications in expressions allow us to interpret systems and understand the conditions under which they are [[Stability|stable]]. Examples are:

1) Since the '''continuous-time solution''' can be written as a linear combination of the modes, we can interpret the solution (in the original coordinate system) as&lt;ref name=l11/&gt;:

&lt;math&gt;x(t) = e^{tA}x(0)  = \sum_{i=1}^n e^{\lambda_i t}(w_i^T x(0)) v_i&lt;/math&gt;

This leads to the following interpretations: (i) left eigenvectors decompose the initial state &lt;math&gt;x(0)&lt;/math&gt; into modal components &lt;math&gt;w^T_i x(0)&lt;/math&gt; --- as can be seen by evaluating the expression above for &lt;math&gt;t=0&lt;/math&gt;. (ii) we can reconstruct state as a linear combination of (right) eigenvectors.

2) '''System stability (continuous)''': The interpretations above allow us to find which initial conditions &lt;math&gt;x(0)&lt;/math&gt; lead to a [[Stability|stable]] system. It can be shown that &lt;ref name=l11&gt; an autonomous system is stable, only if &lt;math&gt;x(0) \in \text{span}\{v_1,\dots,v_s\}&lt;/math&gt;, where &lt;math&gt;v_1, \dots, v_s&lt;/math&gt; are the eigenvectors of &lt;math&gt;A&lt;/math&gt; associated with eigenvalues that have a negative real part.

3) '''System stability (discrete)''': The modal form also shows as that a discrete-time system is [[Stability|stable]] (for all &lt;math&gt;x(0)&lt;/math&gt;) iff the magnitude of all the eigenvalues of &lt;math&gt;A&lt;/math&gt; is smaller than 1. (I.e, the spectral norm of &lt;math&gt;A&lt;/math&gt; is smaller than 1).

==References==</text>
      <sha1>eecztqh0yxcu3h8swavzszgu502mcql</sha1>
    </revision>
  </page>
  <page>
    <title>Stability</title>
    <ns>0</ns>
    <id>37</id>
    <revision>
      <id>82</id>
      <timestamp>2018-07-16T21:07:50Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "===Autonomous systems===  An autonomous LDS (i.e., LDS of the form &lt;math&gt;\dot{x} = Ax&lt;/math&gt;) is called stable if &lt;math&gt;x(t) \to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;. This de..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="245">===Autonomous systems===

An autonomous LDS (i.e., LDS of the form &lt;math&gt;\dot{x} = Ax&lt;/math&gt;) is called stable if &lt;math&gt;x(t) \to 0&lt;/math&gt; as &lt;math&gt;t \to \infty&lt;/math&gt;. This definition is applicable both for continuous- and discrete-time systems.</text>
      <sha1>odiiph06pjhjr1aiohz1ipuw0qmdfds</sha1>
    </revision>
  </page>
  <page>
    <title>State-transition matrix</title>
    <ns>0</ns>
    <id>38</id>
    <redirect title="Laplace Transform" />
    <revision>
      <id>88</id>
      <timestamp>2018-07-16T21:25:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Laplace Transform]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="31">#REDIRECT [[Laplace Transform]]</text>
      <sha1>ph15pd4xieb99jhhnn4z0re0nk0bydk</sha1>
    </revision>
  </page>
  <page>
    <title>Hermitian conjugate</title>
    <ns>0</ns>
    <id>39</id>
    <revision>
      <id>89</id>
      <timestamp>2018-07-16T21:27:54Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Hermitian conjugate (or conjugate transpose) of a matrix &lt;math&gt;A&lt;/math&gt; is obtained by transposing a matrix and then computing the conjugate of every entry in the transposed m..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="181">Hermitian conjugate (or conjugate transpose) of a matrix &lt;math&gt;A&lt;/math&gt; is obtained by transposing a matrix and then computing the conjugate of every entry in the transposed matrix.</text>
      <sha1>1rkpr9a1pl1e96c7efd8amtglvxarca</sha1>
    </revision>
  </page>
  <page>
    <title>Jordan Canonical Form</title>
    <ns>0</ns>
    <id>40</id>
    <revision>
      <id>98</id>
      <timestamp>2018-07-17T23:58:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Jordan Canonical Form (JCF) is a generalization of [[Diagonalization|diagonalization]]. Every square matrix &lt;math&gt;A&lt;/math&gt; has a JCF, even when it's not diagonalizable&lt;ref&gt;Ste..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="542">Jordan Canonical Form (JCF) is a generalization of [[Diagonalization|diagonalization]]. Every square matrix &lt;math&gt;A&lt;/math&gt; has a JCF, even when it's not diagonalizable&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 12&lt;/ref&gt;. The JCF of a matrix &lt;math&gt;A&lt;/math&gt; is obtained with a similarity transformation, that is, 

&lt;math&gt;T^{-1}A T = J = \text{diag}(J_1, \dots, J_q),&lt;/math&gt;

where &lt;math&gt;J&lt;/math&gt; is a block-diagonal matrix and each block &lt;math&gt;J_i&lt;/math&gt; is a ''bidiagonal'' matrix in the form:

&lt;math&gt;J_i = \begin{bmatrix}\ddots\end{bmatrix}&lt;/math&gt;</text>
      <sha1>ms2ph7q4z8b342gvuefoffpmehri065</sha1>
    </revision>
    <revision>
      <id>99</id>
      <parentid>98</parentid>
      <timestamp>2018-07-18T00:01:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="611">Jordan Canonical Form (JCF) is a generalization of [[Diagonalization|diagonalization]]. Every square matrix &lt;math&gt;A&lt;/math&gt; has a JCF, even when it's not diagonalizable&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 12&lt;/ref&gt;. The JCF of a matrix &lt;math&gt;A&lt;/math&gt; is obtained with a similarity transformation, that is, 

&lt;math&gt;T^{-1}A T = J = \text{diag}(J_1, \dots, J_q),&lt;/math&gt;

where &lt;math&gt;J&lt;/math&gt; is a block-diagonal matrix and each block &lt;math&gt;J_i&lt;/math&gt; is a ''bidiagonal'' matrix in the form:

&lt;math&gt;J_i = \begin{bmatrix}\lambda_1 &amp; 1 &amp; &amp; \\
 &amp; \lambda_i &amp; \ddots &amp; \\
&amp;&amp;\ddots &amp;1 \\
&amp;&amp;&amp;\lambda_i\end{bmatrix}&lt;/math&gt;</text>
      <sha1>qyi67vlik1301w5takowwwufg7hhyg6</sha1>
    </revision>
    <revision>
      <id>101</id>
      <parentid>99</parentid>
      <timestamp>2018-07-18T00:19:09Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1372">Jordan Canonical Form (JCF) is a generalization of [[Diagonalization|diagonalization]]. Every square matrix &lt;math&gt;A&lt;/math&gt; has a JCF, even when it's not diagonalizable&lt;ref name=l12&gt;Stephen Boyd, EE263 Lecture Notes 12&lt;/ref&gt;. The JCF of a matrix &lt;math&gt;A&lt;/math&gt; is obtained with a similarity transformation, that is, 

&lt;math&gt;T^{-1}A T = J = \text{diag}(J_1, \dots, J_q),&lt;/math&gt;

where &lt;math&gt;J&lt;/math&gt; is a block-diagonal matrix and each block &lt;math&gt;J_i&lt;/math&gt; is a ''bidiagonal'' matrix in the form:

&lt;math&gt;J_i = \begin{bmatrix}\lambda_1 &amp; 1 &amp; &amp; \\
 &amp; \lambda_i &amp; \ddots &amp; \\
&amp;&amp;\ddots &amp;1 \\
&amp;&amp;&amp;\lambda_i\end{bmatrix} \in \mathbb{C}^{n_i\times n_i}&lt;/math&gt;

is called a ''Jordan block'' of size &lt;math&gt;n_i&lt;/math&gt; with eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;.

The matrix &lt;math&gt;T&lt;/math&gt; that induces the JCF is obtained via the generalized eigenvectors&lt;ref name=l12/&gt;[[Cayley-Hamilton theorem]]

==What's the point of JCF?==

JCFis a conceptual tool (and not used in numerical computations). It simplifies systems; JCF decomposes systems into independent 'Jordan block systems' that are easy to inspect. It also facilitates expressions. It's easier to manipulate the resolvent or the exponential of a matrix in the JCF. Another use of JDF is to show that the trajectory of a system stays in the span of generalized eigenvectors &lt;ref name=l12/&gt;. 

The JCF is also used while proving the</text>
      <sha1>o6xx84hr69b2eg44eh2arxf0v0xqtl8</sha1>
    </revision>
    <revision>
      <id>102</id>
      <parentid>101</parentid>
      <timestamp>2018-07-18T00:19:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1374">Jordan Canonical Form (JCF) is a generalization of [[Diagonalization|diagonalization]]. Every square matrix &lt;math&gt;A&lt;/math&gt; has a JCF, even when it's not diagonalizable&lt;ref name=l12&gt;Stephen Boyd, EE263 Lecture Notes 12&lt;/ref&gt;. The JCF of a matrix &lt;math&gt;A&lt;/math&gt; is obtained with a similarity transformation, that is, 

&lt;math&gt;T^{-1}A T = J = \text{diag}(J_1, \dots, J_q),&lt;/math&gt;

where &lt;math&gt;J&lt;/math&gt; is a block-diagonal matrix and each block &lt;math&gt;J_i&lt;/math&gt; is a ''bidiagonal'' matrix in the form:

&lt;math&gt;J_i = \begin{bmatrix}\lambda_1 &amp; 1 &amp; &amp; \\
 &amp; \lambda_i &amp; \ddots &amp; \\
&amp;&amp;\ddots &amp;1 \\
&amp;&amp;&amp;\lambda_i\end{bmatrix} \in \mathbb{C}^{n_i\times n_i}&lt;/math&gt;

is called a ''Jordan block'' of size &lt;math&gt;n_i&lt;/math&gt; with eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;.

The matrix &lt;math&gt;T&lt;/math&gt; that induces the JCF is obtained via the generalized eigenvectors&lt;ref name=l12/&gt;

==What's the point of JCF?==

JCFis a conceptual tool (and not used in numerical computations). It simplifies systems; JCF decomposes systems into independent 'Jordan block systems' that are easy to inspect. It also facilitates expressions. It's easier to manipulate the resolvent or the exponential of a matrix in the JCF. Another use of JDF is to show that the trajectory of a system stays in the span of generalized eigenvectors &lt;ref name=l12/&gt;. 

The JCF is also used while proving the [[Cayley-Hamilton theorem]].</text>
      <sha1>r6hkq2ckumvfur47qowjxw7ooxww5m1</sha1>
    </revision>
    <revision>
      <id>103</id>
      <parentid>102</parentid>
      <timestamp>2018-07-18T00:20:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1390">Jordan Canonical Form (JCF) is a generalization of [[Diagonalization|diagonalization]]. Every square matrix &lt;math&gt;A&lt;/math&gt; has a JCF, even when it's not diagonalizable&lt;ref name=l12&gt;Stephen Boyd, EE263 Lecture Notes 12&lt;/ref&gt;. The JCF of a matrix &lt;math&gt;A&lt;/math&gt; is obtained with a similarity transformation, that is, 

&lt;math&gt;T^{-1}A T = J = \text{diag}(J_1, \dots, J_q),&lt;/math&gt;

where &lt;math&gt;J&lt;/math&gt; is a block-diagonal matrix and each block &lt;math&gt;J_i&lt;/math&gt; is a ''bidiagonal'' matrix in the form:

&lt;math&gt;J_i = \begin{bmatrix}\lambda_1 &amp; 1 &amp; &amp; \\
 &amp; \lambda_i &amp; \ddots &amp; \\
&amp;&amp;\ddots &amp;1 \\
&amp;&amp;&amp;\lambda_i\end{bmatrix} \in \mathbb{C}^{n_i\times n_i}&lt;/math&gt;

is called a ''Jordan block'' of size &lt;math&gt;n_i&lt;/math&gt; with eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;.

The matrix &lt;math&gt;T&lt;/math&gt; that induces the JCF is obtained via the generalized eigenvectors&lt;ref name=l12/&gt;

==What's the point of JCF?==

JCFis a conceptual tool (and not used in numerical computations). It simplifies systems; JCF decomposes systems into independent 'Jordan block systems' that are easy to inspect. It also facilitates expressions. It's easier to manipulate the resolvent or the exponential of a matrix in the JCF. Another use of JDF is to show that the trajectory of a system stays in the span of generalized eigenvectors &lt;ref name=l12/&gt;. 

The JCF is also used while proving the [[Cayley-Hamilton theorem]].

==References==</text>
      <sha1>albubdzj87pcq6nvp7zb9827rodl90r</sha1>
    </revision>
  </page>
  <page>
    <title>Cayley-Hamilton theorem</title>
    <ns>0</ns>
    <id>41</id>
    <revision>
      <id>100</id>
      <timestamp>2018-07-18T00:18:17Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;p(s) = a_0 +a_1 s+\dots +a_k s^k&lt;/math&gt; be a polynomial and let us overload this polynomial function so that as input it can take a square matrix, i.e.:  &lt;math&gt;p(A)..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1009">Let &lt;math&gt;p(s) = a_0 +a_1 s+\dots +a_k s^k&lt;/math&gt; be a polynomial and let us overload this polynomial function so that as input it can take a square matrix, i.e.:

&lt;math&gt;p(A) = a_0 I +a_1 A + \dots +a_k A^k&lt;/math&gt;

The ''''Cayley-Hamilton theorem'''' states that for any &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; we have &lt;math&gt;\mathcal{X}(A) = 0&lt;/math&gt;, where &lt;math&gt;\mathcal{X}(s)&lt;/math&gt; is the [[Characteristic Polynomial|characteristic polynomial]] and &lt;math&gt;\mathcal{X}(A)&lt;/math&gt; is its version overloaded for matrices.

==What's the point of the C-H theorem?==

At least one of the uses of the C-H theorem is the following corollary, which shows that every power of a matrix can be expressed as a linear combination of its first &lt;math&gt;n-1&lt;/math&gt; powers (including &lt;math&gt;0&lt;/math&gt;th power).

''''Corollary'''': for every &lt;math&gt;k \in \mb{Z}_+&lt;/math&gt;, we have:

&lt;math&gt;A^K \in \text{span}\{I, A, A^2, \dots, A^{n-1}\}&lt;/math&gt;.

If &lt;math&gt;A&lt;/math&gt; is invertible, this holds also for &lt;math&gt;k \in \mathbb{Z}&lt;/math&gt;.</text>
      <sha1>85gfqyj393c0cgz75wy1deugbinzqxp</sha1>
    </revision>
    <revision>
      <id>104</id>
      <parentid>100</parentid>
      <timestamp>2018-07-18T00:49:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1009">Let &lt;math&gt;p(s) = a_0 +a_1 s+\dots +a_k s^k&lt;/math&gt; be a polynomial and let us overload this polynomial function so that as input it can take a square matrix, i.e.:

&lt;math&gt;p(A) = a_0 I +a_1 A + \dots +a_k A^k&lt;/math&gt;

The '''Cayley-Hamilton theore'''' states that for any &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; we have &lt;math&gt;\mathcal{X}(A) = 0&lt;/math&gt;, where &lt;math&gt;\mathcal{X}(s)&lt;/math&gt; is the [[Characteristic Polynomial|characteristic polynomial]] and &lt;math&gt;\mathcal{X}(A)&lt;/math&gt; is its version overloaded for matrices.

==What's the point of the C-H theorem?==

At least one of the uses of the C-H theorem is the following corollary, which shows that every power of a matrix can be expressed as a linear combination of its first &lt;math&gt;n-1&lt;/math&gt; powers (including &lt;math&gt;0&lt;/math&gt;th power).

'''Corollary''': for every &lt;math&gt;k \in \mathbb{Z}_+&lt;/math&gt;, we have:

&lt;math&gt;A^K \in \text{span}\{I, A, A^2, \dots, A^{n-1}\}&lt;/math&gt;.

If &lt;math&gt;A&lt;/math&gt; is invertible, this holds also for &lt;math&gt;k \in \mathbb{Z}&lt;/math&gt;.</text>
      <sha1>oibhyuylnvqk6ol53053ge6ghbpbi7b</sha1>
    </revision>
    <revision>
      <id>105</id>
      <parentid>104</parentid>
      <timestamp>2018-07-18T00:49:53Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of the C-H theorem? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1009">Let &lt;math&gt;p(s) = a_0 +a_1 s+\dots +a_k s^k&lt;/math&gt; be a polynomial and let us overload this polynomial function so that as input it can take a square matrix, i.e.:

&lt;math&gt;p(A) = a_0 I +a_1 A + \dots +a_k A^k&lt;/math&gt;

The '''Cayley-Hamilton theore'''' states that for any &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; we have &lt;math&gt;\mathcal{X}(A) = 0&lt;/math&gt;, where &lt;math&gt;\mathcal{X}(s)&lt;/math&gt; is the [[Characteristic Polynomial|characteristic polynomial]] and &lt;math&gt;\mathcal{X}(A)&lt;/math&gt; is its version overloaded for matrices.

==What's the point of the C-H theorem?==

At least one of the uses of the C-H theorem is the following corollary, which shows that every power of a matrix can be expressed as a linear combination of its first &lt;math&gt;n-1&lt;/math&gt; powers (including &lt;math&gt;0&lt;/math&gt;th power).

'''Corollary''': for every &lt;math&gt;k \in \mathbb{Z}_+&lt;/math&gt;, we have:

&lt;math&gt;A^k \in \text{span}\{I, A, A^2, \dots, A^{n-1}\}&lt;/math&gt;.

If &lt;math&gt;A&lt;/math&gt; is invertible, this holds also for &lt;math&gt;k \in \mathbb{Z}&lt;/math&gt;.</text>
      <sha1>lfruyk44cl58vryw5ixo4o49kmucid9</sha1>
    </revision>
    <revision>
      <id>195</id>
      <parentid>105</parentid>
      <timestamp>2018-07-29T01:24:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1371">Let &lt;math&gt;p(s) = a_0 +a_1 s+\dots +a_k s^k&lt;/math&gt; be a polynomial and let us overload this polynomial function so that as input it can take a square matrix, i.e.:

&lt;math&gt;p(A) = a_0 I +a_1 A + \dots +a_k A^k&lt;/math&gt;

The '''Cayley-Hamilton theore'''' states that for any &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; we have &lt;math&gt;\mathcal{X}(A) = 0&lt;/math&gt;, where &lt;math&gt;\mathcal{X}(s)&lt;/math&gt; is the [[Characteristic Polynomial|characteristic polynomial]] and &lt;math&gt;\mathcal{X}(A)&lt;/math&gt; is its version overloaded for matrices.

==What's the point of the C-H theorem?==

At least one of the uses of the C-H theorem is the following corollary, which shows that every power of a matrix can be expressed as a linear combination of its first &lt;math&gt;n-1&lt;/math&gt; powers (including &lt;math&gt;0&lt;/math&gt;th power).

'''Corollary''': for every &lt;math&gt;k \in \mathbb{Z}_+&lt;/math&gt;, we have:

&lt;math&gt;A^k \in \text{span}\{I, A, A^2, \dots, A^{n-1}\}&lt;/math&gt;.

If &lt;math&gt;A&lt;/math&gt; is invertible, this holds also for &lt;math&gt;k \in \mathbb{Z}&lt;/math&gt;.

=== Reachability===
An important implication of this corollary is that if a discrete LDS with a matrix &lt;math&gt;A\in\mathbb{R}^{n\times n}&lt;/math&gt; can reach a certain state &lt;math&gt;x(t)&lt;/math&gt; it can do so in &lt;math&gt;n&lt;/math&gt; time steps; that is, the additional time steps cannot add more reachable points to the system &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 18&lt;/ref&gt;.</text>
      <sha1>0lncm85iy6dgzhwtk1205bq4irf5swq</sha1>
    </revision>
    <revision>
      <id>196</id>
      <parentid>195</parentid>
      <timestamp>2018-07-29T01:24:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1389">Let &lt;math&gt;p(s) = a_0 +a_1 s+\dots +a_k s^k&lt;/math&gt; be a polynomial and let us overload this polynomial function so that as input it can take a square matrix, i.e.:

&lt;math&gt;p(A) = a_0 I +a_1 A + \dots +a_k A^k&lt;/math&gt;

The '''Cayley-Hamilton theore'''' states that for any &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; we have &lt;math&gt;\mathcal{X}(A) = 0&lt;/math&gt;, where &lt;math&gt;\mathcal{X}(s)&lt;/math&gt; is the [[Characteristic Polynomial|characteristic polynomial]] and &lt;math&gt;\mathcal{X}(A)&lt;/math&gt; is its version overloaded for matrices.

==What's the point of the C-H theorem?==

At least one of the uses of the C-H theorem is the following corollary, which shows that every power of a matrix can be expressed as a linear combination of its first &lt;math&gt;n-1&lt;/math&gt; powers (including &lt;math&gt;0&lt;/math&gt;th power).

'''Corollary''': for every &lt;math&gt;k \in \mathbb{Z}_+&lt;/math&gt;, we have:

&lt;math&gt;A^k \in \text{span}\{I, A, A^2, \dots, A^{n-1}\}&lt;/math&gt;.

If &lt;math&gt;A&lt;/math&gt; is invertible, this holds also for &lt;math&gt;k \in \mathbb{Z}&lt;/math&gt;.

=== Reachability===
An important implication of this corollary is that if a discrete LDS with a matrix &lt;math&gt;A\in\mathbb{R}^{n\times n}&lt;/math&gt; can reach a certain state &lt;math&gt;x(t)&lt;/math&gt; it can do so in &lt;math&gt;n&lt;/math&gt; time steps; that is, the additional time steps cannot add more reachable points to the system &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 18&lt;/ref&gt;.

== References ==</text>
      <sha1>g4gp3vleqqwd4dkk5knspu2rz3htzog</sha1>
    </revision>
  </page>
  <page>
    <title>Quadratic form</title>
    <ns>0</ns>
    <id>42</id>
    <revision>
      <id>106</id>
      <timestamp>2018-07-18T12:45:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The most common form induced by a matrix is arguably the linear form, &lt;math&gt;Ax&lt;/math&gt;, which (in general) yields a vector. Another common form induced by a matrix is the ''qua..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="266">The most common form induced by a matrix is arguably the linear form, &lt;math&gt;Ax&lt;/math&gt;, which (in general) yields a vector. Another common form induced by a matrix is the ''quadratic form'':

&lt;math&gt;x^T Ax&lt;/math&gt;.

The quadratic form is a number. There 

&lt;math&gt;&lt;/math&gt;</text>
      <sha1>atl67mdjbwi0p4ce509p7zse5tz3h46</sha1>
    </revision>
    <revision>
      <id>113</id>
      <parentid>106</parentid>
      <timestamp>2018-07-18T13:38:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1731">The most common form induced by a matrix is arguably the linear form, &lt;math&gt;Ax&lt;/math&gt;, which (in general) yields a vector. Another common form induced by a matrix is the ''quadratic form'':

&lt;math&gt;x^T Ax&lt;/math&gt;.

The quadratic form is a number. A major difference between the linear and the quadratic form is that, while &lt;math&gt;Ax = Bx&lt;/math&gt; for all &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt; implies that &lt;math&gt;A=B&lt;/math&gt;, the analog does not hold for quadratic forms. That is &lt;math&gt;x^T A x = x^T B x&lt;/math&gt; for all &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt; does not imply that &lt;math&gt;A = B&lt;/math&gt;. However, if both &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are symmetric, it is true that &lt;math&gt;x^T A x = x^T B x&lt;/math&gt; for all &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt; implies &lt;math&gt;A = B&lt;/math&gt;. Also, for any quadratic form &lt;math&gt;x^T A x&lt;/math&gt;, one can always obtain a quadratic form that is equivalent but induced by a symmetric matrix &lt;math&gt;\frac{1}{2}(A + A^T)&lt;/math&gt;.

===Bounds by eigenvalues===
If a matrix &lt;math&gt;A&lt;/math&gt; is symmetric, then the smallest and largest values of its quadratic form are bounded by the eigenvalues. That is, if &lt;math&gt;A = A^T&lt;/math&gt;&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;, then

&lt;math&gt;\lambda_{\min} ||x||{}^2 \le  x^T A x \le \lambda_{\max} ||x||{}^{2} &lt;/math&gt;

Those inequalities are ''tight''; the equalities on both sides hold if we replace &lt;math&gt;x&lt;/math&gt; with corresponding eigenvectors. That is, &lt;math&gt;q_{\min}^T A q_{\min} = \lambda_{\min} ||q_1||{}^2&lt;/math&gt; and &lt;math&gt;q_{\max}^T A q = \lambda_{\max} ||q_{\max}||{}^2&lt;/math&gt; where &lt;math&gt;q_{\min}&lt;/math&gt; and &lt;math&gt;q_{\max}&lt;/math&gt; are eigenvectors associated with the eigenvalues &lt;math&gt;\lambda_{\min}&lt;/math&gt; and &lt;math&gt;\lambda_{\max}&lt;/math&gt;, respectively.

==References==</text>
      <sha1>mkaoxgvd2vry3umbmru9390bs9ufcro</sha1>
    </revision>
  </page>
  <page>
    <title>Positive definite matrix</title>
    <ns>0</ns>
    <id>43</id>
    <revision>
      <id>107</id>
      <timestamp>2018-07-18T12:49:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A ''positive definite'' matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is whose [[Quadratic form|quadratic form]] &lt;math&gt;x^T A x&lt;/math&gt; takes strictly positive values for all..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="355">A ''positive definite'' matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is whose [[Quadratic form|quadratic form]] &lt;math&gt;x^T A x&lt;/math&gt; takes strictly positive values for all &lt;math&gt;x\in\mathbb{R}^n&lt;/math&gt;, i.e., &lt;math&gt;x^T A x &gt; 0&lt;/math&gt;.

A matrix is called ''positive semidefinite'' if &lt;math&gt;x^T A x \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt;.</text>
      <sha1>04offrgqm1atao3n6wb6cnspaqtiglh</sha1>
    </revision>
    <revision>
      <id>110</id>
      <parentid>107</parentid>
      <timestamp>2018-07-18T13:12:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="862">A ''positive definite'' matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is whose [[Quadratic form|quadratic form]] &lt;math&gt;x^T A x&lt;/math&gt; takes strictly positive values for all &lt;math&gt;x\in\mathbb{R}^n&lt;/math&gt;, i.e., &lt;math&gt;x^T A x &gt; 0&lt;/math&gt;.

A matrix is called ''positive semidefinite'' if &lt;math&gt;x^T A x \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt;.

Positive definiteness is represented more succintly via [[Matrix inequality|matrix inequality]]; that is, if a matrix &lt;math&gt;A&lt;/math&gt; is positive definite we denote this with &lt;math&gt;A&gt;0&lt;/math&gt;. In a similar manner, we have the following definitions through [[Matrix inequality|matrix inequality]]; we say that

* &lt;math&gt;A&lt;/math&gt; ''positive semidefinite'' if &lt;math&gt;A\ge 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; ''negative definite'' if &lt;math&gt;-A &gt; 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; ''negative semidefinite'' if &lt;math&gt; -A \ge 0&lt;/math&gt;</text>
      <sha1>jbk3sfeiq4sxb8ezgzutvaxo1t5gxw0</sha1>
    </revision>
    <revision>
      <id>112</id>
      <parentid>110</parentid>
      <timestamp>2018-07-18T13:28:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1685">A ''positive definite'' matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is whose [[Quadratic form|quadratic form]] &lt;math&gt;x^T A x&lt;/math&gt; takes strictly positive values for all &lt;math&gt;x\in\mathbb{R}^n&lt;/math&gt;, i.e., &lt;math&gt;x^T A x &gt; 0&lt;/math&gt;.

A matrix is called ''positive semidefinite'' if &lt;math&gt;x^T A x \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt;.

Positive definiteness is represented more succintly via [[Matrix inequality|matrix inequality]]; that is, if a matrix &lt;math&gt;A&lt;/math&gt; is positive definite we denote this with &lt;math&gt;A&gt;0&lt;/math&gt;. In a similar manner, we have the following definitions through [[Matrix inequality|matrix inequality]]; we say that

* &lt;math&gt;A&lt;/math&gt; ''positive semidefinite'' if &lt;math&gt;A\ge 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; ''negative definite'' if &lt;math&gt;-A &gt; 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; ''negative semidefinite'' if &lt;math&gt; -A \ge 0&lt;/math&gt;

===Symmetric positive semidefinite matrices===
When talking about positive definite matrices, it is customary to imply that the matrix is also ''symmetric''. This is not without reason; first of all, even if &lt;math&gt;A&lt;/math&gt; is not symmetric, one can always obtain an equivalent quadratic form that is induced by a symmetric matrix (see [[Quadratic form]]). Secondly, symmetric matrices have the nice property that their positive/negative (semi)definiteness can be verified through the eigenvalues&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* If &lt;math&gt;A = A^T&lt;/math&gt;, then &lt;math&gt;A&lt;/math&gt; is positive definite (semidefinite) iff &lt;math&gt;\lambda_{\text{min}}(A) &gt; 0&lt;/math&gt; (&lt;math&gt;\lambda_{\text{min}(A)} \ge 0&lt;/math&gt;) where &lt;math&gt;\lambda_{\text{min}}(A)&lt;/math&gt; is the smallest eigenvalue of &lt;math&gt;A&lt;/math&gt;

==References==</text>
      <sha1>lwkmd3lup9x065zjrk2dnp9bjfs5ydh</sha1>
    </revision>
    <revision>
      <id>114</id>
      <parentid>112</parentid>
      <timestamp>2018-07-18T13:39:22Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1843">A ''positive definite'' matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is whose [[Quadratic form|quadratic form]] &lt;math&gt;x^T A x&lt;/math&gt; takes strictly positive values for all &lt;math&gt;x\in\mathbb{R}^n&lt;/math&gt;, i.e., &lt;math&gt;x^T A x &gt; 0&lt;/math&gt;.

A matrix is called ''positive semidefinite'' if &lt;math&gt;x^T A x \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt;.

Positive definiteness is represented more succintly via [[Matrix inequality|matrix inequality]]; that is, if a matrix &lt;math&gt;A&lt;/math&gt; is positive definite we denote this with &lt;math&gt;A&gt;0&lt;/math&gt;. In a similar manner, we have the following definitions through [[Matrix inequality|matrix inequality]]; we say that

* &lt;math&gt;A&lt;/math&gt; ''positive semidefinite'' if &lt;math&gt;A\ge 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; ''negative definite'' if &lt;math&gt;-A &gt; 0&lt;/math&gt;
* &lt;math&gt;A&lt;/math&gt; ''negative semidefinite'' if &lt;math&gt; -A \ge 0&lt;/math&gt;

===Symmetric positive semidefinite matrices===
When talking about positive definite matrices, it is customary to imply that the matrix is also ''symmetric''. This is not without reason; first of all, even if &lt;math&gt;A&lt;/math&gt; is not symmetric, one can always obtain an equivalent quadratic form that is induced by a symmetric matrix (see [[Quadratic form]]). Secondly, symmetric matrices have the nice property that their positive/negative (semi)definiteness can be verified through the eigenvalues&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* If &lt;math&gt;A = A^T&lt;/math&gt;, then &lt;math&gt;A&lt;/math&gt; is positive definite (semidefinite) iff &lt;math&gt;\lambda_{\text{min}}(A) &gt; 0&lt;/math&gt; (&lt;math&gt;\lambda_{\text{min}(A)} \ge 0&lt;/math&gt;) where &lt;math&gt;\lambda_{\text{min}}(A)&lt;/math&gt; is the smallest eigenvalue of &lt;math&gt;A&lt;/math&gt;

A related property is that for symmetric matrices, the minimum and maximum bound of their quadratic form is bounded by eigenvalues (see [[Quadratic form]]).

==References==</text>
      <sha1>4qp14x9pv2vwxvjxgmmhfl2gm2lega3</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix inequality</title>
    <ns>0</ns>
    <id>44</id>
    <revision>
      <id>108</id>
      <timestamp>2018-07-18T12:52:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "What is typically meant with matrix inequality is &lt;math&gt;A\ge B&lt;/math&gt; is whether the matrix difference &lt;math&gt;A-B&lt;/math&gt; is [[Positive definite matrix|positive definite]], i.e...."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="198">What is typically meant with matrix inequality is &lt;math&gt;A\ge B&lt;/math&gt; is whether the matrix difference &lt;math&gt;A-B&lt;/math&gt; is [[Positive definite matrix|positive definite]], i.e., &lt;math&gt;A-B\ge 0&lt;/math&gt;</text>
      <sha1>dvv3peurf7nl5qj91i56py35ynhuq7a</sha1>
    </revision>
    <revision>
      <id>109</id>
      <parentid>108</parentid>
      <timestamp>2018-07-18T13:04:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1269">What is typically meant with matrix inequality is &lt;math&gt;A\ge B&lt;/math&gt; is whether the matrix difference &lt;math&gt;A-B&lt;/math&gt; is [[Positive definite matrix|positive definite]], i.e., &lt;math&gt;A-B\ge 0&lt;/math&gt;.
===Properties===
Matrix inequality is a particularly useful concept because it generalizes a number of the properties of regular inequalities&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:
* If &lt;math&gt;A\ge B&lt;/math&gt; and &lt;math&gt;C\ge D&lt;/math&gt;, then &lt;math&gt;A+C \ge B+D&lt;/math&gt;
* If &lt;math&gt;B \le 0&lt;/math&gt;, then &lt;math&gt;A+B \le A&lt;/math&gt;
* If &lt;math&gt;A \ge 0&lt;/math&gt; and &lt;math&gt;\alpha \ge 0&lt;/math&gt;, then &lt;math&gt;\alpha A \ge 0&lt;/math&gt;
* &lt;math&gt;A^2 \ge 0&lt;/math&gt;
* If &lt;math&gt;A&gt;0&lt;/math&gt;, then &lt;math&gt;A^{-1} \ge 0&lt;/math&gt;

A major difference between the regular inequality and the matrix inequality is that ''not all matrices are comparable''. That is, while for any two numbers at least one of the inequalities &lt;math&gt;a\ge b&lt;/math&gt; or &lt;math&gt;a\le b&lt;/math&gt; has to hold, this is not the case for matrix inequalities. We can have &lt;math&gt;A \nleq B&lt;/math&gt; or &lt;math&gt;A \ngeq B&lt;/math&gt;. This makes sense, the quadratic form &lt;math&gt;x^T A x&lt;/math&gt; can be larger than &lt;math&gt;x^T B x&lt;/math&gt; for some values of &lt;math&gt;x&lt;/math&gt; and smaller than &lt;math&gt;x^T B x&lt;/math&gt; for other values of &lt;math&gt;x&lt;/math&gt;.

==References==</text>
      <sha1>duklyzx49ugtnulyhn2rcyjvqj51hxz</sha1>
    </revision>
    <revision>
      <id>111</id>
      <parentid>109</parentid>
      <timestamp>2018-07-18T13:20:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1269">What is typically meant with matrix inequality is &lt;math&gt;A\ge B&lt;/math&gt; is whether the matrix difference &lt;math&gt;A-B&lt;/math&gt; is [[Positive definite matrix|positive definite]], i.e., &lt;math&gt;A-B\ge 0&lt;/math&gt;.
===Properties===
Matrix inequality is a particularly useful concept because it generalizes a number of the properties of regular inequalities&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:
* If &lt;math&gt;A\ge B&lt;/math&gt; and &lt;math&gt;C\ge D&lt;/math&gt;, then &lt;math&gt;A+C \ge B+D&lt;/math&gt;
* If &lt;math&gt;B \le 0&lt;/math&gt;, then &lt;math&gt;A+B \le A&lt;/math&gt;
* If &lt;math&gt;A \ge 0&lt;/math&gt; and &lt;math&gt;\alpha \ge 0&lt;/math&gt;, then &lt;math&gt;\alpha A \ge 0&lt;/math&gt;
* &lt;math&gt;A^2 \ge 0&lt;/math&gt;
* If &lt;math&gt;A&gt;0&lt;/math&gt;, then &lt;math&gt;A^{-1} \ge 0&lt;/math&gt;

A major difference between the regular inequality and the matrix inequality is that ''not all matrices are comparable''. That is, while for any two numbers at least one of the inequalities &lt;math&gt;a\ge b&lt;/math&gt; or &lt;math&gt;a\le b&lt;/math&gt; has to hold, this is not the case for matrix inequalities. We can have &lt;math&gt;A \nleq B&lt;/math&gt; or &lt;math&gt;A \ngeq B&lt;/math&gt;. This makes sense: the quadratic form &lt;math&gt;x^T A x&lt;/math&gt; can be larger than &lt;math&gt;x^T B x&lt;/math&gt; for some values of &lt;math&gt;x&lt;/math&gt; and smaller than &lt;math&gt;x^T B x&lt;/math&gt; for other values of &lt;math&gt;x&lt;/math&gt;.

==References==</text>
      <sha1>qs8ogkqbqpaz0p6hduiwhfqaersufjr</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix gain in a direction</title>
    <ns>0</ns>
    <id>45</id>
    <revision>
      <id>116</id>
      <timestamp>2018-07-18T14:30:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The gain of a matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; in a direction &lt;math&gt;x&lt;/math&gt; is defined as the ratio:  &lt;math&gt;\frac{||Ax||}{||x||}&lt;/math&gt;   ==What's the point o..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="557">The gain of a matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; in a direction &lt;math&gt;x&lt;/math&gt; is defined as the ratio:

&lt;math&gt;\frac{||Ax||}{||x||}&lt;/math&gt;


==What's the point of Matrix gain?==

One of the most important applications of matrix gain is to tell us what's the direction of highest gain. In fact, this is defined as the [[Matrix norm|matrix norm]]. In words, the matrix norm tells us how large we can make &lt;math&gt;||Au||&lt;/math&gt; for a unit vector &lt;math&gt;||u||&lt;/math&gt;. This, for example, would be the the principal component in the context of [[PCA]].</text>
      <sha1>bjwxydskpbr0imqmfmhst4dcjbgprcz</sha1>
    </revision>
    <revision>
      <id>117</id>
      <parentid>116</parentid>
      <timestamp>2018-07-18T14:30:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="573">The gain of a matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; in a direction &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt; is defined as the ratio:

&lt;math&gt;\frac{||Ax||}{||x||}&lt;/math&gt;

==What's the point of Matrix gain?==

One of the most important applications of matrix gain is to tell us what's the direction of highest gain. In fact, this is defined as the [[Matrix norm|matrix norm]]. In words, the matrix norm tells us how large we can make &lt;math&gt;||Au||&lt;/math&gt; for a unit vector &lt;math&gt;||u||&lt;/math&gt;. This, for example, would be the the principal component in the context of [[PCA]].</text>
      <sha1>8p1wxcmnz5574s6sznt1t0813oi4l3q</sha1>
    </revision>
    <revision>
      <id>147</id>
      <parentid>117</parentid>
      <timestamp>2018-07-27T12:58:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1206">The gain of a matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; in a direction &lt;math&gt;x \in \mathbb{R}^n&lt;/math&gt; is defined as the ratio:

&lt;math&gt;\frac{||Ax||}{||x||}&lt;/math&gt;

===Interpretation as a generalization===
An interesting interpretation of matrix gain &lt;math&gt;y=Ax&lt;/math&gt; comes from approaching it as a generalization of the scalar case &lt;math&gt;y=ax&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 Lecture 17 Video (beginning of video)&lt;/ref&gt;: One can interpret the multiplication &lt;math&gt;ax&lt;/math&gt; as how much &lt;math&gt;x&lt;/math&gt; increases by multiplication with &lt;math&gt;a&lt;/math&gt;. Similarly, matrix gain aims to find out how much &lt;math&gt;x&lt;/math&gt; increases by multiplication with &lt;math&gt;A&lt;/math&gt;. However, &lt;math&gt;y=Ax&lt;/math&gt; is much more interesting because now the gain depends on the direction of &lt;math&gt;x&lt;/math&gt;.

==What's the point of Matrix gain?==

One of the most important applications of matrix gain is to tell us what's the direction of highest gain. In fact, this is defined as the [[Matrix norm|matrix norm]]. In words, the matrix norm tells us how large we can make &lt;math&gt;||Au||&lt;/math&gt; for a unit vector &lt;math&gt;||u||&lt;/math&gt;. This, for example, would be the the principal component in the context of [[PCA]].

==References==</text>
      <sha1>jshuctc21z8dwb97tvs5x40hgmrolay</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix norm</title>
    <ns>0</ns>
    <id>46</id>
    <revision>
      <id>118</id>
      <timestamp>2018-07-18T14:33:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Arguably, the most common norm for a matrix is the ''spectral norm'', which is defined for a matrix &lt;math&gt;A \in \mathbb{R}^{n\times m}&lt;/math&gt; as&lt;ref&gt;Stephen Boyd, EE263 Lectur..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="983">Arguably, the most common norm for a matrix is the ''spectral norm'', which is defined for a matrix &lt;math&gt;A \in \mathbb{R}^{n\times m}&lt;/math&gt; as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;

&lt;math&gt;||A||:= \max_{x\neq 0}\frac{||Ax||}{||x||}&lt;/math&gt;.

Using the upper bound on a quadratic form of a symmetric matrix (see [[Quadratic form]]), it is easy to show that &lt;math&gt;||A|| = \sqrt{\lambda_{\max}(A^TA)}&lt;/math&gt;, where &lt;math&gt;\lambda_{\max}(A^T A)&lt;/math&gt; is the largest eigenvalue of the (symmetric) matrix &lt;math&gt;A^T A&lt;/math&gt;:

&lt;math&gt;||A||{}^2 = \max_{x\neq 0}\frac{||Ax||{}^2}{||x||{}^2} = \max_{x\neq 0}\frac{x^T A^T A x}{||x||{}^2} = \lambda_{\max}(A^T A) &lt;/math&gt;.

==Intuitive explanation==
The matrix norm is the matrix norm tells us how "large" a matrix is. This is understood intuitively when matrix norm is thought as the (vector) norm &lt;math&gt;||Au||&lt;/math&gt; for the unit vector &lt;math&gt;u&lt;/math&gt; that maximizes the [[Matrix gain in a direction|gain of a matrix]].

==References==</text>
      <sha1>c05yvka6ui29m77zes8vsabx7csnjdu</sha1>
    </revision>
    <revision>
      <id>121</id>
      <parentid>118</parentid>
      <timestamp>2018-07-20T13:29:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2056">Arguably, the most common norm for a matrix is the ''spectral norm'', which is defined for a matrix &lt;math&gt;A \in \mathbb{R}^{n\times m}&lt;/math&gt; as&lt;ref name=l15&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;

&lt;math&gt;||A||:= \max_{x\neq 0}\frac{||Ax||}{||x||}&lt;/math&gt;.

Using the upper bound on a quadratic form of a symmetric matrix (see [[Quadratic form]]), it is easy to show that &lt;math&gt;||A|| = \sqrt{\lambda_{\max}(A^TA)}&lt;/math&gt;, where &lt;math&gt;\lambda_{\max}(A^T A)&lt;/math&gt; is the largest eigenvalue of the (symmetric) matrix &lt;math&gt;A^T A&lt;/math&gt;:

&lt;math&gt;||A||{}^2 = \max_{x\neq 0}\frac{||Ax||{}^2}{||x||{}^2} = \max_{x\neq 0}\frac{x^T A^T A x}{||x||{}^2} = \lambda_{\max}(A^T A) &lt;/math&gt;.

=== Matrix norm as a generalization ===
Matrix norm is a generalization of concept of the norm for vectors. Technically, the norm &lt;math&gt;||A||&lt;/math&gt; of a vector is a different operator from the norm of a vector &lt;math&gt;||v||&lt;/math&gt;. Fortunately, when the matrix &lt;math&gt;A&lt;/math&gt; is actually a (column) vector, the two definitions coincide. Let &lt;math&gt;a \in \mathbb{R}^n&lt;/math&gt;, and let us compute the ''matrix'' norm on &lt;math&gt;a&lt;/math&gt;:

&lt;math&gt;||a|| = \sqrt{\lambda_{\max}(a^T a)} = sqrt{a^T a}&lt;/math&gt;

Clearly, the rightmost quantity is the L2 (vector) norm of &lt;math&gt;a&lt;/math&gt;. The first property listed below contains both matrix and vector norms.

=== Properties of Matrix Norm ===
Being a generalization of the vector norm, matrix norm possesses a lot of properties similar to that of the vector norm, namely&lt;ref name=l15/&gt;:

* for any &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;||Ax|| \le ||A|| ||x||&lt;/math&gt;
* scaling ||aA|| = |a| ||A||
* triangle inequality: &lt;math&gt;||A+B||&lt;/math&gt;
* Definiteness: &lt;math&gt;||A|| = 0 \iff A = 0&lt;/math&gt;
* norm of product: &lt;math&gt;||AB|| \le ||A|| ||B||&lt;/math&gt;

==Intuitive explanation==
The matrix norm is the matrix norm tells us how "large" a matrix is. This is understood intuitively when matrix norm is thought as the (vector) norm &lt;math&gt;||Au||&lt;/math&gt; for the unit vector &lt;math&gt;u&lt;/math&gt; that maximizes the [[Matrix gain in a direction|gain of a matrix]].

==References==</text>
      <sha1>k7ap7293xhqgcccujrrakc9rmv25mze</sha1>
    </revision>
    <revision>
      <id>122</id>
      <parentid>121</parentid>
      <timestamp>2018-07-20T13:29:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Properties of Matrix Norm */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2069">Arguably, the most common norm for a matrix is the ''spectral norm'', which is defined for a matrix &lt;math&gt;A \in \mathbb{R}^{n\times m}&lt;/math&gt; as&lt;ref name=l15&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;

&lt;math&gt;||A||:= \max_{x\neq 0}\frac{||Ax||}{||x||}&lt;/math&gt;.

Using the upper bound on a quadratic form of a symmetric matrix (see [[Quadratic form]]), it is easy to show that &lt;math&gt;||A|| = \sqrt{\lambda_{\max}(A^TA)}&lt;/math&gt;, where &lt;math&gt;\lambda_{\max}(A^T A)&lt;/math&gt; is the largest eigenvalue of the (symmetric) matrix &lt;math&gt;A^T A&lt;/math&gt;:

&lt;math&gt;||A||{}^2 = \max_{x\neq 0}\frac{||Ax||{}^2}{||x||{}^2} = \max_{x\neq 0}\frac{x^T A^T A x}{||x||{}^2} = \lambda_{\max}(A^T A) &lt;/math&gt;.

=== Matrix norm as a generalization ===
Matrix norm is a generalization of concept of the norm for vectors. Technically, the norm &lt;math&gt;||A||&lt;/math&gt; of a vector is a different operator from the norm of a vector &lt;math&gt;||v||&lt;/math&gt;. Fortunately, when the matrix &lt;math&gt;A&lt;/math&gt; is actually a (column) vector, the two definitions coincide. Let &lt;math&gt;a \in \mathbb{R}^n&lt;/math&gt;, and let us compute the ''matrix'' norm on &lt;math&gt;a&lt;/math&gt;:

&lt;math&gt;||a|| = \sqrt{\lambda_{\max}(a^T a)} = sqrt{a^T a}&lt;/math&gt;

Clearly, the rightmost quantity is the L2 (vector) norm of &lt;math&gt;a&lt;/math&gt;. The first property listed below contains both matrix and vector norms.

=== Properties of Matrix Norm ===
Being a generalization of the vector norm, matrix norm possesses a lot of properties similar to that of the vector norm, namely&lt;ref name=l15/&gt;:

* for any &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;||Ax|| \le ||A|| ||x||&lt;/math&gt;
* scaling &lt;math&gt;||aA|| = |a| ||A||&lt;/math&gt;
* triangle inequality: &lt;math&gt;||A+B||&lt;/math&gt;
* Definiteness: &lt;math&gt;||A|| = 0 \iff A = 0&lt;/math&gt;
* norm of product: &lt;math&gt;||AB|| \le ||A|| ||B||&lt;/math&gt;

==Intuitive explanation==
The matrix norm is the matrix norm tells us how "large" a matrix is. This is understood intuitively when matrix norm is thought as the (vector) norm &lt;math&gt;||Au||&lt;/math&gt; for the unit vector &lt;math&gt;u&lt;/math&gt; that maximizes the [[Matrix gain in a direction|gain of a matrix]].

==References==</text>
      <sha1>1zo85ruadg042hg0ezv2eoiuh8s9v7o</sha1>
    </revision>
    <revision>
      <id>199</id>
      <parentid>122</parentid>
      <timestamp>2018-09-10T15:14:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Matrix norm as a generalization */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2070">Arguably, the most common norm for a matrix is the ''spectral norm'', which is defined for a matrix &lt;math&gt;A \in \mathbb{R}^{n\times m}&lt;/math&gt; as&lt;ref name=l15&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;

&lt;math&gt;||A||:= \max_{x\neq 0}\frac{||Ax||}{||x||}&lt;/math&gt;.

Using the upper bound on a quadratic form of a symmetric matrix (see [[Quadratic form]]), it is easy to show that &lt;math&gt;||A|| = \sqrt{\lambda_{\max}(A^TA)}&lt;/math&gt;, where &lt;math&gt;\lambda_{\max}(A^T A)&lt;/math&gt; is the largest eigenvalue of the (symmetric) matrix &lt;math&gt;A^T A&lt;/math&gt;:

&lt;math&gt;||A||{}^2 = \max_{x\neq 0}\frac{||Ax||{}^2}{||x||{}^2} = \max_{x\neq 0}\frac{x^T A^T A x}{||x||{}^2} = \lambda_{\max}(A^T A) &lt;/math&gt;.

=== Matrix norm as a generalization ===
Matrix norm is a generalization of concept of the norm for vectors. Technically, the norm &lt;math&gt;||A||&lt;/math&gt; of a vector is a different operator from the norm of a vector &lt;math&gt;||v||&lt;/math&gt;. Fortunately, when the matrix &lt;math&gt;A&lt;/math&gt; is actually a (column) vector, the two definitions coincide. Let &lt;math&gt;a \in \mathbb{R}^n&lt;/math&gt;, and let us compute the ''matrix'' norm on &lt;math&gt;a&lt;/math&gt;:

&lt;math&gt;||a|| = \sqrt{\lambda_{\max}(a^T a)} = \sqrt{a^T a}&lt;/math&gt;

Clearly, the rightmost quantity is the L2 (vector) norm of &lt;math&gt;a&lt;/math&gt;. The first property listed below contains both matrix and vector norms.

=== Properties of Matrix Norm ===
Being a generalization of the vector norm, matrix norm possesses a lot of properties similar to that of the vector norm, namely&lt;ref name=l15/&gt;:

* for any &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;||Ax|| \le ||A|| ||x||&lt;/math&gt;
* scaling &lt;math&gt;||aA|| = |a| ||A||&lt;/math&gt;
* triangle inequality: &lt;math&gt;||A+B||&lt;/math&gt;
* Definiteness: &lt;math&gt;||A|| = 0 \iff A = 0&lt;/math&gt;
* norm of product: &lt;math&gt;||AB|| \le ||A|| ||B||&lt;/math&gt;

==Intuitive explanation==
The matrix norm is the matrix norm tells us how "large" a matrix is. This is understood intuitively when matrix norm is thought as the (vector) norm &lt;math&gt;||Au||&lt;/math&gt; for the unit vector &lt;math&gt;u&lt;/math&gt; that maximizes the [[Matrix gain in a direction|gain of a matrix]].

==References==</text>
      <sha1>9z8v89dlpjjlmkrjye8m9tkm4l9zmwz</sha1>
    </revision>
  </page>
  <page>
    <title>Symmetric matrices</title>
    <ns>0</ns>
    <id>47</id>
    <revision>
      <id>125</id>
      <timestamp>2018-07-20T13:52:54Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.  ===Properties of Symmetric Matrices== * The eigenvalues of a symmetric..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1255">A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.

===Properties of Symmetric Matrices==
* The eigenvalues of a symmetric matrix are real.
* There is a set of orthonormal eigenvectors of &lt;math&gt;A&lt;/math&gt;, i.e., &lt;math&gt;\exists q_1, \dots, q_n &lt;/math&gt; s.t. &lt;math&gt;A q_i = \lambda_i q_i&lt;/math&gt; and &lt;math&gt;q_i^t q_j = \delta_{ij}&lt;/math&gt; where &lt;math&gt;\delta_{ij}&lt;/math&gt; is Kronecker's delta. In matrix form, there exists a matrix &lt;math&gt;Q&lt;/math&gt; s.t.:

&lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;

==What's the point of Symmetric matrices?==
Symmetric matrices are worth studying for a number of reasons. Firstly, when working with quadratic forms: one can always find a symmetric equivalent of a quadratic form of a matrix (see [[Quadratic form]]), and it's advantageous to work on the symmetric one because it's easy to find the minimum and maximum of the quadratic form via eigenvalues (see [[Quadratic form]]). A related property is that the positive definiteness of a symmetric matrix can be determined from its eigenvalues (see [[Positive definite matrix]]).

The property listed above (&lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;) is also a very plausible one for symmetric matrices as it leads to a very intuitive decomposition.</text>
      <sha1>f7ycwdbvasb92asq9tpxt3rye3ghu3r</sha1>
    </revision>
    <revision>
      <id>128</id>
      <parentid>125</parentid>
      <timestamp>2018-07-20T14:37:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* =Properties of Symmetric Matrices */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2445">A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.

===Properties of Symmetric Matrices===
* The eigenvalues of a symmetric matrix are real.
* There is a set of orthonormal eigenvectors of &lt;math&gt;A&lt;/math&gt;, i.e., &lt;math&gt;\exists q_1, \dots, q_n &lt;/math&gt; s.t. &lt;math&gt;A q_i = \lambda_i q_i&lt;/math&gt; and &lt;math&gt;q_i^t q_j = \delta_{ij}&lt;/math&gt; where &lt;math&gt;\delta_{ij}&lt;/math&gt; is Kronecker's delta. In matrix form, there exists a matrix &lt;math&gt;Q&lt;/math&gt; s.t. &lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;, and therefore &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt;, where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues of &lt;math&gt;A&lt;/math&gt;.

==== Interpretation ====

The last property above is worth investigating further. Saying that &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt; is also saying that one can obtain &lt;math&gt;y = Ax&lt;/math&gt; via the following block diagram 

[[File:Sym decomposition1.png|frameless]]


This block diagram says that &lt;math&gt;x&lt;/math&gt; is first rotated by &lt;math&gt;Q^T&lt;/math&gt;, then each component in the rotated coordinate system is by its corresponding eigenvalue (i.e., via 'dilation' &lt;math&gt;\Lambda&lt;/math&gt;), and then the vectors are rotated back by &lt;math&gt;Q&lt;/math&gt;.

Another interpretation is a linear combination of 1-dimensional projections:

&lt;math&gt;A = \sum_{i=1}^n \lambda_i q_i q_i^T&lt;/math&gt;

The interpretation of the  latter is illustrated in the example figure below:

[[File:Sym decomposition2.png|frameless]]

That is, the matrix-vector multiplication &lt;math&gt;Ax&lt;/math&gt; can be seen as:

# Projecting x into its &lt;math&gt;q_i q_i^T&lt;/math&gt; components
# Scaling each component by the corresponding eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;
# Adding the scaled components together to obtain &lt;math&gt;Ax&lt;/math&gt;

==What's the point of Symmetric matrices?==
Symmetric matrices are worth studying for a number of reasons. Firstly, when working with quadratic forms: one can always find a symmetric equivalent of a quadratic form of a matrix (see [[Quadratic form]]), and it's advantageous to work on the symmetric one because it's easy to find the minimum and maximum of the quadratic form via eigenvalues (see [[Quadratic form]]). A related property is that the positive definiteness of a symmetric matrix can be determined from its eigenvalues (see [[Positive definite matrix]]).

The property listed above (&lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;) is also a very plausible one for symmetric matrices as it leads to a very intuitive decomposition.</text>
      <sha1>e8qau7rtlfw67t6rpmwp4er5e7qkjhj</sha1>
    </revision>
    <revision>
      <id>130</id>
      <parentid>128</parentid>
      <timestamp>2018-07-20T14:53:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Interpretation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2426">A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.

===Properties of Symmetric Matrices===
* The eigenvalues of a symmetric matrix are real.
* There is a set of orthonormal eigenvectors of &lt;math&gt;A&lt;/math&gt;, i.e., &lt;math&gt;\exists q_1, \dots, q_n &lt;/math&gt; s.t. &lt;math&gt;A q_i = \lambda_i q_i&lt;/math&gt; and &lt;math&gt;q_i^t q_j = \delta_{ij}&lt;/math&gt; where &lt;math&gt;\delta_{ij}&lt;/math&gt; is Kronecker's delta. In matrix form, there exists a matrix &lt;math&gt;Q&lt;/math&gt; s.t. &lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;, and therefore &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt;, where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues of &lt;math&gt;A&lt;/math&gt;.

==== Two Interpretations ====

The last property above is worth investigating further. Saying that &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt; is also saying that one can obtain &lt;math&gt;y = Ax&lt;/math&gt; via the following block diagram 

[[File:Sym decomposition1.png|frameless]]

This block diagram leads to the following interpretation for the linear mapping &lt;math&gt;Ax&lt;/math&gt;:
# Rotate &lt;math&gt;x&lt;/math&gt; by &lt;math&gt;Q^T&lt;/math&gt;
# Diagonal real scale ('dilation') by &lt;math&gt;\Lambda&lt;/math&gt;
# Rotate back by &lt;math&gt;Q&lt;/math&gt;.

Another interpretation is a linear combination of 1-dimensional projections:

&lt;math&gt;A = \sum_{i=1}^n \lambda_i q_i q_i^T&lt;/math&gt;

The interpretation of the latter is illustrated in the example figure below:

[[File:Sym decomposition2.png|frameless]]

According to this interpretation, the matrix-vector multiplication &lt;math&gt;Ax&lt;/math&gt; can be seen as:

# Resolving &lt;math&gt;x&lt;/math&gt; into its &lt;math&gt;q_i q_i^T&lt;/math&gt; components
# Scaling each component by the corresponding eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;
# Reconstituting with basis &lt;math&gt;q_i&lt;/math&gt;.

==What's the point of Symmetric matrices?==
Symmetric matrices are worth studying for a number of reasons. Firstly, when working with quadratic forms: one can always find a symmetric equivalent of a quadratic form of a matrix (see [[Quadratic form]]), and it's advantageous to work on the symmetric one because it's easy to find the minimum and maximum of the quadratic form via eigenvalues (see [[Quadratic form]]). A related property is that the positive definiteness of a symmetric matrix can be determined from its eigenvalues (see [[Positive definite matrix]]).

The property listed above (&lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;) is also a very plausible one for symmetric matrices as it leads to a very intuitive decomposition.</text>
      <sha1>mhxmjnw9z8pztre5pm6eaz1xae7sb3q</sha1>
    </revision>
    <revision>
      <id>131</id>
      <parentid>130</parentid>
      <timestamp>2018-07-20T14:54:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of Symmetric matrices? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2423">A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.

===Properties of Symmetric Matrices===
* The eigenvalues of a symmetric matrix are real.
* There is a set of orthonormal eigenvectors of &lt;math&gt;A&lt;/math&gt;, i.e., &lt;math&gt;\exists q_1, \dots, q_n &lt;/math&gt; s.t. &lt;math&gt;A q_i = \lambda_i q_i&lt;/math&gt; and &lt;math&gt;q_i^t q_j = \delta_{ij}&lt;/math&gt; where &lt;math&gt;\delta_{ij}&lt;/math&gt; is Kronecker's delta. In matrix form, there exists a matrix &lt;math&gt;Q&lt;/math&gt; s.t. &lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;, and therefore &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt;, where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues of &lt;math&gt;A&lt;/math&gt;.

==== Two Interpretations ====

The last property above is worth investigating further. Saying that &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt; is also saying that one can obtain &lt;math&gt;y = Ax&lt;/math&gt; via the following block diagram 

[[File:Sym decomposition1.png|frameless]]

This block diagram leads to the following interpretation for the linear mapping &lt;math&gt;Ax&lt;/math&gt;:
# Rotate &lt;math&gt;x&lt;/math&gt; by &lt;math&gt;Q^T&lt;/math&gt;
# Diagonal real scale ('dilation') by &lt;math&gt;\Lambda&lt;/math&gt;
# Rotate back by &lt;math&gt;Q&lt;/math&gt;.

Another interpretation is a linear combination of 1-dimensional projections:

&lt;math&gt;A = \sum_{i=1}^n \lambda_i q_i q_i^T&lt;/math&gt;

The interpretation of the latter is illustrated in the example figure below:

[[File:Sym decomposition2.png|frameless]]

According to this interpretation, the matrix-vector multiplication &lt;math&gt;Ax&lt;/math&gt; can be seen as:

# Resolving &lt;math&gt;x&lt;/math&gt; into its &lt;math&gt;q_i q_i^T&lt;/math&gt; components
# Scaling each component by the corresponding eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;
# Reconstituting with basis &lt;math&gt;q_i&lt;/math&gt;.

==What's the point of Symmetric matrices?==
Symmetric matrices are worth studying for a number of reasons. Firstly, when working with quadratic forms: one can always find a symmetric equivalent of a quadratic form of a matrix (see [[Quadratic form]]), and it's advantageous to work on the symmetric one because it's easy to find the minimum and maximum of the quadratic form via eigenvalues (see [[Quadratic form]]). A related property is that the positive definiteness of a symmetric matrix can be determined from its eigenvalues (see [[Positive definite matrix]]).

The property listed above (&lt;math&gt;Q^T A Q = \Lambda&lt;/math&gt;) is also a very plausible one for symmetric matrices as it leads to very intuitive decompositions.</text>
      <sha1>p75sok8o6njevq43jundjtz32avjjpv</sha1>
    </revision>
    <revision>
      <id>136</id>
      <parentid>131</parentid>
      <timestamp>2018-07-21T13:39:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of Symmetric matrices? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2513">A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.

===Properties of Symmetric Matrices===
* The eigenvalues of a symmetric matrix are real.
* There is a set of orthonormal eigenvectors of &lt;math&gt;A&lt;/math&gt;, i.e., &lt;math&gt;\exists q_1, \dots, q_n &lt;/math&gt; s.t. &lt;math&gt;A q_i = \lambda_i q_i&lt;/math&gt; and &lt;math&gt;q_i^t q_j = \delta_{ij}&lt;/math&gt; where &lt;math&gt;\delta_{ij}&lt;/math&gt; is Kronecker's delta. In matrix form, there exists a matrix &lt;math&gt;Q&lt;/math&gt; s.t. &lt;math&gt;Q^T A Q_T = \Lambda&lt;/math&gt;, and therefore &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt;, where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues of &lt;math&gt;A&lt;/math&gt;.

==== Two Interpretations ====

The last property above is worth investigating further. Saying that &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt; is also saying that one can obtain &lt;math&gt;y = Ax&lt;/math&gt; via the following block diagram 

[[File:Sym decomposition1.png|frameless]]

This block diagram leads to the following interpretation for the linear mapping &lt;math&gt;Ax&lt;/math&gt;:
# Rotate &lt;math&gt;x&lt;/math&gt; by &lt;math&gt;Q^T&lt;/math&gt;
# Diagonal real scale ('dilation') by &lt;math&gt;\Lambda&lt;/math&gt;
# Rotate back by &lt;math&gt;Q&lt;/math&gt;.

Another interpretation is a linear combination of 1-dimensional projections:

&lt;math&gt;A = \sum_{i=1}^n \lambda_i q_i q_i^T&lt;/math&gt;

The interpretation of the latter is illustrated in the example figure below:

[[File:Sym decomposition2.png|frameless]]

According to this interpretation, the matrix-vector multiplication &lt;math&gt;Ax&lt;/math&gt; can be seen as:

# Resolving &lt;math&gt;x&lt;/math&gt; into its &lt;math&gt;q_i q_i^T&lt;/math&gt; components
# Scaling each component by the corresponding eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;
# Reconstituting with basis &lt;math&gt;q_i&lt;/math&gt;.

==What's the point of Symmetric matrices?==
Symmetric matrices are worth studying for a number of reasons. Firstly, when working with quadratic forms: one can always find a symmetric equivalent of a quadratic form of a matrix (see [[Quadratic form]]), and it's advantageous to work on the symmetric one because it's easy to find the minimum and maximum of the quadratic form via eigenvalues (see [[Quadratic form]]). A related property is that the positive definiteness of a symmetric matrix can be determined from its eigenvalues (see [[Positive definite matrix]]).

The property listed above (&lt;math&gt;Q^T A Q = \Lambda&lt;/math&gt;) is also a very plausible one for symmetric matrices as it leads to very intuitive decompositions.

Symmetric and [[Positive definite matrix|positive definite]] matrices define ellipsoids.</text>
      <sha1>k3db68pd7ro1clc6tbp2ovbnfs1sa38</sha1>
    </revision>
    <revision>
      <id>148</id>
      <parentid>136</parentid>
      <timestamp>2018-07-27T13:10:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Properties of Symmetric Matrices */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2511">A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.

===Properties of Symmetric Matrices===
* The eigenvalues of a symmetric matrix are real.
* There is a set of orthonormal eigenvectors of &lt;math&gt;A&lt;/math&gt;, i.e., &lt;math&gt;\exists q_1, \dots, q_n &lt;/math&gt; s.t. &lt;math&gt;A q_i = \lambda_i q_i&lt;/math&gt; and &lt;math&gt;q_i^t q_j = \delta_{ij}&lt;/math&gt; where &lt;math&gt;\delta_{ij}&lt;/math&gt; is Kronecker's delta. In matrix form, there exists a matrix &lt;math&gt;Q&lt;/math&gt; s.t. &lt;math&gt;Q^T A Q = \Lambda&lt;/math&gt;, and therefore &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt;, where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues of &lt;math&gt;A&lt;/math&gt;.

==== Two Interpretations ====

The last property above is worth investigating further. Saying that &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt; is also saying that one can obtain &lt;math&gt;y = Ax&lt;/math&gt; via the following block diagram 

[[File:Sym decomposition1.png|frameless]]

This block diagram leads to the following interpretation for the linear mapping &lt;math&gt;Ax&lt;/math&gt;:
# Rotate &lt;math&gt;x&lt;/math&gt; by &lt;math&gt;Q^T&lt;/math&gt;
# Diagonal real scale ('dilation') by &lt;math&gt;\Lambda&lt;/math&gt;
# Rotate back by &lt;math&gt;Q&lt;/math&gt;.

Another interpretation is a linear combination of 1-dimensional projections:

&lt;math&gt;A = \sum_{i=1}^n \lambda_i q_i q_i^T&lt;/math&gt;

The interpretation of the latter is illustrated in the example figure below:

[[File:Sym decomposition2.png|frameless]]

According to this interpretation, the matrix-vector multiplication &lt;math&gt;Ax&lt;/math&gt; can be seen as:

# Resolving &lt;math&gt;x&lt;/math&gt; into its &lt;math&gt;q_i q_i^T&lt;/math&gt; components
# Scaling each component by the corresponding eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;
# Reconstituting with basis &lt;math&gt;q_i&lt;/math&gt;.

==What's the point of Symmetric matrices?==
Symmetric matrices are worth studying for a number of reasons. Firstly, when working with quadratic forms: one can always find a symmetric equivalent of a quadratic form of a matrix (see [[Quadratic form]]), and it's advantageous to work on the symmetric one because it's easy to find the minimum and maximum of the quadratic form via eigenvalues (see [[Quadratic form]]). A related property is that the positive definiteness of a symmetric matrix can be determined from its eigenvalues (see [[Positive definite matrix]]).

The property listed above (&lt;math&gt;Q^T A Q = \Lambda&lt;/math&gt;) is also a very plausible one for symmetric matrices as it leads to very intuitive decompositions.

Symmetric and [[Positive definite matrix|positive definite]] matrices define ellipsoids.</text>
      <sha1>mui6h2khpuiqvduwy2qijna3hq86g45</sha1>
    </revision>
    <revision>
      <id>151</id>
      <parentid>148</parentid>
      <timestamp>2018-07-27T13:15:24Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2613">A square matrix &lt;math&gt;A \in \mathbb{R}^{n\times n}&lt;/math&gt; is called symmetric if &lt;math&gt;A^T = A&lt;/math&gt;.

===Properties of Symmetric Matrices===
* The eigenvalues of a symmetric matrix are real.
* There is a set of orthonormal eigenvectors of &lt;math&gt;A&lt;/math&gt;, i.e., &lt;math&gt;\exists q_1, \dots, q_n &lt;/math&gt; s.t. &lt;math&gt;A q_i = \lambda_i q_i&lt;/math&gt; and &lt;math&gt;q_i^t q_j = \delta_{ij}&lt;/math&gt; where &lt;math&gt;\delta_{ij}&lt;/math&gt; is Kronecker's delta. In matrix form, there exists a matrix &lt;math&gt;Q&lt;/math&gt; s.t. &lt;math&gt;Q^T A Q = \Lambda&lt;/math&gt;, and therefore &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt;, where &lt;math&gt;\Lambda&lt;/math&gt; is the diagonal matrix of eigenvalues of &lt;math&gt;A&lt;/math&gt;. The decomposition &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt; is called  ''eigendecomposition'' of &lt;math&gt;A&lt;/math&gt;.

==== Two Interpretations ====

The last property above is worth investigating further. Saying that &lt;math&gt;A = Q\Lambda Q^T&lt;/math&gt; is also saying that one can obtain &lt;math&gt;y = Ax&lt;/math&gt; via the following block diagram 

[[File:Sym decomposition1.png|frameless]]

This block diagram leads to the following interpretation for the linear mapping &lt;math&gt;Ax&lt;/math&gt;:
# Rotate &lt;math&gt;x&lt;/math&gt; by &lt;math&gt;Q^T&lt;/math&gt;
# Diagonal real scale ('dilation') by &lt;math&gt;\Lambda&lt;/math&gt;
# Rotate back by &lt;math&gt;Q&lt;/math&gt;.

Another interpretation is a linear combination of 1-dimensional projections:

&lt;math&gt;A = \sum_{i=1}^n \lambda_i q_i q_i^T&lt;/math&gt;

The interpretation of the latter is illustrated in the example figure below:

[[File:Sym decomposition2.png|frameless]]

According to this interpretation, the matrix-vector multiplication &lt;math&gt;Ax&lt;/math&gt; can be seen as:

# Resolving &lt;math&gt;x&lt;/math&gt; into its &lt;math&gt;q_i q_i^T&lt;/math&gt; components
# Scaling each component by the corresponding eigenvalue &lt;math&gt;\lambda_i&lt;/math&gt;
# Reconstituting with basis &lt;math&gt;q_i&lt;/math&gt;.

==What's the point of Symmetric matrices?==
Symmetric matrices are worth studying for a number of reasons. Firstly, when working with quadratic forms: one can always find a symmetric equivalent of a quadratic form of a matrix (see [[Quadratic form]]), and it's advantageous to work on the symmetric one because it's easy to find the minimum and maximum of the quadratic form via eigenvalues (see [[Quadratic form]]). A related property is that the positive definiteness of a symmetric matrix can be determined from its eigenvalues (see [[Positive definite matrix]]).

The property listed above (&lt;math&gt;Q^T A Q = \Lambda&lt;/math&gt;) is also a very plausible one for symmetric matrices as it leads to very intuitive decompositions.

Symmetric and [[Positive definite matrix|positive definite]] matrices define ellipsoids.</text>
      <sha1>26nfsa9hpp9ffrg1q5kr8ruei6rvg2g</sha1>
    </revision>
  </page>
  <page>
    <title>File:Sym decomposition1.png</title>
    <ns>6</ns>
    <id>48</id>
    <revision>
      <id>126</id>
      <timestamp>2018-07-20T14:13:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-07-20T14:13:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>Sym_decomposition1.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/d/db/Sym_decomposition1.png</src>
      <size>389707</size>
      <sha1base36>f1qfovub1fth1n6z73edeq24ghjgy27</sha1base36>
      <rel>d/db/Sym_decomposition1.png</rel>
    </upload>
  </page>
  <page>
    <title>File:Sym decomposition2.png</title>
    <ns>6</ns>
    <id>49</id>
    <revision>
      <id>127</id>
      <timestamp>2018-07-20T14:14:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-07-20T14:14:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>Sym_decomposition2.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/5/58/Sym_decomposition2.png</src>
      <size>680693</size>
      <sha1base36>avv7p1gcgywjhjsvbzpl9no31fy5dvt</sha1base36>
      <rel>5/58/Sym_decomposition2.png</rel>
    </upload>
  </page>
  <page>
    <title>File:Ellipsoid 2d.png</title>
    <ns>6</ns>
    <id>50</id>
    <revision>
      <id>134</id>
      <timestamp>2018-07-21T13:27:45Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-07-21T13:27:45Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>Ellipsoid_2d.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/1/1e/Ellipsoid_2d.png</src>
      <size>11675</size>
      <sha1base36>cu3p6azhvjwgzx6olligxtvqf9ynte0</sha1base36>
      <rel>1/1e/Ellipsoid_2d.png</rel>
    </upload>
  </page>
  <page>
    <title>Ellipsoid</title>
    <ns>0</ns>
    <id>51</id>
    <revision>
      <id>135</id>
      <timestamp>2018-07-21T13:38:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A [[Symmetric matrices|symmetric]] and [[Positive definite matrix|positive definite]] matrix &lt;math&gt;A=A^T \in \mathbb{R}^{n\times n} &gt; 0&lt;/math&gt; defines an ellipsoid &lt;math&gt;\math..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1383">A [[Symmetric matrices|symmetric]] and [[Positive definite matrix|positive definite]] matrix &lt;math&gt;A=A^T \in \mathbb{R}^{n\times n} &gt; 0&lt;/math&gt; defines an ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; through a [[Quadratic form|quadratic form]] as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

&lt;math&gt;\mathcal E = \{x : x^T A x \le 1 \}&lt;/math&gt;

In this case the ellipsoid is centered on zero. The ellipsoid is illustrated for the 2D case below:

[[File:ellipsoid 2d.png|frameless]]

The ''semi-axes'' are given by &lt;math&gt;s_i = \lambda_i^{-1/2} q_i&lt;/math&gt;. That is, [[Eigenvectors and Eigenvalues|eigenvectors]] determine the directions and [[Eigenvectors and Eigenvalues|eigenvalues]] determine the lengths of semi-axes. &lt;math&gt;\sqrt{\lambda_{\text{max}}/\lambda_{\text{min}}}&lt;/math&gt; gives the maximum ''eccentricity'' of the ellipsoid. 

=== Interpretation === 
The ellipsoids provide an intuitive description of the conditions under which two symmetric and positive definite matrices are [[Matrix inequality|comparable]]. The ellipsoids of two symmetric and positive definite matrices &lt;math&gt;A,B&lt;/math&gt; can be used to [[Matrix inequality|compare]] the two matrices. That is, suppose that &lt;math&gt;\mathcal{\tilde{E}} := \{x : x^T B x \le 1\}&lt;/math&gt;, and &lt;math&gt;\mathcal E := \{ x: x^T A x \le 1 \} &lt;/math&gt;. Then,

&lt;math&gt;A\ge B \iff \mathcal E \subseteq \mathcal{\tilde{E}}&lt;/math&gt;


== References ==</text>
      <sha1>bvpdb7dm1md88arwy0n8p64fj35jskq</sha1>
    </revision>
    <revision>
      <id>190</id>
      <parentid>135</parentid>
      <timestamp>2018-07-29T01:14:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1382">A [[Symmetric matrices|symmetric]] and [[Positive definite matrix|positive definite]] matrix &lt;math&gt;A=A^T \in \mathbb{R}^{n\times n} &gt; 0&lt;/math&gt; defines an ellipsoid &lt;math&gt;\mathcal E&lt;/math&gt; through a [[Quadratic form|quadratic form]] as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

&lt;math&gt;\mathcal E = \{x : x^T A x \le 1 \}&lt;/math&gt;

In this case the ellipsoid is centered on zero. The ellipsoid is illustrated for the 2D case below:

[[File:ellipsoid 2d.png|frameless]]

The ''semi-axes'' are given by &lt;math&gt;s_i = \lambda_i^{-1/2} q_i&lt;/math&gt;. That is, [[Eigenvectors and Eigenvalues|eigenvectors]] determine the directions and [[Eigenvectors and Eigenvalues|eigenvalues]] determine the lengths of semi-axes. &lt;math&gt;\sqrt{\lambda_{\text{max}}/\lambda_{\text{min}}}&lt;/math&gt; gives the maximum ''eccentricity'' of the ellipsoid. 

=== Interpretation === 
The ellipsoids provide an intuitive description of the conditions under which two symmetric and positive definite matrices are [[Matrix inequality|comparable]]. The ellipsoids of two symmetric and positive definite matrices &lt;math&gt;A,B&lt;/math&gt; can be used to [[Matrix inequality|compare]] the two matrices. That is, suppose that &lt;math&gt;\mathcal{\tilde{E}} := \{x : x^T B x \le 1\}&lt;/math&gt;, and &lt;math&gt;\mathcal E := \{ x: x^T A x \le 1 \} &lt;/math&gt;. Then,

&lt;math&gt;A\ge B \iff \mathcal E \subseteq \mathcal{\tilde{E}}&lt;/math&gt;

== References ==</text>
      <sha1>dg9ovqpgplaetc0fel0w8v9clrb31rc</sha1>
    </revision>
  </page>
  <page>
    <title>Gauss-Newton method</title>
    <ns>0</ns>
    <id>52</id>
    <revision>
      <id>137</id>
      <timestamp>2018-07-21T14:02:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The Gauss-Newton method is a standard approach to (approximately/locally) solving non-linear equations. Assume that me aim to minimize a (squared) L2 norm &lt;math&gt;||r(x)||&lt;/math..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="538">The Gauss-Newton method is a standard approach to (approximately/locally) solving non-linear equations. Assume that me aim to minimize a (squared) L2 norm &lt;math&gt;||r(x)||&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt;. The super-simplified pseudo-algorithm of the Gauss-Newton method is&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 7&lt;/ref&gt;:

* Given a starting guess for &lt;math&gt;x&lt;/math&gt;
* Repeat
# Linearize &lt;math&gt;r&lt;/math&gt; near current guess
# Compute new guess, which is the linear LS solution on the linearized &lt;math&gt;r&lt;/math&gt;
* Until convergence

== References ==</text>
      <sha1>rhq6wrd43b2g4zk71lpe1r2nqf5tqyy</sha1>
    </revision>
  </page>
  <page>
    <title>Suggestions for solving math problems</title>
    <ns>0</ns>
    <id>53</id>
    <revision>
      <id>139</id>
      <timestamp>2018-07-21T14:10:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "# Start with a simpler version of the problem! For example, if you want to show an identity on matrices and it becomes much simpler to assume that the matrix is Diagonalizat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="423"># Start with a simpler version of the problem! For example, if you want to show an identity on matrices and it becomes much simpler to assume that the matrix is [[Diagonalization|diagonalizable]], then do this first. This will likely point to how you should attack the problem. Then, in this example, you can use [[Jordan Canonical Form|JCF]], which is a generalization of diagonalization, to solve for the general problem.</text>
      <sha1>hv3ydvaav2j087ap3bt3j9k6a1l24n9</sha1>
    </revision>
  </page>
  <page>
    <title>Eigendecomposition</title>
    <ns>0</ns>
    <id>54</id>
    <redirect title="Diagonalization" />
    <revision>
      <id>150</id>
      <timestamp>2018-07-27T13:12:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Diagonalization]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29">#REDIRECT [[Diagonalization]]</text>
      <sha1>cff9a8ta3dii4ube4j2h3i7kcir7vu3</sha1>
    </revision>
  </page>
  <page>
    <title>Singular Value Decomposition</title>
    <ns>0</ns>
    <id>55</id>
    <revision>
      <id>152</id>
      <timestamp>2018-07-27T13:17:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Singular Value Decomposition is a ubiquitous matrix decomposition"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="65">Singular Value Decomposition is a ubiquitous matrix decomposition</text>
      <sha1>dda5811wtki2mb7hvvblfxjk7kqkqwz</sha1>
    </revision>
    <revision>
      <id>159</id>
      <parentid>152</parentid>
      <timestamp>2018-07-27T14:20:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2308">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

== References ==</text>
      <sha1>98wqqo5pkyadwk4udff2vg4d5n68xby</sha1>
    </revision>
    <revision>
      <id>162</id>
      <parentid>159</parentid>
      <timestamp>2018-07-27T15:16:53Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4264">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

== References ==</text>
      <sha1>i1bw706is0u1k2oe0rkkqs155834hlc</sha1>
    </revision>
    <revision>
      <id>163</id>
      <parentid>162</parentid>
      <timestamp>2018-07-27T15:22:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4611">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

== References ==</text>
      <sha1>nv29a441092ob7vtfnek5cw1nf2n5pt</sha1>
    </revision>
    <revision>
      <id>164</id>
      <parentid>163</parentid>
      <timestamp>2018-07-28T14:01:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4976">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

== References ==</text>
      <sha1>3unzkmto7paqlpnchs2i2daweg00fwf</sha1>
    </revision>
    <revision>
      <id>170</id>
      <parentid>164</parentid>
      <timestamp>2018-07-28T14:29:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5391">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

== Full SVD ==

In some cases it may be preferrable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

== References ==</text>
      <sha1>tn4qfwn7g6ev4l5y89nrmfi91lop7rz</sha1>
    </revision>
    <revision>
      <id>171</id>
      <parentid>170</parentid>
      <timestamp>2018-07-28T14:48:54Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6926">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of 

== Full SVD ==

In some cases it may be preferrable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax +v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order as a threshold value for the significant singular values.  

=== Give picture of matrix gain ===

=== Matrix approximation ===

== References ==</text>
      <sha1>pabgpt56xt1nvju71vk1ald7jf4gj1u</sha1>
    </revision>
    <revision>
      <id>175</id>
      <parentid>171</parentid>
      <timestamp>2018-07-28T14:54:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7156">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).

== Full SVD ==

In some cases it may be preferrable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax +v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order as a threshold value for the significant singular values.  

=== Give picture of matrix gain ===

=== Analysing sensitivity to data error===

=== Matrix approximation ===

== References ==</text>
      <sha1>qjzd2ng7pn2xcpatpxmgkxurmpauufz</sha1>
    </revision>
    <revision>
      <id>176</id>
      <parentid>175</parentid>
      <timestamp>2018-07-28T15:12:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of SVD? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8487">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).

== Full SVD ==

In some cases it may be preferrable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax +v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order as a threshold value for the significant singular values.  

=== Give picture of matrix gain ===

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the esitmated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as:

&lt;math&gt;||delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., 1/\sigma_{\text{min}}) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1}&lt; U^T/math&gt;.)




=== Matrix approximation ===

== References ==</text>
      <sha1>ox6pjuxhf3pm4vt4gno1m5j0w1vafi5</sha1>
    </revision>
    <revision>
      <id>177</id>
      <parentid>176</parentid>
      <timestamp>2018-07-28T15:43:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* What's the point of SVD? */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8731">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).

== Full SVD ==

In some cases it may be preferrable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give picture of matrix gain ===

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).



=== Matrix approximation ===

== References ==</text>
      <sha1>8btlb8hikb3h0ode1ky2jdjc0gg1pl9</sha1>
    </revision>
    <revision>
      <id>180</id>
      <parentid>177</parentid>
      <timestamp>2018-07-29T00:24:02Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9404">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

== Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).

== Full SVD ==

In some cases it may be preferrable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

To distinguish (regular) SVD from full SVD, the former is also called sometimes as ''compact SVD'' .

== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give picture of matrix gain ===

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).



=== Matrix approximation ===

Let &lt;math&gt;\tilde A&lt;/math&gt; be the rank-&lt;math&gt;p&lt;/math&gt; approximation of &lt;math&gt;A&lt;/math&gt;; that is, the approximation that is computed by taking into account only the first &lt;math&gt;p&lt;/math&gt; singular values. Then, &lt;math&gt;\tilde A&lt;/math&gt; is the optimal approximation in that the approximation error &lt;math&gt;||\tilde A - A||&lt;/math&gt; is minimal (for [[Spectral norm]] &lt;math&gt;||\cdot||&lt;/math&gt;) &lt;ref name=l16&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;

In fact, it's also easy to compute the approximation error of this rank-&lt;math&gt;p&lt;/math&gt;: it's &lt;math&gt;\sigma_{p+1}&lt;/math&gt; &lt;ref name=l16/&gt;

== References ==</text>
      <sha1>dkoeayyb05p3h7pc7h4291kh9uye0u5</sha1>
    </revision>
    <revision>
      <id>184</id>
      <parentid>180</parentid>
      <timestamp>2018-07-29T00:49:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10464">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

=== Full SVD ===

In some cases it may be preferable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

To distinguish (regular) SVD from full SVD, the former is also called sometimes as ''compact SVD'' .

== Interpretation: Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Image of unit ball under full SVD transformation ===

The full SVD transformation gives a clear picture of how SVD operates. The figure below shows what full SVD does to the (2D) unit ball at each block
[[File:SVD in action.png|frameless]]

Since we assume full SVD, both &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are orthogonal matrices (and so are their transposes). Therefore, the first operation is a rotation by &lt;math&gt;V^T&lt;/math&gt;, which does not change the image of the ball although each vector on it is rotated. The second step amplifies each dimension according to the corresponding singular value, which turns the ball into an ellipse. The last step does another rotation, one induced by &lt;math&gt;U&lt;/math&gt;. Note that the highest output direction is associated with &lt;math&gt;u_1&lt;/math&gt;.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).


== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give highest/smallest directions of matrix gain ===
Each singular value is associated with an input and an output direction. As described and illustrated in the Interpretation section above, &lt;math&gt;v_1&lt;/math&gt; and &lt;math&gt;u_1&lt;/math&gt; give the respectively highest input and output gain directions for a matrix &lt;math&gt;A&lt;/math&gt;.

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref name=l16&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).



=== Matrix approximation ===

Let &lt;math&gt;\tilde A&lt;/math&gt; be the rank-&lt;math&gt;p&lt;/math&gt; approximation of &lt;math&gt;A&lt;/math&gt;; that is, the approximation that is computed by taking into account only the first &lt;math&gt;p&lt;/math&gt; singular values. Then, &lt;math&gt;\tilde A&lt;/math&gt; is the optimal approximation in that the approximation error &lt;math&gt;||\tilde A - A||&lt;/math&gt; is minimal (for [[Spectral norm]] &lt;math&gt;||\cdot||&lt;/math&gt;) &lt;ref name=l16/&gt;

In fact, it's also easy to compute the approximation error of this rank-&lt;math&gt;p&lt;/math&gt;: it's &lt;math&gt;\sigma_{p+1}&lt;/math&gt; &lt;ref name=l16/&gt;

== References ==</text>
      <sha1>jqg503aiyiq0u36hszqqzivbt09pgr4</sha1>
    </revision>
    <revision>
      <id>185</id>
      <parentid>184</parentid>
      <timestamp>2018-07-29T00:51:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10464">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

=== Full SVD ===

In some cases it may be preferable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

To distinguish (regular) SVD from full SVD, the former is also called sometimes as ''compact SVD''.

== Interpretation: Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Image of unit ball under full SVD transformation ===

The full SVD transformation gives a clear picture of how SVD operates. The figure below shows what full SVD does to the (2D) unit ball at each block

[[File:SVD in action.png|frameless]]

Since we assume full SVD, both &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are orthogonal matrices (and so are their transposes). Therefore, the first operation is a rotation by &lt;math&gt;V^T&lt;/math&gt;, which does not change the image of the ball although each vector on it is rotated. The second step amplifies each dimension according to the corresponding singular value, which turns the ball into an ellipse. The last step does another rotation, one induced by &lt;math&gt;U&lt;/math&gt;. Note that the highest output direction is associated with &lt;math&gt;u_1&lt;/math&gt;.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).


== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give highest/smallest directions of matrix gain ===
Each singular value is associated with an input and an output direction. As described and illustrated in the Interpretation section above, &lt;math&gt;v_1&lt;/math&gt; and &lt;math&gt;u_1&lt;/math&gt; give the respectively highest input and output gain directions for a matrix &lt;math&gt;A&lt;/math&gt;.

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref name=l16&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).



=== Matrix approximation ===

Let &lt;math&gt;\tilde A&lt;/math&gt; be the rank-&lt;math&gt;p&lt;/math&gt; approximation of &lt;math&gt;A&lt;/math&gt;; that is, the approximation that is computed by taking into account only the first &lt;math&gt;p&lt;/math&gt; singular values. Then, &lt;math&gt;\tilde A&lt;/math&gt; is the optimal approximation in that the approximation error &lt;math&gt;||\tilde A - A||&lt;/math&gt; is minimal (for [[Spectral norm]] &lt;math&gt;||\cdot||&lt;/math&gt;) &lt;ref name=l16/&gt;

In fact, it's also easy to compute the approximation error of this rank-&lt;math&gt;p&lt;/math&gt;: it's &lt;math&gt;\sigma_{p+1}&lt;/math&gt; &lt;ref name=l16/&gt;

== References ==</text>
      <sha1>5stzdbtxwjrzg6i0ewd2w492t5h14yn</sha1>
    </revision>
    <revision>
      <id>189</id>
      <parentid>185</parentid>
      <timestamp>2018-07-29T01:00:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10613">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma ^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

=== Full SVD ===

In some cases it may be preferable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

To distinguish (regular) SVD from full SVD, the former is also called sometimes as ''compact SVD''.

== Interpretation: Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Image of unit ball under full SVD transformation ===

The full SVD transformation gives a clear picture of how SVD operates. The figure below shows what full SVD does to the (2D) unit ball at each block

[[File:SVD in action.png|frameless]]

Since we assume full SVD, both &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are orthogonal matrices (and so are their transposes). Therefore, the first operation is a rotation by &lt;math&gt;V^T&lt;/math&gt;, which does not change the image of the ball although each vector on it is rotated. The second step amplifies each dimension according to the corresponding singular value, which turns the ball into an ellipse. The last step does another rotation, one induced by &lt;math&gt;U&lt;/math&gt;. Note that the highest output direction is associated with &lt;math&gt;u_1&lt;/math&gt;.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).


== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give highest/smallest directions of matrix gain ===
Each singular value is associated with an input and an output direction. As described and illustrated in the Interpretation section above, &lt;math&gt;v_1&lt;/math&gt; and &lt;math&gt;u_1&lt;/math&gt; give the respectively highest input and output gain directions for a matrix &lt;math&gt;A&lt;/math&gt;.

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref name=l16&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).

=== Matrix approximation ===

Let &lt;math&gt;\tilde A&lt;/math&gt; be the rank-&lt;math&gt;p&lt;/math&gt; approximation of &lt;math&gt;A&lt;/math&gt;; that is, the approximation that is computed by taking into account only the first &lt;math&gt;p&lt;/math&gt; singular values. Then, &lt;math&gt;\tilde A&lt;/math&gt; is the optimal approximation in that the approximation error &lt;math&gt;||\tilde A - A||&lt;/math&gt; is minimal (for [[Spectral norm]] &lt;math&gt;||\cdot||&lt;/math&gt;) &lt;ref name=l16/&gt;

In fact, it's also easy to compute the approximation error of this rank-&lt;math&gt;p&lt;/math&gt;: it's &lt;math&gt;\sigma_{p+1}&lt;/math&gt; &lt;ref name=l16/&gt;

=== Pseudo-inverse ===

SVD gives the pseudo-inverse of a matrix in the most general case (i.e., when it's not full rank) -- see [[Pseudo-inverse]].


== References ==</text>
      <sha1>ii75zubrwvqlt9ig3x970y1h5cf6y89</sha1>
    </revision>
    <revision>
      <id>200</id>
      <parentid>189</parentid>
      <timestamp>2018-09-11T12:45:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10614">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma V^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

=== Full SVD ===

In some cases it may be preferable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

To distinguish (regular) SVD from full SVD, the former is also called sometimes as ''compact SVD''.

== Interpretation: Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Image of unit ball under full SVD transformation ===

The full SVD transformation gives a clear picture of how SVD operates. The figure below shows what full SVD does to the (2D) unit ball at each block

[[File:SVD in action.png|frameless]]

Since we assume full SVD, both &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are orthogonal matrices (and so are their transposes). Therefore, the first operation is a rotation by &lt;math&gt;V^T&lt;/math&gt;, which does not change the image of the ball although each vector on it is rotated. The second step amplifies each dimension according to the corresponding singular value, which turns the ball into an ellipse. The last step does another rotation, one induced by &lt;math&gt;U&lt;/math&gt;. Note that the highest output direction is associated with &lt;math&gt;u_1&lt;/math&gt;.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).


== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give highest/smallest directions of matrix gain ===
Each singular value is associated with an input and an output direction. As described and illustrated in the Interpretation section above, &lt;math&gt;v_1&lt;/math&gt; and &lt;math&gt;u_1&lt;/math&gt; give the respectively highest input and output gain directions for a matrix &lt;math&gt;A&lt;/math&gt;.

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref name=l16&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).

=== Matrix approximation ===

Let &lt;math&gt;\tilde A&lt;/math&gt; be the rank-&lt;math&gt;p&lt;/math&gt; approximation of &lt;math&gt;A&lt;/math&gt;; that is, the approximation that is computed by taking into account only the first &lt;math&gt;p&lt;/math&gt; singular values. Then, &lt;math&gt;\tilde A&lt;/math&gt; is the optimal approximation in that the approximation error &lt;math&gt;||\tilde A - A||&lt;/math&gt; is minimal (for [[Spectral norm]] &lt;math&gt;||\cdot||&lt;/math&gt;) &lt;ref name=l16/&gt;

In fact, it's also easy to compute the approximation error of this rank-&lt;math&gt;p&lt;/math&gt;: it's &lt;math&gt;\sigma_{p+1}&lt;/math&gt; &lt;ref name=l16/&gt;

=== Pseudo-inverse ===

SVD gives the pseudo-inverse of a matrix in the most general case (i.e., when it's not full rank) -- see [[Pseudo-inverse]].


== References ==</text>
      <sha1>jjfva1s9yuj65rof37dcunn5qf01ny4</sha1>
    </revision>
    <revision>
      <id>201</id>
      <parentid>200</parentid>
      <timestamp>2018-09-11T13:39:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Analysing sensitivity to data error */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10615">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma V^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

=== Full SVD ===

In some cases it may be preferable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

To distinguish (regular) SVD from full SVD, the former is also called sometimes as ''compact SVD''.

== Interpretation: Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Image of unit ball under full SVD transformation ===

The full SVD transformation gives a clear picture of how SVD operates. The figure below shows what full SVD does to the (2D) unit ball at each block

[[File:SVD in action.png|frameless]]

Since we assume full SVD, both &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are orthogonal matrices (and so are their transposes). Therefore, the first operation is a rotation by &lt;math&gt;V^T&lt;/math&gt;, which does not change the image of the ball although each vector on it is rotated. The second step amplifies each dimension according to the corresponding singular value, which turns the ball into an ellipse. The last step does another rotation, one induced by &lt;math&gt;U&lt;/math&gt;. Note that the highest output direction is associated with &lt;math&gt;u_1&lt;/math&gt;.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).


== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give highest/smallest directions of matrix gain ===
Each singular value is associated with an input and an output direction. As described and illustrated in the Interpretation section above, &lt;math&gt;v_1&lt;/math&gt; and &lt;math&gt;u_1&lt;/math&gt; give the respectively highest input and output gain directions for a matrix &lt;math&gt;A&lt;/math&gt;.

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref name=l16&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||\delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).

=== Matrix approximation ===

Let &lt;math&gt;\tilde A&lt;/math&gt; be the rank-&lt;math&gt;p&lt;/math&gt; approximation of &lt;math&gt;A&lt;/math&gt;; that is, the approximation that is computed by taking into account only the first &lt;math&gt;p&lt;/math&gt; singular values. Then, &lt;math&gt;\tilde A&lt;/math&gt; is the optimal approximation in that the approximation error &lt;math&gt;||\tilde A - A||&lt;/math&gt; is minimal (for [[Spectral norm]] &lt;math&gt;||\cdot||&lt;/math&gt;) &lt;ref name=l16/&gt;

In fact, it's also easy to compute the approximation error of this rank-&lt;math&gt;p&lt;/math&gt;: it's &lt;math&gt;\sigma_{p+1}&lt;/math&gt; &lt;ref name=l16/&gt;

=== Pseudo-inverse ===

SVD gives the pseudo-inverse of a matrix in the most general case (i.e., when it's not full rank) -- see [[Pseudo-inverse]].


== References ==</text>
      <sha1>i3lsltq5ydgg45vj6ao3mpibz88d1dt</sha1>
    </revision>
    <revision>
      <id>202</id>
      <parentid>201</parentid>
      <timestamp>2018-09-11T13:39:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Analysing sensitivity to data error */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10616">Singular Value Decomposition (SVD) is a ubiquitous matrix decomposition that applies to any matrix &lt;math&gt;A \in \mathbb{R}^{m\times n}&lt;/math&gt; and takes the form 

&lt;math&gt;A = U \Sigma V^T&lt;/math&gt;.

Let &lt;math&gt;\text{rank}(A) = r&lt;/math&gt;. Then, the columns of &lt;math&gt;U \in \mathbb{R}^{m\times r}&lt;/math&gt; contains an orthonormal set of eigenvectors of &lt;math&gt;A A^T&lt;/math&gt;; the columns of &lt;math&gt;V&lt;/math&gt; form an orthogonal set of eigenvectors of &lt;math&gt;A^T A&lt;/math&gt;; and &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt; contains the square roots of the non-zero eigenvalues of &lt;math&gt;A A^T&lt;/math&gt; (or, [[Eigenvectors and Eigenvalues#Properties of Eigenvectors and Eigenvalues|equivalently]], the non-zero eigenvalues of &lt;math&gt;A^T A&lt;/math&gt;). The values &lt;math&gt;\sigma_i&lt;/math&gt; are listed from largest to smallest. 

Now note that any real matrix that can be expressed in the form of &lt;math&gt;A A^T&lt;/math&gt; or &lt;math&gt;A^T A&lt;/math&gt; has to be symmetric and nonnegative definite (see Theorem 14.3.7&lt;ref&gt;D. Harville, Matrix Algebra for Statisticians&lt;/ref&gt;). Therefore, the entries of &lt;math&gt;\Sigma&lt;/math&gt; are real. We can more succinctly represent the properties of the matrices in SVD as &lt;ref&gt;Stephen Boyd, EE263 Lecture Notes 15&lt;/ref&gt;:

* &lt;math&gt;U = [u_1, \dots, u_r] \in \mathbb{R}^{m\times r}, U^T U = I&lt;/math&gt;; &lt;math&gt;u_i&lt;/math&gt; eigenvector of &lt;math&gt;A A^T&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;V = [v_1, \dots, v_r] \in \mathbb{R}^{n\times r}, V^T V = I&lt;/math&gt;; &lt;math&gt;v_i&lt;/math&gt; eigenvector of &lt;math&gt;A^T A&lt;/math&gt; corresponding to eigenvalue &lt;math&gt;s_i^2&lt;/math&gt;
* &lt;math&gt;\Sigma = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)&lt;/math&gt;, where &lt;math&gt;\sigma_1 \ge \dots \ge \sigma_r &gt; 0&lt;/math&gt;

Now we can write out the SVD decomposition as:

&lt;math&gt;A = U \Sigma V^T = \sum_{i=1}^r \sigma_r u_i v_i^T&lt;/math&gt;

Some terminology:
* &lt;math&gt;s_i&lt;/math&gt; are the (nonzero) ''singular values'' of &lt;math&gt;A&lt;/math&gt;
* &lt;math&gt;v_i&lt;/math&gt; are the ''right'' or ''input singular'' vectors of &lt;math&gt;A&lt;/math&gt; 
* &lt;math&gt;u_i&lt;/math&gt; are the ''left'' or ''output singular'' vectors of &lt;math&gt;A&lt;/math&gt; 

It's easy to show that &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; contain eigenvectors of &lt;math&gt;A A^T&lt;/math&gt; and &lt;math&gt;A^T A&lt;/math&gt;, respectively:

&lt;math&gt;A^T = (U\Sigma V^T)^T (U \Sigma V^T) = V \Sigma V^T&lt;/math&gt;

=== Some properties of SVD ===
* &lt;math&gt;u_1, \dots, u_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\text{range}(A)&lt;/math&gt;
* &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; form an orthonormal basis for &lt;math&gt;\mathcal{N}(A)^\perp&lt;/math&gt;
* &lt;math&gt;\sigma_1 = ||A||&lt;/math&gt; where &lt;math&gt;||\cdot||&lt;/math&gt; is [[Matrix norm]]

The last property is obvious when one considers the definition of [[Matrix norm]]. To understand the first property, we can inspect the multiplication &lt;math&gt;Ax&lt;/math&gt;:

&lt;math&gt;Ax = U \Sigma V^T x = U (\Sigma V^T x) = Uz&lt;/math&gt;.

Now since the rank of &lt;math&gt;V^T \in \mathbb{R}^{r\times n}&lt;/math&gt; is &lt;math&gt;r&lt;/math&gt;, the multiplication &lt;math&gt;(V^T x)&lt;/math&gt; can represent any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. In other words, &lt;math&gt;z&lt;/math&gt; above can be any vector in &lt;math&gt;\mathbb{R}^r&lt;/math&gt;. And since &lt;math&gt;U&lt;/math&gt; is also of rank &lt;math&gt;r&lt;/math&gt;, &lt;math&gt;Uz&lt;/math&gt; spans the range that &lt;math&gt;A&lt;/math&gt; spans.

The second property above is actually another way of saying that &lt;math&gt;v_1, \dots, v_r&lt;/math&gt; are the input spaces that are ''not'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt;. In other words, if you are orthogonal to the span of &lt;math&gt;v_1, \dots, v_r&lt;/math&gt;, then you are ''in'' &lt;math&gt;\mathcal{N}(A)&lt;/math&gt; &lt;ref&gt;Stephen Boyd, EE263 video for lecture 17&lt;/ref&gt;.

=== Full SVD ===

In some cases it may be preferable to have a version of SVD that includes zero values as well, in which case we would be augmenting the &lt;math&gt;\Sigma&lt;/math&gt; matrix with the (possible) zero singular values in the diagonal. Clearly, we would also need to augment &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;, which we would do by completing both of them into bases by adding the sufficient independent columns.

To distinguish (regular) SVD from full SVD, the former is also called sometimes as ''compact SVD''.

== Interpretation: Understanding SVD ==
In SVD, &lt;math&gt;v_1&lt;/math&gt; is the most sensitive ''input'' direction and &lt;math&gt;u_1&lt;/math&gt; is the most sensitive ''output'' direction. The reason becomes clear upon inspecting the block diagram of SVD.

[[File:SVD block.png|frameless]]

Note that the rightmost block [[Orthonormality|does not change the norm of its input]], and the gain of the matrix is determined by the first two blocks. The second block, &lt;math&gt;\Sigma&lt;/math&gt;, is pretty simple: it just scales each entry of its input by a positive number. Since the first entry, &lt;math&gt;\sigma_1&lt;/math&gt;, is the largest one, the outcome of the second block will be maximized if its input is concentrated entirely on the first element, which means that (assuming unit vector) &lt;math&gt;V^T x = e_1&lt;/math&gt;, which becomes possible if &lt;math&gt;x=v_1&lt;/math&gt;. This justifies the qualification of &lt;math&gt;v_1&lt;/math&gt; as the most sensitive input direction.

The qualification of &lt;math&gt;u_1&lt;/math&gt; as the highest gain output direction is also justified because:

&lt;math&gt;A v_1 = \sigma_1 u_1&lt;/math&gt;.

The SVD decompositions acts not unlike the [[Symmetric matrices|eigendecomposition for symmetric matrices]] in that it first rotates the input vector and then scales each entry; the only difference between the two is that while eigendecomposition does at the third step the first rotation in the reverse direction, SVD does a different rotation in the third step.

=== Image of unit ball under full SVD transformation ===

The full SVD transformation gives a clear picture of how SVD operates. The figure below shows what full SVD does to the (2D) unit ball at each block

[[File:SVD in action.png|frameless]]

Since we assume full SVD, both &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt; are orthogonal matrices (and so are their transposes). Therefore, the first operation is a rotation by &lt;math&gt;V^T&lt;/math&gt;, which does not change the image of the ball although each vector on it is rotated. The second step amplifies each dimension according to the corresponding singular value, which turns the ball into an ellipse. The last step does another rotation, one induced by &lt;math&gt;U&lt;/math&gt;. Note that the highest output direction is associated with &lt;math&gt;u_1&lt;/math&gt;.

=== Condition number ===
An important concept that is a product of SVD is the condition number of a matrix &lt;math&gt;A&lt;/math&gt;, which can be denoted with &lt;math&gt;\kappa(A)&lt;/math&gt; and is computed as

&lt;math&gt;\kappa(A) = \sigma_1 / \sigma_r&lt;/math&gt;

Clearly, the condition number is nothing but the ratio between the largest and smallest singular value of &lt;math&gt;A&lt;/math&gt;. The importance of &lt;math&gt;\kappa(A)&lt;/math&gt; is that it places an upper bound to sensitivity to data error, as described below (see [[#Analysing sensitivity to data error]]).


== What's the point of SVD? ==
SVD has a lot of uses, clearly this section will be a meager representation of them.

=== Approximate rank ===
A matrix that is rank-deficient can be easily made full-rank. In this sense, the concept of rank is an extremely sensitive one. SVD can be used to develop a robust alternative to it: the approximate rank. The approximate rank is the number of singular values of a matrix (i.e., the &lt;math&gt;\sigma_i&lt;/math&gt;) that are not very close to zero. Of course, what is very close to zero can be application dependent but in some cases it is pretty clear. For example, if the singular values of a &lt;math&gt;4 \times 4&lt;/math&gt; matrix are as follows

&lt;math&gt;(10, 5, 4, 0.1, 0,01)&lt;/math&gt;

we can say that the approximate rank is 3 as the last two values are comparatively very small. In fact, if we do an estimation problem where we try to find &lt;math&gt;x&lt;/math&gt; from noisy measurements

&lt;math&gt;y = Ax + v&lt;/math&gt;

and if we have an idea about the order of the noise (e.g., the standard deviation if modelled as an RV), then we can use this order to get a threshold value for the significant singular values.  

=== Give highest/smallest directions of matrix gain ===
Each singular value is associated with an input and an output direction. As described and illustrated in the Interpretation section above, &lt;math&gt;v_1&lt;/math&gt; and &lt;math&gt;u_1&lt;/math&gt; give the respectively highest input and output gain directions for a matrix &lt;math&gt;A&lt;/math&gt;.

=== Analysing sensitivity to data error===

Suppose that  &lt;math&gt;y=Ax&lt;/math&gt; where &lt;math&gt;A&lt;/math&gt; is square and invertible, and we are interested in finding &lt;math&gt;x&lt;/math&gt; which clearly is

&lt;math&gt;x = A^{-1}y&lt;/math&gt;.

Suppose that we have an error at &lt;math&gt;y&lt;/math&gt;, i.e., &lt;math&gt;y&lt;/math&gt; becomes &lt;math&gt;y+\delta y&lt;/math&gt;. Then the estimated &lt;math&gt;x&lt;/math&gt; becomes &lt;math&gt;x + \delta x&lt;/math&gt;. Clearly we would like &lt;math&gt;\delta x&lt;/math&gt; to be small. Using a basic inequality (see [[Matrix norm]]), we can place an upper bound on &lt;math&gt;||\delta x||&lt;/math&gt;  as&lt;ref name=l16&gt;Stephen Boyd, EE263 Lecture Notes 16&lt;/ref&gt;:

&lt;math&gt;||\delta x|| = ||A^{-1} \delta y || \le ||A^{-1}|| ||\delta  y||&lt;/math&gt;.

This inequality says that a large &lt;math&gt;|| A^{-1}||&lt;/math&gt; can lead to large errors in &lt;math&gt;x&lt;/math&gt; (although the inequality can be far from tight). The relative error 

&lt;math&gt;\frac{||\delta x||}{||x||} \le ||A|| ||A^{-1}||\frac{||\delta y||}{||y||}&lt;/math&gt;

is obtained by using a similar inequality that leads to &lt;math&gt;||y|| \le ||A|| ||x||&lt;/math&gt;.

The number &lt;math&gt;||A||||A^{-1}||&lt;/math&gt; is nothing but the condition number &lt;math&gt;\kappa(A)&lt;/math&gt;, as one divided by  the smallest singular value of &lt;math&gt;A&lt;/math&gt; (i.e., &lt;math&gt;1/\sigma_{\text{min}}&lt;/math&gt;) is the  largest singular value and hence the norm of &lt;math&gt;A^{-1}&lt;/math&gt;. (To see the latter, note that for invertible &lt;math&gt;A&lt;/math&gt; we have that &lt;math&gt;A^{-1} = V \Sigma^{-1} U^T&lt;/math&gt;.)

To summarize, a small condition number means small amplification of data error (although a large condition number does not necessarily imply a large amplification of data error).

=== Matrix approximation ===

Let &lt;math&gt;\tilde A&lt;/math&gt; be the rank-&lt;math&gt;p&lt;/math&gt; approximation of &lt;math&gt;A&lt;/math&gt;; that is, the approximation that is computed by taking into account only the first &lt;math&gt;p&lt;/math&gt; singular values. Then, &lt;math&gt;\tilde A&lt;/math&gt; is the optimal approximation in that the approximation error &lt;math&gt;||\tilde A - A||&lt;/math&gt; is minimal (for [[Spectral norm]] &lt;math&gt;||\cdot||&lt;/math&gt;) &lt;ref name=l16/&gt;

In fact, it's also easy to compute the approximation error of this rank-&lt;math&gt;p&lt;/math&gt;: it's &lt;math&gt;\sigma_{p+1}&lt;/math&gt; &lt;ref name=l16/&gt;

=== Pseudo-inverse ===

SVD gives the pseudo-inverse of a matrix in the most general case (i.e., when it's not full rank) -- see [[Pseudo-inverse]].


== References ==</text>
      <sha1>2cprvehm3lhqq9ph7hhsbe5oybf9i12</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix decomposition</title>
    <ns>0</ns>
    <id>56</id>
    <revision>
      <id>153</id>
      <timestamp>2018-07-27T13:21:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "There are a lot of interesting matrix decompositions. Below are listed some of them:  # [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an ort..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1087">There are a lot of interesting matrix decompositions. Below are listed some of them:

# [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an orthogonal basis (&lt;math&gt;Q&lt;/math&gt;) and an upper triangular matrix (&lt;math&gt;R&lt;/math&gt;) that gives the coefficients of the linear combination of the columns of &lt;math&gt;Q&lt;/math&gt; that recover &lt;math&gt;A&lt;/math&gt;.
# [[Diagonalization|Eigendecomposition]] of a ''[[Diagonalization|diagonalizable]]'' matrix, &lt;math&gt;A = T \Lambda T^{-1}&lt;/math&gt; where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix that contains eigenvalues and &lt;math&gt;T&lt;/math&gt; is a matrix that contains eigenvectors.
# [[Symmetric matrices|Eigendecomposition]] of a symmetric matrix: &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt;
# [[Singular Value Decomposition]] of ''any'' matrix &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A = U \Sigma V^T&lt;/math&gt;, where &lt;math&gt;\Sigma&lt;/math&gt; is the diagonal matrix that contains the square-roots of the eigenvalues of &lt;math&gt;A A^T&lt;/math&gt;, the columns of &lt;math&gt;U&lt;/math&gt; are eigenvectors of &lt;math&gt;AA^T&lt;/math&gt; and columns of &lt;math&gt;V&lt;/math&gt; are eigenvectors &lt;math&gt;A^T A&lt;/math&gt;</text>
      <sha1>cmmp7j0akt9bjq6bafqgvgqec6aau78</sha1>
    </revision>
    <revision>
      <id>155</id>
      <parentid>153</parentid>
      <timestamp>2018-07-27T13:25:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1255">There are a lot of interesting matrix decompositions. Below are listed some of them:

# [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an orthogonal basis (&lt;math&gt;Q&lt;/math&gt;) and an upper triangular matrix (&lt;math&gt;R&lt;/math&gt;) that gives the coefficients of the linear combination of the columns of &lt;math&gt;Q&lt;/math&gt; that recover &lt;math&gt;A&lt;/math&gt;.
# [[Diagonalization|Eigendecomposition]] of a ''[[Diagonalization|diagonalizable]]'' matrix, &lt;math&gt;A = T \Lambda T^{-1}&lt;/math&gt; where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix that contains eigenvalues and &lt;math&gt;T&lt;/math&gt; is a matrix that contains eigenvectors.
# [[Jordan Canonical Form]] of any square matrix &lt;math&gt;A = T J T^{-1}&lt;/math&gt;, which is a generalization of diagonalization where &lt;math&gt;J&lt;/math&gt; is a bidiagonal matrix.
# [[Symmetric matrices|Eigendecomposition]] of a symmetric matrix: &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt;
# [[Singular Value Decomposition]] of ''any'' matrix &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A = U \Sigma V^T&lt;/math&gt;, where &lt;math&gt;\Sigma&lt;/math&gt; is the diagonal matrix that contains the square-roots of the eigenvalues of &lt;math&gt;A A^T&lt;/math&gt;, the columns of &lt;math&gt;U&lt;/math&gt; are eigenvectors of &lt;math&gt;AA^T&lt;/math&gt; and columns of &lt;math&gt;V&lt;/math&gt; are eigenvectors &lt;math&gt;A^T A&lt;/math&gt;</text>
      <sha1>9uu59yfxbuh1f0c2j5z938q556k56qi</sha1>
    </revision>
    <revision>
      <id>194</id>
      <parentid>155</parentid>
      <timestamp>2018-07-29T01:19:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1513">There are a lot of interesting matrix decompositions. Below are listed some of them.

== Exact decompositions ==

# [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an orthogonal basis (&lt;math&gt;Q&lt;/math&gt;) and an upper triangular matrix (&lt;math&gt;R&lt;/math&gt;) that gives the coefficients of the linear combination of the columns of &lt;math&gt;Q&lt;/math&gt; that recover &lt;math&gt;A&lt;/math&gt;.
# [[Diagonalization|Eigendecomposition]] of a ''[[Diagonalization|diagonalizable]]'' matrix, &lt;math&gt;A = T \Lambda T^{-1}&lt;/math&gt; where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix that contains eigenvalues and &lt;math&gt;T&lt;/math&gt; is a matrix that contains eigenvectors.
# [[Jordan Canonical Form]] of any square matrix &lt;math&gt;A = T J T^{-1}&lt;/math&gt;, which is a generalization of diagonalization where &lt;math&gt;J&lt;/math&gt; is a bidiagonal matrix.
# [[Symmetric matrices|Eigendecomposition]] of a symmetric matrix: &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt;
# [[Singular Value Decomposition]] of ''any'' matrix &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A = U \Sigma V^T&lt;/math&gt;, where &lt;math&gt;\Sigma&lt;/math&gt; is the diagonal matrix that contains the square-roots of the eigenvalues of &lt;math&gt;A A^T&lt;/math&gt;, the columns of &lt;math&gt;U&lt;/math&gt; are eigenvectors of &lt;math&gt;AA^T&lt;/math&gt; and columns of &lt;math&gt;V&lt;/math&gt; are eigenvectors &lt;math&gt;A^T A&lt;/math&gt;

== Approximate decompositions ==
# Using [[Singular_Value_Decomposition#Matrix approximation|SVD]] one can approximate any matrix &lt;math&gt;A&lt;/math&gt; using &lt;math&gt;p&lt;/math&gt; singular values as: &lt;math&gt;A \approx U_1 \Sigma_1 V^T_1&lt;/math&gt;.</text>
      <sha1>3bf7qztxbv5ixlgybmmmwzzvd3j34mh</sha1>
    </revision>
    <revision>
      <id>300</id>
      <parentid>194</parentid>
      <timestamp>2018-10-20T14:37:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1819">There are a lot of interesting matrix decompositions. Below are listed some of them.

== Exact decompositions ==

# [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an orthogonal basis (&lt;math&gt;Q&lt;/math&gt;) and an upper triangular matrix (&lt;math&gt;R&lt;/math&gt;) that gives the coefficients of the linear combination of the columns of &lt;math&gt;Q&lt;/math&gt; that recover &lt;math&gt;A&lt;/math&gt;.
# [[Diagonalization|Eigendecomposition]] of a ''[[Diagonalization|diagonalizable]]'' matrix, &lt;math&gt;A = T \Lambda T^{-1}&lt;/math&gt; where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix that contains eigenvalues and &lt;math&gt;T&lt;/math&gt; is a matrix that contains eigenvectors.
# [[Jordan Canonical Form]] of any square matrix &lt;math&gt;A = T J T^{-1}&lt;/math&gt;, which is a generalization of diagonalization where &lt;math&gt;J&lt;/math&gt; is a bidiagonal matrix.
# [[Symmetric matrices|Eigendecomposition]] of a symmetric matrix: &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt;
# [[Singular Value Decomposition]] of ''any'' matrix &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A = U \Sigma V^T&lt;/math&gt;, where &lt;math&gt;\Sigma&lt;/math&gt; is the diagonal matrix that contains the square-roots of the eigenvalues of &lt;math&gt;A A^T&lt;/math&gt;, the columns of &lt;math&gt;U&lt;/math&gt; are eigenvectors of &lt;math&gt;AA^T&lt;/math&gt; and columns of &lt;math&gt;V&lt;/math&gt; are eigenvectors &lt;math&gt;A^T A&lt;/math&gt;

The following are typical decompositions that are used for (computationally) simplifying the solving of &lt;math&gt;Ax=b&lt;/math&gt;
# [[LU decomposition]] for any nonsingular matrix
# [[Cholesky factorization]] for any symmetric positive definite matrix
# [[LDL factorization]] for any symmetric nonsingular matrix

== Approximate decompositions ==
# Using [[Singular_Value_Decomposition#Matrix approximation|SVD]] one can approximate any matrix &lt;math&gt;A&lt;/math&gt; using &lt;math&gt;p&lt;/math&gt; singular values as: &lt;math&gt;A \approx U_1 \Sigma_1 V^T_1&lt;/math&gt;.</text>
      <sha1>d4jy0itijfg5vxflr5j1llxj5n8zrzg</sha1>
    </revision>
    <revision>
      <id>354</id>
      <parentid>300</parentid>
      <timestamp>2019-03-25T20:06:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Exact decompositions */minor content added</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1829">There are a lot of interesting matrix decompositions. Below are listed some of them.

== Exact decompositions ==

# [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an orthogonal basis (&lt;math&gt;Q&lt;/math&gt;) and an upper triangular matrix (&lt;math&gt;R&lt;/math&gt;) that gives the coefficients of the linear combination of the columns of &lt;math&gt;Q&lt;/math&gt; that recover &lt;math&gt;A&lt;/math&gt;.
# [[Diagonalization|Eigendecomposition]] of a square and''[[Diagonalization|diagonalizable]]'' matrix, &lt;math&gt;A = T \Lambda T^{-1}&lt;/math&gt; where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix that contains eigenvalues and &lt;math&gt;T&lt;/math&gt; is a matrix that contains eigenvectors.
# [[Jordan Canonical Form]] of any square matrix &lt;math&gt;A = T J T^{-1}&lt;/math&gt;, which is a generalization of diagonalization where &lt;math&gt;J&lt;/math&gt; is a bidiagonal matrix.
# [[Symmetric matrices|Eigendecomposition]] of a symmetric matrix: &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt;
# [[Singular Value Decomposition]] of ''any'' matrix &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A = U \Sigma V^T&lt;/math&gt;, where &lt;math&gt;\Sigma&lt;/math&gt; is the diagonal matrix that contains the square-roots of the eigenvalues of &lt;math&gt;A A^T&lt;/math&gt;, the columns of &lt;math&gt;U&lt;/math&gt; are eigenvectors of &lt;math&gt;AA^T&lt;/math&gt; and columns of &lt;math&gt;V&lt;/math&gt; are eigenvectors &lt;math&gt;A^T A&lt;/math&gt;

The following are typical decompositions that are used for (computationally) simplifying the solving of &lt;math&gt;Ax=b&lt;/math&gt;
# [[LU decomposition]] for any nonsingular matrix
# [[Cholesky factorization]] for any symmetric positive definite matrix
# [[LDL factorization]] for any symmetric nonsingular matrix

== Approximate decompositions ==
# Using [[Singular_Value_Decomposition#Matrix approximation|SVD]] one can approximate any matrix &lt;math&gt;A&lt;/math&gt; using &lt;math&gt;p&lt;/math&gt; singular values as: &lt;math&gt;A \approx U_1 \Sigma_1 V^T_1&lt;/math&gt;.</text>
      <sha1>dg61tq0qxjgdvgkw138aqidrpawcery</sha1>
    </revision>
    <revision>
      <id>384</id>
      <parentid>354</parentid>
      <timestamp>2019-06-16T18:33:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2050">There are a lot of interesting matrix decompositions. Below are listed some of them.

== Exact decompositions ==

# [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an orthogonal basis (&lt;math&gt;Q&lt;/math&gt;) and an upper triangular matrix (&lt;math&gt;R&lt;/math&gt;) that gives the coefficients of the linear combination of the columns of &lt;math&gt;Q&lt;/math&gt; that recover &lt;math&gt;A&lt;/math&gt;.
# [[Diagonalization|Eigendecomposition]] of a square and ''[[Diagonalization|diagonalizable]]'' matrix, &lt;math&gt;A = T \Lambda T^{-1}&lt;/math&gt; where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix that contains eigenvalues and &lt;math&gt;T&lt;/math&gt; is a matrix that contains eigenvectors.
# [[Triangularization]] of a square matrix is finding an orthogonal matrix &lt;math&gt;Q&lt;/math&gt; such that for any matrix &lt;math&gt;A&lt;/math&gt; the matrix &lt;math&gt;T=Q'AQ&lt;/math&gt; is upper triangular with eigenvalues of A on its diagonal.
# [[Jordan Canonical Form]] of any square matrix &lt;math&gt;A = T J T^{-1}&lt;/math&gt;, which is a generalization of diagonalization where &lt;math&gt;J&lt;/math&gt; is a bidiagonal matrix.
# [[Symmetric matrices|Eigendecomposition]] of a symmetric matrix: &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt;
# [[Singular Value Decomposition]] of ''any'' matrix &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A = U \Sigma V^T&lt;/math&gt;, where &lt;math&gt;\Sigma&lt;/math&gt; is the diagonal matrix that contains the square-roots of the eigenvalues of &lt;math&gt;A A^T&lt;/math&gt;, the columns of &lt;math&gt;U&lt;/math&gt; are eigenvectors of &lt;math&gt;AA^T&lt;/math&gt; and columns of &lt;math&gt;V&lt;/math&gt; are eigenvectors &lt;math&gt;A^T A&lt;/math&gt;

The following are typical decompositions that are used for (computationally) simplifying the solving of &lt;math&gt;Ax=b&lt;/math&gt;
# [[LU decomposition]] for any nonsingular matrix
# [[Cholesky factorization]] for any symmetric positive definite matrix
# [[LDL factorization]] for any symmetric nonsingular matrix

== Approximate decompositions ==
# Using [[Singular_Value_Decomposition#Matrix approximation|SVD]] one can approximate any matrix &lt;math&gt;A&lt;/math&gt; using &lt;math&gt;p&lt;/math&gt; singular values as: &lt;math&gt;A \approx U_1 \Sigma_1 V^T_1&lt;/math&gt;.</text>
      <sha1>nllgusezirhfxa0evraja5f257r9gcc</sha1>
    </revision>
    <revision>
      <id>521</id>
      <parentid>384</parentid>
      <timestamp>2020-01-06T10:15:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2063">There are a lot of interesting matrix decompositions. Below are listed some of them.

== Exact decompositions ==

# [[QR Factorization]]: When we take a full-rank matrix &lt;math&gt;A&lt;/math&gt; and compute an orthogonal basis (&lt;math&gt;Q&lt;/math&gt;) and an upper triangular matrix (&lt;math&gt;R&lt;/math&gt;) that gives the coefficients of the linear combination of the columns of &lt;math&gt;Q&lt;/math&gt; that recover &lt;math&gt;A&lt;/math&gt;.
# [[Diagonalization|Eigendecomposition]] of a square and ''[[Diagonalization|diagonalizable]]'' matrix, &lt;math&gt;A = T \Lambda T^{-1}&lt;/math&gt; where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix that contains eigenvalues and &lt;math&gt;T&lt;/math&gt; is a matrix that contains eigenvectors.
# [[Triangularization]] of a square matrix is finding an orthogonal matrix &lt;math&gt;Q&lt;/math&gt; such that for any matrix &lt;math&gt;A&lt;/math&gt; the matrix &lt;math&gt;T=Q'AQ&lt;/math&gt; is upper triangular with eigenvalues of &lt;math&gt;A&lt;/math&gt; on its diagonal.
# [[Jordan Canonical Form]] of any square matrix &lt;math&gt;A = T J T^{-1}&lt;/math&gt;, which is a generalization of diagonalization where &lt;math&gt;J&lt;/math&gt; is a bidiagonal matrix.
# [[Symmetric matrices|Eigendecomposition]] of a symmetric matrix: &lt;math&gt;A = Q \Lambda Q^T&lt;/math&gt;
# [[Singular Value Decomposition]] of ''any'' matrix &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A = U \Sigma V^T&lt;/math&gt;, where &lt;math&gt;\Sigma&lt;/math&gt; is the diagonal matrix that contains the square-roots of the eigenvalues of &lt;math&gt;A A^T&lt;/math&gt;, the columns of &lt;math&gt;U&lt;/math&gt; are eigenvectors of &lt;math&gt;AA^T&lt;/math&gt; and columns of &lt;math&gt;V&lt;/math&gt; are eigenvectors &lt;math&gt;A^T A&lt;/math&gt;

The following are typical decompositions that are used for (computationally) simplifying the solving of &lt;math&gt;Ax=b&lt;/math&gt;
# [[LU decomposition]] for any nonsingular matrix
# [[Cholesky factorization]] for any symmetric positive definite matrix
# [[LDL factorization]] for any symmetric nonsingular matrix

== Approximate decompositions ==
# Using [[Singular_Value_Decomposition#Matrix approximation|SVD]] one can approximate any matrix &lt;math&gt;A&lt;/math&gt; using &lt;math&gt;p&lt;/math&gt; singular values as: &lt;math&gt;A \approx U_1 \Sigma_1 V^T_1&lt;/math&gt;.</text>
      <sha1>5bbcyljniq4af5lxkmlmbxeud49gaet</sha1>
    </revision>
  </page>
  <page>
    <title>Spectral decomposition</title>
    <ns>0</ns>
    <id>57</id>
    <revision>
      <id>154</id>
      <timestamp>2018-07-27T13:24:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "#REDIRECT [[[[Eigendecomposition]]]]"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="36">#REDIRECT [[[[Eigendecomposition]]]]</text>
      <sha1>7o1y9ra3g9ux50niy5m2aamonjxofov</sha1>
    </revision>
  </page>
  <page>
    <title>File:SVD block.png</title>
    <ns>6</ns>
    <id>58</id>
    <revision>
      <id>160</id>
      <timestamp>2018-07-27T14:49:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-07-27T14:49:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>SVD_block.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/a/a2/SVD_block.png</src>
      <size>28540</size>
      <sha1base36>7c2v9n7r6g33pz639om7t8gcx25uwiv</sha1base36>
      <rel>a/a2/SVD_block.png</rel>
    </upload>
  </page>
  <page>
    <title>SVD</title>
    <ns>0</ns>
    <id>59</id>
    <redirect title="Singular Value Decomposition" />
    <revision>
      <id>165</id>
      <timestamp>2018-07-28T14:15:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Singular Value Decomposition]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="42">#REDIRECT [[Singular Value Decomposition]]</text>
      <sha1>6q17p63mvbg0vq8tlksg5rv1y45yq8e</sha1>
    </revision>
  </page>
  <page>
    <title>Well conditioned matrix</title>
    <ns>0</ns>
    <id>60</id>
    <redirect title="Singular Value Decomposition" />
    <revision>
      <id>172</id>
      <timestamp>2018-07-28T14:50:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Singular value decomposition#Condition number]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59">#REDIRECT [[Singular value decomposition#Condition number]]</text>
      <sha1>fc780lx30gt1s5s52xcyj34o6z2yc2e</sha1>
    </revision>
    <revision>
      <id>173</id>
      <parentid>172</parentid>
      <timestamp>2018-07-28T14:51:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Changed redirect target from [[Singular value decomposition#Condition number]] to [[Singular Value Decomposition#Condition number]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59">#REDIRECT [[Singular Value Decomposition#Condition number]]</text>
      <sha1>2bdgfi9k7qwx0c38f21gwtlt8crunb3</sha1>
    </revision>
  </page>
  <page>
    <title>Poorly conditioned matrix</title>
    <ns>0</ns>
    <id>61</id>
    <redirect title="Singular Value Decomposition" />
    <revision>
      <id>174</id>
      <timestamp>2018-07-28T14:51:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Singular Value Decomposition#Condition number]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59">#REDIRECT [[Singular Value Decomposition#Condition number]]</text>
      <sha1>2bdgfi9k7qwx0c38f21gwtlt8crunb3</sha1>
    </revision>
  </page>
  <page>
    <title>Spectral norm</title>
    <ns>0</ns>
    <id>62</id>
    <redirect title="Matrix norm" />
    <revision>
      <id>178</id>
      <timestamp>2018-07-29T00:16:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Matrix Norm]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="25">#REDIRECT [[Matrix Norm]]</text>
      <sha1>mcjgjtc4u95r9i6sdrzccmjx46d4r1e</sha1>
    </revision>
    <revision>
      <id>179</id>
      <parentid>178</parentid>
      <timestamp>2018-07-29T00:16:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>Changed redirect target from [[Matrix Norm]] to [[Matrix norm]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="25">#REDIRECT [[Matrix norm]]</text>
      <sha1>efqvfp4mtca031yz0qo3i7ckra7bwuo</sha1>
    </revision>
  </page>
  <page>
    <title>File:SVD in action.png</title>
    <ns>6</ns>
    <id>63</id>
    <revision>
      <id>181</id>
      <timestamp>2018-07-29T00:42:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <revision>
      <id>182</id>
      <parentid>181</parentid>
      <timestamp>2018-07-29T00:48:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Evangelos uploaded a new version of [[File:SVD in action.png]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <revision>
      <id>183</id>
      <parentid>182</parentid>
      <timestamp>2018-07-29T00:48:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Evangelos uploaded a new version of [[File:SVD in action.png]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-07-29T00:42:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>SVD_in_action.png</filename>
      <archivename>20180729004806!SVD_in_action.png</archivename>
      <src>https://static.miraheze.org/engineeringmathwiki/archive/a/ac/20180729004806%21SVD_in_action.png</src>
      <size>77514</size>
      <sha1base36>lndqrxm0a5mhet3s4q5fakunsyfs404</sha1base36>
      <rel>archive/a/ac/20180729004806!SVD_in_action.png</rel>
    </upload>
    <upload>
      <timestamp>2018-07-29T00:48:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>SVD_in_action.png</filename>
      <archivename>20180729004835!SVD_in_action.png</archivename>
      <src>https://static.miraheze.org/engineeringmathwiki/archive/a/ac/20180729004835%21SVD_in_action.png</src>
      <size>77232</size>
      <sha1base36>6zqsdfmucsds0o9jt7dkvf5l31apuj6</sha1base36>
      <rel>archive/a/ac/20180729004835!SVD_in_action.png</rel>
    </upload>
    <upload>
      <timestamp>2018-07-29T00:48:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>SVD_in_action.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/a/ac/SVD_in_action.png</src>
      <size>77232</size>
      <sha1base36>qonw1su8our244as118g0apctf8wjw2</sha1base36>
      <rel>a/ac/SVD_in_action.png</rel>
    </upload>
  </page>
  <page>
    <title>Condition number</title>
    <ns>0</ns>
    <id>64</id>
    <redirect title="Singular Value Decomposition" />
    <revision>
      <id>186</id>
      <timestamp>2018-07-29T00:52:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Singular Value Decomposition#Condition number]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59">#REDIRECT [[Singular Value Decomposition#Condition number]]</text>
      <sha1>2bdgfi9k7qwx0c38f21gwtlt8crunb3</sha1>
    </revision>
  </page>
  <page>
    <title>Decomposition</title>
    <ns>0</ns>
    <id>65</id>
    <redirect title="Matrix decomposition" />
    <revision>
      <id>192</id>
      <timestamp>2018-07-29T01:16:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Matrix decomposition]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34">#REDIRECT [[Matrix decomposition]]</text>
      <sha1>917rkya49nf50pkvsj8oft4rcve17ir</sha1>
    </revision>
  </page>
  <page>
    <title>Factorization</title>
    <ns>0</ns>
    <id>66</id>
    <redirect title="Matrix decomposition" />
    <revision>
      <id>193</id>
      <timestamp>2018-07-29T01:16:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Matrix decomposition]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34">#REDIRECT [[Matrix decomposition]]</text>
      <sha1>917rkya49nf50pkvsj8oft4rcve17ir</sha1>
    </revision>
  </page>
  <page>
    <title>Approximate rank</title>
    <ns>0</ns>
    <id>67</id>
    <redirect title="Singular Value Decomposition" />
    <revision>
      <id>197</id>
      <timestamp>2018-07-29T01:35:02Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Singular Value Decomposition#Approximate rank]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59">#REDIRECT [[Singular Value Decomposition#Approximate rank]]</text>
      <sha1>gbjyxoez0sftqjmcoslpkosr5306220</sha1>
    </revision>
  </page>
  <page>
    <title>EE263 Overview</title>
    <ns>0</ns>
    <id>68</id>
    <revision>
      <id>205</id>
      <timestamp>2018-09-12T18:54:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "== Lecture 1 ==   [[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in co..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="214">== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.</text>
      <sha1>gt5h8njc700cnb351raq9kxibf3fr6r</sha1>
    </revision>
    <revision>
      <id>206</id>
      <parentid>205</parentid>
      <timestamp>2018-09-12T18:56:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="515">The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2 ==</text>
      <sha1>l33sdvh5ty33hebmkd7k5gc94tph8tc</sha1>
    </revision>
    <revision>
      <id>209</id>
      <parentid>206</parentid>
      <timestamp>2018-09-12T19:08:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1515">The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2 ==

[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).</text>
      <sha1>216d8lk83vrr82pif7celxhq33ofg3a</sha1>
    </revision>
    <revision>
      <id>210</id>
      <parentid>209</parentid>
      <timestamp>2018-09-12T19:10:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Lecture 2 */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1566">The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2 ==

[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).</text>
      <sha1>0t3p37wejx1ya96gs25z6a7dama8c1k</sha1>
    </revision>
    <revision>
      <id>211</id>
      <parentid>210</parentid>
      <timestamp>2018-09-12T19:16:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2038">The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2 ==

[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3 ==

Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].</text>
      <sha1>lz99pgq8fd3avtmbaxdum93oma9ld65</sha1>
    </revision>
    <revision>
      <id>212</id>
      <parentid>211</parentid>
      <timestamp>2018-09-12T19:30:48Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3635">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2 ==

[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3 ==

Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4 ==

[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].</text>
      <sha1>mveyxphk0gcjpx1yuzj02asrg2lkz00</sha1>
    </revision>
    <revision>
      <id>213</id>
      <parentid>212</parentid>
      <timestamp>2018-09-12T20:38:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4074">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2 ==

[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3 ==

Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4 ==

[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].


== Lecture 5 == 

[[Least squares]] (LS) to approximately solve an overdetermined set of equations is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.</text>
      <sha1>jxg2f30zzx9yw30iszncayjo9rddrd7</sha1>
    </revision>
    <revision>
      <id>215</id>
      <parentid>213</parentid>
      <timestamp>2018-09-12T20:42:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4304">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1 == 

[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2 ==

[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3 ==

Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4 ==

[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].


== Lecture 5 == 

[[Least squares]] (LS) to approximately solve an overdetermined set of equations is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6 ==

Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).</text>
      <sha1>jrgeflw9ml3p1onc4nptojrsdr96vot</sha1>
    </revision>
    <revision>
      <id>216</id>
      <parentid>215</parentid>
      <timestamp>2018-09-12T20:53:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5458">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].


== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].</text>
      <sha1>3q50szh83lqg4j0erfwz0iaai6jg1jn</sha1>
    </revision>
    <revision>
      <id>217</id>
      <parentid>216</parentid>
      <timestamp>2018-09-12T21:00:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6262">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].


== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture 7: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].

== Lecture 9: Autonomous linear dynamical systems ==

Autonomous LDSs are discussed, examples are given. Linearization near equilibrium and along trajectory point is discussed.

== Lecture 10: Solution via Laplace transform and matrix exponential == 
[[Laplace transform]] is introduced and its usage for solving autonomous LDS is shown. [[Characteristic Polynomial]] is also introduced. Finding the eigenvalues is introduced. [[Matrix exponential]] is introduced, and its usage for solving an autonomous LDS is shown. Properties of [[Matrix exponential|matrix exponential]] as a generalization of the exponential are shown. Interpretations of having real or complex [[Eigenvectors and Eigenvalues|eigenvalues]] are introduced. [[Stability]] of a system and its relation to the eigenvalues are shown.</text>
      <sha1>a3hwxp64h05hp1byhbvv5j4nzyvkltj</sha1>
    </revision>
    <revision>
      <id>218</id>
      <parentid>217</parentid>
      <timestamp>2018-09-12T21:10:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7083">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].


== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture 7: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].

== Lecture 9: Autonomous linear dynamical systems ==

Autonomous LDSs are discussed, examples are given. Linearization near equilibrium and along trajectory point is discussed.

== Lecture 10: Solution via Laplace transform and matrix exponential == 
[[Laplace transform]] is introduced and its usage for solving autonomous LDS is shown. [[Characteristic Polynomial]] is also introduced. Finding the eigenvalues is introduced. [[Matrix exponential]] is introduced, and its usage for solving an autonomous LDS is shown. Properties of [[Matrix exponential|matrix exponential]] as a generalization of the exponential are shown. Interpretations of having real or complex [[Eigenvectors and Eigenvalues|eigenvalues]] are introduced. [[Stability]] of a system and its relation to the eigenvalues are shown.

== Lecture 11: Eigenvectors and diagonalization ==

[[Eigenvectors and Eigenvalues|Eigenvalues and left and right eigenvectors]] are introduced. The scaling interpretation for eigenvectors is given. The relationship between eigenvalues, eigenvectors and the behavior of an LDS is introduced. As an application, the usage of right eignevectors for finding the steady state distribution of a Markov chain is shown.

[[Diagonalization]] is introduced, which is very useful for studying systems and showing certain properties of systems (i.e., for proving things). [[Modal Form|modal form]] is also introduced, and its usages are discussed (i.e., simplifying expressions and analyzing systems -- see WTPO? in [[Modal Form]]). Real Modal Form is shown for to analyze with real matrices systems that have complex eigenvalues.</text>
      <sha1>5jtd62nek9zsbeot6cs0uosv9fg8ojs</sha1>
    </revision>
    <revision>
      <id>219</id>
      <parentid>218</parentid>
      <timestamp>2018-09-12T21:14:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7550">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].

== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture 7: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].

== Lecture 9: Autonomous linear dynamical systems ==

Autonomous LDSs are discussed, examples are given. Linearization near equilibrium and along trajectory point is discussed.

== Lecture 10: Solution via Laplace transform and matrix exponential == 
[[Laplace transform]] is introduced and its usage for solving autonomous LDS is shown. [[Characteristic Polynomial]] is also introduced. Finding the eigenvalues is introduced. [[Matrix exponential]] is introduced, and its usage for solving an autonomous LDS is shown. Properties of [[Matrix exponential|matrix exponential]] as a generalization of the exponential are shown. Interpretations of having real or complex [[Eigenvectors and Eigenvalues|eigenvalues]] are introduced. [[Stability]] of a system and its relation to the eigenvalues are shown.

== Lecture 11: Eigenvectors and diagonalization ==
[[Eigenvectors and Eigenvalues|Eigenvalues and left and right eigenvectors]] are introduced. The scaling interpretation for eigenvectors is given. The relationship between eigenvalues, eigenvectors and the behavior of an LDS is introduced. As an application, the usage of right eignevectors for finding the steady state distribution of a Markov chain is shown.

[[Diagonalization]] is introduced, which is very useful for studying systems and showing certain properties of systems (i.e., for proving things). [[Modal Form|modal form]] is also introduced, and its usages are discussed (i.e., simplifying expressions and analyzing systems -- see WTPO? in [[Modal Form]]). Real Modal Form is shown for to analyze with real matrices systems that have complex eigenvalues.

== Lecture 12: Jordan canonical form ==
[[Jordan Canonical Form]] is introduced, which is a generalization of modal form that works for any square matrix (i.e., works for non-diagonalizable matrices too). The [[Cayley-Hamilton theorem]] is also introduced -- this is a useful theorem for showing that any power of an &lt;math&gt;n \times n&lt;/math&gt; matrix is in the span of the set of the first &lt;math&gt;n-1&lt;/math&gt; powers of the matrix (see WTPO in [[Cayley-Hamilton theorem]]).</text>
      <sha1>arq30eszo5995gkxqipdkj3r88l062s</sha1>
    </revision>
    <revision>
      <id>220</id>
      <parentid>219</parentid>
      <timestamp>2018-09-12T21:21:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8131">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].

== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture 7: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].

== Lecture 9: Autonomous linear dynamical systems ==

Autonomous LDSs are discussed, examples are given. Linearization near equilibrium and along trajectory point is discussed.

== Lecture 10: Solution via Laplace transform and matrix exponential == 
[[Laplace Transform]] is introduced and its usage for solving autonomous LDS is shown. [[Characteristic Polynomial]] is also introduced. Finding the eigenvalues is introduced. [[Matrix exponential]] is introduced, and its usage for solving an autonomous LDS is shown. Properties of [[Matrix exponential|matrix exponential]] as a generalization of the exponential are shown. Interpretations of having real or complex [[Eigenvectors and Eigenvalues|eigenvalues]] are introduced. [[Stability]] of a system and its relation to the eigenvalues are shown.

== Lecture 11: Eigenvectors and diagonalization ==
[[Eigenvectors and Eigenvalues|Eigenvalues and left and right eigenvectors]] are introduced. The scaling interpretation for eigenvectors is given. The relationship between eigenvalues, eigenvectors and the behavior of an LDS is introduced. As an application, the usage of right eignevectors for finding the steady state distribution of a Markov chain is shown.

[[Diagonalization]] is introduced, which is very useful for studying systems and showing certain properties of systems (i.e., for proving things). [[Modal Form|modal form]] is also introduced, and its usages are discussed (i.e., simplifying expressions and analyzing systems -- see WTPO? in [[Modal Form]]). Real Modal Form is shown for to analyze with real matrices systems that have complex eigenvalues.

== Lecture 12: Jordan canonical form ==
[[Jordan Canonical Form]] is introduced, which is a generalization of modal form that works for any square matrix (i.e., works for non-diagonalizable matrices too). The [[Cayley-Hamilton theorem]] is also introduced -- this is a useful theorem for showing that any power of an &lt;math&gt;n \times n&lt;/math&gt; matrix is in the span of the set of the first &lt;math&gt;n-1&lt;/math&gt; powers of the matrix (see WTPO in [[Cayley-Hamilton theorem]]).

== Lecture 13: Linear dynamical systems with inputs &amp; outputs ==
Description of an LDS with i/o is given along with terminology of the matrices and vectors that define the system as well as a block diagram. Of note, the "idea of state" is also presented with 4 bullet points (see slide 13-29). The transfer matrix, impulse matrix (or impulse response), step matrix are discussed. It is shown how to use those concepts and convolution to understand the behavior of a system (e.g. see slide 13-9). The discretization of an LDS and the discrete-time transfer function are shown too.</text>
      <sha1>fxv479fkz5xwb9tywj9vgbhjkbjgyl9</sha1>
    </revision>
    <revision>
      <id>221</id>
      <parentid>220</parentid>
      <timestamp>2018-09-12T22:36:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9103">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].

== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture 7: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].

== Lecture 9: Autonomous linear dynamical systems ==

Autonomous LDSs are discussed, examples are given. Linearization near equilibrium and along trajectory point is discussed.

== Lecture 10: Solution via Laplace transform and matrix exponential == 
[[Laplace Transform]] is introduced and its usage for solving autonomous LDS is shown. [[Characteristic Polynomial]] is also introduced. Finding the eigenvalues is introduced. [[Matrix exponential]] is introduced, and its usage for solving an autonomous LDS is shown. Properties of [[Matrix exponential|matrix exponential]] as a generalization of the exponential are shown. Interpretations of having real or complex [[Eigenvectors and Eigenvalues|eigenvalues]] are introduced. [[Stability]] of a system and its relation to the eigenvalues are shown.

== Lecture 11: Eigenvectors and diagonalization ==
[[Eigenvectors and Eigenvalues|Eigenvalues and left and right eigenvectors]] are introduced. The scaling interpretation for eigenvectors is given. The relationship between eigenvalues, eigenvectors and the behavior of an LDS is introduced. As an application, the usage of right eignevectors for finding the steady state distribution of a Markov chain is shown.

[[Diagonalization]] is introduced, which is very useful for studying systems and showing certain properties of systems (i.e., for proving things). [[Modal Form|modal form]] is also introduced, and its usages are discussed (i.e., simplifying expressions and analyzing systems -- see WTPO? in [[Modal Form]]). Real Modal Form is shown for to analyze with real matrices systems that have complex eigenvalues.

== Lecture 12: Jordan canonical form ==
[[Jordan Canonical Form]] is introduced, which is a generalization of modal form that works for any square matrix (i.e., works for non-diagonalizable matrices too). The [[Cayley-Hamilton theorem]] is also introduced -- this is a useful theorem for showing that any power of an &lt;math&gt;n \times n&lt;/math&gt; matrix is in the span of the set of the first &lt;math&gt;n-1&lt;/math&gt; powers of the matrix (see WTPO in [[Cayley-Hamilton theorem]]).

== Lecture 13: Linear dynamical systems with inputs &amp; outputs ==
Description of an LDS with i/o is given along with terminology of the matrices and vectors that define the system as well as a block diagram. Of note, the "idea of state" is also presented with 4 bullet points (see slide 13-29). The transfer matrix, impulse matrix (or impulse response), step matrix are discussed. It is shown how to use those concepts and convolution to understand the behavior of a system (e.g. see slide 13-9). The discretization of an LDS and the discrete-time transfer function are shown too.

== Lecture 15: Symmetric matrices, quadratic forms, matrix norm, and SVD ==
The title is of the lecture pretty comprehensive. [[Symmetric matrices]] are introduced, which are very useful for studying [[Quadratic form|quadratic forms]] because the eigenvalues of symmetric matrices are real and define upper/lower bounds on quadratic forms. Also, the [[eigendecomposition]] of a symmetric matrix is introduced (see [[Symmetric matrices]] too), which is a very useful decomposition for showing --proving-- things.  The interpretation of the latter decomposition is discussed. [[Positive definite matrix|Positive (semi)-definite matrices]] are introduced and used to define [[Matrix inequality|matrix inequalities]]. [[Ellipsoid|Ellipsoids]] are introduced too. The concept of [[Matrix gain in a direction|matrix gain in a direction]] is discussed and used to define [[Matrix norm|matrix norm]]. [[SVD]] is introduced too, and its interpretation is discussed (see [[SVD]]).</text>
      <sha1>pqsnm6m5p3zx10mhp2e140l2ddhjrbh</sha1>
    </revision>
    <revision>
      <id>222</id>
      <parentid>221</parentid>
      <timestamp>2018-09-12T22:44:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9755">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^M&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].

== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture 7: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].

== Lecture 9: Autonomous linear dynamical systems ==

Autonomous LDSs are discussed, examples are given. Linearization near equilibrium and along trajectory point is discussed.

== Lecture 10: Solution via Laplace transform and matrix exponential == 
[[Laplace Transform]] is introduced and its usage for solving autonomous LDS is shown. [[Characteristic Polynomial]] is also introduced. Finding the eigenvalues is introduced. [[Matrix exponential]] is introduced, and its usage for solving an autonomous LDS is shown. Properties of [[Matrix exponential|matrix exponential]] as a generalization of the exponential are shown. Interpretations of having real or complex [[Eigenvectors and Eigenvalues|eigenvalues]] are introduced. [[Stability]] of a system and its relation to the eigenvalues are shown.

== Lecture 11: Eigenvectors and diagonalization ==
[[Eigenvectors and Eigenvalues|Eigenvalues and left and right eigenvectors]] are introduced. The scaling interpretation for eigenvectors is given. The relationship between eigenvalues, eigenvectors and the behavior of an LDS is introduced. As an application, the usage of right eignevectors for finding the steady state distribution of a Markov chain is shown.

[[Diagonalization]] is introduced, which is very useful for studying systems and showing certain properties of systems (i.e., for proving things). [[Modal Form|modal form]] is also introduced, and its usages are discussed (i.e., simplifying expressions and analyzing systems -- see WTPO? in [[Modal Form]]). Real Modal Form is shown for to analyze with real matrices systems that have complex eigenvalues.

== Lecture 12: Jordan canonical form ==
[[Jordan Canonical Form]] is introduced, which is a generalization of modal form that works for any square matrix (i.e., works for non-diagonalizable matrices too). The [[Cayley-Hamilton theorem]] is also introduced -- this is a useful theorem for showing that any power of an &lt;math&gt;n \times n&lt;/math&gt; matrix is in the span of the set of the first &lt;math&gt;n-1&lt;/math&gt; powers of the matrix (see WTPO in [[Cayley-Hamilton theorem]]).

== Lecture 13: Linear dynamical systems with inputs &amp; outputs ==
Description of an LDS with i/o is given along with terminology of the matrices and vectors that define the system as well as a block diagram. Of note, the "idea of state" is also presented with 4 bullet points (see slide 13-29). The transfer matrix, impulse matrix (or impulse response), step matrix are discussed. It is shown how to use those concepts and convolution to understand the behavior of a system (e.g. see slide 13-9). The discretization of an LDS and the discrete-time transfer function are shown too.

== Lecture 15: Symmetric matrices, quadratic forms, matrix norm, and SVD ==
The title is of the lecture pretty comprehensive. [[Symmetric matrices]] are introduced, which are very useful for studying [[Quadratic form|quadratic forms]] because the eigenvalues of symmetric matrices are real and define upper/lower bounds on quadratic forms. Also, the [[eigendecomposition]] of a symmetric matrix is introduced (see [[Symmetric matrices]] too), which is a very useful decomposition for showing --proving-- things.  The interpretation of the latter decomposition is discussed. [[Positive definite matrix|Positive (semi)-definite matrices]] are introduced and used to define [[Matrix inequality|matrix inequalities]]. [[Ellipsoid|Ellipsoids]] are introduced too. The concept of [[Matrix gain in a direction|matrix gain in a direction]] is discussed and used to define [[Matrix norm|matrix norm]]. [[SVD]] is introduced too, and its interpretation is discussed (see [[SVD]]).

== Lecture 16: SVD Applications ==
Application of SVD as a general [[Pseudo-inverse|pseudo-inverse]] is shown. The full SVD is introduced and used for giving an interpretation to the operation of SVD. SVD is discussed in the context of estimation (&lt;math&gt;y=Ax&lt;/math&gt;) too, and it's used to show the optimality of [[Least-squares]] from another perspective. The usage of [[SVD]] for low-rank matrix approximation is discussed. [[Condition number]] and its utility is introduced. The distance of a matrix to singularity is discussed -- a very useful concept for matrices that are technically non-singular but in practice are pretty close to singularity.</text>
      <sha1>imz8ajg7vp2l9fxht0tg8kgfedxyyc5</sha1>
    </revision>
    <revision>
      <id>525</id>
      <parentid>222</parentid>
      <timestamp>2020-03-15T17:18:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Lecture 2: Linear functions and examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9755">Below are my notes from the online EE263 class taught by Stephen Boyd. The sections in the summary below refer to the lecture notes rather than the videos. Not all lecture notes are included -- only the ones used in the video. Also, some notes mentioned in the video are omitted, because there is more focus on linear algebra parts rather than the LDSs.

== Lecture 1: Overview == 
[[Linear dynamical systems]] (LDSs) are introduced: Equations, terminology, the applications and reasons to study LDSs are provided. They are motivated in comparison to non-linear dynamic systems.

== Lecture 2: Linear functions and examples ==
[[Linear function|Linear functions]] and the matrix multiplication function are introduced. It is shown that every linear function from vector to vector (&lt;math&gt;f:\mathbb{R}^n\to \mathbb{R}^m&lt;/math&gt;) can be written as a matrix multiplication function (&lt;math&gt;f(x) = Ax&lt;/math&gt;) and vice versa.

Interpretations of the matrix multiplication function &lt;math&gt;y=Ax&lt;/math&gt; are given. Specifically, the interpretation of each matrix element of &lt;math&gt;a_{ij}&lt;/math&gt; is discussed: &lt;math&gt;a_{ij}&lt;/math&gt; is the gain factor from &lt;math&gt;j&lt;/math&gt;th input to &lt;math&gt;i&lt;/math&gt;th output.  Examples of applications are given about to illustrate the importance of this intuitive explanation.

Block diagram representation is also illustrated.

[[Linearization]] is introduced.

Broad categories of application are given to illustrate the usefullness of solving &lt;math&gt;y=Ax&lt;/math&gt;.

Different ways of interpreting [[matrix multiplication]] are illustrated both for matrix-vector multiplication and for matrix-matrix multiplication (some are listed in [[Matrix multiplication]]).

== Lecture 3: Linear algebra review ==
Basic of linear algebra are revised, such as: [[Vector Space|vector spaces]], subspaces, independence, basis, dimension, [[Nullspace|nullspace]] (and its interpretations), range and interpretation of range, [[Onto matrices|onto matrices]], [[Inverse matrices|inverse matrices]], [[Rank|rank]] and its usage for fast matrix-vector multiplication, inner product, [[Angle|angle]] and norm, and the [[Cauchy-Schwartz Inequality|Cauchy-Schwartz inequality]].

== Lecture 4: Orthonormal sets of vectors and QR factorization ==
[[Orthonormality|orthonormal]] set of vectors and their geometric properties (i.e., isometric; preserving inner product, norm and [[Angle|angle]]) are discussed. The concept of orthonormal basis and orthogonal matrices are also introduced. Interpretations of the expension in orthonormal basis, &lt;math&gt;x = U U^T x= \sum_{i=1}^n (u_i^T) u_i&lt;/math&gt; (where &lt;math&gt;U&lt;/math&gt; is orthogonal) are given:

* &lt;math&gt;u_i^T x&lt;/math&gt; is called the ''component'' of &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;u_i&lt;/math&gt;
* &lt;math&gt;a = U^T&lt;/math&gt; ''resolves'' &lt;math&gt;x&lt;/math&gt; into the vector of its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; ''reconstitutes'' &lt;math&gt;x&lt;/math&gt; from its &lt;math&gt;u_i&lt;/math&gt; components
* &lt;math&gt;x=Ua&lt;/math&gt; is the &lt;math&gt;(u_i)&lt;/math&gt; expansion of &lt;math&gt;x&lt;/math&gt;

The [[QR Factorization|Gram Schmidt]] (GS) procedure and its general version (i.e., the one that can be used on dependent vectors too) are given. Applications of the GS procedure are listed:
* Yielding orthonormal basis for &lt;math&gt;\mathcal{R}(A)&lt;/math&gt;
* Yielding factorization &lt;math&gt;A=BC&lt;/math&gt; with full-ranked &lt;math&gt;B,C&lt;/math&gt; which is useful for fast matrix-vector multiplication (see also [[Rank]])
* Important: To check if a given vector is in &lt;math&gt;R(A)&lt;/math&gt;
* Revealing rank
* Staircase pattern showing which columns of &lt;math&gt;A&lt;/math&gt; depend on previous ones

The [[QR Factorization|full QR factorization]] is also discussed. Of note, the section nicely ties in the full QR factorization to the [[Fundamental Theorem of Linear Algebra]].

== Lecture 5: Least-squares == 
[[Least squares]] (LS) to approximately solve an overdetermined set of equations (skinny matrix) is introduced. The geometric interpretation of LS (i.e., projecting &lt;math&gt;y&lt;/math&gt; onto &lt;math&gt;\mathcal{R}(A)&lt;/math&gt; and orthogonality between projection and residual) is shown. Properties such as BLUE property are discussed. A motivating example from the first lecture (the one where RMS error is lower than quantization error) is shown.

== Lecture 6: Least-squares applications ==
Applications of LS are shown, e.g., polynomial fitting, system identification. An alternative method to LS that is appropriate to growing sets of data (i.e., online LS) is shown too (see also [[Least squares]]).

== Lecture 7: Regularized least-squares and Gauss-Newton method 7 ==
Regularized LS is shown -- it's nothing but a special case of multi-objective LS (which is also discussed in the same lecture). The [[Gauss-Newton method]] for non-linear least squares is also shown.

== Lecture 8: Least-norm solution ==
Least-norm solution: This time we are dealing with an underdetermined set of equations (fat matrix) and there are infinite possible solutions. As its name suggests, least-norm is the solution with the smallest norm. Geometric interpretation: &lt;math&gt;x_{ln}&lt;/math&gt; is the projection of &lt;math&gt;0&lt;/math&gt; on solution set &lt;math&gt;{x : y = Ax}&lt;/math&gt; (also, &lt;math&gt;x_{ln} \perp \mathcal{N}(A)&lt;/math&gt;).

The relation between least-norm and regularized LS is discussed. Also, the least-norm solution is derived via Lagrange, too. It is shown that both least-norm and least-squares are special cases of [[Norm minimization with equality constraints|norm minimization with equality constraints]].

== Lecture 9: Autonomous linear dynamical systems ==

Autonomous LDSs are discussed, examples are given. Linearization near equilibrium and along trajectory point is discussed.

== Lecture 10: Solution via Laplace transform and matrix exponential == 
[[Laplace Transform]] is introduced and its usage for solving autonomous LDS is shown. [[Characteristic Polynomial]] is also introduced. Finding the eigenvalues is introduced. [[Matrix exponential]] is introduced, and its usage for solving an autonomous LDS is shown. Properties of [[Matrix exponential|matrix exponential]] as a generalization of the exponential are shown. Interpretations of having real or complex [[Eigenvectors and Eigenvalues|eigenvalues]] are introduced. [[Stability]] of a system and its relation to the eigenvalues are shown.

== Lecture 11: Eigenvectors and diagonalization ==
[[Eigenvectors and Eigenvalues|Eigenvalues and left and right eigenvectors]] are introduced. The scaling interpretation for eigenvectors is given. The relationship between eigenvalues, eigenvectors and the behavior of an LDS is introduced. As an application, the usage of right eignevectors for finding the steady state distribution of a Markov chain is shown.

[[Diagonalization]] is introduced, which is very useful for studying systems and showing certain properties of systems (i.e., for proving things). [[Modal Form|modal form]] is also introduced, and its usages are discussed (i.e., simplifying expressions and analyzing systems -- see WTPO? in [[Modal Form]]). Real Modal Form is shown for to analyze with real matrices systems that have complex eigenvalues.

== Lecture 12: Jordan canonical form ==
[[Jordan Canonical Form]] is introduced, which is a generalization of modal form that works for any square matrix (i.e., works for non-diagonalizable matrices too). The [[Cayley-Hamilton theorem]] is also introduced -- this is a useful theorem for showing that any power of an &lt;math&gt;n \times n&lt;/math&gt; matrix is in the span of the set of the first &lt;math&gt;n-1&lt;/math&gt; powers of the matrix (see WTPO in [[Cayley-Hamilton theorem]]).

== Lecture 13: Linear dynamical systems with inputs &amp; outputs ==
Description of an LDS with i/o is given along with terminology of the matrices and vectors that define the system as well as a block diagram. Of note, the "idea of state" is also presented with 4 bullet points (see slide 13-29). The transfer matrix, impulse matrix (or impulse response), step matrix are discussed. It is shown how to use those concepts and convolution to understand the behavior of a system (e.g. see slide 13-9). The discretization of an LDS and the discrete-time transfer function are shown too.

== Lecture 15: Symmetric matrices, quadratic forms, matrix norm, and SVD ==
The title is of the lecture pretty comprehensive. [[Symmetric matrices]] are introduced, which are very useful for studying [[Quadratic form|quadratic forms]] because the eigenvalues of symmetric matrices are real and define upper/lower bounds on quadratic forms. Also, the [[eigendecomposition]] of a symmetric matrix is introduced (see [[Symmetric matrices]] too), which is a very useful decomposition for showing --proving-- things.  The interpretation of the latter decomposition is discussed. [[Positive definite matrix|Positive (semi)-definite matrices]] are introduced and used to define [[Matrix inequality|matrix inequalities]]. [[Ellipsoid|Ellipsoids]] are introduced too. The concept of [[Matrix gain in a direction|matrix gain in a direction]] is discussed and used to define [[Matrix norm|matrix norm]]. [[SVD]] is introduced too, and its interpretation is discussed (see [[SVD]]).

== Lecture 16: SVD Applications ==
Application of SVD as a general [[Pseudo-inverse|pseudo-inverse]] is shown. The full SVD is introduced and used for giving an interpretation to the operation of SVD. SVD is discussed in the context of estimation (&lt;math&gt;y=Ax&lt;/math&gt;) too, and it's used to show the optimality of [[Least-squares]] from another perspective. The usage of [[SVD]] for low-rank matrix approximation is discussed. [[Condition number]] and its utility is introduced. The distance of a matrix to singularity is discussed -- a very useful concept for matrices that are technically non-singular but in practice are pretty close to singularity.</text>
      <sha1>hdwh6tkdkxygnp8izzvhr0k717cja4l</sha1>
    </revision>
  </page>
  <page>
    <title>Epigraph</title>
    <ns>0</ns>
    <id>69</id>
    <revision>
      <id>227</id>
      <timestamp>2018-10-06T18:50:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The epigraph of a function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is the set  &lt;math&gt;\textbf{epi} f = \{(x,t) : x \in \mathbf{dom} f, f(x) \le t\}&lt;/math&gt;  which is a subset o..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="287">The epigraph of a function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is the set

&lt;math&gt;\textbf{epi} f = \{(x,t) : x \in \mathbf{dom} f, f(x) \le t\}&lt;/math&gt;

which is a subset of &lt;math&gt;\mathbb R^{n+1}&lt;/math&gt; &lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. 
== References ==</text>
      <sha1>3lc00ltzkan798r5fj4nlmdt7igkab2</sha1>
    </revision>
  </page>
  <page>
    <title>Convexity</title>
    <ns>0</ns>
    <id>70</id>
    <revision>
      <id>228</id>
      <timestamp>2018-10-06T19:01:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Convexity is a term that applies to sets as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is establish..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="375">Convexity is a term that applies to sets as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is established through the [[Epigraph|epigraph]]: a function &lt;math&gt;f&lt;/math&gt; is [[Convex function|convex]] if and only if its epigraph is a convex set&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.</text>
      <sha1>q6fjamqkykz8rgqurix3qbdzynuxfip</sha1>
    </revision>
    <revision>
      <id>251</id>
      <parentid>228</parentid>
      <timestamp>2018-10-13T13:25:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="466">Convexity is a term that applies to sets as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is established through the [[Epigraph|epigraph]]: a function &lt;math&gt;f&lt;/math&gt; is [[Convex function|convex]] if and only if its epigraph is a convex set&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

See also: [[Convex function#Establishing convexity|operations that preserve convexity ]].</text>
      <sha1>0l82yombwzakmh4m7bmw58cv95bcsf0</sha1>
    </revision>
    <revision>
      <id>526</id>
      <parentid>251</parentid>
      <timestamp>2020-03-21T14:02:43Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="481">Convexity is a term that applies to sets as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is established through the [[Epigraph|epigraph]]: a function &lt;math&gt;f&lt;/math&gt; is [[Convex function|convex]] if and only if its epigraph is a convex set&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

==References==
See also: [[Convex function#Establishing convexity|operations that preserve convexity ]].</text>
      <sha1>4ki3a4ud1gnsrilky9d5howlqr3m3og</sha1>
    </revision>
    <revision>
      <id>527</id>
      <parentid>526</parentid>
      <timestamp>2020-03-21T14:03:24Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="481">Convexity is a term that applies to sets as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is established through the [[Epigraph|epigraph]]: a function &lt;math&gt;f&lt;/math&gt; is [[Convex function|convex]] if and only if its epigraph is a convex set&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

See also: [[Convex function#Establishing convexity|operations that preserve convexity ]].
==References==</text>
      <sha1>kc0ywgr12t4ptzuw79f7p0ygpbvx92z</sha1>
    </revision>
    <revision>
      <id>531</id>
      <parentid>527</parentid>
      <timestamp>2020-03-21T14:25:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="496">Convexity is a term that applies to [[Convex set|sets]] as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is established through the [[Epigraph|epigraph]]: a function &lt;math&gt;f&lt;/math&gt; is [[Convex function|convex]] if and only if its epigraph is a convex set&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

See also: [[Convex function#Establishing convexity|operations that preserve convexity ]].
==References==</text>
      <sha1>0u08qy626nxbw1n3q5kqw2qz8b294aj</sha1>
    </revision>
    <revision>
      <id>559</id>
      <parentid>531</parentid>
      <timestamp>2020-03-22T14:05:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="512">Convexity is a term that applies to [[Convex set|sets]] as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is established through the [[Epigraph|epigraph]]: a function &lt;math&gt;f&lt;/math&gt; is [[Convex function|convex]] if and only if its epigraph is a convex set&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

See also: [[Convex function#Establishing convexity|operations that preserve convexity (for functions) ]].
==References==</text>
      <sha1>07ug00l6hsi766te5mgyyynh7zvr3qf</sha1>
    </revision>
    <revision>
      <id>563</id>
      <parentid>559</parentid>
      <timestamp>2020-03-22T14:20:16Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="619">Convexity is a term that applies to [[Convex set|sets]] as well as functions. The definition of convexity is clearly different for both terms, but the connection between the two is established through the [[Epigraph|epigraph]]: a function &lt;math&gt;f&lt;/math&gt; is [[Convex function|convex]] if and only if its epigraph is a convex set&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

See also: 
* [[Convex set#Operations that preserve set convexity|operations that preserve convexity (for sets) ]].
* [[Convex function#Establishing convexity|operations that preserve convexity (for functions) ]].
==References==</text>
      <sha1>3an2rjj77xvaszhozxbf0grusp4915i</sha1>
    </revision>
  </page>
  <page>
    <title>Convex function</title>
    <ns>0</ns>
    <id>71</id>
    <revision>
      <id>229</id>
      <timestamp>2018-10-06T19:11:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \th..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1117">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.</text>
      <sha1>i3dhcg8hww14cpz48xwk9xqi2te4yt7</sha1>
    </revision>
    <revision>
      <id>230</id>
      <parentid>229</parentid>
      <timestamp>2018-10-06T19:15:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1436">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== References ==</text>
      <sha1>9knyy76mh4ghndg5h5d2301aujhhuse</sha1>
    </revision>
    <revision>
      <id>231</id>
      <parentid>230</parentid>
      <timestamp>2018-10-06T19:30:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2820">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== Establishing convexity == 

There are many practical ways of establishing the convexity of a function (including direct usage of the convexity criteria above). Below is a good list&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 3&lt;/ref&gt;:

# Verify definition
# Verify definition by restricting to a line
# For twice differentiable functions, show that the Hessian is positive
# Show that the function is obtained from simple convex functions by operations that preserve convexity:
#* Nonnegative weighted sum: &lt;math&gt;f(x)=\alpha f_1(x) + \beta f_2(x)&lt;/math&gt; is convex if &lt;math&gt;\alpha,\beta\ge 0&lt;/math&gt; and &lt;math&gt;f_1(x), f_2(x)&lt;/math&gt; are convex. Extends to infinite sums and integrals.
#* Composition with affine function: &lt;math&gt;f(Ax+b)&lt;/math&gt; is convex if &lt;math&gt;f&lt;/math&gt; is convex.
#* Pointwise maximum and supremum: (a) If &lt;math&gt;f_1, \dots, f_m&lt;/math&gt; are convex, then &lt;math&gt;f(x) = \max\{f_1(x),\dots,f_m(x)\}&lt;/math&gt; is convex; (b) If &lt;math&gt;f(x,y)&lt;/math&gt; is convex for each &lt;math&gt;y \in \mathcal A&lt;/math&gt;, then &lt;math&gt;g(x) = \sup_{y \in \mathcal A}f(x,y)&lt;/math&gt; is convex.
#* Composition rules: &lt;math&gt;f(x) = h(g(x))&lt;/math&gt; is convex if (a) &lt;math&gt;g&lt;/math&gt; is convex, &lt;math&gt;h&lt;/math&gt; is convex, &lt;math&gt;\tilde h&lt;/math&gt; (extended value function&lt;ref name=cvbook/&gt;) is nonincreasing; or (b) &lt;math&gt;g&lt;/math&gt; is concave, &lt;math&gt;h&lt;/math&gt; is convex and &lt;math&gt;\tilde h&lt;/math&gt; is nonincreasing

== References ==</text>
      <sha1>psgwid71c568649o39p2qrks8yu5io0</sha1>
    </revision>
    <revision>
      <id>232</id>
      <parentid>231</parentid>
      <timestamp>2018-10-06T19:33:02Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Establishing convexity */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3134">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== Establishing convexity == 

There are many practical ways of establishing the convexity of a function (including direct usage of the convexity criteria above). Below is a good list&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 3&lt;/ref&gt;:

# Verify definition
# Verify definition by restricting to a line
# For twice differentiable functions, show that the Hessian is positive
# Show that the function is obtained from simple convex functions by operations that preserve convexity:
#* Nonnegative weighted sum: &lt;math&gt;f(x)=\alpha f_1(x) + \beta f_2(x)&lt;/math&gt; is convex if &lt;math&gt;\alpha,\beta\ge 0&lt;/math&gt; and &lt;math&gt;f_1(x), f_2(x)&lt;/math&gt; are convex. Extends to infinite sums and integrals.
#* Composition with affine function: &lt;math&gt;f(Ax+b)&lt;/math&gt; is convex if &lt;math&gt;f&lt;/math&gt; is convex.
#* Pointwise maximum and supremum: (a) If &lt;math&gt;f_1, \dots, f_m&lt;/math&gt; are convex, then &lt;math&gt;f(x) = \max\{f_1(x),\dots,f_m(x)\}&lt;/math&gt; is convex; (b) If &lt;math&gt;f(x,y)&lt;/math&gt; is convex for each &lt;math&gt;y \in \mathcal A&lt;/math&gt;, then &lt;math&gt;g(x) = \sup_{y \in \mathcal A}f(x,y)&lt;/math&gt; is convex.
#* Composition rules: &lt;math&gt;f(x) = h(g(x)) = h(g_1(x), \dots, g_k(x))&lt;/math&gt; is convex if (a) &lt;math&gt;g&lt;/math&gt; is convex, &lt;math&gt;h&lt;/math&gt; is convex, &lt;math&gt;\tilde h&lt;/math&gt; (extended value function&lt;ref name=cvbook/&gt;) is nonincreasing; or (b) &lt;math&gt;g&lt;/math&gt; is concave, &lt;math&gt;h&lt;/math&gt; is convex and &lt;math&gt;\tilde h&lt;/math&gt; is nonincreasing in each argument
#* Minimization: If &lt;math&gt;f(x,y)&lt;/math&gt; is convex in (x,y) and &lt;math&gt;C&lt;/math&gt; is a convex set, then &lt;math&gt;g(x) = \inf_{y\in C} f(x,y)&lt;/math&gt; is convex.
#* The perspective of a function &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;g(x,t) = tf(x/t)&lt;/math&gt; is convex if &lt;math&gt;g&lt;/math&gt; is convex.

== References ==</text>
      <sha1>lgtbjq35qqj5wc2ahgw2ulepgjmrpeo</sha1>
    </revision>
    <revision>
      <id>234</id>
      <parentid>232</parentid>
      <timestamp>2018-10-06T19:48:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Establishing convexity */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3875">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== Establishing convexity == 

There are many practical ways of establishing the convexity of a function (including direct usage of the convexity criteria above). Below is a good list&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 3&lt;/ref&gt;:

# Verify definition
# Verify definition by restricting to a line
# For twice differentiable functions, show that the Hessian is positive
# Show that the function is obtained from simple convex functions by operations that preserve convexity:
#* Nonnegative weighted sum: &lt;math&gt;f(x)=\alpha f_1(x) + \beta f_2(x)&lt;/math&gt; is convex if &lt;math&gt;\alpha,\beta\ge 0&lt;/math&gt; and &lt;math&gt;f_1(x), f_2(x)&lt;/math&gt; are convex. Extends to infinite sums and integrals.
#* Composition with affine function: &lt;math&gt;f(Ax+b)&lt;/math&gt; is convex if &lt;math&gt;f&lt;/math&gt; is convex.
#* Pointwise maximum and supremum: (a) If &lt;math&gt;f_1, \dots, f_m&lt;/math&gt; are convex, then &lt;math&gt;f(x) = \max\{f_1(x),\dots,f_m(x)\}&lt;/math&gt; is convex; (b) If &lt;math&gt;f(x,y)&lt;/math&gt; is convex for each &lt;math&gt;y \in \mathcal A&lt;/math&gt;, then &lt;math&gt;g(x) = \sup_{y \in \mathcal A}f(x,y)&lt;/math&gt; is convex.
#* Composition rules: &lt;math&gt;f(x) = h(g(x)) = h(g_1(x), \dots, g_k(x))&lt;/math&gt; is convex if (a) &lt;math&gt;g&lt;/math&gt; is convex, &lt;math&gt;h&lt;/math&gt; is convex, &lt;math&gt;\tilde h&lt;/math&gt; (extended value function&lt;ref name=cvbook/&gt;) is nonincreasing; or (b) &lt;math&gt;g&lt;/math&gt; is concave, &lt;math&gt;h&lt;/math&gt; is convex and &lt;math&gt;\tilde h&lt;/math&gt; is nonincreasing in each argument
#* Minimization: If &lt;math&gt;f(x,y)&lt;/math&gt; is convex in (x,y) and &lt;math&gt;C&lt;/math&gt; is a convex set, then &lt;math&gt;g(x) = \inf_{y\in C} f(x,y)&lt;/math&gt; is convex.
#* The perspective of a function &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;g(x,t) = tf(x/t)&lt;/math&gt; is convex if &lt;math&gt;g&lt;/math&gt; is convex.

== Examples == 

Some typical convex functions are:

* Quadratic function: &lt;math&gt;f(x) = (1/2)x^T P x + q^T + r&lt;/math&gt; where &lt;math&gt;P \ge 0&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; is symmetric.
* Quadratic-over-linear: f(x,y) = x^2/y
* Log-sum-exp: &lt;math&gt;f(x) = \log \sum_{k=1}^n e^{x_k}&lt;/math&gt;
* Geometric mean: f(x) = (\Prod_{k=1}^n x_k)^{1/n}
* Any norm: &lt;math&gt;f(x) = ||x||&lt;/math&gt;
* Distance to farthest point in a set &lt;math&gt;C&lt;/math&gt;:  &lt;math&gt;f(x) = \sup_{y\in C} ||x-y||&lt;/math&gt;
* Distance to a set &lt;math&gt;S&lt;/math&gt;: &lt;math&gt;\inf_{y\in S}||x-y||&lt;/math&gt;.
* Maximum eigenvalue of symmetric matrix: &lt;math&gt;\lambda_{\max} = \sup_{||y||_2 =1}y^T X y&lt;/math&gt;
* Relative entropy:  &lt;math&gt;g(x,t) = t \log t - t\log x&lt;/math&gt; is convex on &lt;math&gt;\mathbb R^{2}_{++}&lt;/math&gt;

== References ==</text>
      <sha1>j7ffbidekidnsajw2wpcibe3s8y3ngg</sha1>
    </revision>
    <revision>
      <id>235</id>
      <parentid>234</parentid>
      <timestamp>2018-10-06T19:48:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3888">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== Establishing convexity == 

There are many practical ways of establishing the convexity of a function (including direct usage of the convexity criteria above). Below is a good list&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 3&lt;/ref&gt;:

# Verify definition
# Verify definition by restricting to a line
# For twice differentiable functions, show that the Hessian is positive
# Show that the function is obtained from simple convex functions by operations that preserve convexity:
#* Nonnegative weighted sum: &lt;math&gt;f(x)=\alpha f_1(x) + \beta f_2(x)&lt;/math&gt; is convex if &lt;math&gt;\alpha,\beta\ge 0&lt;/math&gt; and &lt;math&gt;f_1(x), f_2(x)&lt;/math&gt; are convex. Extends to infinite sums and integrals.
#* Composition with affine function: &lt;math&gt;f(Ax+b)&lt;/math&gt; is convex if &lt;math&gt;f&lt;/math&gt; is convex.
#* Pointwise maximum and supremum: (a) If &lt;math&gt;f_1, \dots, f_m&lt;/math&gt; are convex, then &lt;math&gt;f(x) = \max\{f_1(x),\dots,f_m(x)\}&lt;/math&gt; is convex; (b) If &lt;math&gt;f(x,y)&lt;/math&gt; is convex for each &lt;math&gt;y \in \mathcal A&lt;/math&gt;, then &lt;math&gt;g(x) = \sup_{y \in \mathcal A}f(x,y)&lt;/math&gt; is convex.
#* Composition rules: &lt;math&gt;f(x) = h(g(x)) = h(g_1(x), \dots, g_k(x))&lt;/math&gt; is convex if (a) &lt;math&gt;g&lt;/math&gt; is convex, &lt;math&gt;h&lt;/math&gt; is convex, &lt;math&gt;\tilde h&lt;/math&gt; (extended value function&lt;ref name=cvbook/&gt;) is nonincreasing; or (b) &lt;math&gt;g&lt;/math&gt; is concave, &lt;math&gt;h&lt;/math&gt; is convex and &lt;math&gt;\tilde h&lt;/math&gt; is nonincreasing in each argument
#* Minimization: If &lt;math&gt;f(x,y)&lt;/math&gt; is convex in (x,y) and &lt;math&gt;C&lt;/math&gt; is a convex set, then &lt;math&gt;g(x) = \inf_{y\in C} f(x,y)&lt;/math&gt; is convex.
#* The perspective of a function &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;g(x,t) = tf(x/t)&lt;/math&gt; is convex if &lt;math&gt;g&lt;/math&gt; is convex.

== Examples == 

Some typical convex functions are:

* Quadratic function: &lt;math&gt;f(x) = (1/2)x^T P x + q^T + r&lt;/math&gt; where &lt;math&gt;P \ge 0&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; is symmetric.
* Quadratic-over-linear: f(x,y) = x^2/y
* Log-sum-exp: &lt;math&gt;f(x) = \log \sum_{k=1}^n e^{x_k}&lt;/math&gt;
* Geometric mean: &lt;math&gt;f(x) = (\Prod_{k=1}^n x_k)^{1/n}&lt;/math&gt;
* Any norm: &lt;math&gt;f(x) = ||x||&lt;/math&gt;
* Distance to farthest point in a set &lt;math&gt;C&lt;/math&gt;:  &lt;math&gt;f(x) = \sup_{y\in C} ||x-y||&lt;/math&gt;
* Distance to a set &lt;math&gt;S&lt;/math&gt;: &lt;math&gt;\inf_{y\in S}||x-y||&lt;/math&gt;.
* Maximum eigenvalue of symmetric matrix: &lt;math&gt;\lambda_{\max} = \sup_{||y||_2 =1}y^T X y&lt;/math&gt;
* Relative entropy:  &lt;math&gt;g(x,t) = t \log t - t\log x&lt;/math&gt; is convex on &lt;math&gt;\mathbb R^{2}_{++}&lt;/math&gt;

== References ==</text>
      <sha1>3iopt2bs0ud5ef4wxin4ojdafpm4q72</sha1>
    </revision>
    <revision>
      <id>236</id>
      <parentid>235</parentid>
      <timestamp>2018-10-06T19:49:16Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3901">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== Establishing convexity == 

There are many practical ways of establishing the convexity of a function (including direct usage of the convexity criteria above). Below is a good list&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 3&lt;/ref&gt;:

# Verify definition
# Verify definition by restricting to a line
# For twice differentiable functions, show that the Hessian is positive
# Show that the function is obtained from simple convex functions by operations that preserve convexity:
#* Nonnegative weighted sum: &lt;math&gt;f(x)=\alpha f_1(x) + \beta f_2(x)&lt;/math&gt; is convex if &lt;math&gt;\alpha,\beta\ge 0&lt;/math&gt; and &lt;math&gt;f_1(x), f_2(x)&lt;/math&gt; are convex. Extends to infinite sums and integrals.
#* Composition with affine function: &lt;math&gt;f(Ax+b)&lt;/math&gt; is convex if &lt;math&gt;f&lt;/math&gt; is convex.
#* Pointwise maximum and supremum: (a) If &lt;math&gt;f_1, \dots, f_m&lt;/math&gt; are convex, then &lt;math&gt;f(x) = \max\{f_1(x),\dots,f_m(x)\}&lt;/math&gt; is convex; (b) If &lt;math&gt;f(x,y)&lt;/math&gt; is convex for each &lt;math&gt;y \in \mathcal A&lt;/math&gt;, then &lt;math&gt;g(x) = \sup_{y \in \mathcal A}f(x,y)&lt;/math&gt; is convex.
#* Composition rules: &lt;math&gt;f(x) = h(g(x)) = h(g_1(x), \dots, g_k(x))&lt;/math&gt; is convex if (a) &lt;math&gt;g&lt;/math&gt; is convex, &lt;math&gt;h&lt;/math&gt; is convex, &lt;math&gt;\tilde h&lt;/math&gt; (extended value function&lt;ref name=cvbook/&gt;) is nonincreasing; or (b) &lt;math&gt;g&lt;/math&gt; is concave, &lt;math&gt;h&lt;/math&gt; is convex and &lt;math&gt;\tilde h&lt;/math&gt; is nonincreasing in each argument
#* Minimization: If &lt;math&gt;f(x,y)&lt;/math&gt; is convex in (x,y) and &lt;math&gt;C&lt;/math&gt; is a convex set, then &lt;math&gt;g(x) = \inf_{y\in C} f(x,y)&lt;/math&gt; is convex.
#* The perspective of a function &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;g(x,t) = tf(x/t)&lt;/math&gt; is convex if &lt;math&gt;g&lt;/math&gt; is convex.

== Examples == 

Some typical convex functions are:

* Quadratic function: &lt;math&gt;f(x) = (1/2)x^T P x + q^T + r&lt;/math&gt; where &lt;math&gt;P \ge 0&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; is symmetric.
* Quadratic-over-linear: &lt;math&gt;f(x,y) = x^2/y&lt;/math&gt;
* Log-sum-exp: &lt;math&gt;f(x) = \log \sum_{k=1}^n e^{x_k}&lt;/math&gt;
* Geometric mean: &lt;math&gt;f(x) = (\Prod_{k=1}^n x_k)^{1/n}&lt;/math&gt;
* Any norm: &lt;math&gt;f(x) = ||x||&lt;/math&gt;
* Distance to farthest point in a set &lt;math&gt;C&lt;/math&gt;:  &lt;math&gt;f(x) = \sup_{y\in C} ||x-y||&lt;/math&gt;
* Distance to a set &lt;math&gt;S&lt;/math&gt;: &lt;math&gt;\inf_{y\in S}||x-y||&lt;/math&gt;.
* Maximum eigenvalue of symmetric matrix: &lt;math&gt;\lambda_{\max} = \sup_{||y||_2 =1}y^T X y&lt;/math&gt;
* Relative entropy:  &lt;math&gt;g(x,t) = t \log t - t\log x&lt;/math&gt; is convex on &lt;math&gt;\mathbb R^{2}_{++}&lt;/math&gt;

== References ==</text>
      <sha1>eml65bkcp1u6vyfuj05r17scjgokr5c</sha1>
    </revision>
    <revision>
      <id>237</id>
      <parentid>236</parentid>
      <timestamp>2018-10-06T19:50:24Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3901">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== Establishing convexity == 

There are many practical ways of establishing the convexity of a function (including direct usage of the convexity criteria above). Below is a good list&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 3&lt;/ref&gt;:

# Verify definition
# Verify definition by restricting to a line
# For twice differentiable functions, show that the Hessian is positive
# Show that the function is obtained from simple convex functions by operations that preserve convexity:
#* Nonnegative weighted sum: &lt;math&gt;f(x)=\alpha f_1(x) + \beta f_2(x)&lt;/math&gt; is convex if &lt;math&gt;\alpha,\beta\ge 0&lt;/math&gt; and &lt;math&gt;f_1(x), f_2(x)&lt;/math&gt; are convex. Extends to infinite sums and integrals.
#* Composition with affine function: &lt;math&gt;f(Ax+b)&lt;/math&gt; is convex if &lt;math&gt;f&lt;/math&gt; is convex.
#* Pointwise maximum and supremum: (a) If &lt;math&gt;f_1, \dots, f_m&lt;/math&gt; are convex, then &lt;math&gt;f(x) = \max\{f_1(x),\dots,f_m(x)\}&lt;/math&gt; is convex; (b) If &lt;math&gt;f(x,y)&lt;/math&gt; is convex for each &lt;math&gt;y \in \mathcal A&lt;/math&gt;, then &lt;math&gt;g(x) = \sup_{y \in \mathcal A}f(x,y)&lt;/math&gt; is convex.
#* Composition rules: &lt;math&gt;f(x) = h(g(x)) = h(g_1(x), \dots, g_k(x))&lt;/math&gt; is convex if (a) &lt;math&gt;g&lt;/math&gt; is convex, &lt;math&gt;h&lt;/math&gt; is convex, &lt;math&gt;\tilde h&lt;/math&gt; (extended value function&lt;ref name=cvbook/&gt;) is nonincreasing; or (b) &lt;math&gt;g&lt;/math&gt; is concave, &lt;math&gt;h&lt;/math&gt; is convex and &lt;math&gt;\tilde h&lt;/math&gt; is nonincreasing in each argument
#* Minimization: If &lt;math&gt;f(x,y)&lt;/math&gt; is convex in (x,y) and &lt;math&gt;C&lt;/math&gt; is a convex set, then &lt;math&gt;g(x) = \inf_{y\in C} f(x,y)&lt;/math&gt; is convex.
#* The perspective of a function &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;g(x,t) = tf(x/t)&lt;/math&gt; is convex if &lt;math&gt;g&lt;/math&gt; is convex.

== Examples == 

Some typical convex functions are:

* Quadratic function: &lt;math&gt;f(x) = (1/2)x^T P x + q^T + r&lt;/math&gt; where &lt;math&gt;P \ge 0&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; is symmetric.
* Quadratic-over-linear: &lt;math&gt;f(x,y) = x^2/y&lt;/math&gt;
* Log-sum-exp: &lt;math&gt;f(x) = \log \sum_{k=1}^n e^{x_k}&lt;/math&gt;
* Geometric mean: &lt;math&gt;f(x) = (\prod_{k=1}^n x_k)^{1/n}&lt;/math&gt;
* Any norm: &lt;math&gt;f(x) = ||x||&lt;/math&gt;
* Distance to farthest point in a set &lt;math&gt;C&lt;/math&gt;:  &lt;math&gt;f(x) = \sup_{y\in C} ||x-y||&lt;/math&gt;
* Distance to a set &lt;math&gt;S&lt;/math&gt;: &lt;math&gt;\inf_{y\in S}||x-y||&lt;/math&gt;.
* Maximum eigenvalue of symmetric matrix: &lt;math&gt;\lambda_{\max} = \sup_{||y||_2 =1}y^T X y&lt;/math&gt;
* Relative entropy:  &lt;math&gt;g(x,t) = t \log t - t\log x&lt;/math&gt; is convex on &lt;math&gt;\mathbb R^{2}_{++}&lt;/math&gt;

== References ==</text>
      <sha1>7d97mj2h964zyejyyckanlwfiqog5sb</sha1>
    </revision>
    <revision>
      <id>528</id>
      <parentid>237</parentid>
      <timestamp>2020-03-21T14:07:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Establishing convexity */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3907">A convex function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is a function that satisfies the following inequality for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;0\le \theta \le 1&lt;/math&gt;

&lt;math&gt;f(\theta x + (1-\theta) y) \le \theta f(x) + (1-\theta) f(y)&lt;/math&gt;.

Convexity is an extremely useful property as it renders optimization problems solvable.

== Convexity Criteria ==
While the above-mentioned inequality is the essential property for convexity, there are other equivalent properties.

=== For any function ===

A function &lt;math&gt;f&lt;/math&gt; is convex iff it is convex when restricted to any line that intersects its domain; i.e., if the function &lt;math&gt;g(t) := f(x+tv)&lt;/math&gt; is convex for all direction vectors &lt;math&gt;v&lt;/math&gt;.

=== For differentiable functions ===

If &lt;math&gt;f&lt;/math&gt; is differentiable, then it is convex if and only if &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and 

&lt;math&gt;f(y) \ge f(x) + \nabla f (x)^T(y-x)&lt;/math&gt;

holds for all &lt;math&gt;x,y \in \textbf{dom} f&lt;/math&gt;&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. In other words, &lt;math&gt;f&lt;/math&gt; is convex iff its first-order Taylor approximation is a global underestimator of &lt;math&gt;f&lt;/math&gt; at every point of its domain. If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it is convex iff its Hessian &lt;math&gt;\nabla^2 f&lt;/math&gt; is positive definite, i.e., 

&lt;math&gt;\nabla^2 f(x) \ge 0&lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt;.

== Establishing convexity == 

There are many practical ways of establishing the convexity of a function (including direct usage of the convexity criteria above). Below is a good list&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 3&lt;/ref&gt;:

# Verify definition
# Verify definition by restricting to a line
# For twice differentiable functions, show that the Hessian is positive
# Show that the function is obtained from simple convex functions by '''operations that preserve convexity''':
#* Nonnegative weighted sum: &lt;math&gt;f(x)=\alpha f_1(x) + \beta f_2(x)&lt;/math&gt; is convex if &lt;math&gt;\alpha,\beta\ge 0&lt;/math&gt; and &lt;math&gt;f_1(x), f_2(x)&lt;/math&gt; are convex. Extends to infinite sums and integrals.
#* Composition with affine function: &lt;math&gt;f(Ax+b)&lt;/math&gt; is convex if &lt;math&gt;f&lt;/math&gt; is convex.
#* Pointwise maximum and supremum: (a) If &lt;math&gt;f_1, \dots, f_m&lt;/math&gt; are convex, then &lt;math&gt;f(x) = \max\{f_1(x),\dots,f_m(x)\}&lt;/math&gt; is convex; (b) If &lt;math&gt;f(x,y)&lt;/math&gt; is convex for each &lt;math&gt;y \in \mathcal A&lt;/math&gt;, then &lt;math&gt;g(x) = \sup_{y \in \mathcal A}f(x,y)&lt;/math&gt; is convex.
#* Composition rules: &lt;math&gt;f(x) = h(g(x)) = h(g_1(x), \dots, g_k(x))&lt;/math&gt; is convex if (a) &lt;math&gt;g&lt;/math&gt; is convex, &lt;math&gt;h&lt;/math&gt; is convex, &lt;math&gt;\tilde h&lt;/math&gt; (extended value function&lt;ref name=cvbook/&gt;) is nonincreasing; or (b) &lt;math&gt;g&lt;/math&gt; is concave, &lt;math&gt;h&lt;/math&gt; is convex and &lt;math&gt;\tilde h&lt;/math&gt; is nonincreasing in each argument
#* Minimization: If &lt;math&gt;f(x,y)&lt;/math&gt; is convex in (x,y) and &lt;math&gt;C&lt;/math&gt; is a convex set, then &lt;math&gt;g(x) = \inf_{y\in C} f(x,y)&lt;/math&gt; is convex.
#* The perspective of a function &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;g(x,t) = tf(x/t)&lt;/math&gt; is convex if &lt;math&gt;g&lt;/math&gt; is convex.

== Examples == 

Some typical convex functions are:

* Quadratic function: &lt;math&gt;f(x) = (1/2)x^T P x + q^T + r&lt;/math&gt; where &lt;math&gt;P \ge 0&lt;/math&gt; and &lt;math&gt;P&lt;/math&gt; is symmetric.
* Quadratic-over-linear: &lt;math&gt;f(x,y) = x^2/y&lt;/math&gt;
* Log-sum-exp: &lt;math&gt;f(x) = \log \sum_{k=1}^n e^{x_k}&lt;/math&gt;
* Geometric mean: &lt;math&gt;f(x) = (\prod_{k=1}^n x_k)^{1/n}&lt;/math&gt;
* Any norm: &lt;math&gt;f(x) = ||x||&lt;/math&gt;
* Distance to farthest point in a set &lt;math&gt;C&lt;/math&gt;:  &lt;math&gt;f(x) = \sup_{y\in C} ||x-y||&lt;/math&gt;
* Distance to a set &lt;math&gt;S&lt;/math&gt;: &lt;math&gt;\inf_{y\in S}||x-y||&lt;/math&gt;.
* Maximum eigenvalue of symmetric matrix: &lt;math&gt;\lambda_{\max} = \sup_{||y||_2 =1}y^T X y&lt;/math&gt;
* Relative entropy:  &lt;math&gt;g(x,t) = t \log t - t\log x&lt;/math&gt; is convex on &lt;math&gt;\mathbb R^{2}_{++}&lt;/math&gt;

== References ==</text>
      <sha1>5pdwffm5rsuqpgaudnouslilk41r9fw</sha1>
    </revision>
  </page>
  <page>
    <title>Establishing convexity</title>
    <ns>0</ns>
    <id>72</id>
    <redirect title="Convex function" />
    <revision>
      <id>233</id>
      <timestamp>2018-10-06T19:40:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Convex function#Establishing convexity]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="52">#REDIRECT [[Convex function#Establishing convexity]]</text>
      <sha1>38xw8kcorw9mdr6slnk7wmxfiv8wd1m</sha1>
    </revision>
  </page>
  <page>
    <title>Quasiconvex function</title>
    <ns>0</ns>
    <id>73</id>
    <revision>
      <id>238</id>
      <timestamp>2018-10-08T13:18:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets  &lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1535">A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets

&lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;

for &lt;math&gt;\alpha in \mathbb R&lt;/math&gt;, are convex. A function is quasiconcave if &lt;math&gt;-f&lt;/math&gt; is quasiconcave. A function is ''quasilinear'' if its both quasiconvex and quasiconcave&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Intuitively, quasiconvex functions are unimodal functions.



== Quasiconvexity criteria ==

The following inequality, called modified Jensen inequality, holds for all quasiconvex functions:

&lt;math&gt;f(\theta x+(1-\theta)y) \le \max\{f(x), f(y)\}&lt;/math&gt;

=== Differentiable quasiconvex functions ===

A differentiable function &lt;math&gt;f&lt;/math&gt; is quasiconvex iff &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and for all &lt;math&gt;x,y \in \textbf{dom}f&lt;/math&gt;

&lt;math&gt;f(y) \le f(x) \implies \nabla f(x)^T (y-x) \le 0&lt;/math&gt;




== Examples ==
* &lt;math&gt;\log x&lt;/math&gt; is quasilinear
* The ceiling function &lt;math&gt;\text{ceil}(x) = \inf \{z \in \mathbb Z : z \ge x\}&lt;/math&gt; is quasiconvex
* The function &lt;math&gt;f(x_1,x_2) = x_1 x_2&lt;/math&gt; is neither convex nor concave, but it's quasiconcave.
* Linear-fractional function &lt;math&gt;f(x) = \frac{a^T x+b}{c^Tx +d}&lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \{c^Tx+d &gt; 0\}&lt;/math&gt; is quasilinear.
* Distance ratio function, &lt;math&gt;f(x) = \frac{||x-a||_2}{||x-b||_2}&lt;/math&gt; is quasiconvex on the halfspace \{x: ||x-a||_2 \le ||x-b||_2\}.
* &lt;math&gt;\text{rank}(X+Y)&lt;/math&gt; is quasiconcave

== References ==</text>
      <sha1>52hmtdbau40s0rrb8eisp82kwqld4bb</sha1>
    </revision>
    <revision>
      <id>239</id>
      <parentid>238</parentid>
      <timestamp>2018-10-08T13:23:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2122">A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets

&lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;

for &lt;math&gt;\alpha in \mathbb R&lt;/math&gt;, are convex. A function is quasiconcave if &lt;math&gt;-f&lt;/math&gt; is quasiconcave. A function is ''quasilinear'' if its both quasiconvex and quasiconcave&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Intuitively, quasiconvex functions are unimodal functions


== Quasiconvexity criteria ==

The following inequality, called modified Jensen inequality, holds for all quasiconvex functions:

&lt;math&gt;f(\theta x+(1-\theta)y) \le \max\{f(x), f(y)\}&lt;/math&gt;

=== Differentiable quasiconvex functions ===

A differentiable function &lt;math&gt;f&lt;/math&gt; is quasiconvex iff &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and for all &lt;math&gt;x,y \in \textbf{dom}f&lt;/math&gt;

&lt;math&gt;f(y) \le f(x) \implies \nabla f(x)^T (y-x) \le 0&lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it's quasiconvex iff

&lt;math&gt;y^T \nabla f(x) 0 \implies y^T \nabla^2 f(x) y \ge 0&lt;/math&gt;

=== Operations that preserve quasiconvexity ===

A nonnegative weighted maximum of quasiconvex functions, 

&lt;math&gt;f = \max\{w_1f_1, \dots, w_mf_m\}&lt;/math&gt;,

with &lt;math&gt;w_i\ge 0&lt;/math&gt; and &lt;math&gt;f_i&lt;/math&gt; quasiconvex, is quasiconvex. Extends to supremum as well.




== Examples ==
* &lt;math&gt;\log x&lt;/math&gt; is quasilinear
* The ceiling function &lt;math&gt;\text{ceil}(x) = \inf \{z \in \mathbb Z : z \ge x\}&lt;/math&gt; is quasiconvex
* The function &lt;math&gt;f(x_1,x_2) = x_1 x_2&lt;/math&gt; is neither convex nor concave, but it's quasiconcave.
* Linear-fractional function &lt;math&gt;f(x) = \frac{a^T x+b}{c^Tx +d}&lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \{c^Tx+d &gt; 0\}&lt;/math&gt; is quasilinear.
* Distance ratio function, &lt;math&gt;f(x) = \frac{||x-a||_2}{||x-b||_2}&lt;/math&gt; is quasiconvex on the halfspace \{x: ||x-a||_2 \le ||x-b||_2\}.
* &lt;math&gt;\text{rank}(X+Y)&lt;/math&gt; is quasiconcave
* The maximum generalized eigenvalue, &lt;math&gt;\lambda_{\max}(X,Y) = \sup \frac{u^T X u}{u^T Y u}&lt;/math&gt; is quasiconvex on &lt;math&gt;\textbf{dom} f = \mathbb S^n \times \mathbb S^n_{++} &lt;/math&gt;.

== References ==</text>
      <sha1>9p7zx386sp01uk7d52g9oxf29mkuvct</sha1>
    </revision>
    <revision>
      <id>240</id>
      <parentid>239</parentid>
      <timestamp>2018-10-08T13:24:12Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2118">A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets

&lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;

for &lt;math&gt;\alpha in \mathbb R&lt;/math&gt;, are convex. A function is quasiconcave if &lt;math&gt;-f&lt;/math&gt; is quasiconcave. A function is ''quasilinear'' if its both quasiconvex and quasiconcave&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Intuitively, quasiconvex functions are unimodal functions

== Quasiconvexity criteria ==

The following inequality, called modified Jensen inequality, holds for all quasiconvex functions:

&lt;math&gt;f(\theta x+(1-\theta)y) \le \max\{f(x), f(y)\}&lt;/math&gt;

=== Differentiable quasiconvex functions ===

A differentiable function &lt;math&gt;f&lt;/math&gt; is quasiconvex iff &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and for all &lt;math&gt;x,y \in \textbf{dom}f&lt;/math&gt;

&lt;math&gt;f(y) \le f(x) \implies \nabla f(x)^T (y-x) \le 0&lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it's quasiconvex iff

&lt;math&gt;y^T \nabla f(x) 0 \implies y^T \nabla^2 f(x) y \ge 0&lt;/math&gt;

=== Operations that preserve quasiconvexity ===

A nonnegative weighted maximum of quasiconvex functions, 

&lt;math&gt;f = \max\{w_1f_1, \dots, w_mf_m\}&lt;/math&gt;,

with &lt;math&gt;w_i\ge 0&lt;/math&gt; and &lt;math&gt;f_i&lt;/math&gt; quasiconvex, is quasiconvex. Extends to supremum as well.

== Examples ==
* &lt;math&gt;\log x&lt;/math&gt; is quasilinear
* The ceiling function &lt;math&gt;\text{ceil}(x) = \inf \{z \in \mathbb Z : z \ge x\}&lt;/math&gt; is quasiconvex
* The function &lt;math&gt;f(x_1,x_2) = x_1 x_2&lt;/math&gt; is neither convex nor concave, but it's quasiconcave.
* Linear-fractional function &lt;math&gt;f(x) = \frac{a^T x+b}{c^Tx +d}&lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \{c^Tx+d &gt; 0\}&lt;/math&gt; is quasilinear.
* Distance ratio function, &lt;math&gt;f(x) = \frac{||x-a||_2}{||x-b||_2}&lt;/math&gt; is quasiconvex on the halfspace \{x: ||x-a||_2 \le ||x-b||_2\}.
* &lt;math&gt;\text{rank}(X+Y)&lt;/math&gt; is quasiconcave
* The maximum generalized eigenvalue, &lt;math&gt;\lambda_{\max}(X,Y) = \sup \frac{u^T X u}{u^T Y u}&lt;/math&gt; is quasiconvex on &lt;math&gt;\textbf{dom} f = \mathbb S^n \times \mathbb S^n_{++} &lt;/math&gt;.

== References ==</text>
      <sha1>camzqm5qukmrtrh8ryqny99jkvpu6yo</sha1>
    </revision>
    <revision>
      <id>244</id>
      <parentid>240</parentid>
      <timestamp>2018-10-08T16:22:05Z</timestamp>
      <contributor>
        <ip>159.14.232.34</ip>
      </contributor>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2152">A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets

&lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;

for &lt;math&gt;\alpha in \mathbb R&lt;/math&gt;, are convex. A function is quasiconcave if &lt;math&gt;-f&lt;/math&gt; is quasiconcave. A function is ''quasilinear'' if its both quasiconvex and quasiconcave&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Intuitively, quasiconvex functions are unimodal functions

== Quasiconvexity criteria ==

The following inequality, called modified Jensen inequality, holds for all quasiconvex functions:

&lt;math&gt;f(\theta x+(1-\theta)y) \le \max\{f(x), f(y)\}&lt;/math&gt;

=== Differentiable quasiconvex functions ===

A differentiable function &lt;math&gt;f&lt;/math&gt; is quasiconvex iff &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and for all &lt;math&gt;x,y \in \textbf{dom}f&lt;/math&gt;

&lt;math&gt;f(y) \le f(x) \implies \nabla f(x)^T (y-x) \le 0&lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it's quasiconvex iff

&lt;math&gt;y^T \nabla f(x) 0 \implies y^T \nabla^2 f(x) y \ge 0&lt;/math&gt;

=== Operations that preserve quasiconvexity ===

A nonnegative weighted maximum of quasiconvex functions, 

&lt;math&gt;f = \max\{w_1f_1, \dots, w_mf_m\}&lt;/math&gt;,

with &lt;math&gt;w_i\ge 0&lt;/math&gt; and &lt;math&gt;f_i&lt;/math&gt; quasiconvex, is quasiconvex. Extends to supremum as well.

== Examples ==
* &lt;math&gt;\log x&lt;/math&gt; is quasilinear
* The ceiling function &lt;math&gt;\text{ceil}(x) = \inf \{z \in \mathbb Z : z \ge x\}&lt;/math&gt; is quasiconvex
* The function &lt;math&gt;f(x_1,x_2) = x_1 x_2&lt;/math&gt; is neither convex nor concave, but it's quasiconcave (on &lt;math&gt;\mathbb R^2_{+}&lt;/math&gt;).
* Linear-fractional function &lt;math&gt;f(x) = \frac{a^T x+b}{c^Tx +d}&lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \{c^Tx+d &gt; 0\}&lt;/math&gt; is quasilinear.
* Distance ratio function, &lt;math&gt;f(x) = \frac{||x-a||_2}{||x-b||_2}&lt;/math&gt; is quasiconvex on the halfspace \{x: ||x-a||_2 \le ||x-b||_2\}.
* &lt;math&gt;\text{rank}(X+Y)&lt;/math&gt; is quasiconcave
* The maximum generalized eigenvalue, &lt;math&gt;\lambda_{\max}(X,Y) = \sup \frac{u^T X u}{u^T Y u}&lt;/math&gt; is quasiconvex on &lt;math&gt;\textbf{dom} f = \mathbb S^n \times \mathbb S^n_{++} &lt;/math&gt;.

== References ==</text>
      <sha1>1kj9rvempkcj75y1p6a78wd6l95qzja</sha1>
    </revision>
    <revision>
      <id>245</id>
      <parentid>244</parentid>
      <timestamp>2018-10-08T19:13:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2165">A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets

&lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;

for &lt;math&gt;\alpha in \mathbb R&lt;/math&gt;, are convex. A function is quasiconcave if &lt;math&gt;-f&lt;/math&gt; is quasiconcave. A function is ''quasilinear'' if its both quasiconvex and quasiconcave&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Intuitively, quasiconvex functions are unimodal functions

== Quasiconvexity criteria ==

The following inequality, called modified Jensen inequality, holds for all quasiconvex functions:

&lt;math&gt;f(\theta x+(1-\theta)y) \le \max\{f(x), f(y)\}&lt;/math&gt;

=== Differentiable quasiconvex functions ===

A differentiable function &lt;math&gt;f&lt;/math&gt; is quasiconvex iff &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and for all &lt;math&gt;x,y \in \textbf{dom}f&lt;/math&gt;

&lt;math&gt;f(y) \le f(x) \implies \nabla f(x)^T (y-x) \le 0&lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it's quasiconvex iff

&lt;math&gt;y^T \nabla f(x) 0 \implies y^T \nabla^2 f(x) y \ge 0&lt;/math&gt;

=== Operations that preserve quasiconvexity ===

A nonnegative weighted maximum of quasiconvex functions, 

&lt;math&gt;f = \max\{w_1f_1, \dots, w_mf_m\}&lt;/math&gt;,

with &lt;math&gt;w_i\ge 0&lt;/math&gt; and &lt;math&gt;f_i&lt;/math&gt; quasiconvex, is quasiconvex. Extends to supremum as well.

== Examples ==
* &lt;math&gt;\log x&lt;/math&gt; is quasilinear
* The ceiling function &lt;math&gt;\text{ceil}(x) = \inf \{z \in \mathbb Z : z \ge x\}&lt;/math&gt; is quasiconvex
* The function &lt;math&gt;f(x_1,x_2) = x_1 x_2&lt;/math&gt; is neither convex nor concave, but it's quasiconcave (on &lt;math&gt;\mathbb R^2_{+}&lt;/math&gt;).
* Linear-fractional function &lt;math&gt;f(x) = \frac{a^T x+b}{c^Tx +d}&lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \{c^Tx+d &gt; 0\}&lt;/math&gt; is quasilinear.
* Distance ratio function, &lt;math&gt;f(x) = \frac{||x-a||_2}{||x-b||_2}&lt;/math&gt; is quasiconvex on the halfspace &lt;math&gt;\{x: ||x-a||_2 \le ||x-b||_2\}&lt;/math&gt;.
* &lt;math&gt;\text{rank}(X+Y)&lt;/math&gt; is quasiconcave
* The maximum generalized eigenvalue, &lt;math&gt;\lambda_{\max}(X,Y) = \sup \frac{u^T X u}{u^T Y u}&lt;/math&gt; is quasiconvex on &lt;math&gt;\textbf{dom} f = \mathbb S^n \times \mathbb S^n_{++} &lt;/math&gt;.

== References ==</text>
      <sha1>ddjwyzgabqgy9gdz9lrqn3ekm9zr59u</sha1>
    </revision>
    <revision>
      <id>255</id>
      <parentid>245</parentid>
      <timestamp>2018-10-13T13:35:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2166">A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets

&lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;

for &lt;math&gt;\alpha \in \mathbb R&lt;/math&gt;, are convex. A function is quasiconcave if &lt;math&gt;-f&lt;/math&gt; is quasiconcave. A function is ''quasilinear'' if its both quasiconvex and quasiconcave&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Intuitively, quasiconvex functions are unimodal functions

== Quasiconvexity criteria ==

The following inequality, called modified Jensen inequality, holds for all quasiconvex functions:

&lt;math&gt;f(\theta x+(1-\theta)y) \le \max\{f(x), f(y)\}&lt;/math&gt;

=== Differentiable quasiconvex functions ===

A differentiable function &lt;math&gt;f&lt;/math&gt; is quasiconvex iff &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and for all &lt;math&gt;x,y \in \textbf{dom}f&lt;/math&gt;

&lt;math&gt;f(y) \le f(x) \implies \nabla f(x)^T (y-x) \le 0&lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it's quasiconvex iff

&lt;math&gt;y^T \nabla f(x) 0 \implies y^T \nabla^2 f(x) y \ge 0&lt;/math&gt;

=== Operations that preserve quasiconvexity ===

A nonnegative weighted maximum of quasiconvex functions, 

&lt;math&gt;f = \max\{w_1f_1, \dots, w_mf_m\}&lt;/math&gt;,

with &lt;math&gt;w_i\ge 0&lt;/math&gt; and &lt;math&gt;f_i&lt;/math&gt; quasiconvex, is quasiconvex. Extends to supremum as well.

== Examples ==
* &lt;math&gt;\log x&lt;/math&gt; is quasilinear
* The ceiling function &lt;math&gt;\text{ceil}(x) = \inf \{z \in \mathbb Z : z \ge x\}&lt;/math&gt; is quasiconvex
* The function &lt;math&gt;f(x_1,x_2) = x_1 x_2&lt;/math&gt; is neither convex nor concave, but it's quasiconcave (on &lt;math&gt;\mathbb R^2_{+}&lt;/math&gt;).
* Linear-fractional function &lt;math&gt;f(x) = \frac{a^T x+b}{c^Tx +d}&lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \{c^Tx+d &gt; 0\}&lt;/math&gt; is quasilinear.
* Distance ratio function, &lt;math&gt;f(x) = \frac{||x-a||_2}{||x-b||_2}&lt;/math&gt; is quasiconvex on the halfspace &lt;math&gt;\{x: ||x-a||_2 \le ||x-b||_2\}&lt;/math&gt;.
* &lt;math&gt;\text{rank}(X+Y)&lt;/math&gt; is quasiconcave
* The maximum generalized eigenvalue, &lt;math&gt;\lambda_{\max}(X,Y) = \sup \frac{u^T X u}{u^T Y u}&lt;/math&gt; is quasiconvex on &lt;math&gt;\textbf{dom} f = \mathbb S^n \times \mathbb S^n_{++} &lt;/math&gt;.

== References ==</text>
      <sha1>pw8qeuglhantsww3ullo84m5r7cgi8q</sha1>
    </revision>
    <revision>
      <id>365</id>
      <parentid>255</parentid>
      <timestamp>2019-05-13T13:44:34Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2167">A function &lt;math&gt;f:\mathbb R^n \to R&lt;/math&gt; is called quasiconvex if its domain and all its sublevel sets

&lt;math&gt;S_\alpha = \{x \in \textbf{dom} f : f(x) \le \alpha\}&lt;/math&gt;

for &lt;math&gt;\alpha \in \mathbb R&lt;/math&gt;, are convex. A function is quasiconcave if &lt;math&gt;-f&lt;/math&gt; is quasiconcave. A function is ''quasilinear'' if it's both quasiconvex and quasiconcave&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Intuitively, quasiconvex functions are unimodal functions

== Quasiconvexity criteria ==

The following inequality, called modified Jensen inequality, holds for all quasiconvex functions:

&lt;math&gt;f(\theta x+(1-\theta)y) \le \max\{f(x), f(y)\}&lt;/math&gt;

=== Differentiable quasiconvex functions ===

A differentiable function &lt;math&gt;f&lt;/math&gt; is quasiconvex iff &lt;math&gt;\textbf{dom} f&lt;/math&gt; is convex and for all &lt;math&gt;x,y \in \textbf{dom}f&lt;/math&gt;

&lt;math&gt;f(y) \le f(x) \implies \nabla f(x)^T (y-x) \le 0&lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is twice differentiable, then it's quasiconvex iff

&lt;math&gt;y^T \nabla f(x) 0 \implies y^T \nabla^2 f(x) y \ge 0&lt;/math&gt;

=== Operations that preserve quasiconvexity ===

A nonnegative weighted maximum of quasiconvex functions, 

&lt;math&gt;f = \max\{w_1f_1, \dots, w_mf_m\}&lt;/math&gt;,

with &lt;math&gt;w_i\ge 0&lt;/math&gt; and &lt;math&gt;f_i&lt;/math&gt; quasiconvex, is quasiconvex. Extends to supremum as well.

== Examples ==
* &lt;math&gt;\log x&lt;/math&gt; is quasilinear
* The ceiling function &lt;math&gt;\text{ceil}(x) = \inf \{z \in \mathbb Z : z \ge x\}&lt;/math&gt; is quasiconvex
* The function &lt;math&gt;f(x_1,x_2) = x_1 x_2&lt;/math&gt; is neither convex nor concave, but it's quasiconcave (on &lt;math&gt;\mathbb R^2_{+}&lt;/math&gt;).
* Linear-fractional function &lt;math&gt;f(x) = \frac{a^T x+b}{c^Tx +d}&lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \{c^Tx+d &gt; 0\}&lt;/math&gt; is quasilinear.
* Distance ratio function, &lt;math&gt;f(x) = \frac{||x-a||_2}{||x-b||_2}&lt;/math&gt; is quasiconvex on the halfspace &lt;math&gt;\{x: ||x-a||_2 \le ||x-b||_2\}&lt;/math&gt;.
* &lt;math&gt;\text{rank}(X+Y)&lt;/math&gt; is quasiconcave
* The maximum generalized eigenvalue, &lt;math&gt;\lambda_{\max}(X,Y) = \sup \frac{u^T X u}{u^T Y u}&lt;/math&gt; is quasiconvex on &lt;math&gt;\textbf{dom} f = \mathbb S^n \times \mathbb S^n_{++} &lt;/math&gt;.

== References ==</text>
      <sha1>7vj3x4nt9qz6d7ukogzscnrb4uobc7w</sha1>
    </revision>
  </page>
  <page>
    <title>Log-concavity</title>
    <ns>0</ns>
    <id>74</id>
    <revision>
      <id>241</id>
      <timestamp>2018-10-08T13:31:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is logarithmically concave or log-concave if &lt;math&gt;f(x) &gt; 0 &lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;\lo..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1000">A function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is logarithmically concave or log-concave if &lt;math&gt;f(x) &gt; 0 &lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;\log f&lt;/math&gt; is concave. A function is log-convex iff &lt;math&gt;1/f&lt;/math&gt; is log-concave.

=== Criterion ===

Log-concavity can be expressed directly, without logarithms, through the following inequality:

&lt;math&gt;f(\theta x+(1-\theta y)) \ge f(x)^\theta f(y)^{(1-\theta)} &lt;/math&gt;

=== Operations that preserve log-concavity ===

Log-convexity and log-concavity are closed under multiplication and positive scaling. Sums of log-concave functions are not, in general, log-concave. However, sums of log-convex functions are log-concave. 

Of note, in some special cases, log-concavity is preserved by integration. If &lt;math&gt;f:\mathbb R^n \times \mathbb R^m \to \mathbb R&lt;/math&gt; is log-concave, then

&lt;math&gt;g(x) = \int f(x,y) dy&lt;/math&gt;

is alog-concave function of &lt;math&gt;x&lt;/math&gt; (The integration here is over &lt;math&gt;\mathbb R^m&lt;/math&gt;).</text>
      <sha1>fbgtlto862tmptlkwb6sl949scuxp40</sha1>
    </revision>
    <revision>
      <id>242</id>
      <parentid>241</parentid>
      <timestamp>2018-10-08T13:36:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1709">A function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is logarithmically concave or log-concave if &lt;math&gt;f(x) &gt; 0 &lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;\log f&lt;/math&gt; is concave. A function is log-convex iff &lt;math&gt;1/f&lt;/math&gt; is log-concave.

=== Criterion ===

Log-concavity can be expressed directly, without logarithms, through the following inequality:

&lt;math&gt;f(\theta x+(1-\theta y)) \ge f(x)^\theta f(y)^{(1-\theta)} &lt;/math&gt;

=== Operations that preserve log-concavity ===

Log-convexity and log-concavity are closed under multiplication and positive scaling. Sums of log-concave functions are not, in general, log-concave. However, sums of log-convex functions are log-concave. 

Of note, in some special cases, log-concavity is preserved by integration. If &lt;math&gt;f:\mathbb R^n \times \mathbb R^m \to \mathbb R&lt;/math&gt; is log-concave, then

&lt;math&gt;g(x) = \int f(x,y) dy&lt;/math&gt;

is alog-concave function of &lt;math&gt;x&lt;/math&gt; (The integration here is over &lt;math&gt;\mathbb R^m&lt;/math&gt;). This result has important implications, two of which are:
* Marginal distributions of log-concave probability functions are log-concave. 
* Log-concavity is preserved under convolution, i.e., if &lt;math&gt;f&lt;/math&gt; and &lt;math&gt;g&lt;/math&gt; are log-concave, then so is the convolution &lt;math&gt;(f \ast g)(x) = \int f(x-y)g(y) dy&lt;/math&gt;
* Suppose &lt;math&gt;C\subseteq \mathbb R^n&lt;/math&gt; is a convex set and &lt;math&gt;w&lt;/math&gt; is a random vector in &lt;math&gt;\mathbb R^n&lt;/math&gt; with log-concave probability density function &lt;math&gt;p&lt;/math&gt;. Then, the function &lt;math&gt;f(x) \textbf{prob}(x+w \in C)&lt;/math&gt; is log-concave in &lt;math&gt;x&lt;/math&gt;. Example usage: When &lt;math&gt;x&lt;/math&gt; is a target value to be estimated and &lt;math&gt;w&lt;/math&gt; is the noise.</text>
      <sha1>d5ig2fxte9mfeibhv2xm5awsq703egu</sha1>
    </revision>
    <revision>
      <id>243</id>
      <parentid>242</parentid>
      <timestamp>2018-10-08T13:36:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Operations that preserve log-concavity */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1710">A function &lt;math&gt;f:\mathbb R^n \to \mathbb R&lt;/math&gt; is logarithmically concave or log-concave if &lt;math&gt;f(x) &gt; 0 &lt;/math&gt; for all &lt;math&gt;x \in \textbf{dom} f&lt;/math&gt; and &lt;math&gt;\log f&lt;/math&gt; is concave. A function is log-convex iff &lt;math&gt;1/f&lt;/math&gt; is log-concave.

=== Criterion ===

Log-concavity can be expressed directly, without logarithms, through the following inequality:

&lt;math&gt;f(\theta x+(1-\theta y)) \ge f(x)^\theta f(y)^{(1-\theta)} &lt;/math&gt;

=== Operations that preserve log-concavity ===

Log-convexity and log-concavity are closed under multiplication and positive scaling. Sums of log-concave functions are not, in general, log-concave. However, sums of log-convex functions are log-concave. 

Of note, in some special cases, log-concavity is preserved by integration. If &lt;math&gt;f:\mathbb R^n \times \mathbb R^m \to \mathbb R&lt;/math&gt; is log-concave, then

&lt;math&gt;g(x) = \int f(x,y) dy&lt;/math&gt;

is alog-concave function of &lt;math&gt;x&lt;/math&gt; (The integration here is over &lt;math&gt;\mathbb R^m&lt;/math&gt;). This result has important implications, two of which are:
* Marginal distributions of log-concave probability functions are log-concave. 
* Log-concavity is preserved under convolution, i.e., if &lt;math&gt;f&lt;/math&gt; and &lt;math&gt;g&lt;/math&gt; are log-concave, then so is the convolution &lt;math&gt;(f \ast g)(x) = \int f(x-y)g(y) dy&lt;/math&gt;
* Suppose &lt;math&gt;C\subseteq \mathbb R^n&lt;/math&gt; is a convex set and &lt;math&gt;w&lt;/math&gt; is a random vector in &lt;math&gt;\mathbb R^n&lt;/math&gt; with log-concave probability density function &lt;math&gt;p&lt;/math&gt;. Then, the function &lt;math&gt;f(x)= \textbf{prob}(x+w \in C)&lt;/math&gt; is log-concave in &lt;math&gt;x&lt;/math&gt;. Example usage: When &lt;math&gt;x&lt;/math&gt; is a target value to be estimated and &lt;math&gt;w&lt;/math&gt; is the noise.</text>
      <sha1>r7qvuo73rqlep2izhlxbzwh4qofywg0</sha1>
    </revision>
  </page>
  <page>
    <title>Convex optimization</title>
    <ns>0</ns>
    <id>75</id>
    <revision>
      <id>247</id>
      <timestamp>2018-10-13T13:14:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>,&gt;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="317">A convex optimization problem (in standard form) can be written as&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize} f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

== References ==</text>
      <sha1>5png3vfl7j0dyz2ufwa6sl1phzhsv08</sha1>
    </revision>
    <revision>
      <id>248</id>
      <parentid>247</parentid>
      <timestamp>2018-10-13T13:18:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="846">In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize} f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization problems ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

== References ==</text>
      <sha1>g8qrclj5554rlg9l8oysi3t5odjdbop</sha1>
    </revision>
    <revision>
      <id>249</id>
      <parentid>248</parentid>
      <timestamp>2018-10-13T13:18:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="874">== Optimization problem ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize} f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization problems ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

== References ==</text>
      <sha1>slwt22iuoe6vc0tg9gfwe22ubapq12l</sha1>
    </revision>
    <revision>
      <id>250</id>
      <parentid>249</parentid>
      <timestamp>2018-10-13T13:22:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1340">== Optimization problem ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize} f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

=== Quasiconvex optimization ===

The definition of a ''quasiconvex optimization problem'' is very similar to the definition of a convex optimization problem; the only different is that &lt;math&gt;f_0&lt;/math&gt; is allowed to be a [[Quasiconvex function|quasiconvex function]] and not expected to be strictly a convex function. Note that quasiconvex constraint functions can always be converted to equivalent convex constraint functions (see page 144&lt;ref name=cobook/&gt;).

== References ==</text>
      <sha1>fcqc8xgyzcxvj2edx08am5y18ilrh2k</sha1>
    </revision>
    <revision>
      <id>253</id>
      <parentid>250</parentid>
      <timestamp>2018-10-13T13:32:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1801">== Optimization problems ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize} f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

=== Quasiconvex optimization ===

The definition of a ''quasiconvex optimization problem'' is very similar to the definition of a convex optimization problem; the only different is that &lt;math&gt;f_0&lt;/math&gt; is allowed to be a [[Quasiconvex function|quasiconvex function]] and not expected to be strictly a convex function. Note that quasiconvex constraint functions can always be converted to equivalent convex constraint functions (see page 144&lt;ref name=cobook/&gt;).

=== Convex feasibility problem ===
A problem is called ''convex feasibility problem'' if the goal is to just find any &lt;math&gt;x&lt;/math&gt; that is feasible: 

 &lt;math&gt;\text{find} x&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

Convex feasibility problems are used for solving quasiconvex problems; see the [[Bisection method|bisection method for quasiconvex problems]].
 
== References ==</text>
      <sha1>fntafznuulu9hxnpuif6d3ohh7qs0mm</sha1>
    </revision>
    <revision>
      <id>260</id>
      <parentid>253</parentid>
      <timestamp>2018-10-13T14:08:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2257">== Optimization problems ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

=== Quasiconvex optimization ===

The definition of a ''quasiconvex optimization problem'' is very similar to the definition of a convex optimization problem; the only different is that &lt;math&gt;f_0&lt;/math&gt; is allowed to be a [[Quasiconvex function|quasiconvex function]] and not expected to be strictly a convex function. Note that quasiconvex constraint functions can always be converted to equivalent convex constraint functions (see page 144&lt;ref name=cobook/&gt;).

=== Convex feasibility problem ===
A problem is called ''convex feasibility problem'' if the goal is to just find any &lt;math&gt;x&lt;/math&gt; that is feasible: 

 &lt;math&gt;\text{find } x&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

Convex feasibility problems are used for solving quasiconvex problems; see the [[Bisection method|bisection method for quasiconvex problems]].

=== Geometric program ===

An optimization problem in the form 
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 1, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) = 1, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0, f_i, h_i&lt;/math&gt; are [[Geometric programming#Monomials and posynomials|posynomials]] are called [[Geometric programming|geometric programs]]; GPs can be transformed into convex optimization problems.
 
== References ==</text>
      <sha1>mq20om75oyesum5ilfk7xb670o2ip3k</sha1>
    </revision>
    <revision>
      <id>283</id>
      <parentid>260</parentid>
      <timestamp>2018-10-13T18:51:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2869">== Optimization problems ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

=== Quasiconvex optimization ===

The definition of a ''quasiconvex optimization problem'' is very similar to the definition of a convex optimization problem; the only different is that &lt;math&gt;f_0&lt;/math&gt; is allowed to be a [[Quasiconvex function|quasiconvex function]] and not expected to be strictly a convex function. Note that quasiconvex constraint functions can always be converted to equivalent convex constraint functions (see page 144&lt;ref name=cobook/&gt;).

=== Convex feasibility problem ===
A problem is called ''convex feasibility problem'' if the goal is to just find any &lt;math&gt;x&lt;/math&gt; that is feasible: 

 &lt;math&gt;\text{find } x&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

Convex feasibility problems are used for solving quasiconvex problems; see the [[Bisection method|bisection method for quasiconvex problems]].

=== Geometric program ===

An optimization problem in the form 
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 1, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) = 1, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0, f_i, h_i&lt;/math&gt; are [[Geometric programming#Monomials and posynomials|posynomials]] are called [[Geometric programming|geometric programs]]; GPs can be transformed into convex optimization problems.
 
== How are convex optimization problems actually solved? ==

A curious question is how the problems that involve possibly non-differentiable functions and inequality constraints are solved; after all, all that we know to minimize analytically (without iterations) is a quadratic and unconstrained function.

The page [[How are optimization problems solved?]] tries to very superficially answer the question, but here is the idea: We can solve those problems by converting them to a succession of problems that we know how to solve: unconstrained convex optimization problems with quadratic objective function. 

== References ==</text>
      <sha1>6bjslf446m2q788a2e7tf6vez5thtpl</sha1>
    </revision>
    <revision>
      <id>284</id>
      <parentid>283</parentid>
      <timestamp>2018-10-13T19:05:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4597">== Optimization problems ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

=== Quasiconvex optimization ===

The definition of a ''quasiconvex optimization problem'' is very similar to the definition of a convex optimization problem; the only different is that &lt;math&gt;f_0&lt;/math&gt; is allowed to be a [[Quasiconvex function|quasiconvex function]] and not expected to be strictly a convex function. Note that quasiconvex constraint functions can always be converted to equivalent convex constraint functions (see page 144&lt;ref name=cobook/&gt;).

=== Convex feasibility problem ===
A problem is called ''convex feasibility problem'' if the goal is to just find any &lt;math&gt;x&lt;/math&gt; that is feasible: 

 &lt;math&gt;\text{find } x&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

Convex feasibility problems are used for solving quasiconvex problems; see the [[Bisection method|bisection method for quasiconvex problems]].

=== Geometric program ===

An optimization problem in the form 
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 1, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) = 1, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0, f_i, h_i&lt;/math&gt; are [[Geometric programming#Monomials and posynomials|posynomials]] are called [[Geometric programming|geometric programs]]; GPs can be transformed into convex optimization problems.

== Equivalent convex problems ==

Problems can be described in equivalent forms that are easier to solve. Those problems are not the same problems but they are equivalent, (roughly) meaning that they have the same solution. For example, the problem &lt;math&gt;\text{minimize } ||x||_2&lt;/math&gt; is not the same as the problem &lt;math&gt;\text{minimize } ||x||_2^2&lt;/math&gt; but they have the same solution. 

A convex optimization problem is very often solved by converting it into an equivalent problem; in fact, in terms of implementation, that seems to be almost always the case (see also [[How are optimization problems solved?]]). This is very understandable because one can convert, for example, a problem with a non-differentiable objective function into an equivalent problem with a differentiable function. 

Beyond facilitating the solution, converting problems into an equivalent problem may also make the problem easier to understand. Furthermore, the dual problems of equivalent problems are in general not the same. This is very important, because it means that the dual of a converted problem can have a very simple solution even if the dual of the original problem does not have a solution.

=== Some standard conversions ===
Below are some standard problem conversions.
* Eliminate equality constraint: One can take the constraint &lt;math&gt;Ax=b&lt;/math&gt; and embed it into the objective function.
* Introducing equality constraints: The problem may become easier to solve by adding (affine) equality constraintes
* Introducing slack variables: e.g. converting the inequality constraint &lt;math&gt;a^Tx\le b&lt;/math&gt; into two constraints &lt;math&gt;a^T x+s=b&lt;/math&gt; and &lt;math&gt;s\ge 0&lt;/math&gt;. Here &lt;math&gt;s&lt;/math&gt; is called the slack variable.




 
== How are convex optimization problems actually solved? ==

A curious question is how the problems that involve possibly non-differentiable functions and inequality constraints are solved; after all, all that we know to minimize analytically (without iterations) is a quadratic and unconstrained function.

The page [[How are optimization problems solved?]] tries to very superficially answer the question, but here is the idea: We can solve those problems by converting them to a succession of problems that we know how to solve: unconstrained convex optimization problems with quadratic objective function. 

== References ==</text>
      <sha1>hqjqnjeh53gbw9xpborxi74zud0dpcx</sha1>
    </revision>
    <revision>
      <id>285</id>
      <parentid>284</parentid>
      <timestamp>2018-10-13T19:11:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5111">== Optimization problems ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

=== Quasiconvex optimization ===

The definition of a ''quasiconvex optimization problem'' is very similar to the definition of a convex optimization problem; the only different is that &lt;math&gt;f_0&lt;/math&gt; is allowed to be a [[Quasiconvex function|quasiconvex function]] and not expected to be strictly a convex function. Note that quasiconvex constraint functions can always be converted to equivalent convex constraint functions (see page 144&lt;ref name=cobook/&gt;).

=== Convex feasibility problem ===
A problem is called ''convex feasibility problem'' if the goal is to just find any &lt;math&gt;x&lt;/math&gt; that is feasible: 

 &lt;math&gt;\text{find } x&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

Convex feasibility problems are used for solving quasiconvex problems; see the [[Bisection method|bisection method for quasiconvex problems]].

=== Geometric program ===

An optimization problem in the form 
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 1, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) = 1, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0, f_i, h_i&lt;/math&gt; are [[Geometric programming#Monomials and posynomials|posynomials]] are called [[Geometric programming|geometric programs]]; GPs can be transformed into convex optimization problems.

== Equivalent convex problems ==

Problems can be described in equivalent forms that are easier to solve. Those problems are not the same problems but they are equivalent, (roughly) meaning that they have the same solution. For example, the problem &lt;math&gt;\text{minimize } ||x||_2&lt;/math&gt; is not the same as the problem &lt;math&gt;\text{minimize } ||x||_2^2&lt;/math&gt; but they have the same solution. 

A convex optimization problem is very often solved by converting it into an equivalent problem; in fact, in terms of implementation, that seems to be almost always the case (see also [[How are optimization problems solved?]]). This is very understandable because one can convert, for example, a problem with a non-differentiable objective function into an equivalent problem with a differentiable function. 

Beyond facilitating the solution, converting problems into an equivalent problem may also make the problem easier to understand. Furthermore, the dual problems of equivalent problems are in general not the same. This is very important, because it means that the dual of a converted problem can have a very simple solution even if the dual of the original problem does not have a solution.

=== Some standard conversions ===
Below are some standard problem conversions.
* Eliminate equality constraint: One can take the constraint &lt;math&gt;Ax=b&lt;/math&gt; and embed it into the objective function.
* Introducing equality constraints: The problem may become easier to solve by adding (affine) equality constraintes
* Introducing slack variables: e.g. converting the inequality constraint &lt;math&gt;a^Tx\le b&lt;/math&gt; into two constraints &lt;math&gt;a^T x+s=b&lt;/math&gt; and &lt;math&gt;s\ge 0&lt;/math&gt;. Here &lt;math&gt;s&lt;/math&gt; is called the slack variable.
* Minimizing over some variables: the problem &lt;math&gt;\text{minimize } f_0(x_1,x_2)&lt;/math&gt; is equivalent to &lt;math&gt;\text{minimize } \tilde{f}_0 (x_1)&lt;/math&gt; where &lt;math&gt;\tilde{f}_0(x_1)=\inf_{x_2}f_0(x_1,x_2)&lt;/math&gt;.
* The '''epigraph form''' or the '''epigraph trick''' is one of the most common and useful problem conversion techniques. It goes like the this: &lt;math&gt;\text{minimize (over } x\text{) } f_0(x)&lt;/math&gt; is equivalent to &lt;math&gt;\text{minimize (over }x,t\text{) }t \text{  subject to  }  f_0(x)-t\le 0.&lt;/math&gt; 
 
== How are convex optimization problems actually solved? ==

A curious question is how the problems that involve possibly non-differentiable functions and inequality constraints are solved; after all, all that we know to minimize analytically (without iterations) is a quadratic and unconstrained function.

The page [[How are optimization problems solved?]] tries to very superficially answer the question, but here is the idea: We can solve those problems by converting them to a succession of problems that we know how to solve: unconstrained convex optimization problems with quadratic objective function. 

== References ==</text>
      <sha1>8ag7a8gqcy3yk4ebin3oqo7dy39c2m4</sha1>
    </revision>
    <revision>
      <id>368</id>
      <parentid>285</parentid>
      <timestamp>2019-05-14T13:00:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Some standard conversions */Typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5108">== Optimization problems ==

In general, a (constrained) optimization problem (in standard form) can be written as&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0:\mathbb R^n \to \mathbb R&lt;/math&gt; is the objective function and &lt;math&gt;f_i:\mathbb R^n \to R, \,\, i=1,\dots,m&lt;/math&gt; are the inequality constraint functions and &lt;math&gt;h_i:\mathbb R^n\to R\,\, i=1,\dots,p&lt;/math&gt; are the equality constraint functions. 

=== Convex optimization ===

A problem is a ''convex optimization problem'' if the functions &lt;math&gt;f_0, f_i&lt;/math&gt; are [[Convex function|convex functions]] and the functions &lt;math&gt;h_i&lt;/math&gt; are [[Affine function|affine functions]].

=== Quasiconvex optimization ===

The definition of a ''quasiconvex optimization problem'' is very similar to the definition of a convex optimization problem; the only different is that &lt;math&gt;f_0&lt;/math&gt; is allowed to be a [[Quasiconvex function|quasiconvex function]] and not expected to be strictly a convex function. Note that quasiconvex constraint functions can always be converted to equivalent convex constraint functions (see page 144&lt;ref name=cobook/&gt;).

=== Convex feasibility problem ===
A problem is called ''convex feasibility problem'' if the goal is to just find any &lt;math&gt;x&lt;/math&gt; that is feasible: 

 &lt;math&gt;\text{find } x&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

Convex feasibility problems are used for solving quasiconvex problems; see the [[Bisection method|bisection method for quasiconvex problems]].

=== Geometric program ===

An optimization problem in the form 
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 1, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) = 1, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

where &lt;math&gt;f_0, f_i, h_i&lt;/math&gt; are [[Geometric programming#Monomials and posynomials|posynomials]] are called [[Geometric programming|geometric programs]]; GPs can be transformed into convex optimization problems.

== Equivalent convex problems ==

Problems can be described in equivalent forms that are easier to solve. Those problems are not the same problems but they are equivalent, (roughly) meaning that they have the same solution. For example, the problem &lt;math&gt;\text{minimize } ||x||_2&lt;/math&gt; is not the same as the problem &lt;math&gt;\text{minimize } ||x||_2^2&lt;/math&gt; but they have the same solution. 

A convex optimization problem is very often solved by converting it into an equivalent problem; in fact, in terms of implementation, that seems to be almost always the case (see also [[How are optimization problems solved?]]). This is very understandable because one can convert, for example, a problem with a non-differentiable objective function into an equivalent problem with a differentiable function. 

Beyond facilitating the solution, converting problems into an equivalent problem may also make the problem easier to understand. Furthermore, the dual problems of equivalent problems are in general not the same. This is very important, because it means that the dual of a converted problem can have a very simple solution even if the dual of the original problem does not have a solution.

=== Some standard conversions ===
Below are some standard problem conversions.
* Eliminate equality constraint: One can take the constraint &lt;math&gt;Ax=b&lt;/math&gt; and embed it into the objective function.
* Introducing equality constraints: The problem may become easier to solve by adding (affine) equality constraints
* Introducing slack variables: e.g. converting the inequality constraint &lt;math&gt;a^Tx\le b&lt;/math&gt; into two constraints &lt;math&gt;a^T x+s=b&lt;/math&gt; and &lt;math&gt;s\ge 0&lt;/math&gt;. Here &lt;math&gt;s&lt;/math&gt; is called the slack variable.
* Minimizing over some variables: the problem &lt;math&gt;\text{minimize } f_0(x_1,x_2)&lt;/math&gt; is equivalent to &lt;math&gt;\text{minimize } \tilde{f}_0 (x_1)&lt;/math&gt; where &lt;math&gt;\tilde{f}_0(x_1)=\inf_{x_2}f_0(x_1,x_2)&lt;/math&gt;.
* The '''epigraph form''' or the '''epigraph trick''' is one of the most common and useful problem conversion techniques. It goes like the this: &lt;math&gt;\text{minimize (over } x\text{) } f_0(x)&lt;/math&gt; is equivalent to &lt;math&gt;\text{minimize (over }x,t\text{) }t \text{  subject to  }  f_0(x)-t\le 0.&lt;/math&gt;

== How are convex optimization problems actually solved? ==

A curious question is how the problems that involve possibly non-differentiable functions and inequality constraints are solved; after all, all that we know to minimize analytically (without iterations) is a quadratic and unconstrained function.

The page [[How are optimization problems solved?]] tries to very superficially answer the question, but here is the idea: We can solve those problems by converting them to a succession of problems that we know how to solve: unconstrained convex optimization problems with quadratic objective function. 

== References ==</text>
      <sha1>6x1op8916blnajxvtos51g14m41itka</sha1>
    </revision>
  </page>
  <page>
    <title>Quasiconvex optimization</title>
    <ns>0</ns>
    <id>76</id>
    <redirect title="Convex optimization" />
    <revision>
      <id>252</id>
      <timestamp>2018-10-13T13:26:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Convex optimization#Quasiconvex optimization]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="58">#REDIRECT [[Convex optimization#Quasiconvex optimization]]</text>
      <sha1>ipxlis9ds613k6v9ijqrxm3wavoqhvq</sha1>
    </revision>
  </page>
  <page>
    <title>Bisection method</title>
    <ns>0</ns>
    <id>77</id>
    <revision>
      <id>254</id>
      <timestamp>2018-10-13T13:32:43Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A quasiconvex optimization problem can be solved by solving a series of [[Convex optimization#Convex feasibility problem|convex feasibility]] problems."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="151">A quasiconvex optimization problem can be solved by solving a series of [[Convex optimization#Convex feasibility problem|convex feasibility]] problems.</text>
      <sha1>5anvz7wrwhhi1dlvk4gxfklecjghvfs</sha1>
    </revision>
    <revision>
      <id>256</id>
      <parentid>254</parentid>
      <timestamp>2018-10-13T13:45:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1281">A quasiconvex optimization problem can be solved by solving a series of [[Convex optimization#Convex feasibility problem|convex feasibility]] problems. This is the ''bisection method for quasiconvex optimization''. 

Recall that a function is quasiconvex if the its sublevel sets &lt;math&gt; \phi_t = \{x \in \textbf{dom} f_0 : f_0(x) \le t\} &lt;/math&gt; are convex. The idea of the bisection method is to find the smallest (up to a tolerance level) nonempty sublevel set -- this set will contain the optimal value. For this purpose, we start our search with a large interval (of function values) &lt;math&gt;[l,u]&lt;/math&gt; that contains the optimal value &lt;math&gt;p^*&lt;/math&gt;, and at each step divide this value into half by either updating the upper or the lower bound of this interval.

The algorithm can be written as:
 &lt;math&gt; \textbf{given}\,\, l\le p^*, u\ge p^*, \text{tolerance} \,\,\epsilon &gt; 0\\
 \textbf{repeat} \\
 \,\,1. t:= (l+u)/2
 \,\,2. \text{Solve the convex feasibility problem below}
 \,\,3. \text{If problem feasible, } u:=t, \texbf{else} l:=t
 \textbf{until } u-l\le \epsilon
 &lt;/math&gt;


Convex feasibility problem solved at each iteration:

&lt;math&gt;\begin{matrix}
\text{find} &amp; x \\
\text{subject to} &amp; \phi_t(x) \le 0 \\
&amp; f_i(x) \le 0, i=1,\dots,m \\
&amp; Ax = b
\end{matrix}
&lt;/math&gt;</text>
      <sha1>ccnvnzx79eqn0cndjuxvzus5w70yh9u</sha1>
    </revision>
    <revision>
      <id>257</id>
      <parentid>256</parentid>
      <timestamp>2018-10-13T13:50:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1368">A quasiconvex optimization problem can be solved by solving a series of [[Convex optimization#Convex feasibility problem|convex feasibility]] problems. This is the ''bisection method for quasiconvex optimization''. 

Recall that a function is quasiconvex if the its sublevel sets &lt;math&gt; \phi_t = \{x \in \textbf{dom} f_0 : f_0(x) \le t\} &lt;/math&gt; are convex. The idea of the bisection method is to find the smallest (up to a tolerance level) nonempty sublevel set -- this set will contain the optimal value. For this purpose, we start our search with a large interval (of function values) &lt;math&gt;[l,u]&lt;/math&gt; that contains the optimal value &lt;math&gt;p^*&lt;/math&gt;, and at each step divide this value into half by either updating the upper or the lower bound of this interval.

The algorithm can be written as:
 &lt;math&gt; \textbf{given}\,\, l\le p^*, u\ge p^*, \text{tolerance} \,\,\epsilon &gt; 0\\
 \textbf{repeat} \\
 \,\, \text{1. }t:= (l+u)/2 \\
 \,\, \text{2. Solve the convex feasibility problem below} \\
 \,\, \text{3. If problem feasible, } u:=t, \textbf{else } l:=t\\
 \textbf{until } u-l\le \epsilon  &lt;/math&gt;


Convex feasibility problem solved at each iteration:

&lt;math&gt;\begin{matrix}
\text{find} &amp; x \\
\text{subject to} &amp; \phi_t(x) \le 0 \\
&amp; f_i(x) \le 0, i=1,\dots,m \\
&amp; Ax = b
\end{matrix}
&lt;/math&gt;

where &lt;math&gt;\phi_t&lt;/math&gt; are the sublevel sets defined as above.</text>
      <sha1>sgtd0r8lgn41gp0gt2fdmzvqmlwwco2</sha1>
    </revision>
    <revision>
      <id>315</id>
      <parentid>257</parentid>
      <timestamp>2018-10-23T17:39:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1495">A quasiconvex optimization problem can be solved by solving a series of [[Convex optimization#Convex feasibility problem|convex feasibility]] problems. This is the ''bisection method for quasiconvex optimization''. 

Recall that a function is quasiconvex if the its sublevel sets &lt;math&gt; S_t = \{x \in \textbf{dom} f_0 : f_0(x) \le t\} &lt;/math&gt; are convex. The idea of the bisection method is to find the smallest (up to a tolerance level) nonempty sublevel set -- this set will contain the optimal value. For this purpose, we start our search with a large interval (of function values) &lt;math&gt;[l,u]&lt;/math&gt; that contains the optimal value &lt;math&gt;p^*&lt;/math&gt;, and at each step divide this value into half by either updating the upper or the lower bound of this interval.

The algorithm can be written as:
 &lt;math&gt; \textbf{given}\,\, l\le p^*, u\ge p^*, \text{tolerance} \,\,\epsilon &gt; 0\\
 \textbf{repeat} \\
 \,\, \text{1. }t:= (l+u)/2 \\
 \,\, \text{2. Solve the convex feasibility problem below} \\
 \,\, \text{3. If problem feasible, } u:=t, \textbf{else } l:=t\\
 \textbf{until } u-l\le \epsilon  &lt;/math&gt;


Convex feasibility problem solved at each iteration:

&lt;math&gt;\begin{matrix}
\text{find} &amp; x \\
\text{subject to} &amp; \phi_t(x) \le 0 \\
&amp; f_i(x) \le 0, i=1,\dots,m \\
&amp; Ax = b
\end{matrix}
&lt;/math&gt;

where &lt;math&gt;\phi_t(x)&lt;/math&gt; is a function that satisfies the condition of &lt;math&gt;x&lt;/math&gt; being in the sublevel set &lt;math&gt;S_t&lt;/math&gt;; that is:

&lt;math&gt;\phi_t(x) \le 0 \implies f_0(x) \le t&lt;/math&gt;.</text>
      <sha1>9kdg1lnm7vgn3a8at095mn4qegttfwb</sha1>
    </revision>
  </page>
  <page>
    <title>Geometric programming</title>
    <ns>0</ns>
    <id>78</id>
    <revision>
      <id>258</id>
      <timestamp>2018-10-13T14:02:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "==Geometric programming== Taking products of variables leads in general to nonconvex functions. However, a special case leads to convex functions. In fact, in this special cas..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="966">==Geometric programming==
Taking products of variables leads in general to nonconvex functions. However, a special case leads to convex functions. In fact, in this special case, even taking positive/negative/fractional exponents of the variables in the product is allowed.

=== Monomials and posinomials === 

A function &lt;math&gt;f:\mathbb R^n \to &lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \mathbb R ^n_{++}&lt;/math&gt;, defined as

&lt;math&gt;f(x) = c x_1^{a_1} \dots x_n^{a_n}&lt;/math&gt; 

with &lt;math&gt;c&gt;0&lt;/math&gt; is called a ''monomial'' function. The exponents &lt;math&gt;a_i&lt;/math&gt; can be positive, negative or fractional. A ''posinomial'' is a sum of monomials:

&lt;math&gt;f(x) = \sum_{k=1} c_k x_1^{a_1k} \dots x_n^{a_nk}&lt;/math&gt;.

==Geometric program==

An optimization problem in the form 
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 1, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) = 1, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;
is called a geometric program.</text>
      <sha1>bynkrxrl2xewa05vmazg7k4ati9noxb</sha1>
    </revision>
    <revision>
      <id>259</id>
      <parentid>258</parentid>
      <timestamp>2018-10-13T14:06:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1316">==Geometric programming==
Taking products of variables leads in general to nonconvex functions. However, a special case leads to convex functions. In fact, in this special case, even taking positive/negative/fractional exponents of the variables in the product is allowed.

=== Monomials and posynomials === 

A function &lt;math&gt;f:\mathbb R^n \to &lt;/math&gt; with &lt;math&gt;\textbf{dom} f = \mathbb R ^n_{++}&lt;/math&gt;, defined as

&lt;math&gt;f(x) = c x_1^{a_1} \dots x_n^{a_n}&lt;/math&gt; 

with &lt;math&gt;c&gt;0&lt;/math&gt; is called a ''monomial'' function &lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. The exponents &lt;math&gt;a_i&lt;/math&gt; can be positive, negative or fractional. A ''posynomial'' is a sum of monomials:

&lt;math&gt;f(x) = \sum_{k=1} c_k x_1^{a_1k} \dots x_n^{a_nk}&lt;/math&gt;.

==Geometric program==

An optimization problem in the form 
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 1, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) = 1, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;
is called a geometric program if all the functions &lt;math&gt;f_0, f_i, h_i&lt;/math&gt; are posynomials. (Note that the RHS is 1 instead of the typical 0).


Geometric programs can be converted to convex optimization problems through standard transformations (see Section 4.5.3&lt;ref name=cobook/&gt;).

== References ==</text>
      <sha1>era6iw3qzrzodydhsk1uj0szxiocu66</sha1>
    </revision>
  </page>
  <page>
    <title>Semidefinite programming</title>
    <ns>0</ns>
    <id>79</id>
    <revision>
      <id>261</id>
      <timestamp>2018-10-13T14:59:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A semidefinite program has the form:   &lt;math&gt;\text{minimize } c^T x&lt;/math&gt;  &lt;math&gt;\begin{matrix}\text{subject to} &amp; x_1 F_1 + \dots + x_n F_n + G \le 0 \\  &amp; Ax = b\end{matrix..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="300">A semidefinite program has the form:

 &lt;math&gt;\text{minimize } c^T x&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; x_1 F_1 + \dots + x_n F_n + G \le 0 \\
 &amp; Ax = b\end{matrix}&lt;/math&gt;

where &lt;math&gt;F_i, G&lt;/math&gt; are symmetric matrices and &lt;math&gt;\le &lt;/math&gt; refers to negative definiteness of a matrix.</text>
      <sha1>ld021d5svfgh5ucu5hgisfbeyura0z3</sha1>
    </revision>
  </page>
  <page>
    <title>Scalarization</title>
    <ns>0</ns>
    <id>80</id>
    <revision>
      <id>262</id>
      <timestamp>2018-10-13T15:04:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "To solve multicriterion optimization problems, i.e. problems that have an objective function with a vector output such as &lt;math&gt;f_0:\mathbb R^n \to \mathbb R^m&lt;/math&gt;, we can..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="604">To solve multicriterion optimization problems, i.e. problems that have an objective function with a vector output such as &lt;math&gt;f_0:\mathbb R^n \to \mathbb R^m&lt;/math&gt;, we can convert the output of &lt;math&gt;f_0&lt;/math&gt; into a scalar by multiplying with a vector &lt;math&gt;\lambda&lt;/math&gt; -- i.e., we minimize &lt;math&gt;\lambda^T f_0(x)&lt;/math&gt;. 

Importantly, &lt;math&gt;\lambda&lt;/math&gt; must be positive w.r.t. the generalized inequality  over the cone that we are minimizing, i.e., &lt;math&gt;\lambda \ge_{K^*} 0&lt;/math&gt; (see Figure 4.9&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;).

== References ==</text>
      <sha1>rowp9ls8nrllzgzzjtsnywmdarluiju</sha1>
    </revision>
    <revision>
      <id>529</id>
      <parentid>262</parentid>
      <timestamp>2020-03-21T14:18:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="608">To solve multicriterion optimization problems, i.e. problems that have an objective function with a vector output such as &lt;math&gt;f_0:\mathbb R^n \to \mathbb R^m&lt;/math&gt;, we can convert the output of &lt;math&gt;f_0&lt;/math&gt; into a scalar by multiplying with a vector &lt;math&gt;\lambda&lt;/math&gt; -- i.e., we minimize &lt;math&gt;\lambda^T f_0(x)&lt;/math&gt;. 

Importantly, &lt;math&gt;\lambda&lt;/math&gt; must be positive w.r.t. the generalized inequality  over the [[cone]] that we are minimizing, i.e., &lt;math&gt;\lambda \ge_{K^*} 0&lt;/math&gt; (see Figure 4.9&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;).

== References ==</text>
      <sha1>p495exiz03xh48g1032olj6vpgqdd31</sha1>
    </revision>
  </page>
  <page>
    <title>Dual problem</title>
    <ns>0</ns>
    <id>81</id>
    <revision>
      <id>263</id>
      <timestamp>2018-10-13T15:08:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with " == What's the point of dual problem? ==  Dual problem can be very helpful for defining a lower bound on the minimum value of a function. Note that the primal problem does not..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="425">
== What's the point of dual problem? ==

Dual problem can be very helpful for defining a lower bound on the minimum value of a function. Note that the primal problem does not have to be convex; in fact, the dual can be more informative in such cases as it may allow us to know how far from the optimum (that we'll probably be unable to reach -- because the problem is not convex and we have to go for local optima etc.) are.</text>
      <sha1>itb5xzhmrw118f9g6fl05b90eep8b71</sha1>
    </revision>
    <revision>
      <id>271</id>
      <parentid>263</parentid>
      <timestamp>2018-10-13T15:34:02Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2143">==Lagrange dual problem ==

The [[Lagrangian#The Lagrange dual function|Lagrange dual function]] of an optimization problem, &lt;math&gt;g(\lambda,\nu)&lt;/math&gt;, defines a lower bound on the minimum of a (convex or non-convex) objective function &lt;math&gt;f_0(x)&lt;/math&gt;, i.e. &lt;math&gt;g(\lambda,\nu) \le p^*&lt;/math&gt; for any &lt;math&gt;\lambda,\nu&lt;/math&gt;. Naturally, we would like to investigate how high we can take this lower bound -- we would like to make the inequality as tight as possible. This is the definition of a Langrange dual problem:

 &lt;math&gt;\text{maximize } g(\lambda,\nu)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; \lambda \ge 0.&lt;/math&gt;

In the context of dual problem, the original problem is sometimes called the primal problem.

=== Strong duality vs. Weak duality ===
An interesting question is: how high we can take the upper bound through the optimizatino above? That is, can we have, for example, &lt;math&gt;p^* = g(\lambda,nu)&lt;/math&gt; for some &lt;math&gt;\lambda,\nu&lt;/math&gt;? The answer is: if the primal problem is convex, then, subject to a mild condition (called Slater's condition -- it seems like this is almost always satisfied in practice), it holds that there exists &lt;math&gt;\lambda,nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,nu)&lt;/math&gt;. This is called ''strong duality'', or, equivalently, we say that ''there is no duality gap'' for the problem in question. Conversely, if the there is no &lt;math&gt;\lambda,nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,nu)&lt;/math&gt;, we say that we have ''weak duality'' or, equivalently, that there is a duality gap.

With nonconvex problems there is almost always a duality gap -- there are a few nonconvex problems for which it has been show that there is no duality gap&lt;ref name=cobook/&gt;.

== What's the point of dual problem? ==

Dual problem can be very helpful for defining a lower bound on the minimum value of a function. Note that the primal problem does not have to be convex; in fact, the dual can be more informative in such cases as it may allow us to know how far from the optimum (that we'll probably be unable to reach -- because the problem is not convex and we have to go for local optima etc.) are.</text>
      <sha1>8mw7vxxa3p05sghgphkr7t4pxlfgeme</sha1>
    </revision>
    <revision>
      <id>272</id>
      <parentid>271</parentid>
      <timestamp>2018-10-13T15:35:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2236">==Lagrange dual problem ==

The [[Lagrangian#The Lagrange dual function|Lagrange dual function]] of an optimization problem, &lt;math&gt;g(\lambda,\nu)&lt;/math&gt;, defines a lower bound on the minimum of a (convex or non-convex) objective function &lt;math&gt;f_0(x)&lt;/math&gt;, i.e. &lt;math&gt;g(\lambda,\nu) \le p^*&lt;/math&gt; for any &lt;math&gt;\lambda,\nu&lt;/math&gt;. Naturally, we would like to investigate how high we can take this lower bound -- we would like to make the inequality as tight as possible. This is the definition of a Langrange dual problem&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:

 &lt;math&gt;\text{maximize } g(\lambda,\nu)\\
 \begin{matrix}\text{subject to} &amp; \lambda \ge 0.\end{matrix}&lt;/math&gt;

In the context of dual problem, the original problem is sometimes called the primal problem.

=== Strong duality vs. Weak duality ===
An interesting question is: how high we can take the upper bound through the optimizatino above? That is, can we have, for example, &lt;math&gt;p^* = g(\lambda,nu)&lt;/math&gt; for some &lt;math&gt;\lambda,\nu&lt;/math&gt;? The answer is: if the primal problem is convex, then, subject to a mild condition (called Slater's condition -- it seems like this is almost always satisfied in practice), it holds that there exists &lt;math&gt;\lambda,nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,nu)&lt;/math&gt;. This is called ''strong duality'', or, equivalently, we say that ''there is no duality gap'' for the problem in question. Conversely, if the there is no &lt;math&gt;\lambda,nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,nu)&lt;/math&gt;, we say that we have ''weak duality'' or, equivalently, that there is a duality gap.

With nonconvex problems there is almost always a duality gap -- there are a few nonconvex problems for which it has been show that there is no duality gap&lt;ref name=cobook/&gt;.

== What's the point of dual problem? ==

Dual problem can be very helpful for defining a lower bound on the minimum value of a function. Note that the primal problem does not have to be convex; in fact, the dual can be more informative in such cases as it may allow us to know how far from the optimum (that we'll probably be unable to reach -- because the problem is not convex and we have to go for local optima etc.) are.

== References ==</text>
      <sha1>8vqsay9ueomi7fuskqg934nq3zmm1p2</sha1>
    </revision>
    <revision>
      <id>276</id>
      <parentid>272</parentid>
      <timestamp>2018-10-13T16:38:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3758">==Lagrange dual problem ==

The [[Lagrangian#The Lagrange dual function|Lagrange dual function]] of an optimization problem, &lt;math&gt;g(\lambda,\nu)&lt;/math&gt;, defines a lower bound on the minimum of a (convex or non-convex) objective function &lt;math&gt;f_0(x)&lt;/math&gt;, i.e. &lt;math&gt;g(\lambda,\nu) \le p^*&lt;/math&gt; for any &lt;math&gt;\lambda,\nu&lt;/math&gt;. Naturally, we would like to investigate how high we can take this lower bound -- we would like to make the inequality as tight as possible. This is the definition of a Langrange dual problem&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:

 &lt;math&gt;\text{maximize } g(\lambda,\nu)\\
 \begin{matrix}\text{subject to} &amp; \lambda \ge 0.\end{matrix}&lt;/math&gt;

In the context of dual problem, the original problem is sometimes called the primal problem.

=== Strong duality vs. Weak duality ===
An interesting question is: how high we can take the upper bound through the optimizatino above? That is, can we have, for example, &lt;math&gt;p^* = g(\lambda,\nu)&lt;/math&gt; for some &lt;math&gt;\lambda,\nu&lt;/math&gt;? The answer is: if the primal problem is convex, then, subject to a mild condition (called Slater's condition -- it seems like this is almost always satisfied in practice), it holds that there exists &lt;math&gt;\lambda,nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,\nu)&lt;/math&gt;. This is called ''strong duality'', or, equivalently, we say that ''there is no duality gap'' for the problem in question. Conversely, if the there is no &lt;math&gt;\lambda,\nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,\nu)&lt;/math&gt;, we say that we have ''weak duality'' or, equivalently, that there is a duality gap.

With nonconvex problems there is almost always a duality gap -- there are a few nonconvex problems for which it has been show that there is no duality gap&lt;ref name=cobook/&gt;.

== Interpretation of dual variables ==

=== Perturbation and sensitivity analysis === 
Assume that strong duality holds for the following (primal) problem:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

This means that the [[Lagrangian#Lagrange dual function|Lagrange dual function]] &lt;math&gt;g(\lambda^*, \nu^*)&lt;/math&gt; equals the minimal value for the problem for some &lt;math&gt;\lambda^*, \nu^*&lt;/math&gt;, that is, &lt;math&gt;g(\lambda^*,\nu^*) = p^*&lt;/math&gt;.

The sensitivity of the minimum value of &lt;math&gt;f_0(x)&lt;/math&gt; w.r.t. problem constraints can be analyzed as follows. Suppose that we perturb the boundaries of the constraints; that is, instead of &lt;math&gt;0&lt;/math&gt; we put some values &lt;math&gt;u_i,v_i&lt;/math&gt;:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le u_i, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =v_i, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;.

Now: the optimal dual variables &lt;math&gt;\lambda^*, \nu^*&lt;/math&gt; define very useful information on how sensitive the minimum of &lt;math&gt;f_0(x)&lt;/math&gt; is to perturbations of &lt;math&gt;u_i,v_i&lt;/math&gt;. The details on why are provided in&lt;ref&gt; Stephen Boyd, EE364A Lecture Notes 05 (page 22)&lt;/ref&gt;, but here is a summary:

* If &lt;math&gt;\lambda_i^*&lt;/math&gt; is large: &lt;math&gt;p^*&lt;/math&gt; increases greatly if we tighten the constraint &lt;math&gt;i&lt;/math&gt; (i.e. if &lt;math&gt;u_i&lt;0&lt;/math&gt;).
* If &lt;math&gt;\lambda_i^*&lt;/math&gt; is small: &lt;math&gt;p^*&lt;/math&gt; 



== What's the point of dual problem? ==

Dual problem can be very helpful for defining a lower bound on the minimum value of a function. Note that the primal problem does not have to be convex; in fact, the dual can be more informative in such cases as it may allow us to know how far from the optimum (that we'll probably be unable to reach -- because the problem is not convex and we have to go for local optima etc.) are.

== References ==</text>
      <sha1>bmn58w04dttk6837byxpy8kx008da6p</sha1>
    </revision>
    <revision>
      <id>277</id>
      <parentid>276</parentid>
      <timestamp>2018-10-13T16:41:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4300">==Lagrange dual problem ==

The [[Lagrangian#The Lagrange dual function|Lagrange dual function]] of an optimization problem, &lt;math&gt;g(\lambda,\nu)&lt;/math&gt;, defines a lower bound on the minimum of a (convex or non-convex) objective function &lt;math&gt;f_0(x)&lt;/math&gt;, i.e. &lt;math&gt;g(\lambda,\nu) \le p^*&lt;/math&gt; for any &lt;math&gt;\lambda,\nu&lt;/math&gt;. Naturally, we would like to investigate how high we can take this lower bound -- we would like to make the inequality as tight as possible. This is the definition of a Langrange dual problem&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:

 &lt;math&gt;\text{maximize } g(\lambda,\nu)\\
 \begin{matrix}\text{subject to} &amp; \lambda \ge 0.\end{matrix}&lt;/math&gt;

In the context of dual problem, the original problem is sometimes called the primal problem.

=== Strong duality vs. Weak duality ===
An interesting question is: how high we can take the upper bound through the optimizatino above? That is, can we have, for example, &lt;math&gt;p^* = g(\lambda,\nu)&lt;/math&gt; for some &lt;math&gt;\lambda,\nu&lt;/math&gt;? The answer is: if the primal problem is convex, then, subject to a mild condition (called Slater's condition -- it seems like this is almost always satisfied in practice), it holds that there exists &lt;math&gt;\lambda,nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,\nu)&lt;/math&gt;. This is called ''strong duality'', or, equivalently, we say that ''there is no duality gap'' for the problem in question. Conversely, if the there is no &lt;math&gt;\lambda,\nu&lt;/math&gt; such that &lt;math&gt;p^* = g(\lambda,\nu)&lt;/math&gt;, we say that we have ''weak duality'' or, equivalently, that there is a duality gap.

With nonconvex problems there is almost always a duality gap -- there are a few nonconvex problems for which it has been show that there is no duality gap&lt;ref name=cobook/&gt;.

== Interpretation of dual variables ==

=== Perturbation and sensitivity analysis === 
Assume that strong duality holds for the following (primal) problem:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

This means that the [[Lagrangian#Lagrange dual function|Lagrange dual function]] &lt;math&gt;g(\lambda^*, \nu^*)&lt;/math&gt; equals the minimal value for the problem for some &lt;math&gt;\lambda^*, \nu^*&lt;/math&gt;, that is, &lt;math&gt;g(\lambda^*,\nu^*) = p^*&lt;/math&gt;.

The sensitivity of the minimum value of &lt;math&gt;f_0(x)&lt;/math&gt; w.r.t. problem constraints can be analyzed as follows. Suppose that we perturb the boundaries of the constraints; that is, instead of &lt;math&gt;0&lt;/math&gt; we put some values &lt;math&gt;u_i,v_i&lt;/math&gt;:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le u_i, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =v_i, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;.

Now: the optimal dual variables &lt;math&gt;\lambda^*, \nu^*&lt;/math&gt; define very useful information on how sensitive the minimum of &lt;math&gt;f_0(x)&lt;/math&gt; is to perturbations of &lt;math&gt;u_i,v_i&lt;/math&gt;. The details on why are provided in&lt;ref&gt; Stephen Boyd, EE364A Lecture Notes 05 (page 22)&lt;/ref&gt;, but here is a summary:

* If &lt;math&gt;\lambda_i^*&lt;/math&gt; is large: &lt;math&gt;p^*&lt;/math&gt; increases greatly if we tighten the constraint &lt;math&gt;i&lt;/math&gt; (i.e. if &lt;math&gt;u_i&lt;0&lt;/math&gt;).
* If &lt;math&gt;\lambda_i^*&lt;/math&gt; is small: &lt;math&gt;p^*&lt;/math&gt; does not decrease much if we loosen constraint &lt;math&gt;i&lt;/math&gt; (i.e. if &lt;math&gt;u_i&gt;0&lt;/math&gt;)
* If &lt;math&gt;\nu_i^*&lt;/math&gt; large and positive: &lt;math&gt;p^*&lt;/math&gt; increases greatly if we take &lt;math&gt;v_i&lt;0&lt;/math&gt;
* If &lt;math&gt;\nu_i^*&lt;/math&gt; large and negative: &lt;math&gt;p^*&lt;/math&gt; increases greatly if we take &lt;math&gt;v_i&gt;0&lt;/math&gt;
* If &lt;math&gt;\nu_i^*&lt;/math&gt; small and positive: &lt;math&gt;p^*&lt;/math&gt; does not decrease much if we take &lt;math&gt;v_i&gt;0&lt;/math&gt;
* If &lt;math&gt;\nu_i^*&lt;/math&gt; small and negative: &lt;math&gt;p^*&lt;/math&gt; does not decrease much if we take &lt;math&gt;v_i&lt;0&lt;/math&gt;

== What's the point of dual problem? ==

Dual problem can be very helpful for defining a lower bound on the minimum value of a function. Note that the primal problem does not have to be convex; in fact, the dual can be more informative in such cases as it may allow us to know how far from the optimum (that we'll probably be unable to reach -- because the problem is not convex and we have to go for local optima etc.) are.

== References ==</text>
      <sha1>dvk6p6vhphea2xcrcs485dc9y0927j2</sha1>
    </revision>
  </page>
  <page>
    <title>Lagrangian</title>
    <ns>0</ns>
    <id>82</id>
    <revision>
      <id>264</id>
      <timestamp>2018-10-13T15:11:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Assume that we have an optimization problem in the standard form:  &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;  &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\do..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="236">Assume that we have an optimization problem in the standard form:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;</text>
      <sha1>0sswz3g8yplvscutj9ga86xp8snjgcb</sha1>
    </revision>
    <revision>
      <id>265</id>
      <parentid>264</parentid>
      <timestamp>2018-10-13T15:14:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="827">Assume that we have an optimization problem in the standard form:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

The Lagrangian function associated with the problem above is basically a weighted sum of the objective function as well as the constraints:

&lt;math&gt;L(x,\lambda,\nu) = f_p(x) + \sum_{i}^m \lambda_i f_i(x) + \sum_{i=1}^p\nu_i h_i(x).&lt;/math&gt;

We refer to &lt;math&gt;\lambda_i&lt;/math&gt; as the ''Lagrange multiplier'' associated with the ''i''th inequality constraint and to &lt;math&gt;\nu&lt;/math&gt; as the Lagrange multiplier associated with the ''i''the equality constraint. The vectors &lt;math&gt;\lambda,\nu&lt;/math&gt; are called the ''dual variables'' or ''Lagrange multipler vectors'' associated with the problem.</text>
      <sha1>jk2e2fyw850cqdiuh7hz48730v0ubl3</sha1>
    </revision>
    <revision>
      <id>266</id>
      <parentid>265</parentid>
      <timestamp>2018-10-13T15:18:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1544">Assume that we have an optimization problem in the standard form:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

=== The Lagrangian function ===
The Lagrangian function associated with the problem above is basically a weighted sum of the objective function as well as the constraints&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:

&lt;math&gt;L(x,\lambda,\nu) = f_p(x) + \sum_{i}^m \lambda_i f_i(x) + \sum_{i=1}^p\nu_i h_i(x).&lt;/math&gt;

We refer to &lt;math&gt;\lambda_i&lt;/math&gt; as the ''Lagrange multiplier'' associated with the ''i''th inequality constraint and to &lt;math&gt;\nu&lt;/math&gt; as the Lagrange multiplier associated with the ''i''the equality constraint. The vectors &lt;math&gt;\lambda,\nu&lt;/math&gt; are called the ''dual variables'' or ''Lagrange multipler vectors'' associated with the problem.

=== The Lagrange dual function ===

The ''Lagrange dual function'' &lt;math&gt;g:\mathbb R^m \times \mathbb R^p \to \mathbb R&lt;/math&gt; is defined as the minimum value of the Lagrangian over &lt;math&gt;x&lt;/math&gt;:

&lt;math&gt;g(\lambda,\nu) = \inf_{x\in \mathcal D} &lt;/math&gt;

== What's the point of the Lagrange dual function? ==

The Lagrange dual function defines a lower bound on the (primal) problem that we are solving; that is, if &lt;math&gt;p^*&lt;/math&gt; is the maximal value of a problem, then we have that for any &lt;math&gt;\lambda \in \mathbb R^m, \nu \in \mathbb R^p&lt;/math&gt;:

&lt;math&gt;g(\lambda,\nu) \le p^*.&lt;/math&gt;





==References==</text>
      <sha1>riv3a2f2t7ds53pfs18m1jem6v5ukrn</sha1>
    </revision>
    <revision>
      <id>267</id>
      <parentid>266</parentid>
      <timestamp>2018-10-13T15:19:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1587">== Lagrangan and Lagrangian dual functions ==

Assume that we have an optimization problem in the standard form:
 &lt;math&gt;\text{minimize } f_0(x)&lt;/math&gt;
 &lt;math&gt;\begin{matrix}\text{subject to} &amp; f_i(x) \le 0, \,\,\,\, i=1,\dots,m \\
 &amp; h_i(x) =0, \,\,\,\, i=1,\dots,p\end{matrix}&lt;/math&gt;

=== The Lagrangian function ===
The Lagrangian function associated with the problem above is basically a weighted sum of the objective function as well as the constraints&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;:

&lt;math&gt;L(x,\lambda,\nu) = f_p(x) + \sum_{i}^m \lambda_i f_i(x) + \sum_{i=1}^p\nu_i h_i(x).&lt;/math&gt;

We refer to &lt;math&gt;\lambda_i&lt;/math&gt; as the ''Lagrange multiplier'' associated with the ''i''th inequality constraint and to &lt;math&gt;\nu&lt;/math&gt; as the Lagrange multiplier associated with the ''i''the equality constraint. The vectors &lt;math&gt;\lambda,\nu&lt;/math&gt; are called the ''dual variables'' or ''Lagrange multipler vectors'' associated with the problem.

=== The Lagrange dual function ===

The ''Lagrange dual function'' &lt;math&gt;g:\mathbb R^m \times \mathbb R^p \to \mathbb R&lt;/math&gt; is defined as the minimum value of the Lagrangian over &lt;math&gt;x&lt;/math&gt;:

&lt;math&gt;g(\lambda,\nu) = \inf_{x\in \mathcal D} &lt;/math&gt;

== What's the point of the Lagrange dual function? ==

The Lagrange dual function defines a lower bound on the (primal) problem that we are solving; that is, if &lt;math&gt;p^*&lt;/math&gt; is the maximal value of a problem, then we have that for any &lt;math&gt;\lambda \in \mathbb R^m, \nu \in \mathbb R^p&lt;/math&gt;:

&lt;math&gt;g(\lambda,\nu) \le p^*.&lt;/math&gt;

==References==</text>
      <sha1>29a00bcb54dq0g0tnslykz0i49th873</sha1>
    </revision>
  </page>
  <page>
    <title>Lagrange dual problem</title>
    <ns>0</ns>
    <id>83</id>
    <redirect title="Dual problem" />
    <revision>
      <id>268</id>
      <timestamp>2018-10-13T15:23:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Dual problem]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26">#REDIRECT [[Dual problem]]</text>
      <sha1>svzl3iraayetk41yixoh9atdxajb9c7</sha1>
    </revision>
  </page>
  <page>
    <title>Strong duality</title>
    <ns>0</ns>
    <id>84</id>
    <redirect title="Dual problem" />
    <revision>
      <id>269</id>
      <timestamp>2018-10-13T15:31:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Dual problem#Strong duality vs. Weak duality]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="58">#REDIRECT [[Dual problem#Strong duality vs. Weak duality]]</text>
      <sha1>98h6czt7q6h2szks4n971zlcxs6onfp</sha1>
    </revision>
  </page>
  <page>
    <title>Weak duality</title>
    <ns>0</ns>
    <id>85</id>
    <redirect title="Dual problem" />
    <revision>
      <id>270</id>
      <timestamp>2018-10-13T15:32:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Dual problem#Strong duality vs. Weak duality]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="58">#REDIRECT [[Dual problem#Strong duality vs. Weak duality]]</text>
      <sha1>98h6czt7q6h2szks4n971zlcxs6onfp</sha1>
    </revision>
  </page>
  <page>
    <title>KKT conditions</title>
    <ns>0</ns>
    <id>86</id>
    <revision>
      <id>273</id>
      <timestamp>2018-10-13T16:18:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The Karush-Kuhn-Tucker (KKT) conditions are a set of conditions that are necessary for a point to be the minimal point of any optimization problem with differentiable function..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1005">The Karush-Kuhn-Tucker (KKT) conditions are a set of conditions that are necessary for a point to be the minimal point of any optimization problem with differentiable function. If the problem is convex, then the KKT conditions are necessary and sufficient&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

The KKT conditions are as follows. Let &lt;math&gt;L(x,\lambda,\nu)&lt;/math&gt; be the Lagrangian of an optimization problem. Let &lt;math&gt;x^*&lt;/math&gt; and &lt;math&gt;(\lambda^*,\nu^*)&lt;/math&gt; be any primal and dual optimal points with zero duality gap. If &lt;math&gt;x^*&lt;/math&gt; minimizes &lt;math&gt;L(x,\lambda^*,\nu^*)&lt;/math&gt; over &lt;math&gt;x&lt;/math&gt;, then its gradient must vanish at &lt;math&gt;x^*&lt;/math&gt;. Thus we have that &lt;math&gt;\nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) = 0&lt;/math&gt;

 &lt;math&gt; f_i(x) \le 0, \,\,\,i=1,\dots,m \\
 h_i(x) = 0,\,\,\, i=1,\dots,p \\
 \lambda_i^* \ge 0, \,\,\, i=1,\dots,m \\
 \li^* f_i(x^*) = 0, \,\,\, i=1,\dots,m \\


 &lt;/math&gt;</text>
      <sha1>4oo2prnk721y2xfns2jemgcv508vkcv</sha1>
    </revision>
    <revision>
      <id>274</id>
      <parentid>273</parentid>
      <timestamp>2018-10-13T16:19:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1052">The Karush-Kuhn-Tucker (KKT) conditions are a set of conditions that are necessary for a point to be the minimal point of any optimization problem with differentiable function. If the problem is convex, then the KKT conditions are necessary and sufficient&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

The KKT conditions are as follows. Let &lt;math&gt;L(x,\lambda,\nu)&lt;/math&gt; be the Lagrangian of an optimization problem. Let &lt;math&gt;x^*&lt;/math&gt; and &lt;math&gt;(\lambda^*,\nu^*)&lt;/math&gt; be any primal and dual optimal points with zero duality gap. If &lt;math&gt;x^*&lt;/math&gt; minimizes &lt;math&gt;L(x,\lambda^*,\nu^*)&lt;/math&gt; over &lt;math&gt;x&lt;/math&gt;, then its gradient must vanish at &lt;math&gt;x^*&lt;/math&gt;. Thus we have that &lt;math&gt;\nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) = 0&lt;/math&gt;

 &lt;math&gt;\begin{array} f_i(x) \le 0, \,\,\,i=1,\dots,m \\
 h_i(x) = 0,\,\,\, i=1,\dots,p \\
 \lambda_i^* \ge 0, \,\,\, i=1,\dots,m \\
 \lambda_i^* f_i(x^*) = 0, \,\,\, i=1,\dots,m \\
 \end{array}&lt;/math&gt;


== References ==</text>
      <sha1>synmdy04spqux3vvsqx7iu3cgihlevh</sha1>
    </revision>
    <revision>
      <id>275</id>
      <parentid>274</parentid>
      <timestamp>2018-10-13T16:23:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1229">The Karush-Kuhn-Tucker (KKT) conditions are a set of conditions that are necessary for a point to be the minimal point of any optimization problem with differentiable function. If the problem is convex, then the KKT conditions are necessary and sufficient&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

The KKT conditions are as follows. Let &lt;math&gt;L(x,\lambda,\nu)&lt;/math&gt; be the Lagrangian of an optimization problem. Let &lt;math&gt;x^*&lt;/math&gt; and &lt;math&gt;(\lambda^*,\nu^*)&lt;/math&gt; be any primal and dual optimal points with zero duality gap. If &lt;math&gt;x^*&lt;/math&gt; minimizes &lt;math&gt;L(x,\lambda^*,\nu^*)&lt;/math&gt; over &lt;math&gt;x&lt;/math&gt;, then its gradient must vanish at &lt;math&gt;x^*&lt;/math&gt;. Thus we have that &lt;math&gt;\nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) = 0&lt;/math&gt;

 &lt;math&gt;\begin{eqnarray} 
 f_i(x) &amp;\le 0, \,\,\,i=1,\dots,m \\
 h_i(x) &amp;= 0,\,\,\, i=1,\dots,p \\
 \lambda_i^* &amp;\ge 0, \,\,\, i=1,\dots,m \\
 \lambda_i^* f_i(x^*) &amp;= 0, \,\,\, i=1,\dots,m \\
 \nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) =&amp; 0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\\
 \end{eqnarray}&lt;/math&gt;


== References ==</text>
      <sha1>0udndq14zinc3mv528ulypck79baw74</sha1>
    </revision>
    <revision>
      <id>288</id>
      <parentid>275</parentid>
      <timestamp>2018-10-13T19:14:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1334">The Karush-Kuhn-Tucker (KKT) conditions are a set of conditions that are necessary for a point to be the minimal point of any optimization problem with differentiable function. If the problem is convex, then the KKT conditions are necessary and sufficient&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

The KKT conditions are as follows. Let &lt;math&gt;L(x,\lambda,\nu)&lt;/math&gt; be the Lagrangian of an optimization problem. Let &lt;math&gt;x^*&lt;/math&gt; and &lt;math&gt;(\lambda^*,\nu^*)&lt;/math&gt; be any primal and dual optimal points with zero duality gap. If &lt;math&gt;x^*&lt;/math&gt; minimizes &lt;math&gt;L(x,\lambda^*,\nu^*)&lt;/math&gt; over &lt;math&gt;x&lt;/math&gt;, then its gradient must vanish at &lt;math&gt;x^*&lt;/math&gt;. Thus we have that &lt;math&gt;\nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) = 0&lt;/math&gt;

 &lt;math&gt;\begin{eqnarray} 
 f_i(x) &amp;\le 0, \,\,\,i=1,\dots,m \\
 h_i(x) &amp;= 0,\,\,\, i=1,\dots,p \\
 \lambda_i^* &amp;\ge 0, \,\,\, i=1,\dots,m \\
 \lambda_i^* f_i(x^*) &amp;= 0, \,\,\, i=1,\dots,m \\
 \nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) =&amp; 0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\\
 \end{eqnarray}&lt;/math&gt;

The condition &lt;math&gt;\lambda_i^* f_i(x^*) &amp;= 0&lt;/math&gt; is also referred to as ''complementary slackness''.

== References ==</text>
      <sha1>2yrcoj3euky0t7t8ck7bmbz7wj22vu1</sha1>
    </revision>
    <revision>
      <id>289</id>
      <parentid>288</parentid>
      <timestamp>2018-10-13T19:15:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1333">The Karush-Kuhn-Tucker (KKT) conditions are a set of conditions that are necessary for a point to be the minimal point of any optimization problem with differentiable function. If the problem is convex, then the KKT conditions are necessary and sufficient&lt;ref name=cobook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

The KKT conditions are as follows. Let &lt;math&gt;L(x,\lambda,\nu)&lt;/math&gt; be the Lagrangian of an optimization problem. Let &lt;math&gt;x^*&lt;/math&gt; and &lt;math&gt;(\lambda^*,\nu^*)&lt;/math&gt; be any primal and dual optimal points with zero duality gap. If &lt;math&gt;x^*&lt;/math&gt; minimizes &lt;math&gt;L(x,\lambda^*,\nu^*)&lt;/math&gt; over &lt;math&gt;x&lt;/math&gt;, then its gradient must vanish at &lt;math&gt;x^*&lt;/math&gt;. Thus we have that &lt;math&gt;\nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) = 0&lt;/math&gt;

 &lt;math&gt;\begin{eqnarray} 
 f_i(x) &amp;\le 0, \,\,\,i=1,\dots,m \\
 h_i(x) &amp;= 0,\,\,\, i=1,\dots,p \\
 \lambda_i^* &amp;\ge 0, \,\,\, i=1,\dots,m \\
 \lambda_i^* f_i(x^*) &amp;= 0, \,\,\, i=1,\dots,m \\
 \nabla f_0(x^*) + \sum_{i=1}^m\lambda_i^* \nabla f_i(x^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(x^*) =&amp; 0\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\\
 \end{eqnarray}&lt;/math&gt;

The condition &lt;math&gt;\lambda_i^* f_i(x^*) = 0&lt;/math&gt; is also referred to as ''complementary slackness''.

== References ==</text>
      <sha1>oo2agettkffuyolhqhu5bqxptc1l3ac</sha1>
    </revision>
  </page>
  <page>
    <title>Huber penalty function</title>
    <ns>0</ns>
    <id>87</id>
    <revision>
      <id>278</id>
      <timestamp>2018-10-13T16:51:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The Huber penalty function is a robust penalty function, an alternative to the &lt;math&gt;\ell_2&lt;/math&gt; norm which is sensitive to outliers; it is defined as:  &lt;math&gt;\phi_{\text{hu..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="493">The Huber penalty function is a robust penalty function, an alternative to the &lt;math&gt;\ell_2&lt;/math&gt; norm which is sensitive to outliers; it is defined as:

&lt;math&gt;\phi_{\text{hub}}(x) = \begin{cases}x^2 &amp; |x|\le M\\M(2|x| - M) &amp; |x|&gt;M\end{cases}.&lt;/math&gt;

Note that the function behaves like an &lt;math&gt;\ell_2&lt;/math&gt; penalty function inside a range close to 0, and for larger values it behaves like an &lt;math&gt;\ell_1&lt;/math&gt; norm, which is less sensitive to outliers than the &lt;math&gt;\ell_2&lt;/math&gt; norm.</text>
      <sha1>nfao9vwoe0b0rlaew42mxfcd9i8k50g</sha1>
    </revision>
  </page>
  <page>
    <title>Slab</title>
    <ns>0</ns>
    <id>88</id>
    <revision>
      <id>279</id>
      <timestamp>2018-10-13T18:31:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A slab is a set in the form &lt;math&gt;\{x:\in \mathbb R^n : \alpha &lt; a^T x \le \beta\}&lt;/math&gt;"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="89">A slab is a set in the form &lt;math&gt;\{x:\in \mathbb R^n : \alpha &lt; a^T x \le \beta\}&lt;/math&gt;</text>
      <sha1>ejk509odsupaqjf30oumhq1hlfjgd3w</sha1>
    </revision>
  </page>
  <page>
    <title>How are optimization problems solved?</title>
    <ns>0</ns>
    <id>89</id>
    <revision>
      <id>280</id>
      <timestamp>2018-10-13T18:39:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "To me it was always some kind of mystery how to solve optimization problems. After all, the only way we can compute an unknown is by reducing it into a linear equation; in the..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1065">To me it was always some kind of mystery how to solve optimization problems. After all, the only way we can compute an unknown is by reducing it into a linear equation; in the context of minimization, to get to a linear equation that gives the minimum is always done through taking some derivative. I didn't know anything beyond this to actually do a computation -- without computing derivatives, I thought that we would be completely lost. So how come can we find the minimum in various sorts of problems including those with non-differentiable objective/constraint functions or yet worse with inequality constraints?

It turns out that we somehow convert problems to differentiable ones, no matter how far they initially are from being differentiable.

The order goes like this:

&lt;math&gt;
\begin{matrix}
\text{Non-differentiable objective function, inequality constraints} \\
\downarrow
\text{Smooth objective function, inequality constraints} \\
\downarrow
\text{Smooth unconstrained} \\
\downarrow
\text{Quadratic unconstrained} \\
\downarrow
\end{matrix}
&lt;/math&gt;</text>
      <sha1>6k6oscxjcvlvprqaf5zh5hash8nvrrk</sha1>
    </revision>
    <revision>
      <id>281</id>
      <parentid>280</parentid>
      <timestamp>2018-10-13T18:39:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1080">To me it was always some kind of mystery how to solve optimization problems. After all, the only way we can compute an unknown is by reducing it into a linear equation; in the context of minimization, to get to a linear equation that gives the minimum is always done through taking some derivative. I didn't know anything beyond this to actually do a computation -- without computing derivatives, I thought that we would be completely lost. So how come can we find the minimum in various sorts of problems including those with non-differentiable objective/constraint functions or yet worse with inequality constraints?

It turns out that we somehow convert problems to differentiable ones, no matter how far they initially are from being differentiable.

The order goes like this:

&lt;math&gt;
\begin{matrix}
\text{Non-differentiable objective function, inequality constraints} \\
\downarrow \\
\text{Smooth objective function, inequality constraints} \\
\downarrow  \\
\text{Smooth unconstrained} \\
\downarrow  \\
\text{Quadratic unconstrained} \\
\downarrow  \\
\end{matrix}
&lt;/math&gt;</text>
      <sha1>fjubgdksumg959r18kn09r7icv8hn3w</sha1>
    </revision>
    <revision>
      <id>282</id>
      <parentid>281</parentid>
      <timestamp>2018-10-13T18:48:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2168">To me it was always some kind of mystery how to solve optimization problems. After all, the only way we can compute an unknown is by reducing it into a linear equation; in the context of minimization, to get to a linear equation that gives the minimum is always done through taking some derivative. I didn't know anything beyond this to actually do a computation -- without computing derivatives, I thought that we would be completely lost. So how come can we find the minimum in various sorts of problems including those with non-differentiable objective/constraint functions or yet worse with inequality constraints?

It turns out that we somehow convert problems to differentiable ones, no matter how far they initially are from being differentiable.

For convex optimization problems, the solution hierarchy goes like this:

&lt;math&gt;
\begin{matrix}
\text{Non-differentiable objective function with inequality constraints} \\
\downarrow \\
\text{Smooth objective function, inequality constraints} \\
\downarrow  \\
\text{Smooth unconstrained} \\
\downarrow  \\
\text{Quadratic unconstrained} \\
\downarrow  \\
\text{Linear equations}
\end{matrix}
&lt;/math&gt;

That is, the only things that we know how to solve directly (w/o iterations) are quadratic, unconstrained functions. But if we more generally have a smooth and unconstrained function, then we can solve it iteratively -- by solving a succession of quadratically unconstrained optimization problems. Going higher in the hierarchy, we can solve problems that involve inequality constraints iteratively by solving at each iteration a problem with that involves a smooth and unconstrained objective function. At the top of the hierarchy we have problems with non-differentiable objective functions. We first convert the problem with the non-differentiable objective function to a problem with a differentiable function by adding additional inequality constraints to accurately represent the non-differentiable function. For example, if we want to minimize &lt;math&gt;|x|&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; (assume unconstained), we can instead minimize &lt;math&gt;t&lt;/math&gt; w.r.t. &lt;math&gt;x,t&lt;/math&gt; subject to &lt;math&gt;-t &lt; x &lt; t&lt;/math&gt;.</text>
      <sha1>knvfw2s7vnkwuhtzxaiqjh0gwju795s</sha1>
    </revision>
    <revision>
      <id>287</id>
      <parentid>282</parentid>
      <timestamp>2018-10-13T19:12:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2356">To me it was always some kind of mystery how to solve optimization problems. After all, the only way we can compute an unknown is by reducing it into a linear equation; in the context of minimization, to get to a linear equation that gives the minimum is always done through taking some derivative. I didn't know anything beyond this to actually do a computation -- without computing derivatives, I thought that we would be completely lost. So how come can we find the minimum in various sorts of problems including those with non-differentiable objective/constraint functions or yet worse with inequality constraints?

It turns out that we somehow convert problems to differentiable ones, no matter how far they initially are from being differentiable.

For convex optimization problems, the solution hierarchy goes like this:

&lt;math&gt;
\begin{matrix}
\text{Non-differentiable objective function with inequality constraints} \\
\downarrow \\
\text{Smooth objective function, inequality constraints} \\
\downarrow  \\
\text{Smooth unconstrained} \\
\downarrow  \\
\text{Quadratic unconstrained} \\
\downarrow  \\
\text{Linear equations}
\end{matrix}
&lt;/math&gt;

That is, the only things that we know how to solve directly (w/o iterations) are quadratic, unconstrained functions. But if we more generally have a smooth and unconstrained function, then we can solve it iteratively -- by solving a succession of quadratically unconstrained optimization problems. Going higher in the hierarchy, we can solve problems that involve inequality constraints iteratively by solving at each iteration a problem with that involves a smooth and unconstrained objective function. At the top of the hierarchy we have problems with non-differentiable objective functions. We first convert the problem with the non-differentiable objective function to a problem with a differentiable function by adding additional inequality constraints to accurately represent the non-differentiable function. For example, if we want to minimize &lt;math&gt;|x|&lt;/math&gt; w.r.t. &lt;math&gt;x&lt;/math&gt; (assume unconstained), we can instead minimize &lt;math&gt;t&lt;/math&gt; w.r.t. &lt;math&gt;x,t&lt;/math&gt; subject to &lt;math&gt;-t &lt; x &lt; t&lt;/math&gt;. The [[Epigraph trick|epigraph trick]] that is used here is a very common trick to convert problems with non-differentiable objective functions into problems with differentiable functions.</text>
      <sha1>q48abwxm330a6r9qumerfmxprtbsw1w</sha1>
    </revision>
  </page>
  <page>
    <title>Epigraph trick</title>
    <ns>0</ns>
    <id>90</id>
    <redirect title="Convex optimization" />
    <revision>
      <id>286</id>
      <timestamp>2018-10-13T19:12:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Convex optimization#Some standard conversions]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59">#REDIRECT [[Convex optimization#Some standard conversions]]</text>
      <sha1>6rzoirzpe9bevu6flzrqy43wqln2q2t</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix factorization</title>
    <ns>0</ns>
    <id>91</id>
    <redirect title="Matrix decomposition" />
    <revision>
      <id>290</id>
      <timestamp>2018-10-20T12:48:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Matrix decomposition]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34">#REDIRECT [[Matrix decomposition]]</text>
      <sha1>917rkya49nf50pkvsj8oft4rcve17ir</sha1>
    </revision>
  </page>
  <page>
    <title>Solving linear equations</title>
    <ns>0</ns>
    <id>92</id>
    <revision>
      <id>291</id>
      <timestamp>2018-10-20T13:59:48Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "In general, solving a system of linear equations &lt;math&gt;Ax=b&lt;/math&gt; is pretty difficult. However, there are certain subsets of systems that are easy to solve. The idea is to ta..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="747">In general, solving a system of linear equations &lt;math&gt;Ax=b&lt;/math&gt; is pretty difficult. However, there are certain subsets of systems that are easy to solve. The idea is to take any system and convert it to this form that's easy to solve. Below are the main sets of systems that are easy to solve (we consider square matrices, &lt;math&gt;A \in \mathbb R^{n\times n}&lt;/math&gt;):
* &lt;math&gt;A&lt;/math&gt; is diagonal (&lt;math&gt;x_i = b_i/a_{ii}&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is lower triangular (start with &lt;math&gt;x_1 = b_1/a_{11}&lt;/math&gt; and substitute it going forward)
* &lt;math&gt;A&lt;/math&gt; is upper triangular (start with &lt;math&gt;x_n = b_n/a_{nn}&lt;/math&gt; and substitute it going backward)
* &lt;math&gt;A&lt;/math&gt; is orthogonal (&lt;math&gt;x=A^Tb&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is permutation matrix</text>
      <sha1>cxwa40to2my6xagbgy6vj5ecw0f2buv</sha1>
    </revision>
    <revision>
      <id>292</id>
      <parentid>291</parentid>
      <timestamp>2018-10-20T14:12:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2197">In general, solving a system of linear equations &lt;math&gt;Ax=b&lt;/math&gt; is pretty difficult. However, there are certain subsets of systems that are easy to solve. The idea is to take any system and convert it to this form that's easy to solve. Below are the main sets of systems that are easy to solve (we consider square matrices, &lt;math&gt;A \in \mathbb R^{n\times n}&lt;/math&gt;)&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:
* &lt;math&gt;A&lt;/math&gt; is diagonal (&lt;math&gt;x_i = b_i/a_{ii}&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is lower triangular (start with &lt;math&gt;x_1 = b_1/a_{11}&lt;/math&gt; and substitute it going forward)
* &lt;math&gt;A&lt;/math&gt; is upper triangular (start with &lt;math&gt;x_n = b_n/a_{nn}&lt;/math&gt; and substitute it going backward)
* &lt;math&gt;A&lt;/math&gt; is orthogonal (&lt;math&gt;x=A^Tb&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is permutation matrix
 
== Simplifying the solution of linear equations ==

=== Factorization ===
The main strategy for solving general &lt;math&gt;Ax=b&lt;/math&gt; can now be stated as factoring &lt;math&gt;A&lt;/math&gt; into a favorable form &lt;math&gt;A=A_1A_2\dots A_k&lt;/math&gt; and then solving recursively by starting with &lt;math&gt;A_1z_1 = b&lt;/math&gt;, then &lt;math&gt;A_2 z_2 = z_1&lt;/math&gt;, until &lt;math&gt;A_k z_k = z_{k-1}&lt;/math&gt;.

The factorizations that facilitate the solving of equations are:
* LU factorization, &lt;math&gt;A=LU&lt;/math&gt;. Every nonsingular matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLU&lt;/math&gt;, where &lt;math&gt;P&lt;/math&gt; is a permutation matrix
* Cholesky factorization: every positive definite &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A = LL^T&lt;/math&gt;
* LDL^T: Every nonsingular symmetric matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLDL^TP^T&lt;/math&gt; where &lt;math&gt;P^T&lt;/math&gt; is a permutation matrix.

=== Equations with structured sub-blocks ===
If a matrix &lt;math&gt;A&lt;/math&gt; has as structured form according to which &lt;math&gt;Ax =b&lt;/math&gt; can be written as

 &lt;math&gt;\begin{bmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22}\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix} = \begin{bmatrix}b_1 \\ b_2\end{bmatrix} &lt;/math&gt;

where &lt;math&gt;A_{11}&lt;/math&gt; is nonsingular, then we can eliminate &lt;math&gt;x_{11} = A_{11}^{-1}(b1-A_{21}A_{11}^{-1}b_1)&lt;/math&gt;, compute &lt;math&gt;x_2&lt;/math&gt; from here and then use &lt;math&gt;x_2&lt;/math&gt; to find &lt;math&gt;x_1&lt;/math&gt;.

== References ==</text>
      <sha1>00trvj0w5r3qh415q94dsdpy4j1veju</sha1>
    </revision>
    <revision>
      <id>293</id>
      <parentid>292</parentid>
      <timestamp>2018-10-20T14:17:45Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2675">In general, solving a system of linear equations &lt;math&gt;Ax=b&lt;/math&gt; is pretty difficult. However, there are certain subsets of systems that are easy to solve. The idea is to take any system and convert it to this form that's easy to solve. Below are the main sets of systems that are easy to solve (we consider square matrices, &lt;math&gt;A \in \mathbb R^{n\times n}&lt;/math&gt;)&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:
* &lt;math&gt;A&lt;/math&gt; is diagonal (&lt;math&gt;x_i = b_i/a_{ii}&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is lower triangular (start with &lt;math&gt;x_1 = b_1/a_{11}&lt;/math&gt; and substitute it going forward)
* &lt;math&gt;A&lt;/math&gt; is upper triangular (start with &lt;math&gt;x_n = b_n/a_{nn}&lt;/math&gt; and substitute it going backward)
* &lt;math&gt;A&lt;/math&gt; is orthogonal (&lt;math&gt;x=A^Tb&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is permutation matrix
 
== Simplifying the solution of linear equations ==

=== Factorization ===
The main strategy for solving general &lt;math&gt;Ax=b&lt;/math&gt; can now be stated as factoring &lt;math&gt;A&lt;/math&gt; into a favorable form &lt;math&gt;A=A_1A_2\dots A_k&lt;/math&gt; and then solving recursively by starting with &lt;math&gt;A_1z_1 = b&lt;/math&gt;, then &lt;math&gt;A_2 z_2 = z_1&lt;/math&gt;, until &lt;math&gt;A_k z_k = z_{k-1}&lt;/math&gt;.

The factorizations that facilitate the solving of equations are:
* LU factorization, &lt;math&gt;A=LU&lt;/math&gt;. Every nonsingular matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLU&lt;/math&gt;, where &lt;math&gt;P&lt;/math&gt; is a permutation matrix
* Cholesky factorization: every positive definite &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A = LL^T&lt;/math&gt;
* LDL^T: Every nonsingular symmetric matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLDL^TP^T&lt;/math&gt; where &lt;math&gt;P^T&lt;/math&gt; is a permutation matrix.

=== Equations with structured sub-blocks ===
If a matrix &lt;math&gt;A&lt;/math&gt; has as structured form according to which &lt;math&gt;Ax =b&lt;/math&gt; can be written as

 &lt;math&gt;\begin{bmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22}\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix} = \begin{bmatrix}b_1 \\ b_2\end{bmatrix} &lt;/math&gt;

where &lt;math&gt;A_{11}&lt;/math&gt; is nonsingular, then we can eliminate &lt;math&gt;x_{11} = A_{11}^{-1}(b1-A_{21}A_{11}^{-1}b_1)&lt;/math&gt;, compute &lt;math&gt;x_2&lt;/math&gt; from here and then use &lt;math&gt;x_2&lt;/math&gt; to find &lt;math&gt;x_1&lt;/math&gt;.

=== Structured matrix plus low rank term ===

If we try to solve a system that can be written as
 &lt;math&gt;(A+BC)x = b&lt;/math&gt; 
where &lt;math&gt;Ax=b&lt;/math&gt; is easy to solve and &lt;math&gt;BC&lt;/math&gt; is low-rank, then we can write this system as:
 &lt;math&gt;\begin{bmatrix}A &amp; B \\ C &amp; -I\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} = \begin{bmatrix}b \\ 0\end{bmatrix}&lt;/math&gt;

now apply block elimination and solve &lt;math&gt;(I+CA^{-1}B)y = CA^{-1}b&lt;/math&gt;, and then solve &lt;math&gt;Ax = b-By&lt;/math&gt;

== References ==</text>
      <sha1>lq85uhzc92nhd2872ey5qj12lkjo5c9</sha1>
    </revision>
    <revision>
      <id>294</id>
      <parentid>293</parentid>
      <timestamp>2018-10-20T14:19:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Factorization */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2774">In general, solving a system of linear equations &lt;math&gt;Ax=b&lt;/math&gt; is pretty difficult. However, there are certain subsets of systems that are easy to solve. The idea is to take any system and convert it to this form that's easy to solve. Below are the main sets of systems that are easy to solve (we consider square matrices, &lt;math&gt;A \in \mathbb R^{n\times n}&lt;/math&gt;)&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:
* &lt;math&gt;A&lt;/math&gt; is diagonal (&lt;math&gt;x_i = b_i/a_{ii}&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is lower triangular (start with &lt;math&gt;x_1 = b_1/a_{11}&lt;/math&gt; and substitute it going forward)
* &lt;math&gt;A&lt;/math&gt; is upper triangular (start with &lt;math&gt;x_n = b_n/a_{nn}&lt;/math&gt; and substitute it going backward)
* &lt;math&gt;A&lt;/math&gt; is orthogonal (&lt;math&gt;x=A^Tb&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is permutation matrix
 
== Simplifying the solution of linear equations ==

=== Factorization ===
The main strategy for solving general &lt;math&gt;Ax=b&lt;/math&gt; can now be stated as factoring &lt;math&gt;A&lt;/math&gt; into a favorable form &lt;math&gt;A=A_1A_2\dots A_k&lt;/math&gt; and then solving recursively by starting with &lt;math&gt;A_1z_1 = b&lt;/math&gt;, then &lt;math&gt;A_2 z_2 = z_1&lt;/math&gt;, until &lt;math&gt;A_k z_k = z_{k-1}&lt;/math&gt;.

The factorizations that facilitate the solving of equations are (below &lt;math&gt;L,U&lt;/math&gt; denote a lower- and an upper-triangular matrix, respectively):
* LU factorization, &lt;math&gt;A=LU&lt;/math&gt;. Every nonsingular matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLU&lt;/math&gt;, where &lt;math&gt;P&lt;/math&gt; is a permutation matrix
* Cholesky factorization: every positive definite &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A = LL^T&lt;/math&gt;
* &lt;math&gt;LDL^T&lt;/math&gt;: Every nonsingular symmetric matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLDL^TP^T&lt;/math&gt; where &lt;math&gt;P^T&lt;/math&gt; is a permutation matrix.

=== Equations with structured sub-blocks ===
If a matrix &lt;math&gt;A&lt;/math&gt; has as structured form according to which &lt;math&gt;Ax =b&lt;/math&gt; can be written as

 &lt;math&gt;\begin{bmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22}\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix} = \begin{bmatrix}b_1 \\ b_2\end{bmatrix} &lt;/math&gt;

where &lt;math&gt;A_{11}&lt;/math&gt; is nonsingular, then we can eliminate &lt;math&gt;x_{11} = A_{11}^{-1}(b1-A_{21}A_{11}^{-1}b_1)&lt;/math&gt;, compute &lt;math&gt;x_2&lt;/math&gt; from here and then use &lt;math&gt;x_2&lt;/math&gt; to find &lt;math&gt;x_1&lt;/math&gt;.

=== Structured matrix plus low rank term ===

If we try to solve a system that can be written as
 &lt;math&gt;(A+BC)x = b&lt;/math&gt; 
where &lt;math&gt;Ax=b&lt;/math&gt; is easy to solve and &lt;math&gt;BC&lt;/math&gt; is low-rank, then we can write this system as:
 &lt;math&gt;\begin{bmatrix}A &amp; B \\ C &amp; -I\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} = \begin{bmatrix}b \\ 0\end{bmatrix}&lt;/math&gt;

now apply block elimination and solve &lt;math&gt;(I+CA^{-1}B)y = CA^{-1}b&lt;/math&gt;, and then solve &lt;math&gt;Ax = b-By&lt;/math&gt;

== References ==</text>
      <sha1>3arso6eq1ue7dmh6xmdmrvakynh3zvf</sha1>
    </revision>
    <revision>
      <id>295</id>
      <parentid>294</parentid>
      <timestamp>2018-10-20T14:22:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2752">In general, solving a system of linear equations &lt;math&gt;Ax=b&lt;/math&gt; is pretty difficult. However, there are certain subsets of systems that are easy to solve. The idea is to take any system and convert it to this form that's easy to solve. Below are the main sets of systems that are easy to solve (we consider square matrices, &lt;math&gt;A \in \mathbb R^{n\times n}&lt;/math&gt;)&lt;ref&gt;Stephen Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:
* &lt;math&gt;A&lt;/math&gt; is diagonal (&lt;math&gt;x_i = b_i/a_{ii}&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is lower triangular (start with &lt;math&gt;x_1 = b_1/a_{11}&lt;/math&gt; and substitute it going forward)
* &lt;math&gt;A&lt;/math&gt; is upper triangular (start with &lt;math&gt;x_n = b_n/a_{nn}&lt;/math&gt; and substitute it going backward)
* &lt;math&gt;A&lt;/math&gt; is orthogonal (&lt;math&gt;x=A^Tb&lt;/math&gt;)
* &lt;math&gt;A&lt;/math&gt; is permutation matrix
 
== Strategies for solving ==

=== Factorization ===
The main strategy for solving general &lt;math&gt;Ax=b&lt;/math&gt; can now be stated as factoring &lt;math&gt;A&lt;/math&gt; into a favorable form &lt;math&gt;A=A_1A_2\dots A_k&lt;/math&gt; and then solving recursively by starting with &lt;math&gt;A_1z_1 = b&lt;/math&gt;, then &lt;math&gt;A_2 z_2 = z_1&lt;/math&gt;, until &lt;math&gt;A_k z_k = z_{k-1}&lt;/math&gt;.

The factorizations that facilitate the solving of equations are (below &lt;math&gt;L,U&lt;/math&gt; denote a lower- and an upper-triangular matrix, respectively):
* LU factorization, &lt;math&gt;A=LU&lt;/math&gt;. Every nonsingular matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLU&lt;/math&gt;, where &lt;math&gt;P&lt;/math&gt; is a permutation matrix
* Cholesky factorization: every positive definite &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A = LL^T&lt;/math&gt;
* &lt;math&gt;LDL^T&lt;/math&gt;: Every nonsingular symmetric matrix &lt;math&gt;A&lt;/math&gt; can be factored as &lt;math&gt;A=PLDL^TP^T&lt;/math&gt; where &lt;math&gt;P^T&lt;/math&gt; is a permutation matrix.

=== Equations with structured sub-blocks ===
If a matrix &lt;math&gt;A&lt;/math&gt; has as structured form according to which &lt;math&gt;Ax =b&lt;/math&gt; can be written as

 &lt;math&gt;\begin{bmatrix}A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22}\end{bmatrix}\begin{bmatrix}x_1 \\ x_2\end{bmatrix} = \begin{bmatrix}b_1 \\ b_2\end{bmatrix} &lt;/math&gt;

where &lt;math&gt;A_{11}&lt;/math&gt; is nonsingular, then we can eliminate &lt;math&gt;x_{11} = A_{11}^{-1}(b1-A_{21}A_{11}^{-1}b_1)&lt;/math&gt;, compute &lt;math&gt;x_2&lt;/math&gt; from here and then use &lt;math&gt;x_2&lt;/math&gt; to find &lt;math&gt;x_1&lt;/math&gt;.

=== Structured matrix plus low rank term ===

If we try to solve a system that can be written as
 &lt;math&gt;(A+BC)x = b&lt;/math&gt; 
where &lt;math&gt;Ax=b&lt;/math&gt; is easy to solve and &lt;math&gt;BC&lt;/math&gt; is low-rank, then we can write this system as:
 &lt;math&gt;\begin{bmatrix}A &amp; B \\ C &amp; -I\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} = \begin{bmatrix}b \\ 0\end{bmatrix}&lt;/math&gt;

now apply block elimination and solve &lt;math&gt;(I+CA^{-1}B)y = CA^{-1}b&lt;/math&gt;, and then solve &lt;math&gt;Ax = b-By&lt;/math&gt;

== References ==</text>
      <sha1>dp5tsogylrdhddoaw6agtg6blvswtbs</sha1>
    </revision>
  </page>
  <page>
    <title>LU decomposition</title>
    <ns>0</ns>
    <id>93</id>
    <revision>
      <id>296</id>
      <timestamp>2018-10-20T14:27:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Every nonsingular matrix &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;  &lt;math&gt;A=PLU&lt;/math&gt; where &lt;math&gt;P&lt;/math&gt; is a permutation matrix, &lt;math&gt;L&lt;..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="322">Every nonsingular matrix &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;
 &lt;math&gt;A=PLU&lt;/math&gt;
where &lt;math&gt;P&lt;/math&gt; is a permutation matrix, &lt;math&gt;L&lt;/math&gt; lower triangular and &lt;math&gt;U&lt;/math&gt; is an upper triangular matrix. The cost of this factorization is &lt;math&gt;(2/3)n^3&lt;/math&gt;

== References ==</text>
      <sha1>b30vw03xm6sqmdsobtmp3xsj6cbh261</sha1>
    </revision>
  </page>
  <page>
    <title>Cholesky factorization</title>
    <ns>0</ns>
    <id>94</id>
    <revision>
      <id>297</id>
      <timestamp>2018-10-20T14:30:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Every positive definite &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:  &lt;math&gt;A = LL^T&lt;/math&gt;  with &lt;math&gt;L&lt;/math&gt; lower triangular. The cost is..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="284">Every positive definite &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:

&lt;math&gt;A = LL^T&lt;/math&gt;

with &lt;math&gt;L&lt;/math&gt; lower triangular. The cost is &lt;math&gt;(1/3)n^3&lt;/math&gt; flops. This factorization is used to simplify the solving of systems &lt;math&gt;Ax=b&lt;/math&gt;.</text>
      <sha1>8kjwd7ne2vptqzgop9rokjv45ubjw8u</sha1>
    </revision>
    <revision>
      <id>298</id>
      <parentid>297</parentid>
      <timestamp>2018-10-20T14:30:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="302">Every positive definite &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:

&lt;math&gt;A = LL^T&lt;/math&gt;

with &lt;math&gt;L&lt;/math&gt; lower triangular. The cost is &lt;math&gt;(1/3)n^3&lt;/math&gt; flops. This factorization is used to simplify the solving of systems &lt;math&gt;Ax=b&lt;/math&gt;.

== References ==</text>
      <sha1>2gzfuaazlpt85cxubudc981kdsqpqs6</sha1>
    </revision>
    <revision>
      <id>301</id>
      <parentid>298</parentid>
      <timestamp>2018-10-20T14:38:12Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="312">Every symmetric positive definite &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;:

&lt;math&gt;A = LL^T&lt;/math&gt;

with &lt;math&gt;L&lt;/math&gt; lower triangular. The cost is &lt;math&gt;(1/3)n^3&lt;/math&gt; flops. This factorization is used to simplify the solving of systems &lt;math&gt;Ax=b&lt;/math&gt;.

== References ==</text>
      <sha1>i56bm6jabg65n5t2k8k6meop1xsp65t</sha1>
    </revision>
  </page>
  <page>
    <title>LDL factorization</title>
    <ns>0</ns>
    <id>95</id>
    <revision>
      <id>299</id>
      <timestamp>2018-10-20T14:34:31Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Every nonsingular symmetric matrix &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;  &lt;math&gt;A=PLDL^TP^T&lt;/math&gt;   with &lt;math&gt;P&lt;/math&gt; a permutation ma..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="390">Every nonsingular symmetric matrix &lt;math&gt;A&lt;/math&gt; can be factored as&lt;ref&gt;S. Boyd, EE364A Lecture Notes 9&lt;/ref&gt;

&lt;math&gt;A=PLDL^TP^T&lt;/math&gt; 

with &lt;math&gt;P&lt;/math&gt; a permutation matrix, &lt;math&gt;L&lt;/math&gt; lower triangular, &lt;math&gt;D&lt;/math&gt; block diagonal with &lt;math&gt;1\times 1&lt;/math&gt; or &lt;math&gt;2 \times 2&lt;/math&gt; diagonal blocks. The cost of this factorization is &lt;math&gt;(1/3)n^3&lt;/math&gt;.

== References ==</text>
      <sha1>o2uwnsabq7bnku1ads4zrsbz8uir3xy</sha1>
    </revision>
  </page>
  <page>
    <title>Descent methods</title>
    <ns>0</ns>
    <id>96</id>
    <revision>
      <id>303</id>
      <timestamp>2018-10-20T16:59:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;.  Following is the general outline of descent methods:  &lt;math&gt;\textbf{given} \text{ a..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="430">Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;.  Following is the general outline of descent methods:
 &lt;math&gt;\textbf{given} \text{ a starting point } x \in \textbf{dom}f \\
\textbf{repeat} \\
\text{ 1. Determine a descent direction }\Delta x\\
\text{ 2. Line search. Choose a step step size} t&gt;0\\
\text{3. Update}. x = x+t\Delta x \\
\textbf{until } stopping criterion satisfied
&lt;/math&gt;</text>
      <sha1>orsi87v5tyeb87sj8ox78mnp3b88cke</sha1>
    </revision>
    <revision>
      <id>304</id>
      <parentid>303</parentid>
      <timestamp>2018-10-20T17:26:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1698">Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;. 

== Outline ==

Following is the general outline of descent methods:
 &lt;math&gt;\textbf{given} \text{ a starting point } x \in \textbf{dom}f \\
\textbf{repeat} \\
\text{ 1. Determine a descent direction }\Delta x\\
\text{ 2. Line search. Choose a step step size } t&gt;0\\
\text{ 3. Update}. x = x+t\Delta x \\
\textbf{until } \text{stopping criterion satisfied}
&lt;/math&gt;

There are two key ingredients that determine the gradient method: The descent direction and the line search. Some methods also have their own stopping criteria.

=== Descent direction ===

There are several standard methods to compute descent direction. The simplest is to compute the [[Gradient descent|gradient descent]], which doesn't require the Hessian. Another standard method is [[Newton's method]] which does require the Hessian. There are simpler alternatives to Newton's method which use simple approximations of the Hessian, such as a diagonal approximation. The advantage of Newton's method is that it's affine invariant (see [[Newton's method]]).

=== Line search ===
Once the descent direction is determined, a natural question is how big of a step one should take in that direction.

==== Exact line search ====
The most straightforward approach to line search would be exact line search: &lt;math&gt;t=\arg_{t&gt;0}\min f(x+t\Delta x)&lt;/math&gt;. However, this is typically costly and unnecessary. 

=== Backtracking line search ===
A simpler and typically more efficient approach to line search is backtracking line search. The algorithm has two parameters, &lt;math&gt;\alpha \in (0,1/2)&lt;/math&gt; and &lt;math&gt;\beta \in (0,1)&lt;/math&gt;

&lt;math&gt;&lt;/math&gt;</text>
      <sha1>b7fzczh9n3ootwjjwgmahecsxobjauy</sha1>
    </revision>
    <revision>
      <id>306</id>
      <parentid>304</parentid>
      <timestamp>2018-10-20T17:40:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1884">Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;. 

== Outline ==

Following is the general outline of descent methods:
 &lt;math&gt;\textbf{given} \text{ a starting point } x \in \textbf{dom}f \\
\textbf{repeat} \\
\text{ 1. Determine a descent direction }\Delta x\\
\text{ 2. Line search. Choose a step step size } t&gt;0\\
\text{ 3. Update}. x = x+t\Delta x \\
\textbf{until } \text{stopping criterion satisfied}
&lt;/math&gt;

There are two key ingredients that determine the gradient method: The descent direction and the line search. Some methods also have their own stopping criteria.

=== Descent direction ===

There are several standard methods to compute descent direction. The simplest is to compute the [[Gradient descent|gradient descent]], which doesn't require the Hessian. Another standard method is [[Newton's method]] which does require the Hessian. There are simpler alternatives to Newton's method which use simple approximations of the Hessian, such as a diagonal approximation. The advantage of Newton's method is that it's affine invariant (see [[Newton's method]]).

=== Line search ===
Once the descent direction is determined, a natural question is how big of a step one should take in that direction.

==== Exact line search ====
The most straightforward approach to line search would be exact line search: &lt;math&gt;t=\arg_{t&gt;0}\min f(x+t\Delta x)&lt;/math&gt;. However, this is typically costly and unnecessary. 

=== Backtracking line search ===
A simpler and typically more efficient approach to line search is backtracking line search. The algorithm has two parameters, &lt;math&gt;\alpha \in (0,1/2)&lt;/math&gt; and &lt;math&gt;\beta \in (0,1)&lt;/math&gt;. The outline of the algorithm is then 

 &lt;math&gt;\textbf{given } t=1, \alpha, \beta \\
 \textbf{repeat } t = \beta t \\
 \textbf{until } f(x+t \Delta x) &lt; f(x) + \alpha t \nabla f(x)^T \Delta x 
&lt;/math&gt;</text>
      <sha1>9e4xeiencz5b5hvqdzk0nhxwjvjcary</sha1>
    </revision>
    <revision>
      <id>307</id>
      <parentid>306</parentid>
      <timestamp>2018-10-20T17:42:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1929">Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;. 

== Outline ==

Following is the general outline of descent methods&lt;ref&gt;S. Boyd, EE364A Lecture Notes 10&lt;/ref&gt;:
 &lt;math&gt;\textbf{given} \text{ a starting point } x \in \textbf{dom}f \\
\textbf{repeat} \\
\text{ 1. Determine a descent direction }\Delta x\\
\text{ 2. Line search. Choose a step step size } t&gt;0\\
\text{ 3. Update}. x = x+t\Delta x \\
\textbf{until } \text{stopping criterion satisfied}
&lt;/math&gt;

There are two key ingredients that determine the gradient method: The descent direction and the line search. Some methods also have their own stopping criteria.

=== Descent direction ===

There are several standard methods to compute descent direction. The simplest is to compute the [[Gradient descent|gradient descent]], which doesn't require the Hessian. Another standard method is [[Newton's method]] which does require the Hessian. There are simpler alternatives to Newton's method which use simple approximations of the Hessian, such as a diagonal approximation. The advantage of Newton's method is that it's affine invariant (see [[Newton's method]]).

=== Line search ===
Once the descent direction is determined, a natural question is how big of a step one should take in that direction.

==== Exact line search ====
The most straightforward approach to line search would be exact line search: &lt;math&gt;t=\arg_{t&gt;0}\min f(x+t\Delta x)&lt;/math&gt;. However, this is typically costly and unnecessary. 

==== Backtracking line search ====
A simpler and typically more efficient approach to line search is backtracking line search. The algorithm has two parameters, &lt;math&gt;\alpha \in (0,1/2)&lt;/math&gt; and &lt;math&gt;\beta \in (0,1)&lt;/math&gt;. The outline of the algorithm is then 

 &lt;math&gt;\textbf{given } t=1, \alpha, \beta \\
 \textbf{repeat } t = \beta t \\
 \textbf{until } f(x+t \Delta x) &lt; f(x) + \alpha t \nabla f(x)^T \Delta x 
&lt;/math&gt;</text>
      <sha1>hso3399bwz694kofjr49lvs1aavny69</sha1>
    </revision>
    <revision>
      <id>308</id>
      <parentid>307</parentid>
      <timestamp>2018-10-20T18:42:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3042">Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;. 

Following is the general outline of descent methods&lt;ref&gt;S. Boyd, EE364A Lecture Notes 10&lt;/ref&gt;:
 &lt;math&gt;\textbf{given} \text{ a starting point } x \in \textbf{dom}f \\
\textbf{repeat} \\
\text{ 1. Determine a descent direction }\Delta x\\
\text{ 2. Line search. Choose a step step size } t&gt;0\\
\text{ 3. Update}. x = x+t\Delta x \\
\textbf{until } \text{stopping criterion satisfied}
&lt;/math&gt;

There are two key ingredients that determine the gradient method: The descent direction and the line search. Some methods also have their own stopping criteria.

== Descent direction ==


=== Gradient descent === 
There are several standard methods to compute descent direction. The simplest is to compute the [[Gradient descent|gradient descent]]:
 &lt;math&gt;\Delta x = -\nabla f(x).&lt;/math&gt;

This method doesn't require the Hessian, however, it is sensitive to the coordinate system of the data.

=== Steepest descent === 
Gradient descent is actually a special case of steepest descent direction, which is defined, for a given norm &lt;math&gt;||\cdot||&lt;/math&gt; as:

 &lt;math&gt;\Delta x_{\text{nsd}} = \arg\min\{ \nabla f(x) ^T v : ||v|| = 1\}.&lt;/math&gt;

The gradient descent is recovered if the norm is set to the euclidean norm. Turns out that a very good choice of norm is the Hessian (at each iteration), which gives the Newton's method.

=== Newton's method ===

Newton's method is obtained by setting the gradient to 

 &lt;math&gt;\Delta x_{nt} = -\nabla^2 f(x)^{-1}\nabla f(x)&lt;/math&gt;.

In general, the Hessian is not constant, so Newton's method can be very costly especially for high dimensional problems. However, Newton's method has the great advantage of being affine invariant and converging in a predictable number of iterations (typically around 25-30 at most). That is, minimizing &lt;math&gt;f(x)&lt;/math&gt; and &lt;math&gt;f(Ax+b)&lt;/math&gt; takes the same number of iterations.

=== Quasi-Newton methods ===
Quasi-Newton methods try avoid the costly Hessian by approximating it with simpler matrices (e.g., diagonal) or by reusing the same Hessian for a number of iterations before updating it. They provide a good compromise and are generally worth trying if the problem is computionally complex.

== Line search ==
Once the descent direction is determined, a natural question is how big of a step one should take in that direction.

=== Exact line search ===
The most straightforward approach to line search would be exact line search: &lt;math&gt;t=\arg_{t&gt;0}\min f(x+t\Delta x)&lt;/math&gt;. However, this is typically costly and unnecessary. 

=== Backtracking line search ===
A simpler and typically more efficient approach to line search is backtracking line search. The algorithm has two parameters, &lt;math&gt;\alpha \in (0,1/2)&lt;/math&gt; and &lt;math&gt;\beta \in (0,1)&lt;/math&gt;. The outline of the algorithm is then 

 &lt;math&gt;\textbf{given } t=1, \alpha, \beta \\
 \textbf{repeat } t = \beta t \\
 \textbf{until } f(x+t \Delta x) &lt; f(x) + \alpha t \nabla f(x)^T \Delta x 
&lt;/math&gt;

== References ==</text>
      <sha1>nl6npbwt0m52en8qsu9f7hoqpfxqvho</sha1>
    </revision>
    <revision>
      <id>309</id>
      <parentid>308</parentid>
      <timestamp>2018-10-20T18:42:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Gradient descent */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3021">Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;. 

Following is the general outline of descent methods&lt;ref&gt;S. Boyd, EE364A Lecture Notes 10&lt;/ref&gt;:
 &lt;math&gt;\textbf{given} \text{ a starting point } x \in \textbf{dom}f \\
\textbf{repeat} \\
\text{ 1. Determine a descent direction }\Delta x\\
\text{ 2. Line search. Choose a step step size } t&gt;0\\
\text{ 3. Update}. x = x+t\Delta x \\
\textbf{until } \text{stopping criterion satisfied}
&lt;/math&gt;

There are two key ingredients that determine the gradient method: The descent direction and the line search. Some methods also have their own stopping criteria.

== Descent direction ==


=== Gradient descent === 
There are several standard methods to compute descent direction. The simplest is to compute the gradient descent:
 &lt;math&gt;\Delta x = -\nabla f(x).&lt;/math&gt;

This method doesn't require the Hessian, however, it is sensitive to the coordinate system of the data.

=== Steepest descent === 
Gradient descent is actually a special case of steepest descent direction, which is defined, for a given norm &lt;math&gt;||\cdot||&lt;/math&gt; as:

 &lt;math&gt;\Delta x_{\text{nsd}} = \arg\min\{ \nabla f(x) ^T v : ||v|| = 1\}.&lt;/math&gt;

The gradient descent is recovered if the norm is set to the euclidean norm. Turns out that a very good choice of norm is the Hessian (at each iteration), which gives the Newton's method.

=== Newton's method ===

Newton's method is obtained by setting the gradient to 

 &lt;math&gt;\Delta x_{nt} = -\nabla^2 f(x)^{-1}\nabla f(x)&lt;/math&gt;.

In general, the Hessian is not constant, so Newton's method can be very costly especially for high dimensional problems. However, Newton's method has the great advantage of being affine invariant and converging in a predictable number of iterations (typically around 25-30 at most). That is, minimizing &lt;math&gt;f(x)&lt;/math&gt; and &lt;math&gt;f(Ax+b)&lt;/math&gt; takes the same number of iterations.

=== Quasi-Newton methods ===
Quasi-Newton methods try avoid the costly Hessian by approximating it with simpler matrices (e.g., diagonal) or by reusing the same Hessian for a number of iterations before updating it. They provide a good compromise and are generally worth trying if the problem is computionally complex.

== Line search ==
Once the descent direction is determined, a natural question is how big of a step one should take in that direction.

=== Exact line search ===
The most straightforward approach to line search would be exact line search: &lt;math&gt;t=\arg_{t&gt;0}\min f(x+t\Delta x)&lt;/math&gt;. However, this is typically costly and unnecessary. 

=== Backtracking line search ===
A simpler and typically more efficient approach to line search is backtracking line search. The algorithm has two parameters, &lt;math&gt;\alpha \in (0,1/2)&lt;/math&gt; and &lt;math&gt;\beta \in (0,1)&lt;/math&gt;. The outline of the algorithm is then 

 &lt;math&gt;\textbf{given } t=1, \alpha, \beta \\
 \textbf{repeat } t = \beta t \\
 \textbf{until } f(x+t \Delta x) &lt; f(x) + \alpha t \nabla f(x)^T \Delta x 
&lt;/math&gt;

== References ==</text>
      <sha1>8xwisk996qxiy2u9ips4az5mrwmvnur</sha1>
    </revision>
    <revision>
      <id>313</id>
      <parentid>309</parentid>
      <timestamp>2018-10-20T18:46:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <comment>/* Newton's method */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3028">Descent methods are a set of methods to iteratively minimize a function &lt;math&gt;f(x)&lt;/math&gt;. 

Following is the general outline of descent methods&lt;ref&gt;S. Boyd, EE364A Lecture Notes 10&lt;/ref&gt;:
 &lt;math&gt;\textbf{given} \text{ a starting point } x \in \textbf{dom}f \\
\textbf{repeat} \\
\text{ 1. Determine a descent direction }\Delta x\\
\text{ 2. Line search. Choose a step step size } t&gt;0\\
\text{ 3. Update}. x = x+t\Delta x \\
\textbf{until } \text{stopping criterion satisfied}
&lt;/math&gt;

There are two key ingredients that determine the gradient method: The descent direction and the line search. Some methods also have their own stopping criteria.

== Descent direction ==


=== Gradient descent === 
There are several standard methods to compute descent direction. The simplest is to compute the gradient descent:
 &lt;math&gt;\Delta x = -\nabla f(x).&lt;/math&gt;

This method doesn't require the Hessian, however, it is sensitive to the coordinate system of the data.

=== Steepest descent === 
Gradient descent is actually a special case of steepest descent direction, which is defined, for a given norm &lt;math&gt;||\cdot||&lt;/math&gt; as:

 &lt;math&gt;\Delta x_{\text{nsd}} = \arg\min\{ \nabla f(x) ^T v : ||v|| = 1\}.&lt;/math&gt;

The gradient descent is recovered if the norm is set to the euclidean norm. Turns out that a very good choice of norm is the Hessian (at each iteration), which gives the Newton's method.

=== Newton's method ===

Newton's method is obtained by setting the gradient to 

 &lt;math&gt;\Delta x_{\text{nt}} = -\nabla^2 f(x)^{-1}\nabla f(x)&lt;/math&gt;.

In general, the Hessian is not constant, so Newton's method can be very costly especially for high dimensional problems. However, Newton's method has the great advantage of being affine invariant and converging in a predictable number of iterations (typically around 25-30 at most). That is, minimizing &lt;math&gt;f(x)&lt;/math&gt; and &lt;math&gt;f(Ax+b)&lt;/math&gt; takes the same number of iterations.

=== Quasi-Newton methods ===
Quasi-Newton methods try avoid the costly Hessian by approximating it with simpler matrices (e.g., diagonal) or by reusing the same Hessian for a number of iterations before updating it. They provide a good compromise and are generally worth trying if the problem is computionally complex.

== Line search ==
Once the descent direction is determined, a natural question is how big of a step one should take in that direction.

=== Exact line search ===
The most straightforward approach to line search would be exact line search: &lt;math&gt;t=\arg_{t&gt;0}\min f(x+t\Delta x)&lt;/math&gt;. However, this is typically costly and unnecessary. 

=== Backtracking line search ===
A simpler and typically more efficient approach to line search is backtracking line search. The algorithm has two parameters, &lt;math&gt;\alpha \in (0,1/2)&lt;/math&gt; and &lt;math&gt;\beta \in (0,1)&lt;/math&gt;. The outline of the algorithm is then 

 &lt;math&gt;\textbf{given } t=1, \alpha, \beta \\
 \textbf{repeat } t = \beta t \\
 \textbf{until } f(x+t \Delta x) &lt; f(x) + \alpha t \nabla f(x)^T \Delta x 
&lt;/math&gt;

== References ==</text>
      <sha1>ghph9dqfgta8ve7oujcwrxj3sndf2nt</sha1>
    </revision>
  </page>
  <page>
    <title>Line search</title>
    <ns>0</ns>
    <id>97</id>
    <redirect title="Descent methods" />
    <revision>
      <id>305</id>
      <timestamp>2018-10-20T17:28:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Descent methods#Line search]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="41">#REDIRECT [[Descent methods#Line search]]</text>
      <sha1>orqk3at7w4l9j5eh98b9zgscrm8oct1</sha1>
    </revision>
  </page>
  <page>
    <title>Newton's method</title>
    <ns>0</ns>
    <id>98</id>
    <redirect title="Descent methods" />
    <revision>
      <id>310</id>
      <timestamp>2018-10-20T18:43:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Descent methods#Newton's method]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="45">#REDIRECT [[Descent methods#Newton's method]]</text>
      <sha1>cj4lqg7i5lclh4ue7plvna40hoc04a3</sha1>
    </revision>
  </page>
  <page>
    <title>Gradient descent</title>
    <ns>0</ns>
    <id>99</id>
    <redirect title="Descent methods" />
    <revision>
      <id>311</id>
      <timestamp>2018-10-20T18:44:12Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Descent methods#Gradient descent]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="46">#REDIRECT [[Descent methods#Gradient descent]]</text>
      <sha1>trlgo806xxx3pu26o3a5kjf516y1w7t</sha1>
    </revision>
  </page>
  <page>
    <title>Quasi-Newton methods</title>
    <ns>0</ns>
    <id>100</id>
    <redirect title="Descent methods" />
    <revision>
      <id>312</id>
      <timestamp>2018-10-20T18:45:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Descent methods#Quasi-Newton methods]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="50">#REDIRECT [[Descent methods#Quasi-Newton methods]]</text>
      <sha1>bwsd5ic904a4vg4bzki7qb1m9cksgup</sha1>
    </revision>
  </page>
  <page>
    <title>File:V structure.png</title>
    <ns>6</ns>
    <id>101</id>
    <revision>
      <id>318</id>
      <timestamp>2018-11-17T22:10:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-11-17T22:10:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>V_structure.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/d/d9/V_structure.png</src>
      <size>14636</size>
      <sha1base36>5avy2h5bznzstr4mogl9878dwa4h9b5</sha1base36>
      <rel>d/d9/V_structure.png</rel>
    </upload>
  </page>
  <page>
    <title>V-structure</title>
    <ns>0</ns>
    <id>102</id>
    <revision>
      <id>319</id>
      <timestamp>2018-11-17T22:18:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "In the context of a [[Probabilistic graphical model]], a V-structure is a trio of nodes &lt;math&gt;A, B, C&lt;/math&gt; such that &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; both influence &lt;math&gt;C..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="782">In the context of a [[Probabilistic graphical model]], a V-structure is a trio of nodes &lt;math&gt;A, B, C&lt;/math&gt; such that &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; both influence &lt;math&gt;C&lt;/math&gt;:

[[File:v_structure.png]]

V-structures are critical for identifying conditional independence or for d-separated nodes. Similarly, they are used in finding [[Active trail|Active trails]].

Often, we talk about ''activating'' v-structures, which happens when we observe the variable &lt;math&gt;C&lt;/math&gt;. The consequences of observing &lt;math&gt;C&lt;/math&gt; are important and counterintuitive: The variables &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt;, which are originally independent, become dependent when &lt;math&gt;C&lt;/math&gt; is observed. Formally, we have that

 &lt;math&gt;P(A,B) = P(A)P(B) \\
P(A,B|C) \neq P(A|C)P(B|C)
&lt;/math&gt;</text>
      <sha1>862s9xk69jkxvydvqwtfl95xj7lumr1</sha1>
    </revision>
  </page>
  <page>
    <title>Active trail</title>
    <ns>0</ns>
    <id>103</id>
    <revision>
      <id>320</id>
      <timestamp>2018-11-17T22:19:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/m..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="260">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;.</text>
      <sha1>0n3ik9edk9cpxlurfurk5dslvzr1gas</sha1>
    </revision>
    <revision>
      <id>322</id>
      <parentid>320</parentid>
      <timestamp>2018-11-17T22:26:50Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1112">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. A more formal definition of active trails will be given below for two separate cases: When some variables are and aren't observed.

== Active Trails ==

As an example, in the graph below &lt;math&gt;A&lt;/math&gt; influences &lt;math&gt;B&lt;/math&gt; which in turn infleunces &lt;math&gt;C&lt;/math&gt;.

[[File:linear_path.png]]

This can also be seen in the conditional probability distribution:

 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|B)&lt;/math&gt;

However, things become interesting and counterintuitive when we observe a set of variables &lt;math&gt;Z&lt;/math&gt;, which is equivalent to conditioning the probability distribution &lt;math&gt;P&lt;/math&gt; on the variables &lt;math&gt;Z&lt;/math&gt;. The following sections will describe under which conditions a path of graphs is and isn't active.

=== Unconditioned active trails ===









==References==</text>
      <sha1>81omhrme56ro3i7a6fizq9j9j1pbt99</sha1>
    </revision>
    <revision>
      <id>324</id>
      <parentid>322</parentid>
      <timestamp>2018-11-17T22:33:16Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1493">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. A more formal definition of active trails will be given below for two separate cases: When some variables are and aren't observed.

== Active Trails ==

As an example, in the graph below &lt;math&gt;A&lt;/math&gt; influences &lt;math&gt;B&lt;/math&gt; which in turn infleunces &lt;math&gt;C&lt;/math&gt;.

[[File:linear_path.png]]

This can also be seen in the conditional probability distribution:

 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|B)&lt;/math&gt;

However, things become interesting and counterintuitive when we observe a set of variables &lt;math&gt;Z&lt;/math&gt;, which is equivalent to conditioning the probability distribution &lt;math&gt;P&lt;/math&gt; on the variables &lt;math&gt;Z&lt;/math&gt;. The following sections will describe under which conditions a path of graphs is and isn't active.

=== Unconditioned active trails ===

A variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections:
* &lt;math&gt;X \to Y &lt;/math&gt;
* &lt;math&gt;X \leftarrow Y&lt;/math&gt;
* &lt;math&gt;X \to W \to Y&lt;/math&gt;
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt;
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt;
&lt;math&gt;X&lt;/math&gt; can '''not''' influence &lt;math&gt;Y&lt;/math&gt; only in the a [[V-structure]]:
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt;







==References==</text>
      <sha1>4dvrbhj6603qqbi3urfcogmq99mxmzn</sha1>
    </revision>
    <revision>
      <id>325</id>
      <parentid>324</parentid>
      <timestamp>2018-11-17T22:35:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1745">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. A more formal definition of active trails will be given below for two separate cases: When some variables are and aren't observed.

== Active Trails ==

As an example, in the graph below &lt;math&gt;A&lt;/math&gt; influences &lt;math&gt;B&lt;/math&gt; which in turn infleunces &lt;math&gt;C&lt;/math&gt;.

[[File:linear_path.png]]

This can also be seen in the conditional probability distribution:

 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|B)&lt;/math&gt;

However, things become interesting and counterintuitive when we observe a set of variables &lt;math&gt;Z&lt;/math&gt;, which is equivalent to conditioning the probability distribution &lt;math&gt;P&lt;/math&gt; on the variables &lt;math&gt;Z&lt;/math&gt;. The following sections will describe under which conditions a path of graphs is and isn't active.

=== Unconditioned active trails ===

A variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see the figure below the item list for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (e.g. &lt;math&gt;A, C&lt;/math&gt; in the figure below)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;C, A&lt;/math&gt;)
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)
&lt;math&gt;X&lt;/math&gt; can '''not''' influence &lt;math&gt;Y&lt;/math&gt; only in the a [[V-structure]]:
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)

[[File:general_pgm.png]]








==References==</text>
      <sha1>m7p6vfibkiyfkupcaszj71nap3xtxlo</sha1>
    </revision>
    <revision>
      <id>327</id>
      <parentid>325</parentid>
      <timestamp>2018-11-17T22:52:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3973">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. The conditions for having an active trail will be given below for two separate cases: When some variables are and aren't observed.

== Active Trails ==

As an example, in the graph on the right &lt;math&gt;A&lt;/math&gt; influences &lt;math&gt;B&lt;/math&gt; which in turn infleunces &lt;math&gt;C&lt;/math&gt;.

[[File:linear_path.png|frame|right]]

This can also be seen in the conditional probability distribution:

 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|B)&lt;/math&gt;

However, things become interesting and counterintuitive when we observe a set of variables &lt;math&gt;Z&lt;/math&gt;, which is equivalent to conditioning the probability distribution &lt;math&gt;P&lt;/math&gt; on the variables &lt;math&gt;Z&lt;/math&gt;. The following sections will describe under which conditions a path of graphs is and isn't active.

=== Active trails without observed variables ===

A variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (e.g. &lt;math&gt;A, C&lt;/math&gt; in the figure below)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;C, A&lt;/math&gt;)
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)
&lt;math&gt;X&lt;/math&gt; can '''not''' influence &lt;math&gt;Y&lt;/math&gt; only in the a [[V-structure]]:
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)

[[File:general_pgm.png]

All those cases are fairly intuitive. The only exception (i.e, V-structure) certainly makes sense: think of the example where &lt;math&gt;A, B, C&lt;/math&gt; are respectively class difficulty, student intelligence and student's grade.

==== Condition for active trail ====
A sequence of RVs &lt;math&gt;X_1, \dots, X_{k}&lt;/math&gt; form an active path if there is a path among them and this path contains no v-structures.

=== Active trails with observed variables ===
Assume that we have observed a set of RVs, &lt;math&gt;Z&lt;/math&gt; and that &lt;math&gt;W \in Z&lt;/math&gt;. Then, a variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (they are directly connected, &lt;math&gt;Z&lt;/math&gt; is irrelevant)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (ditto)
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)
Note that the pictured changed for V-structure: it does form an active trail now (see also [[V-structure]]).
The following cases do not form an active trail; that is, &lt;math&gt;X&lt;/math&gt; can not influence Y:
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)

For example to &lt;math&gt;X\to W \to Y&lt;/math&gt;, see figure on the right. The shaded node &lt;math&gt;B&lt;/math&gt; is observed. If it weren't observed, &lt;math&gt;A&lt;/math&gt; would influence &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;. But when we observe &lt;math&gt;B&lt;/math&gt;, we get all the possible information about the influence that &lt;math&gt;A&lt;/math&gt; can have on &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;, therefore seeing &lt;math&gt;A&lt;/math&gt; can not provide any further information about &lt;math&gt;C&lt;/math&gt;.
[[File:observed_linear_path.png|frame|right]]
==== Conditions for active trail====
When we have a set of observed RVs &lt;math&gt;Z&lt;/math&gt;, the conditions for &lt;math&gt;X_1,\dots,X_k&lt;/math&gt; a being an active trail are that:
* For any v-structure in the trail &lt;math&gt;X_{i-1}\to X_i \leftarrow X_{i+1}&lt;/math&gt;, we have that &lt;math&gt;X_i&lt;/math&gt; or one of its descendants is in &lt;math&gt;Z&lt;/math&gt;
* No other &lt;math&gt;X_i&lt;/math&gt; is in &lt;math&gt;Z&lt;/math&gt; (otherwise &lt;math&gt;X_i&lt;/math&gt; would block the path see figure on right with shaded B).



==References==</text>
      <sha1>ozjrab2dch18xnzd0stc9bqts9o606r</sha1>
    </revision>
    <revision>
      <id>339</id>
      <parentid>327</parentid>
      <timestamp>2018-11-22T13:52:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4210">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. The conditions for having an active trail will be given below for two separate cases: When some variables are and aren't observed.

== Directed Bayesian Networks ==

As an example, in the graph on the right &lt;math&gt;A&lt;/math&gt; influences &lt;math&gt;B&lt;/math&gt; which in turn infleunces &lt;math&gt;C&lt;/math&gt;.

[[File:linear_path.png|frame|right]]

This can also be seen in the conditional probability distribution:

 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|B)&lt;/math&gt;

However, things become interesting and counterintuitive when we observe a set of variables &lt;math&gt;Z&lt;/math&gt;, which is equivalent to conditioning the probability distribution &lt;math&gt;P&lt;/math&gt; on the variables &lt;math&gt;Z&lt;/math&gt;. The following sections will describe under which conditions a path of graphs is and isn't active.

=== Active trails without observed variables ===

A variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (e.g. &lt;math&gt;A, C&lt;/math&gt; in the figure below)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;C, A&lt;/math&gt;)
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)
&lt;math&gt;X&lt;/math&gt; can '''not''' influence &lt;math&gt;Y&lt;/math&gt; only in the a [[V-structure]]:
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)

[[File:general_pgm.png]

All those cases are fairly intuitive. The only exception (i.e, V-structure) certainly makes sense: think of the example where &lt;math&gt;A, B, C&lt;/math&gt; are respectively class difficulty, student intelligence and student's grade.

==== Condition for active trail ====
A sequence of RVs &lt;math&gt;X_1, \dots, X_{k}&lt;/math&gt; form an active path if there is a path among them and this path contains no v-structures.

=== Active trails with observed variables ===
Assume that we have observed a set of RVs, &lt;math&gt;Z&lt;/math&gt; and that &lt;math&gt;W \in Z&lt;/math&gt;. Then, a variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (they are directly connected, &lt;math&gt;Z&lt;/math&gt; is irrelevant)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (ditto)
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)
Note that the pictured changed for V-structure: it does form an active trail now (see also [[V-structure]]).
The following cases do not form an active trail; that is, &lt;math&gt;X&lt;/math&gt; can not influence Y:
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)

For example to &lt;math&gt;X\to W \to Y&lt;/math&gt;, see figure on the right. The shaded node &lt;math&gt;B&lt;/math&gt; is observed. If it weren't observed, &lt;math&gt;A&lt;/math&gt; would influence &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;. But when we observe &lt;math&gt;B&lt;/math&gt;, we get all the possible information about the influence that &lt;math&gt;A&lt;/math&gt; can have on &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;, therefore seeing &lt;math&gt;A&lt;/math&gt; can not provide any further information about &lt;math&gt;C&lt;/math&gt;.
[[File:observed_linear_path.png|frame|right]]
==== Conditions for active trail====
When we have a set of observed RVs &lt;math&gt;Z&lt;/math&gt;, the conditions for &lt;math&gt;X_1,\dots,X_k&lt;/math&gt; a being an active trail are that:
* For any v-structure in the trail &lt;math&gt;X_{i-1}\to X_i \leftarrow X_{i+1}&lt;/math&gt;, we have that &lt;math&gt;X_i&lt;/math&gt; or one of its descendants is in &lt;math&gt;Z&lt;/math&gt;
* No other &lt;math&gt;X_i&lt;/math&gt; is in &lt;math&gt;Z&lt;/math&gt; (otherwise &lt;math&gt;X_i&lt;/math&gt; would block the path see figure on right with shaded B).

== Undirected Graphs ==

Active trails are very easy to identify in an undirected graph: Given that we observed RVs &lt;math&gt;Z&lt;/math&gt;, a trail &lt;math&gt;X_1 - ... X_{k}&lt;/math&gt; is active if no &lt;math&gt;X_i&lt;/math&gt; is in &lt;math&gt;Z&lt;/math&gt;.


==References==</text>
      <sha1>2zqr6urur1lhqlkhzfa4p0vrqu4wjws</sha1>
    </revision>
    <revision>
      <id>341</id>
      <parentid>339</parentid>
      <timestamp>2019-01-10T11:17:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Undirected Graphs */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4238">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. The conditions for having an active trail will be given below for two separate cases: When some variables are and aren't observed.

== Directed Bayesian Networks ==

As an example, in the graph on the right &lt;math&gt;A&lt;/math&gt; influences &lt;math&gt;B&lt;/math&gt; which in turn infleunces &lt;math&gt;C&lt;/math&gt;.

[[File:linear_path.png|frame|right]]

This can also be seen in the conditional probability distribution:

 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|B)&lt;/math&gt;

However, things become interesting and counterintuitive when we observe a set of variables &lt;math&gt;Z&lt;/math&gt;, which is equivalent to conditioning the probability distribution &lt;math&gt;P&lt;/math&gt; on the variables &lt;math&gt;Z&lt;/math&gt;. The following sections will describe under which conditions a path of graphs is and isn't active.

=== Active trails without observed variables ===

A variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (e.g. &lt;math&gt;A, C&lt;/math&gt; in the figure below)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;C, A&lt;/math&gt;)
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)
&lt;math&gt;X&lt;/math&gt; can '''not''' influence &lt;math&gt;Y&lt;/math&gt; only in the a [[V-structure]]:
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)

[[File:general_pgm.png]

All those cases are fairly intuitive. The only exception (i.e, V-structure) certainly makes sense: think of the example where &lt;math&gt;A, B, C&lt;/math&gt; are respectively class difficulty, student intelligence and student's grade.

==== Condition for active trail ====
A sequence of RVs &lt;math&gt;X_1, \dots, X_{k}&lt;/math&gt; form an active path if there is a path among them and this path contains no v-structures.

=== Active trails with observed variables ===
Assume that we have observed a set of RVs, &lt;math&gt;Z&lt;/math&gt; and that &lt;math&gt;W \in Z&lt;/math&gt;. Then, a variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (they are directly connected, &lt;math&gt;Z&lt;/math&gt; is irrelevant)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (ditto)
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)
Note that the pictured changed for V-structure: it does form an active trail now (see also [[V-structure]]).
The following cases do not form an active trail; that is, &lt;math&gt;X&lt;/math&gt; can not influence Y:
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)

For example to &lt;math&gt;X\to W \to Y&lt;/math&gt;, see figure on the right. The shaded node &lt;math&gt;B&lt;/math&gt; is observed. If it weren't observed, &lt;math&gt;A&lt;/math&gt; would influence &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;. But when we observe &lt;math&gt;B&lt;/math&gt;, we get all the possible information about the influence that &lt;math&gt;A&lt;/math&gt; can have on &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;, therefore seeing &lt;math&gt;A&lt;/math&gt; can not provide any further information about &lt;math&gt;C&lt;/math&gt;.
[[File:observed_linear_path.png|frame|right]]
==== Conditions for active trail====
When we have a set of observed RVs &lt;math&gt;Z&lt;/math&gt;, the conditions for &lt;math&gt;X_1,\dots,X_k&lt;/math&gt; a being an active trail are that:
* For any v-structure in the trail &lt;math&gt;X_{i-1}\to X_i \leftarrow X_{i+1}&lt;/math&gt;, we have that &lt;math&gt;X_i&lt;/math&gt; or one of its descendants is in &lt;math&gt;Z&lt;/math&gt;
* No other &lt;math&gt;X_i&lt;/math&gt; is in &lt;math&gt;Z&lt;/math&gt; (otherwise &lt;math&gt;X_i&lt;/math&gt; would block the path see figure on right with shaded B).

== Undirected Graphs ==

Active trails are very easy to identify in an undirected graph (i.e., a [[Markov network]]): Given that we observed RVs &lt;math&gt;Z&lt;/math&gt;, a trail &lt;math&gt;X_1 - ... X_{k}&lt;/math&gt; is active if no &lt;math&gt;X_i&lt;/math&gt; is in &lt;math&gt;Z&lt;/math&gt;.

==References==</text>
      <sha1>2kx76a1b6o31mxvbd5r2q85sc83206x</sha1>
    </revision>
    <revision>
      <id>524</id>
      <parentid>341</parentid>
      <timestamp>2020-01-30T15:43:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4240">Within the context of a [[Probabilistic graphical model]], an active trail is one where the random variable &lt;math&gt;X_1&lt;/math&gt; can influence another random variable &lt;math&gt;X_k&lt;/math&gt; through possible intermediate random variables &lt;math&gt;X_2, \dots, X_{k-1}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. The conditions for having an active trail will be given below for two separate cases: When some variables are and aren't observed.

== Directed Bayesian Networks ==

As an example, in the graph on the right &lt;math&gt;A&lt;/math&gt; influences &lt;math&gt;B&lt;/math&gt; which in turn infleunces &lt;math&gt;C&lt;/math&gt;.

[[File:linear_path.png|frame|right]]

This can also be seen in the conditional probability distribution:

 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|B)&lt;/math&gt;

However, things become interesting and counterintuitive when we observe a set of variables &lt;math&gt;Z&lt;/math&gt;, which is equivalent to conditioning the probability distribution &lt;math&gt;P&lt;/math&gt; on the variables &lt;math&gt;Z&lt;/math&gt;. The following sections will describe under which conditions a path of graphs is and isn't active.

=== Active trails without observed variables ===

A variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (e.g. &lt;math&gt;A, C&lt;/math&gt; in the figure below)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;C, A&lt;/math&gt;)
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)
&lt;math&gt;X&lt;/math&gt; can '''not''' influence &lt;math&gt;Y&lt;/math&gt; only in the a [[V-structure]]:
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)

[[File:general_pgm.png]

All those cases are fairly intuitive. The only exception (i.e, V-structure) certainly makes sense: think of the example where &lt;math&gt;A, B, C&lt;/math&gt; are respectively class difficulty, student intelligence and student's grade.

==== Condition for active trail ====
A sequence of RVs &lt;math&gt;X_1, \dots, X_{k}&lt;/math&gt; form an active path if there is a path among them and this path contains no v-structures.

=== Active trails with observed variables ===
Assume that we have observed a set of RVs, &lt;math&gt;Z&lt;/math&gt; and that &lt;math&gt;W \in Z&lt;/math&gt;. Then, a variable &lt;math&gt;X&lt;/math&gt; can influence &lt;math&gt;Y&lt;/math&gt; in the following types of path connections (see figure below for examples to each):
* &lt;math&gt;X \to Y &lt;/math&gt; (they are directly connected, &lt;math&gt;Z&lt;/math&gt; is irrelevant)
* &lt;math&gt;X \leftarrow Y&lt;/math&gt; (ditto)
* &lt;math&gt;X \to W \leftarrow Y&lt;/math&gt; (e.g. A, C, B)
Note that the pictured changed for V-structure: it does form an active trail now (see also [[V-structure]]).
The following cases do not form an active trail; that is, &lt;math&gt;X&lt;/math&gt; can not influence Y:
* &lt;math&gt;X \to W \to Y&lt;/math&gt; (e.g. &lt;math&gt;A,C,E&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \leftarrow Y&lt;/math&gt; (e.g. &lt;math&gt;E, C, A&lt;/math&gt;)
* &lt;math&gt;X \leftarrow W \to Y&lt;/math&gt; (e.g. &lt;math&gt;C, B, D&lt;/math&gt;)

For example to &lt;math&gt;X\to W \to Y&lt;/math&gt;, see figure on the right. The shaded node &lt;math&gt;B&lt;/math&gt; is observed. If it weren't observed, &lt;math&gt;A&lt;/math&gt; would influence &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;. But when we observe &lt;math&gt;B&lt;/math&gt;, we get all the possible information about the influence that &lt;math&gt;A&lt;/math&gt; can have on &lt;math&gt;C&lt;/math&gt; through &lt;math&gt;B&lt;/math&gt;, therefore seeing &lt;math&gt;A&lt;/math&gt; can not provide any further information about &lt;math&gt;C&lt;/math&gt;.
[[File:observed_linear_path.png|frame|right]]
==== Conditions for active trail====
When we have a set of observed RVs &lt;math&gt;Z&lt;/math&gt;, the conditions for &lt;math&gt;X_1,\dots,X_k&lt;/math&gt; a being an active trail are that:
* For any v-structure in the trail &lt;math&gt;X_{i-1}\to X_i \leftarrow X_{i+1}&lt;/math&gt;, we have that &lt;math&gt;X_i&lt;/math&gt; or one of its descendants is in &lt;math&gt;Z&lt;/math&gt;
* No other &lt;math&gt;X_i&lt;/math&gt; is in &lt;math&gt;Z&lt;/math&gt; (otherwise &lt;math&gt;X_i&lt;/math&gt; would block the path see figure on right with shaded B).

== Undirected Graphs ==

Active trails are very easy to identify in an undirected graph (i.e., a [[Markov network]]): Given that we observed RVs &lt;math&gt;Z&lt;/math&gt;, a trail &lt;math&gt;X_1 - ... - X_{k}&lt;/math&gt; is active if no &lt;math&gt;X_i&lt;/math&gt; is in &lt;math&gt;Z&lt;/math&gt;.

==References==</text>
      <sha1>2o6099eqn73ysmpxk7l3psmrb6f1y6b</sha1>
    </revision>
  </page>
  <page>
    <title>File:Linear path.png</title>
    <ns>6</ns>
    <id>104</id>
    <revision>
      <id>321</id>
      <timestamp>2018-11-17T22:21:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-11-17T22:21:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>Linear_path.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/8/8d/Linear_path.png</src>
      <size>10571</size>
      <sha1base36>dk5hnsactikfk3t0g9efeqadt2zawe9</sha1base36>
      <rel>8/8d/Linear_path.png</rel>
    </upload>
  </page>
  <page>
    <title>File:General pgm.png</title>
    <ns>6</ns>
    <id>105</id>
    <revision>
      <id>323</id>
      <timestamp>2018-11-17T22:29:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-11-17T22:29:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>General_pgm.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/3/32/General_pgm.png</src>
      <size>12143</size>
      <sha1base36>afa6abjlhlc9aq4ot1ag0bn40yc4awm</sha1base36>
      <rel>3/32/General_pgm.png</rel>
    </upload>
  </page>
  <page>
    <title>File:Observed linear path.png</title>
    <ns>6</ns>
    <id>106</id>
    <revision>
      <id>326</id>
      <timestamp>2018-11-17T22:47:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="0" />
      <sha1>phoiac9h4m842xq45sp7s6u21eteeq1</sha1>
    </revision>
    <upload>
      <timestamp>2018-11-17T22:47:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment />
      <filename>Observed_linear_path.png</filename>
      <src>https://static.miraheze.org/engineeringmathwiki/3/31/Observed_linear_path.png</src>
      <size>10393</size>
      <sha1base36>bdnx9awydjmdor9q93ek477at8tjdk9</sha1base36>
      <rel>3/31/Observed_linear_path.png</rel>
    </upload>
  </page>
  <page>
    <title>D-separation</title>
    <ns>0</ns>
    <id>107</id>
    <revision>
      <id>328</id>
      <timestamp>2018-11-17T22:56:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "In the context of a probabilistic graphical model, the RVs &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; are ''d-separated'' in graph &lt;math&gt;G&lt;/math&gt; given &lt;math&gt;Z&lt;/math&gt; if there is no ..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="368">In the context of a probabilistic graphical model, the RVs &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; are ''d-separated'' in graph &lt;math&gt;G&lt;/math&gt; given &lt;math&gt;Z&lt;/math&gt; if there is no [[Active trail|active trail]] in &lt;math&gt;G&lt;/math&gt; between &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; given &lt;math&gt;Z&lt;/math&gt;. &lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;

==References ==</text>
      <sha1>35kswhwmuly9tjyg1gyal3eakbyx8qa</sha1>
    </revision>
    <revision>
      <id>329</id>
      <parentid>328</parentid>
      <timestamp>2018-11-17T23:00:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="598">In the context of a probabilistic graphical model, the RVs &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; are ''d-separated'' in graph &lt;math&gt;G&lt;/math&gt; given &lt;math&gt;Z&lt;/math&gt; if there is no [[Active trail|active trail]] in &lt;math&gt;G&lt;/math&gt; between &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; given &lt;math&gt;Z&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. This is denoted as &lt;math&gt;\text{d-sep}_G(X,Y|Z)&lt;/math&gt;.

'''Theorem:''' If a distribution &lt;math&gt;P&lt;/math&gt; factorizes over &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;\text{d-sep}_G(X,Y|Z)&lt;/math&gt;, then &lt;math&gt;P&lt;/math&gt; satisfies &lt;math&gt;(X\perp Y|Z)&lt;/math&gt;

==References ==</text>
      <sha1>59x99cubj9dm2h0q0nsrkildgix7p45</sha1>
    </revision>
    <revision>
      <id>330</id>
      <parentid>329</parentid>
      <timestamp>2018-11-17T23:02:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1057">In the context of a probabilistic graphical model, the RVs &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; are ''d-separated'' in graph &lt;math&gt;G&lt;/math&gt; given &lt;math&gt;Z&lt;/math&gt; if there is no [[Active trail|active trail]] in &lt;math&gt;G&lt;/math&gt; between &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; given &lt;math&gt;Z&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;. This is denoted as &lt;math&gt;\text{d-sep}_G(X,Y|Z)&lt;/math&gt;.

'''Theorem:''' If a distribution &lt;math&gt;P&lt;/math&gt; factorizes over &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;\text{d-sep}_G(X,Y|Z)&lt;/math&gt;, then &lt;math&gt;P&lt;/math&gt; satisfies &lt;math&gt;(X\perp Y|Z)&lt;/math&gt;. This theorem may be a bit difficult to understand because it sounds extremely obvious. What it does is simply related d-separatedness (a concept about active trails) to statistical independence.

Any node is d-separated from its non-descendants given its parents. Accoring to the theorem above, this implies that: If &lt;math&gt;P&lt;/math&gt; factorizes over &lt;math&gt;G&lt;/math&gt;, then in &lt;math&gt;P&lt;/math&gt;, any variable is independent of its non-descendants given its parents.

==References ==</text>
      <sha1>17mwidwzvxzwirlh3xl5u34zx0o2gn8</sha1>
    </revision>
  </page>
  <page>
    <title>I-map</title>
    <ns>0</ns>
    <id>108</id>
    <revision>
      <id>331</id>
      <timestamp>2018-11-18T16:00:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "An I-map (independency map) is a concept in [[Probabilistic graphical models]]. Although it is not a complicated concept, one needs to carefully analyze the semantics of an I-..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="544">An I-map (independency map) is a concept in [[Probabilistic graphical models]]. Although it is not a complicated concept, one needs to carefully analyze the semantics of an I-map otherwise it gets confusing.

An I-map is a ''function'' that takes as input a [[Graph|graph]] &lt;math&gt;G&lt;/math&gt; and outputs a set whose elements are (conditional) independences in a probability distribution &lt;math&gt;P&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;:

 &lt;math&gt;I(G) = \{(X\perp Y| Z): \text{d-sep}_G(X,Y|Z)\}&lt;/math&gt;

==References==</text>
      <sha1>6s067hx3zgdrvw4zaqbdw1yl4e3aw6e</sha1>
    </revision>
    <revision>
      <id>332</id>
      <parentid>331</parentid>
      <timestamp>2018-11-18T16:06:12Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1114">An I-map (independency map) is a concept in [[Probabilistic graphical models]]. Although it is not a complicated concept, one needs to carefully analyze the semantics of an I-map otherwise it gets confusing.

An I-map is a ''function'' that takes as input a [[Graph|graph]] &lt;math&gt;G&lt;/math&gt; and outputs a set whose elements are (conditional) independences in a probability distribution &lt;math&gt;P&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;:

 &lt;math&gt;I(G) = \{(X\perp Y| Z): \text{d-sep}_G(X,Y|Z)\}&lt;/math&gt;

'''Definition: Legal factorization'''. [[File:V_structure.png|frame|right]]I declare that a legal factorization of a probabilistic model is one that is faithful to the (conditional) independences in it. For example, the legal definition of the probability distribution distributed with the graph on the right is:
 &lt;math&gt;P(A,B,C) = P(A)P(B)P(C|A,B)&lt;/math&gt;
Note that the factorization that assumes no independences is always a legal distribution, i.e. the following is also a legal factorization for the distro on the right:
 &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|A,B)&lt;/math&gt;



==References==</text>
      <sha1>qmzdu2gwxjv2v2hgvlwo1qq8cmynkuc</sha1>
    </revision>
    <revision>
      <id>334</id>
      <parentid>332</parentid>
      <timestamp>2018-11-18T16:30:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2958">An I-map (independency map) is a concept in [[Probabilistic graphical models]]. Although it is not a complicated concept, one needs to carefully analyze the semantics of an I-map otherwise it gets confusing.

First of all, we say that &lt;math&gt;G&lt;/math&gt; (a ''graph'') is an I-map for ''probability distribution'' &lt;math&gt;P&lt;/math&gt;. So being an I-map in itself doesn't make sense: you can only  be an I-map ''of something''.

With these in mind, let us define a function &lt;math&gt;I(G)&lt;/math&gt; as the set of conditional independences warranted through the [[D-separation|d-separated]] nodes in graph &lt;math&gt;G&lt;/math&gt;, that is,
 &lt;math&gt;I(G) = \{(X\perp Y| Z): \text{d-sep}_G(X,Y|Z)\}&lt;/math&gt;

'''Definition (I-map)''': If a probability satisfies all the independences in &lt;math&gt;I(G)&lt;/math&gt;, then we say that &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

I-maps provide an alternative view to the intimate relationship between a graph &lt;math&gt;G&lt;/math&gt; and the probability distribution related to it, &lt;math&gt;P&lt;/math&gt;:
* First view puts [[Factorization|factorization]] in the center: &lt;math&gt;G&lt;/math&gt; allows &lt;math&gt;P&lt;/math&gt; to be represented
* Second view, based on I-map: Independences encoded &lt;math&gt;G&lt;/math&gt; hold in &lt;math&gt;P&lt;/math&gt;.

Clearly, if &lt;math&gt;P&lt;/math&gt; factorizes over a graph &lt;math&gt;G&lt;/math&gt;, we can read from the graph the independeces that must hold in &lt;math&gt;P&lt;/math&gt;.

== Alternative definitions == 
I believe that the following definition, legal factorization, is useful for understanding the intuition behind an I-map.
[[File:V_structure.png|frame|right]]
'''Definition: Legal factorization'''. I declare that a legal factorization of a probabilistic model is one that is faithful to the (conditional) independences in it. For example, the legal definition of the probability distribution distributed with the graph on the right is &lt;math&gt;P(A,B,C) = P(A)P(B)P(C|A,B)&lt;/math&gt;. Note that the factorization that assumes no independences is always a legal distribution, i.e. the following is also a legal factorization for the distro on the right: &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|A,B)&lt;/math&gt;.

'''I-map: Alternative definition''': An I-map of &lt;math&gt;G&lt;/math&gt;, &lt;math&gt;I(G)&lt;/math&gt;, provides a legal factorization for &lt;math&gt;P&lt;/math&gt;.

The proposition above is a definition that's easier to remember for me. It simply says that if &lt;math&gt;I(G)&lt;/math&gt; is an I-map for distro &lt;math&gt;P&lt;/math&gt;, then any independence included in &lt;math&gt;I(G)&lt;/math&gt; must hold for the distro &lt;math&gt;P&lt;/math&gt;. Clearly, &lt;math&gt;I(G)&lt;/math&gt; does not have to contain ''all'' independences -- it only needs to provide a legal factorization (note that the distro that assumes no independences always provides a legal factorization -- see above).

== Theorems ==

'''Theorem 1''' (Factorization &lt;math&gt;\implies&lt;/math&gt; Independence): If &lt;math&gt;P&lt;/math&gt; factorizes over graph &lt;math&gt;G&lt;/math&gt;, then &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;

==References==</text>
      <sha1>alt315xqoa8klkxmy4bwrf45jpggkue</sha1>
    </revision>
    <revision>
      <id>335</id>
      <parentid>334</parentid>
      <timestamp>2018-11-18T16:30:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2957">An I-map (independency map) is a concept in [[Probabilistic graphical models]]. Although it is not a complicated concept, one needs to carefully analyze the semantics of an I-map otherwise it gets confusing.

First of all, we say that &lt;math&gt;G&lt;/math&gt; (a ''graph'') is an I-map for ''probability distribution'' &lt;math&gt;P&lt;/math&gt;. So being an I-map in itself doesn't make sense: you can only  be an I-map ''of something''. With these in mind, let us define a function &lt;math&gt;I(G)&lt;/math&gt; as the set of conditional independences warranted through the [[D-separation|d-separated]] nodes in graph &lt;math&gt;G&lt;/math&gt;, that is,
 &lt;math&gt;I(G) = \{(X\perp Y| Z): \text{d-sep}_G(X,Y|Z)\}&lt;/math&gt;

'''Definition (I-map)''': If a probability satisfies all the independences in &lt;math&gt;I(G)&lt;/math&gt;, then we say that &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

I-maps provide an alternative view to the intimate relationship between a graph &lt;math&gt;G&lt;/math&gt; and the probability distribution related to it, &lt;math&gt;P&lt;/math&gt;:
* First view puts [[Factorization|factorization]] in the center: &lt;math&gt;G&lt;/math&gt; allows &lt;math&gt;P&lt;/math&gt; to be represented
* Second view, based on I-map: Independences encoded &lt;math&gt;G&lt;/math&gt; hold in &lt;math&gt;P&lt;/math&gt;.

Clearly, if &lt;math&gt;P&lt;/math&gt; factorizes over a graph &lt;math&gt;G&lt;/math&gt;, we can read from the graph the independeces that must hold in &lt;math&gt;P&lt;/math&gt;.

== Alternative definitions == 
I believe that the following definition, legal factorization, is useful for understanding the intuition behind an I-map.
[[File:V_structure.png|frame|right]]
'''Definition: Legal factorization'''. I declare that a legal factorization of a probabilistic model is one that is faithful to the (conditional) independences in it. For example, the legal definition of the probability distribution distributed with the graph on the right is &lt;math&gt;P(A,B,C) = P(A)P(B)P(C|A,B)&lt;/math&gt;. Note that the factorization that assumes no independences is always a legal distribution, i.e. the following is also a legal factorization for the distro on the right: &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|A,B)&lt;/math&gt;.

'''I-map: Alternative definition''': An I-map of &lt;math&gt;G&lt;/math&gt;, &lt;math&gt;I(G)&lt;/math&gt;, provides a legal factorization for &lt;math&gt;P&lt;/math&gt;.

The proposition above is a definition that's easier to remember for me. It simply says that if &lt;math&gt;I(G)&lt;/math&gt; is an I-map for distro &lt;math&gt;P&lt;/math&gt;, then any independence included in &lt;math&gt;I(G)&lt;/math&gt; must hold for the distro &lt;math&gt;P&lt;/math&gt;. Clearly, &lt;math&gt;I(G)&lt;/math&gt; does not have to contain ''all'' independences -- it only needs to provide a legal factorization (note that the distro that assumes no independences always provides a legal factorization -- see above).

== Theorems ==

'''Theorem 1''' (Factorization &lt;math&gt;\implies&lt;/math&gt; Independence): If &lt;math&gt;P&lt;/math&gt; factorizes over graph &lt;math&gt;G&lt;/math&gt;, then &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;

==References==</text>
      <sha1>mvw8war79nu1shmin7afqv4hz0p9dsa</sha1>
    </revision>
    <revision>
      <id>336</id>
      <parentid>335</parentid>
      <timestamp>2018-11-18T16:32:09Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2989">An I-map (independency map) is a concept in [[Probabilistic graphical models]]. Although it is not a complicated concept, one needs to carefully analyze the semantics of an I-map otherwise it gets confusing.

First of all, we say that &lt;math&gt;G&lt;/math&gt; (a ''graph'') is an I-map for ''probability distribution'' &lt;math&gt;P&lt;/math&gt;. So being an I-map in itself doesn't make sense: you can only  be an I-map ''of something''. With these in mind, let us define a function &lt;math&gt;I(G)&lt;/math&gt; as the set of conditional independences warranted through the [[D-separation|d-separated]] nodes in graph &lt;math&gt;G&lt;/math&gt;, that is,
 &lt;math&gt;I(G) = \{(X\perp Y| Z): \text{d-sep}_G(X,Y|Z)\}&lt;/math&gt;

'''Definition (I-map)''': If a probability distribution &lt;math&gt;P&lt;/math&gt; satisfies all the independences in &lt;math&gt;I(G)&lt;/math&gt;, then we say that &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

I-maps provide an alternative view to the intimate relationship between a graph &lt;math&gt;G&lt;/math&gt; and the probability distribution related to it, &lt;math&gt;P&lt;/math&gt;:
* First view puts [[Factorization|factorization]] in the center: &lt;math&gt;G&lt;/math&gt; allows &lt;math&gt;P&lt;/math&gt; to be represented
* Second view, based on I-map: Independences encoded &lt;math&gt;G&lt;/math&gt; hold in &lt;math&gt;P&lt;/math&gt;.

Clearly, if &lt;math&gt;P&lt;/math&gt; factorizes over a graph &lt;math&gt;G&lt;/math&gt;, we can read from the graph the independeces that must hold in &lt;math&gt;P&lt;/math&gt;.

== Alternative definitions == 
I believe that the following definition, legal factorization, is useful for understanding the intuition behind an I-map.
[[File:V_structure.png|frame|right]]
'''Definition: Legal factorization'''. I declare that a legal factorization of a probabilistic model is one that is faithful to the (conditional) independences in it. For example, the legal definition of the probability distribution distributed with the graph on the right is &lt;math&gt;P(A,B,C) = P(A)P(B)P(C|A,B)&lt;/math&gt;. Note that the factorization that assumes no independences is always a legal distribution, i.e. the following is also a legal factorization for the distro on the right: &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|A,B)&lt;/math&gt;.

'''I-map: Alternative definition''': &lt;math&gt;G&lt;/math&gt; is an I-map of &lt;math&gt;P&lt;/math&gt; if it provides a legal factorization for &lt;math&gt;P&lt;/math&gt;.

The proposition above is a definition that's easier to remember for me. It simply says that if &lt;math&gt;I(G)&lt;/math&gt; is an I-map for distro &lt;math&gt;P&lt;/math&gt;, then any independence included in &lt;math&gt;I(G)&lt;/math&gt; must hold for the distro &lt;math&gt;P&lt;/math&gt;. Clearly, &lt;math&gt;I(G)&lt;/math&gt; does not have to contain ''all'' independences -- it only needs to provide a legal factorization (note that the distro that assumes no independences always provides a legal factorization -- see above).

== Theorems ==

'''Theorem 1''' (Factorization &lt;math&gt;\implies&lt;/math&gt; Independence): If &lt;math&gt;P&lt;/math&gt; factorizes over graph &lt;math&gt;G&lt;/math&gt;, then &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;

==References==</text>
      <sha1>5flea8hipjn9qhnu7ezmnu551tzme61</sha1>
    </revision>
    <revision>
      <id>337</id>
      <parentid>336</parentid>
      <timestamp>2018-11-18T16:33:24Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Theorems */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3161">An I-map (independency map) is a concept in [[Probabilistic graphical models]]. Although it is not a complicated concept, one needs to carefully analyze the semantics of an I-map otherwise it gets confusing.

First of all, we say that &lt;math&gt;G&lt;/math&gt; (a ''graph'') is an I-map for ''probability distribution'' &lt;math&gt;P&lt;/math&gt;. So being an I-map in itself doesn't make sense: you can only  be an I-map ''of something''. With these in mind, let us define a function &lt;math&gt;I(G)&lt;/math&gt; as the set of conditional independences warranted through the [[D-separation|d-separated]] nodes in graph &lt;math&gt;G&lt;/math&gt;, that is,
 &lt;math&gt;I(G) = \{(X\perp Y| Z): \text{d-sep}_G(X,Y|Z)\}&lt;/math&gt;

'''Definition (I-map)''': If a probability distribution &lt;math&gt;P&lt;/math&gt; satisfies all the independences in &lt;math&gt;I(G)&lt;/math&gt;, then we say that &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

I-maps provide an alternative view to the intimate relationship between a graph &lt;math&gt;G&lt;/math&gt; and the probability distribution related to it, &lt;math&gt;P&lt;/math&gt;:
* First view puts [[Factorization|factorization]] in the center: &lt;math&gt;G&lt;/math&gt; allows &lt;math&gt;P&lt;/math&gt; to be represented
* Second view, based on I-map: Independences encoded &lt;math&gt;G&lt;/math&gt; hold in &lt;math&gt;P&lt;/math&gt;.

Clearly, if &lt;math&gt;P&lt;/math&gt; factorizes over a graph &lt;math&gt;G&lt;/math&gt;, we can read from the graph the independeces that must hold in &lt;math&gt;P&lt;/math&gt;.

== Alternative definitions == 
I believe that the following definition, legal factorization, is useful for understanding the intuition behind an I-map.
[[File:V_structure.png|frame|right]]
'''Definition: Legal factorization'''. I declare that a legal factorization of a probabilistic model is one that is faithful to the (conditional) independences in it. For example, the legal definition of the probability distribution distributed with the graph on the right is &lt;math&gt;P(A,B,C) = P(A)P(B)P(C|A,B)&lt;/math&gt;. Note that the factorization that assumes no independences is always a legal distribution, i.e. the following is also a legal factorization for the distro on the right: &lt;math&gt;P(A,B,C) = P(A)P(B|A)P(C|A,B)&lt;/math&gt;.

'''I-map: Alternative definition''': &lt;math&gt;G&lt;/math&gt; is an I-map of &lt;math&gt;P&lt;/math&gt; if it provides a legal factorization for &lt;math&gt;P&lt;/math&gt;.

The proposition above is a definition that's easier to remember for me. It simply says that if &lt;math&gt;I(G)&lt;/math&gt; is an I-map for distro &lt;math&gt;P&lt;/math&gt;, then any independence included in &lt;math&gt;I(G)&lt;/math&gt; must hold for the distro &lt;math&gt;P&lt;/math&gt;. Clearly, &lt;math&gt;I(G)&lt;/math&gt; does not have to contain ''all'' independences -- it only needs to provide a legal factorization (note that the distro that assumes no independences always provides a legal factorization -- see above).

== Theorems ==

'''Theorem 1''' (Factorization &lt;math&gt;\implies&lt;/math&gt; Independence): If &lt;math&gt;P&lt;/math&gt; factorizes over graph &lt;math&gt;G&lt;/math&gt;, then &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;.

'''Theorem 2''' (Independence &lt;math&gt;\implies&lt;/math&gt; Factorization): If &lt;math&gt;G&lt;/math&gt; is an I-map for &lt;math&gt;P&lt;/math&gt;, then &lt;math&gt;P&lt;/math&gt; factorizes over &lt;math&gt;G&lt;/math&gt;.

==References==</text>
      <sha1>hxjpy33at23y7ogi7q0o67850d64dhg</sha1>
    </revision>
  </page>
  <page>
    <title>Factorization (Probability distribution)</title>
    <ns>0</ns>
    <id>109</id>
    <revision>
      <id>333</id>
      <timestamp>2018-11-18T16:22:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The factorization of a probability distribution &lt;math&gt;P&lt;/math&gt; is a set of probability distributions whose multiplication gives &lt;math&gt;P&lt;/math&gt;.   In general, the factorization..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="542">The factorization of a probability distribution &lt;math&gt;P&lt;/math&gt; is a set of probability distributions whose multiplication gives &lt;math&gt;P&lt;/math&gt;. 

In general, the factorization of a 2-RV probability distro is:
&lt;math&gt;P(X,Y) = P(Y|X)P(X) = P(X|Y)P(Y)&lt;/math&gt;

However, if &lt;math&gt;X,Y&lt;/math&gt; are independent, then the following is ''also'' a factorization:
&lt;math&gt;P(X,Y)=P(X)P(Y)&lt;/math&gt;.

Therefore, if &lt;math&gt;X,Y&lt;/math&gt; are independent, then &lt;math&gt;P(X,Y)&lt;/math&gt; has three factorizations: &lt;math&gt;P(X|Y)P(Y), P(Y|X)P(X)&lt;/math&gt; and &lt;math&gt;P(X)P(Y)&lt;/math&gt;.</text>
      <sha1>9o1ynfssdavovptq0n48jvjlg6lqtnl</sha1>
    </revision>
  </page>
  <page>
    <title>Graph</title>
    <ns>0</ns>
    <id>110</id>
    <revision>
      <id>347</id>
      <timestamp>2019-01-14T13:05:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A graph is a data structure &lt;math&gt;K&lt;/math&gt; consisting of nodes &lt;math&gt;\mathcal X = X_1, \dots, X_n&lt;/math&gt; and edges between the nodes, &lt;math&gt;K=(\mathcal X, \mathcal E)&lt;/math&gt;...."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="634">A graph is a data structure &lt;math&gt;K&lt;/math&gt; consisting of nodes &lt;math&gt;\mathcal X = X_1, \dots, X_n&lt;/math&gt; and edges between the nodes, &lt;math&gt;K=(\mathcal X, \mathcal E)&lt;/math&gt;. A pair of nodes &lt;math&gt;X_i&lt;/math&gt; and &lt;math&gt;X_j&lt;/math&gt; can be connected by a 'directed' edge &lt;math&gt;\to&lt;/math&gt; or an undirected edge &lt;math&gt;\----&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

== Types of Graphs ==
There are two often used classes of graphs, namely (i) 'directed acyclic graph' (DAG), which is also often called a 'directed Bayesian network' and (ii) 'undirected graph', which is also often called a 'Markov network.'</text>
      <sha1>hhcvdz2u3511kuunnd5p7qhgksawv0d</sha1>
    </revision>
    <revision>
      <id>348</id>
      <parentid>347</parentid>
      <timestamp>2019-01-14T13:07:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="641">A graph is a data structure &lt;math&gt;K&lt;/math&gt; consisting of nodes &lt;math&gt;\mathcal X = X_1, \dots, X_n&lt;/math&gt; and edges between the nodes, &lt;math&gt;K=(\mathcal X, \mathcal E)&lt;/math&gt;. A pair of nodes &lt;math&gt;X_i&lt;/math&gt; and &lt;math&gt;X_j&lt;/math&gt; can be connected by a 'directed' edge &lt;math&gt;\to&lt;/math&gt; or an undirected edge &lt;math&gt;\text{\----}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

== Types of Graphs ==
There are two often used classes of graphs, namely (i) 'directed acyclic graph' (DAG), which is also often called a 'directed Bayesian network' and (ii) 'undirected graph', which is also often called a 'Markov network.'</text>
      <sha1>n5vv00yr7704t0gw1obbk2fkuzxx8lz</sha1>
    </revision>
    <revision>
      <id>349</id>
      <parentid>348</parentid>
      <timestamp>2019-01-14T13:08:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="652">A graph is a data structure &lt;math&gt;K&lt;/math&gt; consisting of nodes &lt;math&gt;\mathcal X = X_1, \dots, X_n&lt;/math&gt; and edges between the nodes, &lt;math&gt;K=(\mathcal X, \mathcal E)&lt;/math&gt;. A pair of nodes &lt;math&gt;X_i&lt;/math&gt; and &lt;math&gt;X_j&lt;/math&gt; can be connected by a 'directed' edge &lt;math&gt;\to&lt;/math&gt; or an undirected edge &lt;math&gt;\text{\----}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

== Types of Graphs ==
There are two often used classes of graphs, namely (i) 'directed acyclic graph' (DAG), which is also often called a 'directed Bayesian network' (DBN) and (ii) 'undirected graph', which is also often called a 'Markov network' (MN).</text>
      <sha1>iuuny8qmd6f7fdy7fzn6mgms1g2odi8</sha1>
    </revision>
    <revision>
      <id>350</id>
      <parentid>349</parentid>
      <timestamp>2019-01-15T03:29:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Minor edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="633">A graph is a data structure &lt;math&gt;K&lt;/math&gt; consisting of nodes &lt;math&gt;\mathcal X = X_1, \dots, X_n&lt;/math&gt; and edges &lt;math&gt;K=(\mathcal X, \mathcal E)&lt;/math&gt;. A pair of nodes &lt;math&gt;X_i&lt;/math&gt; and &lt;math&gt;X_j&lt;/math&gt; can be connected by a 'directed' edge &lt;math&gt;\to&lt;/math&gt; or an undirected edge &lt;math&gt;\text{\----}&lt;/math&gt;&lt;ref&gt;D. Koller, Probabilistic Graphical Models lecture notes&lt;/ref&gt;.

== Types of Graphs ==
There are two often used classes of graphs, namely (i) 'directed acyclic graph' (DAG), which is also often called a 'directed Bayesian network' (DBN) and (ii) 'undirected graph', which is also often called a 'Markov network' (MN).</text>
      <sha1>azow42bt32v7ymknhgae0zd2qmj6u35</sha1>
    </revision>
  </page>
  <page>
    <title>Trigonometric Identities</title>
    <ns>0</ns>
    <id>111</id>
    <revision>
      <id>352</id>
      <timestamp>2019-02-04T21:29:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "&lt;math&gt;\sin(a\pm b) = \sin(a)\cos(b)\pm\cos(a)\sin(b)&lt;/math&gt;  &lt;math&gt;\cos(a\pm b) = \cos(a)\cos(b)\mp\sin(a)\sin(b)&lt;/math&gt;"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="120">&lt;math&gt;\sin(a\pm b) = \sin(a)\cos(b)\pm\cos(a)\sin(b)&lt;/math&gt;

&lt;math&gt;\cos(a\pm b) = \cos(a)\cos(b)\mp\sin(a)\sin(b)&lt;/math&gt;</text>
      <sha1>se6olgwenmmegnt0s65t1ojwhsv0pn5</sha1>
    </revision>
  </page>
  <page>
    <title>Stochastic Process</title>
    <ns>0</ns>
    <id>112</id>
    <revision>
      <id>355</id>
      <timestamp>2019-04-04T18:45:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;m..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="854">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;&lt;ref&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

==References==</text>
      <sha1>qre6bwl9y85q0bdu1w1pkmb6qzmzlc4</sha1>
    </revision>
    <revision>
      <id>356</id>
      <parentid>355</parentid>
      <timestamp>2019-04-04T19:11:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1750">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

If &lt;math&gt;R&lt;/math&gt; is the real axis, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a continuous-time process. If &lt;math&gt;R&lt;/math&gt; is the set of integers, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a discrete-time process. A discrete-time process is, thus, a sequence of RVs. Such a sequence is typically denoted by &lt;math&gt;\mathbf x_n&lt;/math&gt; or &lt;math&gt;\mathbf x[n]&lt;/math&gt;.

The symbol &lt;math&gt;\mathbf x(t)&lt;/math&gt; has multiple interpretations:

* It's a family of functions &lt;math&gt;\mathbf x(t,\zeta)&lt;/math&gt;. In this interpretation, &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are variables.
* It's a single time function (or a ''sample'' of the given process). In this case, &lt;math&gt;t&lt;/math&gt; is a variable and &lt;math&gt;\zeta&lt;/math&gt; is fixed.
* If &lt;math&gt;t&lt;/math&gt; is fixed and &lt;math&gt;\zeta&lt;/math&gt; is variable, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is an RV.
* If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are fixed, then &lt;math&gt;x(t)&lt;/math&gt; is a ''number''.

==References==</text>
      <sha1>ma1x0sd8wnx9d6ztrvyow25y7586ge8</sha1>
    </revision>
    <revision>
      <id>493</id>
      <parentid>356</parentid>
      <timestamp>2019-08-04T14:52:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2507">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

If &lt;math&gt;R&lt;/math&gt; is the real axis, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a continuous-time process. If &lt;math&gt;R&lt;/math&gt; is the set of integers, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a discrete-time process. A discrete-time process is, thus, a sequence of RVs. Such a sequence is typically denoted by &lt;math&gt;\mathbf x_n&lt;/math&gt; or &lt;math&gt;\mathbf x[n]&lt;/math&gt;.

The symbol &lt;math&gt;\mathbf x(t)&lt;/math&gt; has multiple interpretations:

* It's a family of functions &lt;math&gt;\mathbf x(t,\zeta)&lt;/math&gt;. In this interpretation, &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are variables.
* It's a single time function (or a ''sample'' of the given process). In this case, &lt;math&gt;t&lt;/math&gt; is a variable and &lt;math&gt;\zeta&lt;/math&gt; is fixed.
* If &lt;math&gt;t&lt;/math&gt; is fixed and &lt;math&gt;\zeta&lt;/math&gt; is variable, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is an RV.
* If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are fixed, then &lt;math&gt;x(t)&lt;/math&gt; is a ''number''.

=== Discrete stochastic processes ===
A probably even simpler definition can be given as below. First of all, note that random processes are infinite. Consider the coin tossing experiment. Then, there are an infinite number of ''infinite long'' possible experimental outcomes, 

&lt;math&gt;\mathcal S = \{(H, H, T, \dots), (H, T, H, \dots), (T, H, H, \dots), (T, T, T, \dots), \dots\}&lt;/math&gt;.

Recall that a [[random variable]] is simply a mapping between all possible experimental outcomes and a numerical value. Similarly, a random process is a mapping that maps every possible outcome in &lt;math&gt;\mathcal S&lt;/math&gt; to a series of numbers. That is, 

&lt;math&gt;\mathcal S_{\mathcal X} = {(1,1,0, \dots), (1,0,1, \dots), (0,1,1, \dots), (0,0,0, \dots}, \dots&lt;/math&gt;


==References==</text>
      <sha1>s7t01b7hktuhtrvwiig782axfid2oqb</sha1>
    </revision>
    <revision>
      <id>494</id>
      <parentid>493</parentid>
      <timestamp>2019-08-04T14:54:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2803">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

If &lt;math&gt;R&lt;/math&gt; is the real axis, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a continuous-time process. If &lt;math&gt;R&lt;/math&gt; is the set of integers, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a discrete-time process. A discrete-time process is, thus, a sequence of RVs. Such a sequence is typically denoted by &lt;math&gt;\mathbf x_n&lt;/math&gt; or &lt;math&gt;\mathbf x[n]&lt;/math&gt;.

The symbol &lt;math&gt;\mathbf x(t)&lt;/math&gt; has multiple interpretations:

* It's a family of functions &lt;math&gt;\mathbf x(t,\zeta)&lt;/math&gt;. In this interpretation, &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are variables.
* It's a single time function (or a ''sample'' of the given process). In this case, &lt;math&gt;t&lt;/math&gt; is a variable and &lt;math&gt;\zeta&lt;/math&gt; is fixed.
* If &lt;math&gt;t&lt;/math&gt; is fixed and &lt;math&gt;\zeta&lt;/math&gt; is variable, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is an RV.
* If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are fixed, then &lt;math&gt;x(t)&lt;/math&gt; is a ''number''.

=== Discrete stochastic processes ===
A probably even simpler definition can be given as below. First of all, note that random processes are infinite. Consider the coin tossing experiment. Then, there are an infinite number of ''infinite long'' possible experimental outcomes, 

&lt;math&gt;\mathcal S = \{(H, H, T, \dots), (H, T, H, \dots), (T, H, H, \dots), (T, T, T, \dots), \dots\}&lt;/math&gt;.

Recall that a [[random variable]] is simply a mapping between all possible experimental outcomes and a numerical value. Similarly, a random process is a mapping that maps every possible outcome in &lt;math&gt;\mathcal S&lt;/math&gt; to a series of numbers. That is, 

&lt;math&gt;\mathcal S_{\mathcal X} = {(1,1,0, \dots), (1,0,1, \dots), (0,1,1, \dots), (0,0,0, \dots}), \dots&lt;/math&gt;.

The random process does nothing but mapping each sequence in &lt;math&gt;\mathcal S&lt;/math&gt; to a sequence in &lt;math&gt;\mathcal S&lt;/math&gt;. [[Random variable|Again]], there is nothing random about this procedure, it's a pretty deterministic mapping and the name ''random process'' is somewhat unfortunate.


==References==</text>
      <sha1>qzlpzwlutotj24g9kmp4u3oli1d11mr</sha1>
    </revision>
    <revision>
      <id>495</id>
      <parentid>494</parentid>
      <timestamp>2019-08-04T15:05:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3057">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

If &lt;math&gt;R&lt;/math&gt; is the real axis, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a continuous-time process. If &lt;math&gt;R&lt;/math&gt; is the set of integers, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a discrete-time process. A discrete-time process is, thus, a sequence of RVs. Such a sequence is typically denoted by &lt;math&gt;\mathbf x_n&lt;/math&gt; or &lt;math&gt;\mathbf x[n]&lt;/math&gt;.

The symbol &lt;math&gt;\mathbf x(t)&lt;/math&gt; has multiple interpretations:

* It's a family of functions &lt;math&gt;\mathbf x(t,\zeta)&lt;/math&gt;. In this interpretation, &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are variables.
* It's a single time function (or a ''sample'' of the given process). In this case, &lt;math&gt;t&lt;/math&gt; is a variable and &lt;math&gt;\zeta&lt;/math&gt; is fixed.
* If &lt;math&gt;t&lt;/math&gt; is fixed and &lt;math&gt;\zeta&lt;/math&gt; is variable, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is an RV.
* If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are fixed, then &lt;math&gt;x(t)&lt;/math&gt; is a ''number''.

=== Discrete stochastic processes ===
A probably even simpler definition can be given as below. First of all, note that random processes are infinite. Consider the coin tossing experiment. Then, there are an infinite number of ''infinite long'' possible experimental outcomes, 

&lt;math&gt;\mathcal S = \{(H, H, T, \dots), (H, T, H, \dots), (T, H, H, \dots), (T, T, T, \dots), \dots\}&lt;/math&gt;.

Recall that a [[random variable]] is simply a mapping between all possible experimental outcomes and a numerical value. Similarly, a random process is a mapping that maps every possible outcome in &lt;math&gt;\mathcal S&lt;/math&gt; to a series of numbers. That is, 

&lt;math&gt;\mathcal S_{\mathcal X} = {(1,1,0, \dots), (1,0,1, \dots), (0,1,1, \dots), (0,0,0, \dots}), \dots&lt;/math&gt;.

The random process does nothing but mapping each sequence in &lt;math&gt;\mathcal S&lt;/math&gt; to a sequence in &lt;math&gt;\mathcal S&lt;/math&gt;. [[Random variable|Again]], there is nothing random about this procedure, it's a pretty deterministic mapping and the name ''random process'' is somewhat unfortunate.

== Types of random processes==

=== According to time resolution and output type ===
* Discrete time discrete valued (DTDV)
* Discrete time continuous valued (DTCV)
* Continuous time discrete   valued (CTDV)
* Continuous time continuous valued (CTCV)




==References==</text>
      <sha1>l9djxu0qjtjkjrwmp9nlx0v7rnqnrkx</sha1>
    </revision>
    <revision>
      <id>496</id>
      <parentid>495</parentid>
      <timestamp>2019-08-04T15:06:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3319">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

If &lt;math&gt;R&lt;/math&gt; is the real axis, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a continuous-time process. If &lt;math&gt;R&lt;/math&gt; is the set of integers, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a discrete-time process. A discrete-time process is, thus, a sequence of RVs. Such a sequence is typically denoted by &lt;math&gt;\mathbf x_n&lt;/math&gt; or &lt;math&gt;\mathbf x[n]&lt;/math&gt;.

The symbol &lt;math&gt;\mathbf x(t)&lt;/math&gt; has multiple interpretations:

* It's a family of functions &lt;math&gt;\mathbf x(t,\zeta)&lt;/math&gt;. In this interpretation, &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are variables.
* It's a single time function (or a ''sample'' of the given process). In this case, &lt;math&gt;t&lt;/math&gt; is a variable and &lt;math&gt;\zeta&lt;/math&gt; is fixed.
* If &lt;math&gt;t&lt;/math&gt; is fixed and &lt;math&gt;\zeta&lt;/math&gt; is variable, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is an RV.
* If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are fixed, then &lt;math&gt;x(t)&lt;/math&gt; is a ''number''.

=== Discrete stochastic processes ===
A probably even simpler definition can be given as below. First of all, note that random processes are infinite. Consider the coin tossing experiment. Then, there are an infinite number of ''infinite long'' possible experimental outcomes, 

&lt;math&gt;\mathcal S = \{(H, H, T, \dots), (H, T, H, \dots), (T, H, H, \dots), (T, T, T, \dots), \dots\}&lt;/math&gt;.

Recall that a [[random variable]] is simply a mapping between all possible experimental outcomes and a numerical value. Similarly, a random process is a mapping that maps every possible outcome in &lt;math&gt;\mathcal S&lt;/math&gt; to a series of numbers. That is, 

&lt;math&gt;\mathcal S_{\mathcal X} = {(1,1,0, \dots), (1,0,1, \dots), (0,1,1, \dots), (0,0,0, \dots}), \dots&lt;/math&gt;.

The random process does nothing but mapping each sequence in &lt;math&gt;\mathcal S&lt;/math&gt; to a sequence in &lt;math&gt;\mathcal S&lt;/math&gt;. [[Random variable|Again]], there is nothing random about this procedure, it's a pretty deterministic mapping and the name ''random process'' is somewhat unfortunate.

== Types of random processes==

=== According to time resolution and output type ===
* Discrete time discrete valued (DTDV)
* Discrete time continuous valued (DTCV)
* Continuous time discrete   valued (CTDV)
* Continuous time continuous valued (CTCV)

=== According to time dependence ===

Random processes can be categorized from simplest to most complicated (in terms of time dependence) as below
* IID processes (independent and identically distributed)
* Stationary processes
* Wide-sense stationary processes



==References==</text>
      <sha1>thfbg06uvxiibnc8oca0slubqmw9t7y</sha1>
    </revision>
    <revision>
      <id>497</id>
      <parentid>496</parentid>
      <timestamp>2019-08-04T15:07:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3421">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

If &lt;math&gt;R&lt;/math&gt; is the real axis, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a continuous-time process. If &lt;math&gt;R&lt;/math&gt; is the set of integers, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a discrete-time process. A discrete-time process is, thus, a sequence of RVs. Such a sequence is typically denoted by &lt;math&gt;\mathbf x_n&lt;/math&gt; or &lt;math&gt;\mathbf x[n]&lt;/math&gt;.

The symbol &lt;math&gt;\mathbf x(t)&lt;/math&gt; has multiple interpretations:

* It's a family of functions &lt;math&gt;\mathbf x(t,\zeta)&lt;/math&gt;. In this interpretation, &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are variables.
* It's a single time function (or a ''sample'' of the given process). In this case, &lt;math&gt;t&lt;/math&gt; is a variable and &lt;math&gt;\zeta&lt;/math&gt; is fixed.
* If &lt;math&gt;t&lt;/math&gt; is fixed and &lt;math&gt;\zeta&lt;/math&gt; is variable, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is an RV.
* If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are fixed, then &lt;math&gt;x(t)&lt;/math&gt; is a ''number''.

=== Discrete stochastic processes ===
A probably even simpler definition can be given as below. First of all, note that random processes are infinite. Consider the coin tossing experiment. Then, there are an infinite number of ''infinite long'' possible experimental outcomes, 

&lt;math&gt;\mathcal S = \{(H, H, T, \dots), (H, T, H, \dots), (T, H, H, \dots), (T, T, T, \dots), \dots\}&lt;/math&gt;.

Recall that a [[random variable]] is simply a mapping between all possible experimental outcomes and a numerical value. Similarly, a random process is a mapping that maps every possible outcome in &lt;math&gt;\mathcal S&lt;/math&gt; to a series of numbers. That is, 

&lt;math&gt;\mathcal S_{\mathcal X} = {(1,1,0, \dots), (1,0,1, \dots), (0,1,1, \dots), (0,0,0, \dots}), \dots&lt;/math&gt;.

The random process does nothing but mapping each sequence in &lt;math&gt;\mathcal S&lt;/math&gt; to a sequence in &lt;math&gt;\mathcal S&lt;/math&gt;. [[Random variable|Again]], there is nothing random about this procedure, it's a pretty deterministic mapping and the name ''random process'' is somewhat unfortunate.

== Types of random processes==

=== According to time resolution and output type ===
* Discrete time discrete valued (DTDV)
* Discrete time continuous valued (DTCV)
* Continuous time discrete   valued (CTDV)
* Continuous time continuous valued (CTCV)

=== According to time dependence ===

Random processes can be categorized from simplest to most complicated (in terms of time dependence) as below
* IID processes (independent and identically distributed)
* Stationary processes
* Wide-sense stationary processes

The lower items in this list encompass the higher items (i.e. a stationary process is also IID etc.)



==References==</text>
      <sha1>lbfaw6mdtte0xfufqmjqp8dlam00avj</sha1>
    </revision>
    <revision>
      <id>519</id>
      <parentid>497</parentid>
      <timestamp>2019-12-08T12:03:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>/* Discrete stochastic processes */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3434">It's helpful to define a stochastic process via an analogy to RVs. An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a ''rule'' for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt;&lt;ref&gt;A. Papoulis and U.Pillai, Probability, Random Variables and Stochastic Processes 4th edition&lt;/ref&gt;. A stochastic process is a rule for assigning to every outcome &lt;math&gt;\zeta&lt;/math&gt; a ''function'' &lt;math&gt;\mathbf x(t, \zeta)&lt;/math&gt;. Thus a stochastic process is a family of time functions depending on the parameter &lt;math&gt;\zeta&lt;/math&gt; or, equivalently, a function of &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt;. The domain of &lt;math&gt;\zeta&lt;/math&gt; is the set of all experimental outcomes and the domain of &lt;math&gt;t&lt;/math&gt; is a set &lt;math&gt;R&lt;/math&gt; of real numbers. (Note: It's not ''the'' set of real numbers, it's ''a'' set of real numbers.

If &lt;math&gt;R&lt;/math&gt; is the real axis, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a continuous-time process. If &lt;math&gt;R&lt;/math&gt; is the set of integers, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is a discrete-time process. A discrete-time process is, thus, a sequence of RVs. Such a sequence is typically denoted by &lt;math&gt;\mathbf x_n&lt;/math&gt; or &lt;math&gt;\mathbf x[n]&lt;/math&gt;.

The symbol &lt;math&gt;\mathbf x(t)&lt;/math&gt; has multiple interpretations:

* It's a family of functions &lt;math&gt;\mathbf x(t,\zeta)&lt;/math&gt;. In this interpretation, &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are variables.
* It's a single time function (or a ''sample'' of the given process). In this case, &lt;math&gt;t&lt;/math&gt; is a variable and &lt;math&gt;\zeta&lt;/math&gt; is fixed.
* If &lt;math&gt;t&lt;/math&gt; is fixed and &lt;math&gt;\zeta&lt;/math&gt; is variable, then &lt;math&gt;\mathbf x(t)&lt;/math&gt; is an RV.
* If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;\zeta&lt;/math&gt; are fixed, then &lt;math&gt;x(t)&lt;/math&gt; is a ''number''.

=== Discrete stochastic processes ===
A probably even simpler definition can be given as below. First of all, note that random processes are infinite. Consider the coin tossing experiment. Then, there are an infinite number of ''infinite long'' possible experimental outcomes, 

&lt;math&gt;\mathcal S = \{(H, H, T, \dots), (H, T, H, \dots), (T, H, H, \dots), (T, T, T, \dots), \dots\}&lt;/math&gt;.

Recall that a [[random variable]] is simply a mapping between all possible experimental outcomes and a numerical value. Similarly, a random process is a mapping that maps every possible outcome in &lt;math&gt;\mathcal S&lt;/math&gt; to a series of numbers. That is, 

&lt;math&gt;\mathcal S_{\mathcal X} = {(1,1,0, \dots), (1,0,1, \dots), (0,1,1, \dots), (0,0,0, \dots}), \dots&lt;/math&gt;.

The random process does nothing but mapping each sequence in &lt;math&gt;\mathcal S&lt;/math&gt; to a sequence in &lt;math&gt;\mathcal S_{\mathcal X}&lt;/math&gt;. [[Random variable|Again]], there is nothing random about this procedure, it's a pretty deterministic mapping and the name ''random process'' is somewhat unfortunate.

== Types of random processes==

=== According to time resolution and output type ===
* Discrete time discrete valued (DTDV)
* Discrete time continuous valued (DTCV)
* Continuous time discrete   valued (CTDV)
* Continuous time continuous valued (CTCV)

=== According to time dependence ===

Random processes can be categorized from simplest to most complicated (in terms of time dependence) as below
* IID processes (independent and identically distributed)
* Stationary processes
* Wide-sense stationary processes

The lower items in this list encompass the higher items (i.e. a stationary process is also IID etc.)



==References==</text>
      <sha1>09nwm9j887b4u9p5ktg6z44eaij01ot</sha1>
    </revision>
  </page>
  <page>
    <title>Essentially disjoint</title>
    <ns>0</ns>
    <id>113</id>
    <revision>
      <id>358</id>
      <timestamp>2019-04-28T17:41:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Two subspaces &lt;math&gt;\mathcal{U}&lt;/math&gt; and &lt;math&gt;\mathcal V&lt;/math&gt; of &lt;math&gt;\mathcal W&lt;/math&gt; are called essentially disjoint if &lt;math&gt;\mathcal{U}\cap \mathcal{V} = \{\mathbf..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="458">Two subspaces &lt;math&gt;\mathcal{U}&lt;/math&gt; and &lt;math&gt;\mathcal V&lt;/math&gt; of &lt;math&gt;\mathcal W&lt;/math&gt; are called essentially disjoint if &lt;math&gt;\mathcal{U}\cap \mathcal{V} = \{\mathbf 0\}&lt;/math&gt;. Essential disjointness can be seen as a generalization of orthogonality; orthogonal subspaces are essentially disjoint (see Lemma 17.1.9&lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;) but the converse is not in general true.

== References ==</text>
      <sha1>qj37ep513if2dbnfbrpdpftiqkmyurk</sha1>
    </revision>
    <revision>
      <id>372</id>
      <parentid>358</parentid>
      <timestamp>2019-06-16T16:01:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1280">Two subspaces &lt;math&gt;\mathcal{U}&lt;/math&gt; and &lt;math&gt;\mathcal V&lt;/math&gt; of &lt;math&gt;\mathcal W&lt;/math&gt; are called essentially disjoint if &lt;math&gt;\mathcal{U}\cap \mathcal{V} = \{\mathbf 0\}&lt;/math&gt;. Essential disjointness can be seen as a generalization of orthogonality; orthogonal subspaces are essentially disjoint (see Lemma 17.1.9&lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;) but the converse is not in general true.

For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;

=== What's the point of essential disjointness? ===

The concept of essential disjoint allows us to express the conditions under which the rank of sums of matrices equals the sum of the ranks of the matrices (see [[rank]]).



== References ==</text>
      <sha1>blx2t7rp9w1v3aju3qdev2dbda3qrf9</sha1>
    </revision>
    <revision>
      <id>373</id>
      <parentid>372</parentid>
      <timestamp>2019-06-16T16:05:43Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1706">Two subspaces &lt;math&gt;\mathcal{U}&lt;/math&gt; and &lt;math&gt;\mathcal V&lt;/math&gt; of &lt;math&gt;\mathcal W&lt;/math&gt; are called essentially disjoint if &lt;math&gt;\mathcal{U}\cap \mathcal{V} = \{\mathbf 0\}&lt;/math&gt;. Essential disjointness can be seen as a generalization of orthogonality; orthogonal subspaces are essentially disjoint (see Lemma 17.1.9&lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;) but the converse is not in general true.

For any appropriately sized matrices &lt;math&gt;\mathbf A&lt;/math&gt; and &lt;math&gt;\mathbf B&lt;/math&gt;, &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt; and &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; are essentially disjoint. Intuitively terms, this result says that  &lt;math&gt;\mathcal{C}[(\mathbf{I-AA^-})\mathbf B]&lt;/math&gt; ''hits'' the part of the  &lt;math&gt;\mathbf B&lt;/math&gt;'s column space that cannot be hit by &lt;math&gt;\mathcal{C}(\mathbf A)&lt;/math&gt;. A direct consequence of this is that &lt;math&gt;\text{rank}[\mathbf A, (\mathbf{I-AA^-})\mathbf B] = \text{rank}(\mathbf A) + \text{rank}[(\mathbf{I-AA^-})\mathbf B].&lt;/math&gt;

=== What's the point of essential disjointness? ===

The concept of essential disjoint allows us to express the conditions under which the rank of sums of matrices equals the sum of the ranks of the matrices (see [[rank]]). It also allows us to come up with expressions for ranks of products of matrices (see [[rank]]). Moreover, it closely relates to projection and allows us to look at the concept of projection from a different perspective. It also relates closely to idempotent matrices (a matrix &lt;math&gt;\mathbf{A}&lt;/math&gt; is idempotent iff &lt;math&gt;\mathcal{C}(\mathbf{A})&lt;/math&gt; and &lt;math&gt;\mathcal{C}(I-\mathbf{A})&lt;/math&gt; are essentially disjoint).



== References ==</text>
      <sha1>faeioseiblrik2i9xev7s5u6l67gqv9</sha1>
    </revision>
  </page>
  <page>
    <title>Spectrum</title>
    <ns>0</ns>
    <id>114</id>
    <revision>
      <id>375</id>
      <timestamp>2019-06-16T18:13:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Spectrum of a matrix &lt;math&gt;A&lt;/math&gt; is the set of (distinct) scalars that are [[Eigenvectors and Eigenvalues|eigenvalues]] of &lt;math&gt;A&lt;/math&gt;."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="141">Spectrum of a matrix &lt;math&gt;A&lt;/math&gt; is the set of (distinct) scalars that are [[Eigenvectors and Eigenvalues|eigenvalues]] of &lt;math&gt;A&lt;/math&gt;.</text>
      <sha1>mfn20zy8eam4cset6jruhmqju4be0e0</sha1>
    </revision>
    <revision>
      <id>376</id>
      <parentid>375</parentid>
      <timestamp>2019-06-16T18:14:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="252">Spectrum of a matrix &lt;math&gt;A&lt;/math&gt; is the set of (distinct) scalars that are [[Eigenvectors and Eigenvalues|eigenvalues]] of &lt;math&gt;A&lt;/math&gt;. That's why the [[Spectral decomposition|spectral decomposition]] decomposes a matrix based on its eigenvalues.</text>
      <sha1>ddxfhaj6i8wathb965rubeobr9cyln3</sha1>
    </revision>
  </page>
  <page>
    <title>Similar matrices</title>
    <ns>0</ns>
    <id>115</id>
    <redirect title="Similarity Transformation" />
    <revision>
      <id>380</id>
      <timestamp>2019-06-16T18:24:02Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Similarity transformation]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="39">#REDIRECT [[Similarity transformation]]</text>
      <sha1>fkqd0es1720mi98fcf809daxeyr2ojc</sha1>
    </revision>
    <revision>
      <id>381</id>
      <parentid>380</parentid>
      <timestamp>2019-06-16T18:24:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Changed redirect target from [[Similarity transformation]] to [[Similarity Transformation]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="39">#REDIRECT [[Similarity Transformation]]</text>
      <sha1>l4tmcskf8jzkvggcyzvsiygd20r9tz5</sha1>
    </revision>
  </page>
  <page>
    <title>Triangularization</title>
    <ns>0</ns>
    <id>116</id>
    <revision>
      <id>383</id>
      <timestamp>2019-06-16T18:31:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;A&lt;/math&gt; represent an &lt;math&gt;n\times n&lt;/math&gt; matrix that have &lt;math&gt;n&lt;/math&gt; (distinct or non-distinct) eigenvalues, and let &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; repres..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="576">Let &lt;math&gt;A&lt;/math&gt; represent an &lt;math&gt;n\times n&lt;/math&gt; matrix that have &lt;math&gt;n&lt;/math&gt; (distinct or non-distinct) eigenvalues, and let &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; represent the (not necessarily distinct) eigenvalues of &lt;math&gt;A&lt;/math&gt; in arbitrary order. Then, there exists an orthogonal matrix &lt;math&gt;Q&lt;/math&gt; such that

&lt;math&gt;Q'AQ = T&lt;/math&gt;

where &lt;math&gt;T&lt;/math&gt; is an &lt;math&gt;n\times n&lt;/math&gt; upper triangular matrix whose diagonal elements are &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; (see T21.5.11 &lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;).</text>
      <sha1>2r30u2eubr54scoehfxc9f2zt0vjzvw</sha1>
    </revision>
    <revision>
      <id>385</id>
      <parentid>383</parentid>
      <timestamp>2019-06-16T18:35:24Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="818">Let &lt;math&gt;A&lt;/math&gt; represent an &lt;math&gt;n\times n&lt;/math&gt; matrix that have &lt;math&gt;n&lt;/math&gt; (distinct or non-distinct) eigenvalues, and let &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; represent the (not necessarily distinct) eigenvalues of &lt;math&gt;A&lt;/math&gt; in arbitrary order. Then, there exists an orthogonal matrix &lt;math&gt;Q&lt;/math&gt; such that

&lt;math&gt;Q'AQ = T&lt;/math&gt;

where &lt;math&gt;T&lt;/math&gt; is an &lt;math&gt;n\times n&lt;/math&gt; upper triangular matrix whose diagonal elements are &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; (see T21.5.11 &lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;).

===What's the poitn of triangularization?===
Among other possible advantages, triangularization facilitates the computation of determinant and trace 

 &lt;math&gt;\det(A) = \prod_{i=1}^n d_i&lt;/math&gt;  
 &lt;math&gt;\text{tr}(A) = \sum_{i=1}^n d_i&lt;/math&gt;</text>
      <sha1>9degyahc31uy5ybbgxmk7oh9jnp9v96</sha1>
    </revision>
    <revision>
      <id>386</id>
      <parentid>385</parentid>
      <timestamp>2019-06-16T18:36:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="806">Let &lt;math&gt;A&lt;/math&gt; represent an &lt;math&gt;n\times n&lt;/math&gt; matrix that have &lt;math&gt;n&lt;/math&gt; (distinct or non-distinct) eigenvalues, and let &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; represent the (not necessarily distinct) eigenvalues of &lt;math&gt;A&lt;/math&gt; in arbitrary order. Then, there exists an orthogonal matrix &lt;math&gt;Q&lt;/math&gt; such that

&lt;math&gt;Q'AQ = T&lt;/math&gt;

where &lt;math&gt;T&lt;/math&gt; is an &lt;math&gt;n\times n&lt;/math&gt; upper triangular matrix whose diagonal elements are &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; (see T21.5.11 &lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;).

===What's the poitn of triangularization?===
Among other possible advantages, triangularization facilitates the computation of determinant and trace 

 &lt;math&gt;\det(A) = \prod_{i=1}^n d_i \\

\text{tr}(A) = \sum_{i=1}^n d_i&lt;/math&gt;</text>
      <sha1>emy8pubun0p1c29205t6gp6zqrmasv6</sha1>
    </revision>
    <revision>
      <id>569</id>
      <parentid>386</parentid>
      <timestamp>2020-04-03T18:25:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="824">Let &lt;math&gt;A&lt;/math&gt; represent an &lt;math&gt;n\times n&lt;/math&gt; matrix that have &lt;math&gt;n&lt;/math&gt; (distinct or non-distinct) eigenvalues, and let &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; represent the (not necessarily distinct) eigenvalues of &lt;math&gt;A&lt;/math&gt; in arbitrary order. Then, there exists an orthogonal matrix &lt;math&gt;Q&lt;/math&gt; such that

&lt;math&gt;Q'AQ = T&lt;/math&gt;

where &lt;math&gt;T&lt;/math&gt; is an &lt;math&gt;n\times n&lt;/math&gt; upper triangular matrix whose diagonal elements are &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; (see T21.5.11 &lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;).

===What's the poitn of triangularization?===
Among other possible advantages, triangularization facilitates the computation of determinant and trace 

 &lt;math&gt;\det(A) = \prod_{i=1}^n d_i \\

\text{tr}(A) = \sum_{i=1}^n d_i&lt;/math&gt;

== References ==</text>
      <sha1>op2rl05w2grf7l7v4deagzsjgl20td7</sha1>
    </revision>
    <revision>
      <id>570</id>
      <parentid>569</parentid>
      <timestamp>2020-04-03T18:26:22Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="824">Let &lt;math&gt;A&lt;/math&gt; represent an &lt;math&gt;n\times n&lt;/math&gt; matrix that have &lt;math&gt;n&lt;/math&gt; (distinct or non-distinct) eigenvalues, and let &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; represent the (not necessarily distinct) eigenvalues of &lt;math&gt;A&lt;/math&gt; in arbitrary order. Then, there exists an orthogonal matrix &lt;math&gt;Q&lt;/math&gt; such that

&lt;math&gt;Q'AQ = T&lt;/math&gt;

where &lt;math&gt;T&lt;/math&gt; is an &lt;math&gt;n\times n&lt;/math&gt; upper triangular matrix whose diagonal elements are &lt;math&gt;d_1, d_2, \dots, d_n&lt;/math&gt; (see T21.5.11 &lt;ref&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;).

===What's the point of triangularization?===
Among other possible advantages, triangularization facilitates the computation of determinant and trace 

 &lt;math&gt;\det(A) = \prod_{i=1}^n d_i \\

\text{tr}(A) = \sum_{i=1}^n d_i&lt;/math&gt;

== References ==</text>
      <sha1>8hgdsw5wxz3e5nisfr0w8piuik1kqun</sha1>
    </revision>
  </page>
  <page>
    <title>Square root of matrix</title>
    <ns>0</ns>
    <id>117</id>
    <revision>
      <id>390</id>
      <timestamp>2019-06-16T18:52:50Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The concept of square root can be generalized from scalars to (symmetric and nonnegative definite) matrices. For any symmetric and nonnegative matrix &lt;math&gt;A&lt;/math&gt;, there exi..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="588">The concept of square root can be generalized from scalars to (symmetric and nonnegative definite) matrices. For any symmetric and nonnegative matrix &lt;math&gt;A&lt;/math&gt;, there exists a symmetric and nonnegative definite matrix &lt;math&gt;R&lt;/math&gt; such that &lt;math&gt;A=R^2&lt;/math&gt;. Moreover, &lt;math&gt;R&lt;/math&gt; is unique and is expressible as 

&lt;math&gt;R = Q \text{diag}(\sqrt{d_1}, \sqrt{d_2}, \dots, \sqrt{d_n})Q'&lt;/math&gt;

where &lt;math&gt;A = Q \text{diag}({d_1}, {d_2}, \dots, {d_n})Q'&lt;/math&gt; (T21.9.1&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;).

== References ==</text>
      <sha1>imb94xbfv09witbw7fguobjdblpnl4m</sha1>
    </revision>
  </page>
  <page>
    <title>Injective</title>
    <ns>0</ns>
    <id>118</id>
    <revision>
      <id>393</id>
      <timestamp>2019-06-22T14:32:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is injective if &lt;math&gt;T\mathbf..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="644">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is injective if &lt;math&gt;T\mathbf{v} = \mathbf{0} \implies \mathbf{v} = \mathbf{0}.&lt;/math&gt;

If &lt;math&gt;T&lt;/math&gt; is a matrix, this means that it has full column rank, because the definition above suggests that the [[nullspace]] of &lt;math&gt;T&lt;/math&gt; is of dimension 0, which, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], suggests that &lt;math&gt;T&lt;/math&gt; has to have full column rank. An injective matrix &lt;math&gt;T&lt;/math&gt; is also called [[One-to-one|one-to-one]].</text>
      <sha1>9d6prmvagm7u2xdlz7uhni11x3dryi7</sha1>
    </revision>
    <revision>
      <id>403</id>
      <parentid>393</parentid>
      <timestamp>2019-06-22T14:44:34Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="724">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is injective(see Def 2.4.5&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) if &lt;math&gt;T\mathbf{v} = \mathbf{0} \implies \mathbf{v} = \mathbf{0}.&lt;/math&gt;

If &lt;math&gt;T&lt;/math&gt; is a matrix, this means that it has full column rank, because the definition above suggests that the [[nullspace]] of &lt;math&gt;T&lt;/math&gt; is of dimension 0, which, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], suggests that &lt;math&gt;T&lt;/math&gt; has to have full column rank. An injective matrix &lt;math&gt;T&lt;/math&gt; is also called [[One-to-one|one-to-one]].</text>
      <sha1>sx37lfrrxnmusyjvfsgxgeivtxup1i6</sha1>
    </revision>
    <revision>
      <id>404</id>
      <parentid>403</parentid>
      <timestamp>2019-06-22T14:44:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="742">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is injective(see Def 2.4.5&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) if &lt;math&gt;T\mathbf{v} = \mathbf{0} \implies \mathbf{v} = \mathbf{0}.&lt;/math&gt;

If &lt;math&gt;T&lt;/math&gt; is a matrix, this means that it has full column rank, because the definition above suggests that the [[nullspace]] of &lt;math&gt;T&lt;/math&gt; is of dimension 0, which, according to the [[Fundamental Theorem of Linear Algebra|fundamental theorem of linear algebra]], suggests that &lt;math&gt;T&lt;/math&gt; has to have full column rank. An injective matrix &lt;math&gt;T&lt;/math&gt; is also called [[One-to-one|one-to-one]].

== References ==</text>
      <sha1>eb17ewhepmssssxascmi8rimae6ct7o</sha1>
    </revision>
  </page>
  <page>
    <title>Surjective</title>
    <ns>0</ns>
    <id>119</id>
    <revision>
      <id>394</id>
      <timestamp>2019-06-22T14:34:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is surjective if for each &lt;mat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="474">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is surjective if for each &lt;math&gt;\mathbf{w} \in V_2&lt;/math&gt; there exists &lt;math&gt;\mathbf{v} \in V_1&lt;/math&gt; such that &lt;math&gt;T\mathbf{v} = w&lt;/math&gt;.

If &lt;math&gt;T&lt;/math&gt; is a matrix, this means that it has full row rank. A surjective matrix &lt;math&gt;T&lt;/math&gt; is also called [[Onto matrices|onto]].

See also [[injective]] and [[bijective]].</text>
      <sha1>onsyin9w39p3p2jfb7t3ulsrxgaivnp</sha1>
    </revision>
    <revision>
      <id>405</id>
      <parentid>394</parentid>
      <timestamp>2019-06-22T14:45:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="573">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is surjective (see Def 2.4.5&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) if for each &lt;math&gt;\mathbf{w} \in V_2&lt;/math&gt; there exists &lt;math&gt;\mathbf{v} \in V_1&lt;/math&gt; such that &lt;math&gt;T\mathbf{v} = w&lt;/math&gt;.

If &lt;math&gt;T&lt;/math&gt; is a matrix, this means that it has full row rank. A surjective matrix &lt;math&gt;T&lt;/math&gt; is also called [[Onto matrices|onto]].

See also [[injective]] and [[bijective]].

== References ==</text>
      <sha1>7kt5wob22l5c0jo5b9qiuzintydwknt</sha1>
    </revision>
  </page>
  <page>
    <title>Bijective</title>
    <ns>0</ns>
    <id>120</id>
    <revision>
      <id>395</id>
      <timestamp>2019-06-22T14:35:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "An operator is called bijective if it is [[injective]] and [[surjective]]."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="74">An operator is called bijective if it is [[injective]] and [[surjective]].</text>
      <sha1>8e6m2ysjof7eu7iortvr32omuet71fe</sha1>
    </revision>
    <revision>
      <id>406</id>
      <parentid>395</parentid>
      <timestamp>2019-06-22T14:45:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="173">An operator is called bijective if it is [[injective]] and [[surjective]] (see Def 2.4.5&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

== References ==</text>
      <sha1>6j0fncl18rih5ijp8rfct2bygwfkwzp</sha1>
    </revision>
  </page>
  <page>
    <title>Isometry</title>
    <ns>0</ns>
    <id>121</id>
    <revision>
      <id>399</id>
      <timestamp>2019-06-22T14:38:55Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is injective if &lt;math&gt;T\mathbf..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="344">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be an operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. Then, &lt;math&gt;T&lt;/math&gt; is injective if &lt;math&gt;T\mathbf{v} = \mathbf{0} \implies \mathbf{v} = \mathbf{0}.&lt;/math&gt; The operator &lt;math&gt;T&lt;/math&gt; is called an isometry if &lt;math&gt;||T\mathbf{v}||_{V_2} = ||\mathbf{v}||_{V_1}&lt;/math&gt;.</text>
      <sha1>0apjb7d54vw0i2p8txlypo06fhkp54d</sha1>
    </revision>
  </page>
  <page>
    <title>Bounded linear operator</title>
    <ns>0</ns>
    <id>122</id>
    <revision>
      <id>400</id>
      <timestamp>2019-06-22T14:42:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be a linear operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. &lt;math&gt;T&lt;/math&gt; is bounded if there exists a c..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="474">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be a linear operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. &lt;math&gt;T&lt;/math&gt; is bounded if there exists a constant &lt;math&gt;K\ge 0&lt;/math&gt; such that
&lt;math&gt;||T\mathbf{v}||_{V_2} \le K||\mathbf{v}||_{V_1}&lt;/math&gt; &lt;math&gt;\forall \mathbf{v} \in V_1&lt;/math&gt;. 

The smallest possible value of &lt;math&gt;K&lt;/math&gt; that can be used above is called the norm of the operator &lt;math&gt;T&lt;/math&gt;, and is denoted by &lt;math&gt;||T||&lt;/math&gt;.</text>
      <sha1>dmfyxtgdbeoj1rwu136k4tcwyfymtxq</sha1>
    </revision>
    <revision>
      <id>401</id>
      <parentid>400</parentid>
      <timestamp>2019-06-22T14:42:51Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="549">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be a linear operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. &lt;math&gt;T&lt;/math&gt; is bounded&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions&lt;/ref&gt; if there exists a constant &lt;math&gt;K\ge 0&lt;/math&gt; such that
&lt;math&gt;||T\mathbf{v}||_{V_2} \le K||\mathbf{v}||_{V_1}&lt;/math&gt; &lt;math&gt;\forall \mathbf{v} \in V_1&lt;/math&gt;. 

The smallest possible value of &lt;math&gt;K&lt;/math&gt; that can be used above is called the norm of the operator &lt;math&gt;T&lt;/math&gt;, and is denoted by &lt;math&gt;||T||&lt;/math&gt;.

==References==</text>
      <sha1>eimsjqufc0t0hps5ll4f52svefoo90x</sha1>
    </revision>
    <revision>
      <id>402</id>
      <parentid>401</parentid>
      <timestamp>2019-06-22T14:44:17Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="571">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be a linear operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. &lt;math&gt;T&lt;/math&gt; is bounded (see Def 2.4.1&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) if there exists a constant &lt;math&gt;K\ge 0&lt;/math&gt; such that
&lt;math&gt;||T\mathbf{v}||_{V_2} \le K||\mathbf{v}||_{V_1}&lt;/math&gt; &lt;math&gt;\forall \mathbf{v} \in V_1&lt;/math&gt;. 

The smallest possible value of &lt;math&gt;K&lt;/math&gt; that can be used above is called the norm of the operator &lt;math&gt;T&lt;/math&gt;, and is denoted by &lt;math&gt;||T||&lt;/math&gt;.

==References==</text>
      <sha1>c4oo8jvk810n9turj5gzo5ak5aycy6j</sha1>
    </revision>
    <revision>
      <id>469</id>
      <parentid>402</parentid>
      <timestamp>2019-07-07T14:05:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1135">Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be normed spaces and let &lt;math&gt;T&lt;/math&gt; be a linear operator &lt;math&gt;T:V_1\to V_2&lt;/math&gt;. &lt;math&gt;T&lt;/math&gt; is bounded (see Def 2.4.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) if there exists a constant &lt;math&gt;K\ge 0&lt;/math&gt; such that
&lt;math&gt;||T\mathbf{v}||_{V_2} \le K||\mathbf{v}||_{V_1}&lt;/math&gt; &lt;math&gt;\forall \mathbf{v} \in V_1&lt;/math&gt;. 

The smallest possible value of &lt;math&gt;K&lt;/math&gt; that can be used above is called the norm of the operator &lt;math&gt;T&lt;/math&gt;, and is denoted by &lt;math&gt;||T||&lt;/math&gt;.

===Linearity in infinite sums===
Note that linearity is in general defined for finite sums. However, for a bounded linear operator it can apply subject to the following convergence criteria (Exercise 2.14&lt;ref name=ole/&gt;).

Assume that &lt;math&gt;\{\mathbf v_k\}_{k=1}^{\infty}&lt;/math&gt; is a sequence of elements in &lt;math&gt;V_1&lt;/math&gt; and that &lt;math&gt;\sum_{k=1}^{\infty} c_k \mathbf v_k&lt;/math&gt; is convergent for some scalar sequence &lt;math&gt;\{c_k\}_{k=1}^\infty&lt;/math&gt;. Then,

 &lt;math&gt;T\sum_{k=1}^\infty c_k \mathbf v_k = \sum_{k=1}^\infty c_k T_k \mathbf v_k&lt;/math&gt;.

==References==</text>
      <sha1>qbf44qe9u8ne3m39jdnrz5p9dquxkdr</sha1>
    </revision>
  </page>
  <page>
    <title>Operator norm</title>
    <ns>0</ns>
    <id>123</id>
    <redirect title="Bounded linear operator" />
    <revision>
      <id>407</id>
      <timestamp>2019-06-22T14:46:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Bounded linear operator]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="37">#REDIRECT [[Bounded linear operator]]</text>
      <sha1>esgodzqxw92wcdf0ykbxdslsuqqqgiy</sha1>
    </revision>
  </page>
  <page>
    <title>Convergence</title>
    <ns>0</ns>
    <id>124</id>
    <revision>
      <id>408</id>
      <timestamp>2019-06-23T14:52:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "== Convergence in normed spaces ==  A sequence &lt;math&gt;\{\mathbf{v}_k\}_{k=1}^{\infty}&lt;/math&gt; in a normed vector space &lt;math&gt;V&lt;/math&gt; converges to &lt;math&gt;\mathbf{v} \in V&lt;/math&gt;..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="730">== Convergence in normed spaces ==

A sequence &lt;math&gt;\{\mathbf{v}_k\}_{k=1}^{\infty}&lt;/math&gt; in a normed vector space &lt;math&gt;V&lt;/math&gt; converges to &lt;math&gt;\mathbf{v} \in V&lt;/math&gt; if(Def 2.1.5&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)

 &lt;math&gt;||\mathbf{v-v}_k||\to 0&lt;/math&gt; as &lt;math&gt;k\to \infty&lt;/math&gt;. 

Precisely, this means that for any &lt;math&gt;\epsilon&gt;0&lt;/math&gt;, there exists &lt;math&gt;N\in \mathbb{N}&lt;/math&gt; such that &lt;math&gt;||\mathbf{v-v}_k||&lt;\epsilon&lt;/math&gt; for any &lt;math&gt;k\ge N&lt;/math&gt;.

Convergence is also denoted alternatively alternatively in the following two forms

(i) &lt;math&gt;\mathbf{v}_k\to \mathbf{v}&lt;/math&gt; as &lt;math&gt;k\to\infty&lt;/math&gt;

(ii) &lt;math&gt;v=\lim_{k\to\infty}\mathbf{v}_k&lt;/math&gt;

== References ==</text>
      <sha1>raur7roqglhr2tur1vaffo56nmyj3ws</sha1>
    </revision>
  </page>
  <page>
    <title>Cauchy sequence</title>
    <ns>0</ns>
    <id>125</id>
    <revision>
      <id>409</id>
      <timestamp>2019-06-23T14:54:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A sequence &lt;math&gt;\{\mathbf{v}_k\}_{k=1}^{\infty}&lt;/math&gt; in a normed vector space &lt;math&gt;V&lt;/math&gt; is called a Cauchy sequence if (Def. 3.1.1&lt;ref&gt;O. Christensen, Functions, Space..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="394">A sequence &lt;math&gt;\{\mathbf{v}_k\}_{k=1}^{\infty}&lt;/math&gt; in a normed vector space &lt;math&gt;V&lt;/math&gt; is called a Cauchy sequence if (Def. 3.1.1&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) for any &lt;math&gt;\epsilon&gt;0&lt;/math&gt;, there exists &lt;math&gt;N\in \mathbb{N}&lt;/math&gt; such that 

 &lt;math&gt;||\mathbf{v}_k-\mathbf{v}_l||&lt;\epsilon&lt;/math&gt; whenever &lt;math&gt;k,l\ge N&lt;/math&gt;.

== References ==</text>
      <sha1>7sgybtuqibcutuertf3n2io14h9auwn</sha1>
    </revision>
    <revision>
      <id>472</id>
      <parentid>409</parentid>
      <timestamp>2019-07-13T13:26:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="962">A sequence &lt;math&gt;\{\mathbf{v}_k\}_{k=1}^{\infty}&lt;/math&gt; in a normed vector space &lt;math&gt;V&lt;/math&gt; is called a Cauchy sequence if (Def. 3.1.1&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) for any &lt;math&gt;\epsilon&gt;0&lt;/math&gt;, there exists &lt;math&gt;N\in \mathbb{N}&lt;/math&gt; such that 

 &lt;math&gt;||\mathbf{v}_k-\mathbf{v}_l||&lt;\epsilon&lt;/math&gt; whenever &lt;math&gt;k,l\ge N&lt;/math&gt;.

Note that Cauchy sequences are very closely related to [[convergence]]. Clearly, every convergent sequence is a Cauchy sequence, but the converse does not necessarily holds; it holds, by definition, for [[Banach spaces]].

The advantage of Cauchy sequences compared to convergent sequences is that for Cauchy sequences we don't have to ''guess'' or to know to what the sequence convergences too (i.e., the &lt;math&gt;v&lt;/math&gt;; see [[convergence]]). We just need to show that the norm difference shown above gets smaller and smaller. This is simpler to do Mathematically. 

== References ==</text>
      <sha1>8s57m1z6ec0kno5ronkqqrm2pmp3lp8</sha1>
    </revision>
  </page>
  <page>
    <title>Banach spaces</title>
    <ns>0</ns>
    <id>126</id>
    <revision>
      <id>410</id>
      <timestamp>2019-06-23T14:56:41Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A normed vector space &lt;math&gt;V&lt;/math&gt; is called a Banach space (Def 3.1.4) if every [[Cauchy sequence]] in &lt;math&gt;V&lt;/math&gt; converges to some &lt;math&gt;\mathbf{v}\in V&lt;/math&gt;.  ==Ref..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="185">A normed vector space &lt;math&gt;V&lt;/math&gt; is called a Banach space (Def 3.1.4) if every [[Cauchy sequence]] in &lt;math&gt;V&lt;/math&gt; converges to some &lt;math&gt;\mathbf{v}\in V&lt;/math&gt;.

==References ==</text>
      <sha1>gfmim9p3npngfjkzbd6t4oyvl4ubm36</sha1>
    </revision>
    <revision>
      <id>411</id>
      <parentid>410</parentid>
      <timestamp>2019-06-23T15:04:50Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="694">A normed vector space &lt;math&gt;V&lt;/math&gt; is called a Banach space (Def 3.1.4) if every [[Cauchy sequence]] in &lt;math&gt;V&lt;/math&gt; converges to some &lt;math&gt;\mathbf{v}\in V&lt;/math&gt;.

Examples to Banach space are &lt;math&gt;\mathbb{R}^n, \mathbb{C}^n&lt;/math&gt;. The space of [[continuous functions on a bounded interval]], &lt;math&gt;C[a,b]&lt;/math&gt;, is also a Banach space. Moreover, the spaces &lt;math&gt;\ell^p(\mathbb{N})&lt;/math&gt; (&lt;math&gt;p \in [1,\infty)&lt;/math&gt;) defined as

&lt;math&gt;\ell^p(\mathbb{N}) := \{  \{x_k\}_{k=1}^\infty : x_k \in \mathbb{C}, \sum_{k=1}^\infty |x_k|^p &lt; \infty \}&lt;/math&gt;

are also Banach spaces w.r.t. the norm &lt;math&gt;||\mathbf{x}||+p\left(\sum_{k=1}^\infty |x_k|^p\right)^{1/p}&lt;/math&gt;


==References ==</text>
      <sha1>oxfs5g8s96e8hqs9pfh1z9xiu1ww8j4</sha1>
    </revision>
    <revision>
      <id>412</id>
      <parentid>411</parentid>
      <timestamp>2019-06-23T15:05:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="759">A normed vector space &lt;math&gt;V&lt;/math&gt; is called a Banach space (Def 3.1.4&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) if every [[Cauchy sequence]] in &lt;math&gt;V&lt;/math&gt; converges to some &lt;math&gt;\mathbf{v}\in V&lt;/math&gt;.

Examples to Banach space are &lt;math&gt;\mathbb{R}^n, \mathbb{C}^n&lt;/math&gt;. The space of [[continuous functions on a bounded interval]], &lt;math&gt;C[a,b]&lt;/math&gt;, is also a Banach space. Moreover, the spaces &lt;math&gt;\ell^p(\mathbb{N})&lt;/math&gt; (&lt;math&gt;p \in [1,\infty)&lt;/math&gt;) defined as

&lt;math&gt;\ell^p(\mathbb{N}) := \{  \{x_k\}_{k=1}^\infty : x_k \in \mathbb{C}, \sum_{k=1}^\infty |x_k|^p &lt; \infty \}&lt;/math&gt;

are also Banach spaces w.r.t. the norm &lt;math&gt;||\mathbf{x}||+p\left(\sum_{k=1}^\infty |x_k|^p\right)^{1/p}&lt;/math&gt;


==References ==</text>
      <sha1>qawlayxp56u4m95v8jludvyltlhtwe4</sha1>
    </revision>
    <revision>
      <id>413</id>
      <parentid>412</parentid>
      <timestamp>2019-06-23T15:05:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="757">A normed vector space &lt;math&gt;V&lt;/math&gt; is called a Banach space (Def 3.1.4&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) if every [[Cauchy sequence]] in &lt;math&gt;V&lt;/math&gt; converges to some &lt;math&gt;\mathbf{v}\in V&lt;/math&gt;.

Examples to Banach space are &lt;math&gt;\mathbb{R}^n, \mathbb{C}^n&lt;/math&gt;. The space of [[continuous functions on a bounded interval]], &lt;math&gt;C[a,b]&lt;/math&gt;, is also a Banach space. Moreover, the spaces &lt;math&gt;\ell^p(\mathbb{N})&lt;/math&gt; (&lt;math&gt;p \in [1,\infty)&lt;/math&gt;) defined as

&lt;math&gt;\ell^p(\mathbb{N}) := \{  \{x_k\}_{k=1}^\infty : x_k \in \mathbb{C}, \sum_{k=1}^\infty |x_k|^p &lt; \infty \}&lt;/math&gt;

are also Banach spaces w.r.t. the norm &lt;math&gt;||\mathbf{x}||=p\left(\sum_{k=1}^\infty |x_k|^p\right)^{1/p}&lt;/math&gt;
==References ==</text>
      <sha1>i4yed0wr5u08msc80i3pgu9bzka5h9q</sha1>
    </revision>
  </page>
  <page>
    <title>Inner product</title>
    <ns>0</ns>
    <id>127</id>
    <revision>
      <id>414</id>
      <timestamp>2019-06-23T15:17:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;V&lt;/math&gt; be a (complex) vector space. An inner product on &lt;math&gt;V&lt;/math&gt; is a mapping(Def 4.1.1&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician'..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1197">Let &lt;math&gt;V&lt;/math&gt; be a (complex) vector space. An inner product on &lt;math&gt;V&lt;/math&gt; is a mapping(Def 4.1.1&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;)

&lt;math&gt;\langle \cdot, \cdot\rangle  : V\times V \to \mathbb{C}&lt;/math&gt;

that satisfies the following three properties:

(i) (linearity in first entry): &lt;math&gt;\langle \alpha \mathbf{v}+\beta \mathbf{w}, \mathbf u\rangle  = \alpha \langle \mathbf{v,u} \rangle +\beta\langle \mathbf{w,u}\rangle \,\,\,\forall \mathbf{v,w,u}\in V,\,\,\forall \alpha, \beta \in \mathbb{C}&lt;/math&gt;

(ii) &lt;math&gt; \langle \mathbf{v,w}\rangle  = \bar{\langle \mathbf{w,v}\rangle },\,\,\forall\mathbf{v,w}\in V&lt;/math&gt;

(iii) &lt;math&gt;\langle \mathbf{v,v}\rangle \ge 0,\,\,\forall \mathbf{v}\in V&lt;/math&gt; and &lt;math&gt;\langle v,v\rangle =0\iff\mathbf{v=0}&lt;/math&gt;

Note that (i) and (ii) imply that &lt;math&gt;\langle v,\alpha \mathbf{w}+\beta\mathbf{u}\rangle  =\bar{\alpha}\langle v,w\rangle +\bar{\beta}\langle \mathbf{v,u}\rangle \,\,\forall\mathbf{v,w,u}\in V, \alpha, \beta \in \mathbb{C}.&lt;/math&gt;. This property is called ''antilinearity''.

A vector space equipped with an inner product is called an inner product space.

== References ==</text>
      <sha1>1uf3n9x9pbf4j9tisxr6mmaev760ydj</sha1>
    </revision>
    <revision>
      <id>522</id>
      <parentid>414</parentid>
      <timestamp>2020-01-21T21:01:05Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1198">Let &lt;math&gt;V&lt;/math&gt; be a (complex) vector space. An inner product on &lt;math&gt;V&lt;/math&gt; is a mapping (Def 4.1.1&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;)

&lt;math&gt;\langle \cdot, \cdot\rangle  : V\times V \to \mathbb{C}&lt;/math&gt;

that satisfies the following three properties:

(i) (linearity in first entry): &lt;math&gt;\langle \alpha \mathbf{v}+\beta \mathbf{w}, \mathbf u\rangle  = \alpha \langle \mathbf{v,u} \rangle +\beta\langle \mathbf{w,u}\rangle \,\,\,\forall \mathbf{v,w,u}\in V,\,\,\forall \alpha, \beta \in \mathbb{C}&lt;/math&gt;

(ii) &lt;math&gt; \langle \mathbf{v,w}\rangle  = \bar{\langle \mathbf{w,v}\rangle },\,\,\forall\mathbf{v,w}\in V&lt;/math&gt;

(iii) &lt;math&gt;\langle \mathbf{v,v}\rangle \ge 0,\,\,\forall \mathbf{v}\in V&lt;/math&gt; and &lt;math&gt;\langle v,v\rangle =0\iff\mathbf{v=0}&lt;/math&gt;

Note that (i) and (ii) imply that &lt;math&gt;\langle v,\alpha \mathbf{w}+\beta\mathbf{u}\rangle  =\bar{\alpha}\langle v,w\rangle +\bar{\beta}\langle \mathbf{v,u}\rangle \,\,\forall\mathbf{v,w,u}\in V, \alpha, \beta \in \mathbb{C}.&lt;/math&gt;. This property is called ''antilinearity''.

A vector space equipped with an inner product is called an inner product space.

== References ==</text>
      <sha1>73wapjlx5ld07y925klbqmu7u1wrvkx</sha1>
    </revision>
    <revision>
      <id>523</id>
      <parentid>522</parentid>
      <timestamp>2020-01-21T21:07:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1203">Let &lt;math&gt;V&lt;/math&gt; be a (complex) vector space. An inner product on &lt;math&gt;V&lt;/math&gt; is a mapping (Def 4.1.1&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;)

&lt;math&gt;\langle \cdot, \cdot\rangle  : V\times V \to \mathbb{C}&lt;/math&gt;

that satisfies the following three properties:

(i) (linearity in first entry): &lt;math&gt;\langle \alpha \mathbf{v}+\beta \mathbf{w}, \mathbf u\rangle  = \alpha \langle \mathbf{v,u} \rangle +\beta\langle \mathbf{w,u}\rangle \,\,\,\forall \mathbf{v,w,u}\in V,\,\,\forall \alpha, \beta \in \mathbb{C}&lt;/math&gt;

(ii) &lt;math&gt; \langle \mathbf{v,w}\rangle  = \overline{\langle \mathbf{w,v}\rangle },\,\,\forall\mathbf{v,w}\in V&lt;/math&gt;

(iii) &lt;math&gt;\langle \mathbf{v,v}\rangle \ge 0,\,\,\forall \mathbf{v}\in V&lt;/math&gt; and &lt;math&gt;\langle v,v\rangle =0\iff\mathbf{v=0}&lt;/math&gt;

Note that (i) and (ii) imply that &lt;math&gt;\langle v,\alpha \mathbf{w}+\beta\mathbf{u}\rangle  =\bar{\alpha}\langle v,w\rangle +\bar{\beta}\langle \mathbf{v,u}\rangle \,\,\forall\mathbf{v,w,u}\in V, \alpha, \beta \in \mathbb{C}.&lt;/math&gt;. This property is called ''antilinearity''.

A vector space equipped with an inner product is called an inner product space.

== References ==</text>
      <sha1>6b20yqr3zid61yypgy5s0y6w94i3z5p</sha1>
    </revision>
  </page>
  <page>
    <title>Hilbert spaces</title>
    <ns>0</ns>
    <id>128</id>
    <revision>
      <id>415</id>
      <timestamp>2019-06-23T15:24:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "An [[Inner product|inner product space]] &lt;math&gt;V&lt;/math&gt; that is a [[Banach spaces|Banach space]] equipped with the norm (Def 4.1.5&lt;ref name=harville&gt;D. A. Harville, Matrix Alg..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="613">An [[Inner product|inner product space]] &lt;math&gt;V&lt;/math&gt; that is a [[Banach spaces|Banach space]] equipped with the norm (Def 4.1.5&lt;ref name=harville&gt;D. A. Harville, Matrix Algebra from a Statistician's Perspective&lt;/ref&gt;)

&lt;math&gt;||\cdot || = \sqrt{\langle \cdot, \cdot \rangle}&lt;/math&gt; 

is called a Hilbert space. A Hilbert space is typically denoted as &lt;math&gt;\mathcal{H}&lt;/math&gt;. Clearly, by definition Hilbert spaces are a subset of Banach spaces. 

Examples to Hilbert spaces are &lt;math&gt;\mathbb{R}^n, \mathbb C^n&lt;/math&gt; and &lt;math&gt;\ell^2(\mathbb{N})&lt;/math&gt; (see [[Banach spaces]] for the latter).

== References ==</text>
      <sha1>ml6xqfjcrs1mwpqxr94hfi6y1u68e2m</sha1>
    </revision>
  </page>
  <page>
    <title>Functional</title>
    <ns>0</ns>
    <id>129</id>
    <revision>
      <id>416</id>
      <timestamp>2019-06-23T15:30:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "== In Hilbert spaces ==  Let &lt;math&gt;\mathcal H&lt;/math&gt; be a Hilbert space. A linear operator &lt;math&gt;\Phi:\mathcal H \to \mathbb C&lt;/math&gt; is called a functional (Def 4.4.1&lt;ref&gt;O...."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="461">== In Hilbert spaces ==

Let &lt;math&gt;\mathcal H&lt;/math&gt; be a Hilbert space. A linear operator &lt;math&gt;\Phi:\mathcal H \to \mathbb C&lt;/math&gt; is called a functional (Def 4.4.1&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

An interesting property of functionals is that, any [[Bounded linear operator|bounded]] functional can be defined as 

&lt;math&gt;\Phi \mathbf v :=\langle\mathbf{v,w}\rangle&lt;/math&gt; for some &lt;math&gt;\mathbf w&lt;/math&gt;

== References ==</text>
      <sha1>8kp5lr80khc5almm0pdqhtjvyz7qmm5</sha1>
    </revision>
    <revision>
      <id>421</id>
      <parentid>416</parentid>
      <timestamp>2019-06-28T11:48:34Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="551">== In Hilbert spaces ==

Let &lt;math&gt;\mathcal H&lt;/math&gt; be a Hilbert space. A linear operator &lt;math&gt;\Phi:\mathcal H \to \mathbb C&lt;/math&gt; is called a functional (Def 4.4.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

An interesting property of functionals is that, any [[Bounded linear operator|bounded]] functional can be defined as 

&lt;math&gt;\Phi \mathbf v :=\langle\mathbf{v,w}\rangle&lt;/math&gt; for some &lt;math&gt;\mathbf w&lt;/math&gt;

This property is due to Riesz' Representation Theorem (T 4.4.3&lt;ref name=ole/&gt;).

== References ==</text>
      <sha1>cpijwcw1bpgp8cwoxxcnbwcnuwv4q2h</sha1>
    </revision>
  </page>
  <page>
    <title>Hlder's inequality</title>
    <ns>0</ns>
    <id>130</id>
    <redirect title="Inequalities in sequences" />
    <revision>
      <id>417</id>
      <timestamp>2019-06-23T15:38:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "== Inequalities in sequences ==  Consider any scalar sequences &lt;math&gt;\{x_k\}_{k=1}^\infty, \{y_k\}_{k=1}^\infty&lt;/math&gt;, the following inequalities hold (Th. 1.7.3., L1.7.4&lt;ref..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="858">== Inequalities in sequences ==

Consider any scalar sequences &lt;math&gt;\{x_k\}_{k=1}^\infty, \{y_k\}_{k=1}^\infty&lt;/math&gt;, the following inequalities hold (Th. 1.7.3., L1.7.4&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)

(i) (Hlder's inequality) For any numberes &lt;math&gt;p,q \in (1,\infty)&lt;/math&gt; with &lt;math&gt;1/p+1/q=1&lt;/math&gt;:

&lt;math&gt;\sum_{k=1}^\infty|x_k y_k|\le \left(\sum_{k=1}^\infty |x_k|^p\right)^{1/p} \left(\sum_{k=1}^\infty |y_k|^q \right)^{1/q}&lt;/math&gt;

(ii) (Minkowski's inequality) For any &lt;math&gt;p \in [1,\infty)&lt;/math&gt;

&lt;math&gt;\left( \sum_{k=1}^{\infty} |x_k+y_k|^{p} \right)^{1/p} \le \left( \sum_{k=1}^\infty |x_k|^p\right)^{1/p}+\left( \sum_{k=1}^\infty |y_k|^p\right)^{1/p}&lt;/math&gt;

(iii) (Absolute convergence implies convergence)

&lt;math&gt;\left|  \sum_{k=1}^\infty x_k\right|\le \sum_{k=1}^\infty |x_k|&lt;/math&gt;

== References ==</text>
      <sha1>a7eyjjomk6glyjr0g9xmp1aw8ambgr2</sha1>
    </revision>
    <revision>
      <id>419</id>
      <parentid>417</parentid>
      <timestamp>2019-06-23T15:39:57Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Inequalities in sequences]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="39">#REDIRECT [[Inequalities in sequences]]</text>
      <sha1>opf1d9wo99o1c239chheyih603kyon3</sha1>
    </revision>
  </page>
  <page>
    <title>Inequalities in sequences</title>
    <ns>0</ns>
    <id>131</id>
    <revision>
      <id>418</id>
      <timestamp>2019-06-23T15:39:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "== Inequalities in sequences ==  Consider any scalar sequences &lt;math&gt;\{x_k\}_{k=1}^\infty, \{y_k\}_{k=1}^\infty&lt;/math&gt;, the following inequalities hold (Th. 1.7.3., L1.7.4&lt;ref..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="858">== Inequalities in sequences ==

Consider any scalar sequences &lt;math&gt;\{x_k\}_{k=1}^\infty, \{y_k\}_{k=1}^\infty&lt;/math&gt;, the following inequalities hold (Th. 1.7.3., L1.7.4&lt;ref&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)

(i) (Hlder's inequality) For any numberes &lt;math&gt;p,q \in (1,\infty)&lt;/math&gt; with &lt;math&gt;1/p+1/q=1&lt;/math&gt;:

&lt;math&gt;\sum_{k=1}^\infty|x_k y_k|\le \left(\sum_{k=1}^\infty |x_k|^p\right)^{1/p} \left(\sum_{k=1}^\infty |y_k|^q \right)^{1/q}&lt;/math&gt;

(ii) (Minkowski's inequality) For any &lt;math&gt;p \in [1,\infty)&lt;/math&gt;

&lt;math&gt;\left( \sum_{k=1}^{\infty} |x_k+y_k|^{p} \right)^{1/p} \le \left( \sum_{k=1}^\infty |x_k|^p\right)^{1/p}+\left( \sum_{k=1}^\infty |y_k|^p\right)^{1/p}&lt;/math&gt;

(iii) (Absolute convergence implies convergence)

&lt;math&gt;\left|  \sum_{k=1}^\infty x_k\right|\le \sum_{k=1}^\infty |x_k|&lt;/math&gt;

== References ==</text>
      <sha1>a7eyjjomk6glyjr0g9xmp1aw8ambgr2</sha1>
    </revision>
  </page>
  <page>
    <title>Basis</title>
    <ns>0</ns>
    <id>132</id>
    <revision>
      <id>420</id>
      <timestamp>2019-06-28T11:46:48Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "== Finite vs. Infinite spaces ==  === In finite dimensional spaces === A collection of vectors &lt;math&gt;\{\mathbf{e}_k\}_{k=1}^N&lt;/math&gt; (Def. 1.3.2&lt;ref name=ole&gt;O. Christensen, F..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="883">== Finite vs. Infinite spaces ==

=== In finite dimensional spaces ===
A collection of vectors &lt;math&gt;\{\mathbf{e}_k\}_{k=1}^N&lt;/math&gt; (Def. 1.3.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) is a basis for a (complex) vector space &lt;math&gt;V&lt;/math&gt; if &lt;math&gt;\text{span}\{\mathbf{e}_k\}_{k=1}^N = V&lt;/math&gt; and the vectors &lt;math&gt;\{\mathbf{e}_k\}_{k=1}^N&lt;/math&gt; are linearly independent.

=== Basis in normed vector space (possibly infinite dimensional) ===
A collection of vectors &lt;math&gt;\{\mathbf{v}_k\}_{k=1}^N&lt;/math&gt; (Def. 2.5.4&lt;ref name=ole/&gt;) is a basis for a ''normed'' vector space &lt;math&gt;V&lt;/math&gt;  if for each &lt;math&gt;\mathbf{v} \in V&lt;/math&gt; there exists ''unique'' scalar coefficients &lt;math&gt;\{c_k\}_{k=1}^\infty&lt;/math&gt; such that 

&lt;math&gt;\mathbf{v} = \sum_{k=1}^{\infty} c_k \mathbf{v}_k&lt;/math&gt;. This definition is also the Schauder basis.

==References==</text>
      <sha1>5x2w1wt2vmgzvbb9dpk0mhouwvcac8b</sha1>
    </revision>
  </page>
  <page>
    <title>Riesz Representation Theorem</title>
    <ns>0</ns>
    <id>133</id>
    <redirect title="Functional" />
    <revision>
      <id>422</id>
      <timestamp>2019-06-28T11:49:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Functional]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24">#REDIRECT [[Functional]]</text>
      <sha1>2uvrih4gmairx9kq775dpaxo386n4zl</sha1>
    </revision>
  </page>
  <page>
    <title>Support</title>
    <ns>0</ns>
    <id>134</id>
    <revision>
      <id>423</id>
      <timestamp>2019-06-28T11:52:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The support of a function (Def. 5.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) is the smallest set outside which the function is equal to zer..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="447">The support of a function (Def. 5.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) is the smallest set outside which the function is equal to zero:

&lt;math&gt;\text{supp} f= \bar{\{x\in R: f(x) \neq = 0\}}&lt;/math&gt;

If &lt;math&gt;\text{supp}f&lt;/math&gt; is bounded (i.e., contained in an interval in the form [a,b] for some &lt;math&gt;a,b \in \mathbb{R}&lt;/math&gt;), then we say that &lt;math&gt;f&lt;/math&gt; has ''compact support''.

== References ==</text>
      <sha1>9zd9f7ev35edfol7trwv8pyanc3h4xs</sha1>
    </revision>
    <revision>
      <id>425</id>
      <parentid>423</parentid>
      <timestamp>2019-06-28T11:56:53Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1002">The support of a function (Def. 5.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) is the smallest set outside which the function is equal to zero:

&lt;math&gt;\text{supp} f= \bar{\{x\in R: f(x) \neq = 0\}}&lt;/math&gt;

If &lt;math&gt;\text{supp}f&lt;/math&gt; is bounded (i.e., contained in an interval in the form [a,b] for some &lt;math&gt;a,b \in \mathbb{R}&lt;/math&gt;), then we say that &lt;math&gt;f&lt;/math&gt; has ''compact support''.

=== Special vector spaces ===

There are two special vector spaces w.r.t. support. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;



== References ==</text>
      <sha1>3fd3isbup0gh74yp7udh5jzogf2ds1m</sha1>
    </revision>
  </page>
  <page>
    <title>Compact support</title>
    <ns>0</ns>
    <id>135</id>
    <redirect title="Support" />
    <revision>
      <id>424</id>
      <timestamp>2019-06-28T11:53:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Support]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21">#REDIRECT [[Support]]</text>
      <sha1>gde8mh667slbzmx7h4ubhkjnv5lbjhl</sha1>
    </revision>
  </page>
  <page>
    <title>Special vector spaces</title>
    <ns>0</ns>
    <id>136</id>
    <revision>
      <id>426</id>
      <timestamp>2019-06-28T12:00:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Here is a list of special vector spaces   === W.r.t function support === There are two special vector spaces w.r.t. [[support]].  * &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="656">Here is a list of special vector spaces


=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach Spaces ===
* &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt;</text>
      <sha1>gkkdkdsau7ve0cgm6knoig2k601f0ol</sha1>
    </revision>
    <revision>
      <id>427</id>
      <parentid>426</parentid>
      <timestamp>2019-06-29T14:51:42Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1674">Here is a list of special vector spaces

=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach spaces ===
* &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;

=== Some Hilbert spaces ===
* &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;

== Advantages/disadvantages of some spaces ==

Those notes are from Week 6 notes of Real Analysis course of Ole Christensen.

==== The space &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;: ====
* + The assumption of compact support is realistic for applications
* - Not all signals are continuous
* - Not a Banach space

==== The space &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* - The assumption of &lt;math&gt;f(x)\to 0&lt;/math&gt; as &lt;math&gt;x\to\pm\infty&lt;/math&gt; is realistic for applications
* - Not all signals are continuous

==== The space &lt;math&gt;L^1(\mathbb{R})&lt;/math&gt; ====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* - Not a Hilbert space
* - Not a space of equivalence classes

==== The space &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* + Is a Hilbert space
* - Not a space of equivalence classes</text>
      <sha1>r9iuqrcz30jauvjsnf57ltl95jj84hp</sha1>
    </revision>
    <revision>
      <id>429</id>
      <parentid>427</parentid>
      <timestamp>2019-06-29T15:01:09Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1885">Here is a list of special vector spaces

=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach spaces ===
* &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;

=== Some Hilbert spaces ===
* &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;

== Advantages/disadvantages of some spaces ==

Those notes are from Week 6 notes of Real Analysis course of Ole Christensen.

==== The space &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;: ====
* + The assumption of compact support is realistic for applications
* + &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt; is [[dense subset|dense]] in &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt; for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt; (T.5.4.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
* - Not all signals are continuous
* - Not a Banach space

==== The space &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* - The assumption of &lt;math&gt;f(x)\to 0&lt;/math&gt; as &lt;math&gt;x\to\pm\infty&lt;/math&gt; is realistic for applications
* - Not all signals are continuous

==== The space &lt;math&gt;L^1(\mathbb{R})&lt;/math&gt; ====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* - Not a Hilbert space
* - Not a space of equivalence classes

==== The space &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* + Is a Hilbert space
* - Not a space of equivalence classes</text>
      <sha1>kxolrqhnrpv9yssnqu7oc0la3k7ua0i</sha1>
    </revision>
    <revision>
      <id>430</id>
      <parentid>429</parentid>
      <timestamp>2019-06-29T15:01:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1903">Here is a list of special vector spaces

=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach spaces ===
* &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;

=== Some Hilbert spaces ===
* &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;

== Advantages/disadvantages of some spaces ==

Those notes are from Week 6 notes of Real Analysis course of Ole Christensen.

==== The space &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;: ====
* + The assumption of compact support is realistic for applications
* + &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt; is [[dense subset|dense]] in &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt; for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt; (T.5.4.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
* - Not all signals are continuous
* - Not a Banach space

==== The space &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* - The assumption of &lt;math&gt;f(x)\to 0&lt;/math&gt; as &lt;math&gt;x\to\pm\infty&lt;/math&gt; is realistic for applications
* - Not all signals are continuous

==== The space &lt;math&gt;L^1(\mathbb{R})&lt;/math&gt; ====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* - Not a Hilbert space
* - Not a space of equivalence classes

==== The space &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* + Is a Hilbert space
* - Not a space of equivalence classes

== References ==</text>
      <sha1>tk873yuuyf79vbdvhkub66b8trszumo</sha1>
    </revision>
    <revision>
      <id>441</id>
      <parentid>430</parentid>
      <timestamp>2019-07-04T16:48:02Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2135">Here is a list of special vector spaces

=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach spaces ===
* &lt;math&gt;L^p(\mathbb{R}):=\left\{f:\mathbb R \to \mathbb C \mid \int_{-\infty}^{\infty}|f(x)|^p dx &lt; \infty \right\}&lt;/math&gt; for &lt;math&gt;p \in [1,\infty)&lt;/math&gt;
* &lt;math&gt;L^\infty(\mathbb R) := \left\{f : \mathbb R \to \mathbb C | f \text{ is bounded } \right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;

=== Some Hilbert spaces ===
* &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;

== Advantages/disadvantages of some spaces ==

Those notes are from Week 6 notes of Real Analysis course of Ole Christensen.

==== The space &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;: ====
* + The assumption of compact support is realistic for applications
* + &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt; is [[dense subset|dense]] in &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt; for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt; (T.5.4.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
* - Not all signals are continuous
* - Not a Banach space

==== The space &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* - The assumption of &lt;math&gt;f(x)\to 0&lt;/math&gt; as &lt;math&gt;x\to\pm\infty&lt;/math&gt; is realistic for applications
* - Not all signals are continuous

==== The space &lt;math&gt;L^1(\mathbb{R})&lt;/math&gt; ====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* - Not a Hilbert space
* - Not a space of equivalence classes

==== The space &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* + Is a Hilbert space
* - Not a space of equivalence classes

== References ==</text>
      <sha1>4388j6wtyzf8fu1tbzrcjtfoksp4auj</sha1>
    </revision>
    <revision>
      <id>442</id>
      <parentid>441</parentid>
      <timestamp>2019-07-04T16:48:53Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2137">Here is a list of special vector spaces

=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{ is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{ is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach spaces ===
* &lt;math&gt;L^p(\mathbb{R}):=\left\{f:\mathbb R \to \mathbb C \mid \int_{-\infty}^{\infty}|f(x)|^p dx &lt; \infty \right\}&lt;/math&gt; for &lt;math&gt;p \in [1,\infty)&lt;/math&gt;
* &lt;math&gt;L^\infty(\mathbb R) := \left\{f : \mathbb R \to \mathbb C | f \text{ is bounded } \right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;

=== Some Hilbert spaces ===
* &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;

== Advantages/disadvantages of some spaces ==

Those notes are from Week 6 notes of Real Analysis course of Ole Christensen.

==== The space &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;: ====
* + The assumption of compact support is realistic for applications
* + &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt; is [[dense subset|dense]] in &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt; for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt; (T.5.4.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
* - Not all signals are continuous
* - Not a Banach space

==== The space &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* - The assumption of &lt;math&gt;f(x)\to 0&lt;/math&gt; as &lt;math&gt;x\to\pm\infty&lt;/math&gt; is realistic for applications
* - Not all signals are continuous

==== The space &lt;math&gt;L^1(\mathbb{R})&lt;/math&gt; ====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* - Not a Hilbert space
* - Not a space of equivalence classes

==== The space &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* + Is a Hilbert space
* - Not a space of equivalence classes

== References ==</text>
      <sha1>26x2tud0hxbfodl9wo17npdw7cnwaqa</sha1>
    </revision>
    <revision>
      <id>444</id>
      <parentid>442</parentid>
      <timestamp>2019-07-04T17:09:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2384">Here is a list of special vector spaces

=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{ is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{ is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach spaces ===
* &lt;math&gt;L^p(\mathbb{R}):=\left\{f:\mathbb R \to \mathbb C \mid \int_{-\infty}^{\infty}|f(x)|^p dx &lt; \infty \right\}&lt;/math&gt; for &lt;math&gt;p \in [1,\infty)&lt;/math&gt;
* &lt;math&gt;L^\infty(\mathbb R) := \left\{f : \mathbb R \to \mathbb C | f \text{ is bounded } \right\}&lt;/math&gt;
* The space of continuous functions on a bounded interval &lt;math&gt;[a,b]&lt;/math&gt;: &lt;math&gt;C[a,b]:=\{f[a,b]\to \mathbb C | f \text{ is continuous }\}&lt;/math&gt; (w.r.t. norm &lt;math&gt;||f||_\infty = max_{x\in[a,b]} |f(x)|&lt;/math&gt; --- see Th. 3.1.6&lt;ref name=ole&gt;)
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;

=== Some Hilbert spaces ===
* &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;

== Advantages/disadvantages of some spaces ==

Those notes are from Week 6 notes of Real Analysis course of Ole Christensen.

==== The space &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;: ====
* + The assumption of compact support is realistic for applications
* + &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt; is [[dense subset|dense]] in &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt; for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt; (T.5.4.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
* - Not all signals are continuous
* - Not a Banach space

==== The space &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* - The assumption of &lt;math&gt;f(x)\to 0&lt;/math&gt; as &lt;math&gt;x\to\pm\infty&lt;/math&gt; is realistic for applications
* - Not all signals are continuous

==== The space &lt;math&gt;L^1(\mathbb{R})&lt;/math&gt; ====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* - Not a Hilbert space
* - Not a space of equivalence classes

==== The space &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* + Is a Hilbert space
* - Not a space of equivalence classes

== References ==</text>
      <sha1>gk6tm3gw6y4x8ek99ih9rlhbjz3srj1</sha1>
    </revision>
    <revision>
      <id>445</id>
      <parentid>444</parentid>
      <timestamp>2019-07-04T17:09:17Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2385">Here is a list of special vector spaces

=== W.r.t function support ===
There are two special vector spaces w.r.t. [[support]]. 
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;, the space of all continuous functions with compact support: &lt;math&gt;C_c(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{ is continuous and has compact support}\right\}&lt;/math&gt;
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;, the space of all continuous functions that tend to zero as &lt;math&gt;x\to\pm\infty&lt;/math&gt;: &lt;math&gt;C_0(\mathbb{R}):=\left\{f:\mathbb{R}\to\mathbb{C} : f \text{ is continuous and } f(x)\to 0 \text{ as } x\to\pm\infty\right\}&lt;/math&gt;

=== Some Banach spaces ===
* &lt;math&gt;L^p(\mathbb{R}):=\left\{f:\mathbb R \to \mathbb C \mid \int_{-\infty}^{\infty}|f(x)|^p dx &lt; \infty \right\}&lt;/math&gt; for &lt;math&gt;p \in [1,\infty)&lt;/math&gt;
* &lt;math&gt;L^\infty(\mathbb R) := \left\{f : \mathbb R \to \mathbb C | f \text{ is bounded } \right\}&lt;/math&gt;
* The space of continuous functions on a bounded interval &lt;math&gt;[a,b]&lt;/math&gt;: &lt;math&gt;C[a,b]:=\{f[a,b]\to \mathbb C | f \text{ is continuous }\}&lt;/math&gt; (w.r.t. norm &lt;math&gt;||f||_\infty = max_{x\in[a,b]} |f(x)|&lt;/math&gt; --- see Th. 3.1.6&lt;ref name=ole/&gt;)
* &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;

=== Some Hilbert spaces ===
* &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;

== Advantages/disadvantages of some spaces ==

Those notes are from Week 6 notes of Real Analysis course of Ole Christensen.

==== The space &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt;: ====
* + The assumption of compact support is realistic for applications
* + &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt; is [[dense subset|dense]] in &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt; for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt; (T.5.4.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
* - Not all signals are continuous
* - Not a Banach space

==== The space &lt;math&gt;C_0(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* - The assumption of &lt;math&gt;f(x)\to 0&lt;/math&gt; as &lt;math&gt;x\to\pm\infty&lt;/math&gt; is realistic for applications
* - Not all signals are continuous

==== The space &lt;math&gt;L^1(\mathbb{R})&lt;/math&gt; ====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* - Not a Hilbert space
* - Not a space of equivalence classes

==== The space &lt;math&gt;L^2(\mathbb{R})&lt;/math&gt;====
* + Is a Banach space
* + Contains non-continuous functions too
* + Realistic for applications
* + Is a Hilbert space
* - Not a space of equivalence classes

== References ==</text>
      <sha1>fpx7913sd1mlvf1sj2gbaxsdl6rlcgf</sha1>
    </revision>
  </page>
  <page>
    <title>Dense subset</title>
    <ns>0</ns>
    <id>137</id>
    <revision>
      <id>428</id>
      <timestamp>2019-06-29T14:58:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The concept of denseness is at the center of approximation theory. That is, a space &lt;math&gt;W \subseteq V&lt;/math&gt; may not contain all elements of &lt;math&gt;V&lt;/math&gt;, but if &lt;math&gt;W&lt;/..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="705">The concept of denseness is at the center of approximation theory. That is, a space &lt;math&gt;W \subseteq V&lt;/math&gt; may not contain all elements of &lt;math&gt;V&lt;/math&gt;, but if &lt;math&gt;W&lt;/math&gt; is dense in &lt;math&gt;V&lt;/math&gt;, then any element of &lt;math&gt;V&lt;/math&gt; can be approximated arbitrarily well by elements of &lt;math&gt;W&lt;/math&gt;.

Formally, a subset &lt;math&gt;W&lt;/math&gt; of a normed vector space &lt;math&gt;V&lt;/math&gt; is said to be ''dense'' in &lt;math&gt;V&lt;/math&gt; if for each &lt;math&gt;\mathbf{v} \in V&lt;/math&gt; and each &lt;math&gt;\epsilon &gt; 0&lt;/math&gt; there exists &lt;math&gt;\mathbf{w}\in W&lt;/math&gt; such that (Def. 2.3.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
&lt;math&gt;||\mathbf{w-v}||\le \epsilon&lt;/math&gt;.

== References ==</text>
      <sha1>1uw5hbnlujna3kpr5ui3wmbjcfqgaoq</sha1>
    </revision>
    <revision>
      <id>454</id>
      <parentid>428</parentid>
      <timestamp>2019-07-04T17:42:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1127">The concept of denseness is at the center of approximation theory. That is, a space &lt;math&gt;W \subseteq V&lt;/math&gt; may not contain all elements of &lt;math&gt;V&lt;/math&gt;, but if &lt;math&gt;W&lt;/math&gt; is dense in &lt;math&gt;V&lt;/math&gt;, then any element of &lt;math&gt;V&lt;/math&gt; can be approximated arbitrarily well by elements of &lt;math&gt;W&lt;/math&gt;.

Formally, a subset &lt;math&gt;W&lt;/math&gt; of a normed vector space &lt;math&gt;V&lt;/math&gt; is said to be ''dense'' in &lt;math&gt;V&lt;/math&gt; if for each &lt;math&gt;\mathbf{v} \in V&lt;/math&gt; and each &lt;math&gt;\epsilon &gt; 0&lt;/math&gt; there exists &lt;math&gt;\mathbf{w}\in W&lt;/math&gt; such that (Def. 2.3.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)
&lt;math&gt;||\mathbf{w-v}||\le \epsilon&lt;/math&gt;.

=== Examples ===

* The set of polynomials is dense in &lt;math&gt;L^2(a,b)&lt;/math&gt; -- see Th. 6.3.1&lt;ref name=ole/&gt;.
* The set of polynomials is dense in &lt;math&gt;C[a,b]&lt;/math&gt; (see [[Weierstrass' theorem]] and [[Special vector spaces]]).
* &lt;math&gt;C_c(\mathbb{R})&lt;/math&gt; is dense in &lt;math&gt;L^p(\mathbb{R})&lt;/math&gt; for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt; (T.5.4.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)

== References ==</text>
      <sha1>blgl194af6r7rpx01qb8fun60kr3vy2</sha1>
    </revision>
  </page>
  <page>
    <title>Adjoint</title>
    <ns>0</ns>
    <id>138</id>
    <revision>
      <id>431</id>
      <timestamp>2019-06-29T15:09:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Under a Hilbert space &lt;math&gt;\mathcal H&lt;/math&gt;, an adjoint operator is defined in terms of inner product. Let &lt;math&gt;T&lt;/math&gt; be a Bounded linear operator|bounded linear opera..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="613">Under a Hilbert space &lt;math&gt;\mathcal H&lt;/math&gt;, an adjoint operator is defined in terms of inner product. Let &lt;math&gt;T&lt;/math&gt; be a [[Bounded linear operator|bounded linear operator]]. Then, there exists (T.4.4.3&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) an operator &lt;math&gt;T^*&lt;/math&gt; such that 

 &lt;math&gt;\langle T\mathbf{v}, \mathbf{w}\rangle_{\mathcal H} = \langle \mathbf{v}, T^*\mathbf{w} \rangle.&lt;/math&gt;

This operator is called the adjoint operator.

=== Self-adjoint operators ===
If &lt;math&gt;T = T^*&lt;/math&gt;, then &lt;math&gt;T&lt;/math&gt; is called a self-adjoint operator.

== References ==</text>
      <sha1>a68b9dpwzry3jkejntw5wg2eq7o10ig</sha1>
    </revision>
    <revision>
      <id>432</id>
      <parentid>431</parentid>
      <timestamp>2019-06-29T15:12:03Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="887">Under a Hilbert space &lt;math&gt;\mathcal H&lt;/math&gt;, an adjoint operator is defined in terms of inner product. Let &lt;math&gt;T&lt;/math&gt; be a [[Bounded linear operator|bounded linear operator]]. Then, there exists (T.4.4.3&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) an operator &lt;math&gt;T^*&lt;/math&gt; such that 

 &lt;math&gt;\langle T\mathbf{v}, \mathbf{w}\rangle_{\mathcal H} = \langle \mathbf{v}, T^*\mathbf{w} \rangle.&lt;/math&gt;

This operator is called the adjoint operator.

=== Self-adjoint operators ===
If &lt;math&gt;T = T^*&lt;/math&gt;, then &lt;math&gt;T&lt;/math&gt; is called a self-adjoint operator (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

=== Unitary operators ===
If  &lt;math&gt;TT^* = T^*T  = I&lt;/math&gt;, then  &lt;math&gt;T&lt;/math&gt; is called ''unitary'' (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

== References ==</text>
      <sha1>0q7tfsnu7qgmolu1vg0mxdsgevyj2y2</sha1>
    </revision>
    <revision>
      <id>435</id>
      <parentid>432</parentid>
      <timestamp>2019-06-29T15:18:53Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1064">Under a Hilbert space &lt;math&gt;\mathcal H&lt;/math&gt;, an adjoint operator is defined in terms of inner product. Let &lt;math&gt;T&lt;/math&gt; be a [[Bounded linear operator|bounded linear operator]]. Then, there exists (T.4.4.3&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) an operator &lt;math&gt;T^*&lt;/math&gt; such that 

 &lt;math&gt;\langle T\mathbf{v}, \mathbf{w}\rangle_{\mathcal H} = \langle \mathbf{v}, T^*\mathbf{w} \rangle.&lt;/math&gt;

This operator is called the adjoint operator.

If the operator &lt;math&gt;T&lt;/math&gt; is a matrix, then &lt;math&gt;T^*&lt;/math&gt; is the Hermitian transpose (i.e., conjugate transpose) of &lt;math&gt;T&lt;/math&gt;; that is, &lt;math&gt;T^*=\bar{T}^T&lt;/math&gt;

=== Self-adjoint operators ===
If &lt;math&gt;T = T^*&lt;/math&gt;, then &lt;math&gt;T&lt;/math&gt; is called a self-adjoint operator (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

=== Unitary operators ===
If  &lt;math&gt;TT^* = T^*T  = I&lt;/math&gt;, then  &lt;math&gt;T&lt;/math&gt; is called ''unitary'' (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

== References ==</text>
      <sha1>6mt59san0cligbku1cefk2c32zwn2dh</sha1>
    </revision>
    <revision>
      <id>436</id>
      <parentid>435</parentid>
      <timestamp>2019-06-29T15:23:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1077">Under a Hilbert space &lt;math&gt;\mathcal H&lt;/math&gt;, an adjoint operator is defined in terms of inner product. Let &lt;math&gt;T&lt;/math&gt; be a [[Bounded linear operator|bounded linear operator]]. Then, there exists (T.4.4.3&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) an operator &lt;math&gt;T^*&lt;/math&gt; such that 

 &lt;math&gt;\langle T\mathbf{v}, \mathbf{w}\rangle_{\mathcal H} = \langle \mathbf{v}, T^*\mathbf{w} \rangle_{\mathcal H}.&lt;/math&gt;

This operator is called the adjoint operator.

If the operator &lt;math&gt;T&lt;/math&gt; is a matrix, then &lt;math&gt;T^*&lt;/math&gt; is the Hermitian transpose (i.e., conjugate transpose) of &lt;math&gt;T&lt;/math&gt;; that is, &lt;math&gt;T^*=\bar{T}^T&lt;/math&gt;

=== Self-adjoint operators ===
If &lt;math&gt;T = T^*&lt;/math&gt;, then &lt;math&gt;T&lt;/math&gt; is called a self-adjoint operator (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

=== Unitary operators ===
If  &lt;math&gt;TT^* = T^*T  = I&lt;/math&gt;, then  &lt;math&gt;T&lt;/math&gt; is called ''unitary'' (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

== References ==</text>
      <sha1>dgokd44bgbmi693228cqw21ry1neqg9</sha1>
    </revision>
    <revision>
      <id>461</id>
      <parentid>436</parentid>
      <timestamp>2019-07-06T13:41:51Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1252">Under a Hilbert space &lt;math&gt;\mathcal H&lt;/math&gt;, an adjoint operator is defined in terms of inner product. Let &lt;math&gt;T&lt;/math&gt; be a [[Bounded linear operator|bounded linear operator]]. Then, there exists (T.4.4.3&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;) an operator &lt;math&gt;T^*&lt;/math&gt; such that 

 &lt;math&gt;\langle T\mathbf{v}, \mathbf{w}\rangle_{\mathcal H} = \langle \mathbf{v}, T^*\mathbf{w} \rangle_{\mathcal H}.&lt;/math&gt;

This operator is called the adjoint operator.

If the operator &lt;math&gt;T&lt;/math&gt; is a matrix, then &lt;math&gt;T^*&lt;/math&gt; is the Hermitian transpose (i.e., conjugate transpose) of &lt;math&gt;T&lt;/math&gt;; that is, &lt;math&gt;T^*=\bar{T}^T&lt;/math&gt;

=== Self-adjoint operators ===
If &lt;math&gt;T = T^*&lt;/math&gt;, then &lt;math&gt;T&lt;/math&gt; is called a self-adjoint operator (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

=== Unitary operators ===
If  &lt;math&gt;TT^* = T^*T  = I&lt;/math&gt;, then  &lt;math&gt;T&lt;/math&gt; is called ''unitary'' (D.4.5.4&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

=== (Not so) special cases===
If &lt;math&gt;T&lt;/math&gt; is a [[bounded linear operator]], then (L4.5.2&lt;ref name=ole/&gt;)
# &lt;math&gt; (T^*)^* = T &lt;/math&gt;
# &lt;math&gt;||T || = || T^*|| &lt;/math&gt;

== References ==</text>
      <sha1>nrqpj9y462r54zxb54ma0mn4fpbqw6r</sha1>
    </revision>
  </page>
  <page>
    <title>Self-adjoint</title>
    <ns>0</ns>
    <id>139</id>
    <redirect title="Adjoint" />
    <revision>
      <id>433</id>
      <timestamp>2019-06-29T15:12:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Adjoint]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21">#REDIRECT [[Adjoint]]</text>
      <sha1>2z8tts3ppv7tsnnv2avvloneymsiirk</sha1>
    </revision>
  </page>
  <page>
    <title>Unitary</title>
    <ns>0</ns>
    <id>140</id>
    <redirect title="Adjoint" />
    <revision>
      <id>434</id>
      <timestamp>2019-06-29T15:14:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Adjoint]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21">#REDIRECT [[Adjoint]]</text>
      <sha1>2z8tts3ppv7tsnnv2avvloneymsiirk</sha1>
    </revision>
  </page>
  <page>
    <title>Operator extension</title>
    <ns>0</ns>
    <id>141</id>
    <revision>
      <id>438</id>
      <timestamp>2019-07-04T15:33:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Whenever a space &lt;math&gt;W&lt;/math&gt; is [[Dense subset|dense]] in another space &lt;math&gt;V&lt;/math&gt;, a natural question to ask is whether the an operator defined in &lt;math&gt;W&lt;/math&gt; can b..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1267">Whenever a space &lt;math&gt;W&lt;/math&gt; is [[Dense subset|dense]] in another space &lt;math&gt;V&lt;/math&gt;, a natural question to ask is whether the an operator defined in &lt;math&gt;W&lt;/math&gt; can be extended to &lt;math&gt;V&lt;/math&gt;. This is possible for [[bounded linear operator]] as the theorem below specifies (Th. 3.3.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be [[Banach spaces]], &lt;math&gt;W&lt;/math&gt; a [[Dense subset|dense]] subspace of &lt;math&gt;V_1&lt;/math&gt;, and &lt;math&gt;T:W\to V_2&lt;/math&gt; a bounded linear operator. Then, there exists a ''unique'' bounded linear operator

 &lt;math&gt;\tilde{T}:V_1\to V_2&lt;/math&gt;

for which &lt;math&gt;\tilde{T}\mathbf{v} =  T \mathbf{v}&lt;/math&gt; for all &lt;math&gt;\mathbf{w} in W&lt;/math&gt;. The operator &lt;math&gt;\tilde{T}&lt;/math&gt; satisfies &lt;math&gt;||\tilde{T}|| =||T||&lt;/math&gt;.

As the theorem states, the behavior of &lt;math&gt;\tilde T&lt;/math&gt; is obvious for &lt;math&gt;\mathbf v in W&lt;/math&gt;. If &lt;math&gt;\mathbf v \in V_1\setminus W&lt;/math&gt;, then

&lt;math&gt;\tilde T \mathbf{v} := \lim_{k\to \infty} T\mathbf{v}_k&lt;/math&gt;,

where &lt;math&gt;\{\mathbf v_k\}_k&lt;/math&gt; is a sequence in &lt;math&gt;W&lt;/math&gt; that converges to &lt;math&gt;V_1&lt;/math&gt;. (The existence of such a sequence is guaranteed by [[Dense subset|denseness]]).

  
== References ==</text>
      <sha1>2dhgg0drtxdt83f8g7u1fw6mkmb5uku</sha1>
    </revision>
    <revision>
      <id>439</id>
      <parentid>438</parentid>
      <timestamp>2019-07-04T15:33:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1266">Whenever a space &lt;math&gt;W&lt;/math&gt; is [[Dense subset|dense]] in another space &lt;math&gt;V&lt;/math&gt;, a natural question to ask is whether the an operator defined in &lt;math&gt;W&lt;/math&gt; can be extended to &lt;math&gt;V&lt;/math&gt;. This is possible for [[bounded linear operator]] as the theorem below specifies (Th. 3.3.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be [[Banach spaces]], &lt;math&gt;W&lt;/math&gt; a [[Dense subset|dense]] subspace of &lt;math&gt;V_1&lt;/math&gt;, and &lt;math&gt;T:W\to V_2&lt;/math&gt; a bounded linear operator. Then, there exists a ''unique'' bounded linear operator

 &lt;math&gt;\tilde{T}:V_1\to V_2&lt;/math&gt;

for which &lt;math&gt;\tilde{T}\mathbf{v} =  T \mathbf{v}&lt;/math&gt; for all &lt;math&gt;\mathbf{w} in W&lt;/math&gt;. The operator &lt;math&gt;\tilde{T}&lt;/math&gt; satisfies &lt;math&gt;||\tilde{T}|| =||T||&lt;/math&gt;.

As the theorem states, the behavior of &lt;math&gt;\tilde T&lt;/math&gt; is obvious for &lt;math&gt;\mathbf v in W&lt;/math&gt;. If &lt;math&gt;\mathbf v \in V_1\setminus W&lt;/math&gt;, then

&lt;math&gt;\tilde T \mathbf{v} := \lim_{k\to \infty} T\mathbf{v}_k&lt;/math&gt;,

where &lt;math&gt;\{\mathbf v_k\}_k&lt;/math&gt; is a sequence in &lt;math&gt;W&lt;/math&gt; that converges to &lt;math&gt;V_1&lt;/math&gt;. (The existence of such a sequence is guaranteed by [[Dense subset|denseness]]).
  
== References ==</text>
      <sha1>7zrjyixc8yz8a7sjqg67ehqyzzp6rcv</sha1>
    </revision>
    <revision>
      <id>440</id>
      <parentid>439</parentid>
      <timestamp>2019-07-04T15:39:12Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1268">Whenever a space &lt;math&gt;W&lt;/math&gt; is [[Dense subset|dense]] in another space &lt;math&gt;V&lt;/math&gt;, a natural question to ask is whether the an operator defined in &lt;math&gt;W&lt;/math&gt; can be extended to &lt;math&gt;V&lt;/math&gt;. This is possible for [[bounded linear operator]]s as the theorem below specifies (Th. 3.3.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

Let &lt;math&gt;V_1&lt;/math&gt; and &lt;math&gt;V_2&lt;/math&gt; be [[Banach spaces]], &lt;math&gt;W&lt;/math&gt; a [[Dense subset|dense]] subspace of &lt;math&gt;V_1&lt;/math&gt;, and &lt;math&gt;T:W\to V_2&lt;/math&gt; a bounded linear operator. Then, there exists a ''unique'' bounded linear operator

 &lt;math&gt;\tilde{T}:V_1\to V_2&lt;/math&gt;

for which &lt;math&gt;\tilde{T}\mathbf{v} =  T \mathbf{v}&lt;/math&gt; for all &lt;math&gt;\mathbf{w} in W&lt;/math&gt;. The operator &lt;math&gt;\tilde{T}&lt;/math&gt; satisfies &lt;math&gt;||\tilde{T}|| =||T||&lt;/math&gt;.

As the theorem states, the behavior of &lt;math&gt;\tilde T&lt;/math&gt; is obvious for &lt;math&gt;\mathbf v \in W&lt;/math&gt;. If &lt;math&gt;\mathbf v \in V_1\setminus W&lt;/math&gt;, then

&lt;math&gt;\tilde T \mathbf{v} := \lim_{k\to \infty} T\mathbf{v}_k&lt;/math&gt;,

where &lt;math&gt;\{\mathbf v_k\}_k&lt;/math&gt; is a sequence in &lt;math&gt;W&lt;/math&gt; that converges to &lt;math&gt;V_1&lt;/math&gt;. (The existence of such a sequence is guaranteed by [[Dense subset|denseness]]).
  
== References ==</text>
      <sha1>7uwd8y3pcn6spbiqpolo8vbqumcvfi2</sha1>
    </revision>
  </page>
  <page>
    <title>Weierstrass' theorem</title>
    <ns>0</ns>
    <id>142</id>
    <revision>
      <id>443</id>
      <timestamp>2019-07-04T16:51:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Weierstrass' theorem is an interesting theorem that says that any continuous function can be approximated arbitrarily well by polynomials (Th 2.3.4 &lt;ref name=ole&gt;O. Christense..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="555">Weierstrass' theorem is an interesting theorem that says that any continuous function can be approximated arbitrarily well by polynomials (Th 2.3.4 &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

To compare with Taylor series, which also approximates continuous functions; Weierstrass' does not require differentiability. Taylor series, however, is able to give approximation coefficients at the assuming differential functions. Obtaining approximation coefficients is not trivial with Weierstrass' theorem.

== References ==</text>
      <sha1>lh5p2vcwav1ftrsum1lgqfykc1f3viz</sha1>
    </revision>
  </page>
  <page>
    <title>Fourier transform</title>
    <ns>0</ns>
    <id>143</id>
    <revision>
      <id>446</id>
      <timestamp>2019-07-04T17:12:51Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The Fourier transform associated to each function &lt;math&gt;f \in L^1(\mathbb R)&lt;/math&gt; a new function &lt;math&gt;\hat f:\mathbb R\to C&lt;/math&gt; given by (D.7.1.1&lt;ref name=ole&gt;O. Christe..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="447">The Fourier transform associated to each function &lt;math&gt;f \in L^1(\mathbb R)&lt;/math&gt; a new function &lt;math&gt;\hat f:\mathbb R\to C&lt;/math&gt; given by (D.7.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)

 &lt;math&gt;\hat f(\gamma) := \int_{-\infty}^\infty f(x) e^{-2\pi i x \gamma} dx, \,\,\gamma \in \mathbb R&lt;/math&gt;.

The Fourier transform is also denoted as an operator:
 &lt;math&gt;(\mathcal F f)(\gamma) := \hat f(\gamma)&lt;/math&gt;</text>
      <sha1>ioet3c73zl6chm8wt5edpjdt3834836</sha1>
    </revision>
    <revision>
      <id>447</id>
      <parentid>446</parentid>
      <timestamp>2019-07-04T17:17:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="714">The Fourier transform (FT) associated to each function &lt;math&gt;f \in L^1(\mathbb R)&lt;/math&gt; a new function &lt;math&gt;\hat f:\mathbb R\to C&lt;/math&gt; given by (D.7.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)

 &lt;math&gt;\hat f(\gamma) := \int_{-\infty}^\infty f(x) e^{-2\pi i x \gamma} dx, \,\,\gamma \in \mathbb R&lt;/math&gt;.

FT is linear and bounded.

The FT is also denoted as an operator:

 &lt;math&gt;(\mathcal F f)(\gamma) := \hat f(\gamma)&lt;/math&gt;

The FT can be [[Operator extension|extended]] to an operator from &lt;math&gt;L^2(\mathbb R) \to L^2(\mathbb R)&lt;/math&gt;. This extension satisfies the following:
* Is [[Unitary|unitary]]
* ||\hat f||_2 = ||f||_2
* &lt;\hat f,\hat g&gt; = &lt;f,g&gt;

== References ==</text>
      <sha1>3pvun5povmytttmiiq9utmeqo6gchq5</sha1>
    </revision>
    <revision>
      <id>448</id>
      <parentid>447</parentid>
      <timestamp>2019-07-04T17:17:34Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="740">The Fourier transform (FT) associated to each function &lt;math&gt;f \in L^1(\mathbb R)&lt;/math&gt; a new function &lt;math&gt;\hat f:\mathbb R\to C&lt;/math&gt; given by (D.7.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;)

 &lt;math&gt;\hat f(\gamma) := \int_{-\infty}^\infty f(x) e^{-2\pi i x \gamma} dx, \,\,\gamma \in \mathbb R&lt;/math&gt;.

FT is linear and bounded.

The FT is also denoted as an operator:

 &lt;math&gt;(\mathcal F f)(\gamma) := \hat f(\gamma)&lt;/math&gt;

The FT can be [[Operator extension|extended]] to an operator from &lt;math&gt;L^2(\mathbb R) \to L^2(\mathbb R)&lt;/math&gt;. This extension satisfies the following:
* Is [[Unitary|unitary]]
* &lt;math&gt;||\hat f||_2 = ||f||_2&lt;/math&gt;
* &lt;math&gt;&lt;\hat f,\hat g&gt; = &lt;f,g&gt;&lt;/math&gt;

== References ==</text>
      <sha1>c373av7p5k4b0vw2t8iln9c8hcq89kn</sha1>
    </revision>
  </page>
  <page>
    <title>Parseval's theorem</title>
    <ns>0</ns>
    <id>144</id>
    <revision>
      <id>449</id>
      <timestamp>2019-07-04T17:25:08Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Parseval's theorem says that the norm of a function can be recovered by its Fourier coefficients (L.6.4.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="689">Parseval's theorem says that the norm of a function can be recovered by its Fourier coefficients (L.6.4.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;):

&lt;math&gt;\frac{1}{2\pi} = \int_{-\pi}^\pi |f(x)|^2 dx = \sum_{k\in \mathbb Z} |c_k|^2&lt;/math&gt;

where 

&lt;math&gt;c_k:= \frac{1}{2\pi}\int_{-\pi}^\pi f(x) e ^{ikx}dx&lt;/math&gt;.

But in fact Parseval's theorem is a more general theorem  -- it says that, given any orthonormal basis for a space &lt;math&gt;V&lt;/math&gt;, the norm of an element in &lt;math&gt;V&lt;/math&gt; can be recovered by its coefficients wrt this basis (see (iv) in Th. 4.7.2&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;).

== References ==</text>
      <sha1>2119hcqb89fkp5r7h1pzjfnajozqdsn</sha1>
    </revision>
  </page>
  <page>
    <title>Translation operator</title>
    <ns>0</ns>
    <id>145</id>
    <revision>
      <id>450</id>
      <timestamp>2019-07-04T17:30:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The translation operator is defined on &lt;math&gt;L^(\mathbb R)&lt;/math&gt; (see Def. 6.2.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;; not sure if it can..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="488">The translation operator is defined on &lt;math&gt;L^(\mathbb R)&lt;/math&gt; (see Def. 6.2.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;; not sure if it can be defined for other &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces too). By definition, for a given &lt;math&gt;a &lt;/math&gt; this operator acts as:

 &lt;math&gt;(T_a f)(x):=f(x-a), \,x\in \mathbb R.&lt;/math&gt;

&lt;math&gt;T_a&lt;/math&gt; is a [[Unitary|unitary]] operator and it holds that
* &lt;math&gt;T_a^{-1} = T_{-a} = (T_a)^*&lt;/math&gt;.

== References ==</text>
      <sha1>qgk4k1b0wis8abcjxdmfg5yvtbex6mq</sha1>
    </revision>
    <revision>
      <id>451</id>
      <parentid>450</parentid>
      <timestamp>2019-07-04T17:31:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="505">The translation operator &lt;math&gt;T_a&lt;/math&gt; is defined on &lt;math&gt;L^(\mathbb R)&lt;/math&gt; (see Def. 6.2.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;; not sure if it can be defined for other &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces too). By definition, for a given &lt;math&gt;a &lt;/math&gt; this operator acts as:

 &lt;math&gt;(T_a f)(x):=f(x-a), \,x\in \mathbb R.&lt;/math&gt;

&lt;math&gt;T_a&lt;/math&gt; is a [[Unitary|unitary]] operator and it holds that
* &lt;math&gt;T_a^{-1} = T_{-a} = (T_a)^*&lt;/math&gt;.

== References ==</text>
      <sha1>gi0l79tcc8mtkvs6jzyrgpnqst1913g</sha1>
    </revision>
  </page>
  <page>
    <title>Modulation operator</title>
    <ns>0</ns>
    <id>146</id>
    <revision>
      <id>452</id>
      <timestamp>2019-07-04T17:32:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The modulation operator &lt;math&gt;E_b&lt;/math&gt; is defined on &lt;math&gt;L^(\mathbb R)&lt;/math&gt; (see Def. 6.2.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;; no..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="515">The modulation operator &lt;math&gt;E_b&lt;/math&gt; is defined on &lt;math&gt;L^(\mathbb R)&lt;/math&gt; (see Def. 6.2.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;; not sure if it can be defined for other &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces too). By definition, for a given &lt;math&gt;b&lt;/math&gt; this operator acts as:

 &lt;math&gt;(E_b f)(x):=e^{-2\pi ibx} f(x), \,x\in \mathbb R.&lt;/math&gt;

&lt;math&gt;E_b&lt;/math&gt; is a [[Unitary|unitary]] operator and it holds that
* &lt;math&gt;E_b^{-1} = E_{-b} = (E_b)^*&lt;/math&gt;.

== References ==</text>
      <sha1>kdba2az1chukhzm6ju5qt6g1kmug8qu</sha1>
    </revision>
  </page>
  <page>
    <title>Dilation operator</title>
    <ns>0</ns>
    <id>147</id>
    <revision>
      <id>453</id>
      <timestamp>2019-07-04T17:34:21Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The dilation operator &lt;math&gt;D_c&lt;/math&gt; is defined on &lt;math&gt;L^(\mathbb R)&lt;/math&gt; (see Def. 6.2.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;; not..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="531">The dilation operator &lt;math&gt;D_c&lt;/math&gt; is defined on &lt;math&gt;L^(\mathbb R)&lt;/math&gt; (see Def. 6.2.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;; not sure if it can be defined for other &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces too). By definition, for a given &lt;math&gt;c&gt;0&lt;/math&gt; this operator acts as:

 &lt;math&gt;(D_c f)(x):=\frac{1}{\sqrt{c}} f(\frac{x}{c}), \,x\in \mathbb R.&lt;/math&gt;

&lt;math&gt;D_c&lt;/math&gt; is a [[Unitary|unitary]] operator and it holds that
* &lt;math&gt;D_c^{-1} = D_{1/c} = (D_c)^*&lt;/math&gt;.

== References ==</text>
      <sha1>09keutsqoxw8fbhkluluxuq24rm1gzi</sha1>
    </revision>
  </page>
  <page>
    <title>Geometric series</title>
    <ns>0</ns>
    <id>148</id>
    <revision>
      <id>455</id>
      <timestamp>2019-07-06T13:17:17Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The series &lt;math&gt;1+r+r^2+r^3\dots&lt;/math&gt; is called a ''geometric'' series. This series is convergent if &lt;math&gt;|r| &lt; 1&lt;/math&gt;, in which case:   &lt;math&gt;1+r+r^2+r^3\dots = \frac{1..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="189">The series &lt;math&gt;1+r+r^2+r^3\dots&lt;/math&gt; is called a ''geometric'' series. This series is convergent if &lt;math&gt;|r| &lt; 1&lt;/math&gt;, in which case:

 &lt;math&gt;1+r+r^2+r^3\dots = \frac{1}{1-r}&lt;/math&gt;.</text>
      <sha1>pobl85necw8mnskbku9lihoeqqid9xs</sha1>
    </revision>
  </page>
  <page>
    <title>Young's inequality</title>
    <ns>0</ns>
    <id>149</id>
    <revision>
      <id>456</id>
      <timestamp>2019-07-06T13:19:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Let &lt;math&gt;a, b&lt;/math&gt; be nonnegative real numbers and &lt;math&gt;p, q&lt;/math&gt; real numbers greater than 1 such that &lt;math&gt;1/p+1/q =1&lt;/math&gt;. Then, Young's inequality says that:   &lt;m..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="223">Let &lt;math&gt;a, b&lt;/math&gt; be nonnegative real numbers and &lt;math&gt;p, q&lt;/math&gt; real numbers greater than 1 such that &lt;math&gt;1/p+1/q =1&lt;/math&gt;. Then, Young's inequality says that:

 &lt;math&gt;ab \le \frac{a^p}{p} + \frac{b^q}{q}&lt;/math&gt;.</text>
      <sha1>155hml28mav17zivcs6k5nckp0afr1r</sha1>
    </revision>
  </page>
  <page>
    <title>Paley-Wiener space</title>
    <ns>0</ns>
    <id>150</id>
    <revision>
      <id>457</id>
      <timestamp>2019-07-06T13:24:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A function &lt;math&gt;f\in L^{2}(\mathbb R)&lt;/math&gt; is band-limited if the [[Fourier transform]] &lt;math&gt;\hat f&lt;/math&gt; has [[compact support.]]  The Paley-Wiener space, &lt;math&gt;PW&lt;/math..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="361">A function &lt;math&gt;f\in L^{2}(\mathbb R)&lt;/math&gt; is band-limited if the [[Fourier transform]] &lt;math&gt;\hat f&lt;/math&gt; has [[compact support.]]

The Paley-Wiener space, &lt;math&gt;PW&lt;/math&gt;, is the space of functions whose ''[[Fourier transform]]'' has compact support:

 &lt;math&gt;PW = :\{f\in L^2(\mathbb R) | \text{ supp}\hat{f} \subseteq [-\frac{1}{2}, \frac{1}{2}]\}&lt;/math&gt;</text>
      <sha1>6axt0zwxr7jfzruou35zsggly2qaytw</sha1>
    </revision>
    <revision>
      <id>459</id>
      <parentid>457</parentid>
      <timestamp>2019-07-06T13:38:09Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="440">A function &lt;math&gt;f\in L^{2}(\mathbb R)&lt;/math&gt; is band-limited if the [[Fourier transform]] &lt;math&gt;\hat f&lt;/math&gt; has [[compact support.]]

The Paley-Wiener space, &lt;math&gt;PW&lt;/math&gt;, is the space of functions whose ''[[Fourier transform]]'' has compact support:

 &lt;math&gt;PW = :\{f\in L^2(\mathbb R) | \text{ supp}\hat{f} \subseteq [-\frac{1}{2}, \frac{1}{2}]\}&lt;/math&gt;

The Paley Wiener space is directly related to [[Shannon's sampling theorem]].</text>
      <sha1>l6bjgma63zd1c14mnauk3c64yumvzk0</sha1>
    </revision>
    <revision>
      <id>460</id>
      <parentid>459</parentid>
      <timestamp>2019-07-06T13:38:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="440">A function &lt;math&gt;f\in L^{2}(\mathbb R)&lt;/math&gt; is band-limited if the [[Fourier transform]] &lt;math&gt;\hat f&lt;/math&gt; has [[compact support]].

The Paley-Wiener space, &lt;math&gt;PW&lt;/math&gt;, is the space of functions whose ''[[Fourier transform]]'' has compact support:

 &lt;math&gt;PW = :\{f\in L^2(\mathbb R) | \text{ supp}\hat{f} \subseteq [-\frac{1}{2}, \frac{1}{2}]\}&lt;/math&gt;

The Paley Wiener space is directly related to [[Shannon's sampling theorem]].</text>
      <sha1>se6tiw97xqxk26y32h8sj1mch9zukbo</sha1>
    </revision>
  </page>
  <page>
    <title>Shannon's sampling theorem</title>
    <ns>0</ns>
    <id>151</id>
    <revision>
      <id>458</id>
      <timestamp>2019-07-06T13:33:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The functions &lt;math&gt;\{\text{sinc}(\cdot -k)\}_{k\in \mathbb Z}&lt;/math&gt; form an orthonormal basis for the Paley-Wiener space (Th. 7.4.5&lt;ref name=ole&gt;O. Christensen, Functions, S..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="563">The functions &lt;math&gt;\{\text{sinc}(\cdot -k)\}_{k\in \mathbb Z}&lt;/math&gt; form an orthonormal basis for the Paley-Wiener space (Th. 7.4.5&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;). In other words, any whose [[Fourier transform]] has compact support can be represented in the afore-listed basis. The coefficients for a function &lt;math&gt;f(x)&lt;/math&gt; in this basis expansion are simply the values &lt;math&gt;f(k)&lt;/math&gt; for &lt;math&gt;k \in \mathbb Z&lt;/math&gt;. That is

 &lt;math&gt;f(x) = \sum_{k\in \mathbb Z} f(k) \text{sinc}(x-k)&lt;/math&gt;

== References ==</text>
      <sha1>ms3qg71l8azlukzakwduxxmy93ukh7z</sha1>
    </revision>
  </page>
  <page>
    <title>Wavelet</title>
    <ns>0</ns>
    <id>152</id>
    <revision>
      <id>462</id>
      <timestamp>2019-07-07T13:33:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="553">A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More precisely and following the book definition (Def. 8.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;), a wavelet is defined as below.

Let &lt;math&gt;\psi(x) \in L^2(\mathbb R)&lt;/math&gt;. For &lt;math&gt;j,k \in \mathbb Z&lt;/math&gt; define the function &lt;math&gt;\psi_{j,k}&lt;/math&gt; by

 &lt;math&gt;\psi_{j,k}(x):=2^{j/2} = D^j T_k \psi = \psi (2^jx-k),\,\,x\in \mathbb R&lt;/math&gt;</text>
      <sha1>6nvarv7216u0llspgn464swsstew37y</sha1>
    </revision>
    <revision>
      <id>463</id>
      <parentid>462</parentid>
      <timestamp>2019-07-07T13:42:50Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1245">A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More precisely and following the book definition (Def. 8.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;), a wavelet is defined as below.

Let &lt;math&gt;\psi(x) \in L^2(\mathbb R)&lt;/math&gt;. For &lt;math&gt;j,k \in \mathbb Z&lt;/math&gt; define the function &lt;math&gt;\psi_{j,k}&lt;/math&gt; by

 &lt;math&gt;\psi_{j,k}(x):=2^{j/2} = D^j T_k \psi = \psi (2^jx-k),\,\,x\in \mathbb R&lt;/math&gt;

where &lt;math&gt;D^j:=D_{2^{-j}}&lt;/math&gt; is the [[dilation operator]] and &lt;math&gt;T_k&lt;/math&gt; is the [[translation operator]]. Then, the function &lt;math&gt;\psi&lt;/math&gt; is called a ''wavelet'' if the functions &lt;math&gt;\{\psi\}_{j,k\in \mathbb Z}&lt;/math&gt; define an [[orthonormal basis]] for &lt;math&gt;L^(\mathbb R)&lt;/math&gt;.

=== How to obtain a wavelet function? ===

It is by no means clear how to find a function that satisfies the property above. The main tool to establish that a function is a wavelet is a Multiresolution analysis (Def. 8.2.1&lt;ref name=ole/&gt;), which is a combination of sequence of closed subspaces &lt;math&gt;\{V_j\}_{j\in \mathbb Z}&lt;/math&gt; and a function &lt;math&gt;\phi V_0&lt;/math&gt;.

== References ==</text>
      <sha1>bv7oxxf1hct3z1opyftt0il8ejc4kjp</sha1>
    </revision>
    <revision>
      <id>464</id>
      <parentid>463</parentid>
      <timestamp>2019-07-07T13:50:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2173">A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More precisely and following the book definition (Def. 8.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;), a wavelet is defined as below.

Let &lt;math&gt;\psi(x) \in L^2(\mathbb R)&lt;/math&gt;. For &lt;math&gt;j,k \in \mathbb Z&lt;/math&gt; define the function &lt;math&gt;\psi_{j,k}&lt;/math&gt; by

 &lt;math&gt;\psi_{j,k}(x):=2^{j/2} = D^j T_k \psi = \psi (2^jx-k),\,\,x\in \mathbb R&lt;/math&gt;

where &lt;math&gt;D^j:=D_{2^{-j}}&lt;/math&gt; is the [[dilation operator]] and &lt;math&gt;T_k&lt;/math&gt; is the [[translation operator]]. Then, the function &lt;math&gt;\psi&lt;/math&gt; is called a ''wavelet'' if the functions &lt;math&gt;\{\psi\}_{j,k\in \mathbb Z}&lt;/math&gt; define an [[orthonormal basis]] for &lt;math&gt;L^(\mathbb R)&lt;/math&gt;.

=== How to obtain a wavelet function? ===

It is by no means clear how to find a function that satisfies the property above. The main tool to establish that a function is a wavelet is a Multiresolution analysis (Def. 8.2.1&lt;ref name=ole/&gt;), which is a combination of sequence of closed subspaces &lt;math&gt;\{V_j\}_{j\in \mathbb Z}&lt;/math&gt; and a function &lt;math&gt;\phi V_0&lt;/math&gt; that satisfy five properties (see Def. 8.2.1). Note that we say that Multiresolution analysis is the main tool that leads to wavelets but it has no function &lt;math&gt;\psi&lt;/math&gt; but a function &lt;math&gt;\phi&lt;/math&gt;. The key results in Prop. 8.2.6, Th. 8.2.7 and Prop. 8.2.8 establish the relationship between &lt;math&gt;\psi&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;. The relationship between those two functions is defined in terms of the [[Fourier transform]]. Specifically, those results show that a function &lt;math&gt;\psi&lt;/math&gt; is a wavelet if 

 &lt;math&gt;\hat{\psi}(2\gamma):=H_1(\gamma)\hat{\phi}(\gamma)&lt;/math&gt;

for a function &lt;math&gt;H_1(\gamma)&lt;/math&gt;, which is defined in terms of another function &lt;math&gt;H_{0}(\gamma)&lt;/math&gt; as &lt;math&gt;H_1(\gamma):=\bar{H_0(\gamma + \frac{1}{2})}e^{-2\pi i \gamma}&lt;/math&gt;. Moreover, the function &lt;math&gt;H_0(\gamma)&lt;/math&gt; must satisfy the relationship &lt;math&gt;\hat{\phi}(2\gamma) = H_0(\gamma) \hat{\phi}(\gamma)&lt;/math&gt;. 

== References ==</text>
      <sha1>2srhfyr5hwwq06z129r8aby37b81bhy</sha1>
    </revision>
    <revision>
      <id>465</id>
      <parentid>464</parentid>
      <timestamp>2019-07-07T13:58:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2178">A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More precisely and following the book definition (Def. 8.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;), a wavelet is defined as below.

Let &lt;math&gt;\psi(x) \in L^2(\mathbb R)&lt;/math&gt;. For &lt;math&gt;j,k \in \mathbb Z&lt;/math&gt; define the function &lt;math&gt;\psi_{j,k}&lt;/math&gt; by

 &lt;math&gt;\psi_{j,k}(x):=2^{j/2} = D^j T_k \psi = \psi (2^jx-k),\,\,x\in \mathbb R&lt;/math&gt;

where &lt;math&gt;D^j:=D_{2^{-j}}&lt;/math&gt; is the [[dilation operator]] and &lt;math&gt;T_k&lt;/math&gt; is the [[translation operator]]. Then, the function &lt;math&gt;\psi&lt;/math&gt; is called a ''wavelet'' if the functions &lt;math&gt;\{\psi\}_{j,k\in \mathbb Z}&lt;/math&gt; define an [[orthonormal basis]] for &lt;math&gt;L^(\mathbb R)&lt;/math&gt;.

=== How to obtain a wavelet function? ===

It is by no means clear how to find a function that satisfies the property above. The main tool to establish that a function is a wavelet is a Multiresolution analysis (Def. 8.2.1&lt;ref name=ole/&gt;), which is a combination of sequence of closed subspaces &lt;math&gt;\{V_j\}_{j\in \mathbb Z}&lt;/math&gt; and a function &lt;math&gt;\phi V_0&lt;/math&gt; that satisfy five properties (see Def. 8.2.1). Note that we say that Multiresolution analysis is the main tool that leads to wavelets but it has no function &lt;math&gt;\psi&lt;/math&gt; but a function &lt;math&gt;\phi&lt;/math&gt;. The key results in Prop. 8.2.6, Th. 8.2.7 and Prop. 8.2.8 establish the relationship between &lt;math&gt;\psi&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;. The relationship between those two functions is defined in terms of the [[Fourier transform]]. Specifically, those results show that a function &lt;math&gt;\psi&lt;/math&gt; is a wavelet if 

 &lt;math&gt;\hat{\psi}(2\gamma):=H_1(\gamma)\hat{\phi}(\gamma)&lt;/math&gt;

for a function &lt;math&gt;H_1(\gamma)&lt;/math&gt;, which is defined in terms of another function &lt;math&gt;H_{0}(\gamma)&lt;/math&gt; as &lt;math&gt;H_1(\gamma):=\overline{H_0(\gamma + \frac{1}{2})}e^{-2\pi i \gamma}&lt;/math&gt;. Moreover, the function &lt;math&gt;H_0(\gamma)&lt;/math&gt; must satisfy the relationship &lt;math&gt;\hat{\phi}(2\gamma) = H_0(\gamma) \hat{\phi}(\gamma)&lt;/math&gt;. 

== References ==</text>
      <sha1>heh2e05fxcaycu7jr2h3uexwfnmpsk4</sha1>
    </revision>
    <revision>
      <id>466</id>
      <parentid>465</parentid>
      <timestamp>2019-07-07T13:59:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2341">A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More precisely and following the book definition (Def. 8.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;), a wavelet is defined as below.

Let &lt;math&gt;\psi(x) \in L^2(\mathbb R)&lt;/math&gt;. For &lt;math&gt;j,k \in \mathbb Z&lt;/math&gt; define the function &lt;math&gt;\psi_{j,k}&lt;/math&gt; by

 &lt;math&gt;\psi_{j,k}(x):=2^{j/2} = D^j T_k \psi = \psi (2^jx-k),\,\,x\in \mathbb R&lt;/math&gt;

where &lt;math&gt;D^j:=D_{2^{-j}}&lt;/math&gt; is the [[dilation operator]] and &lt;math&gt;T_k&lt;/math&gt; is the [[translation operator]]. Then, the function &lt;math&gt;\psi&lt;/math&gt; is called a ''wavelet'' if the functions &lt;math&gt;\{\psi\}_{j,k\in \mathbb Z}&lt;/math&gt; define an [[orthonormal basis]] for &lt;math&gt;L^(\mathbb R)&lt;/math&gt;.

=== How to obtain a wavelet function? ===

It is by no means clear how to find a function that satisfies the property above. The main tool to establish that a function is a wavelet is a Multiresolution analysis (Def. 8.2.1&lt;ref name=ole/&gt;), which is a combination of sequence of closed subspaces &lt;math&gt;\{V_j\}_{j\in \mathbb Z}&lt;/math&gt; and a function &lt;math&gt;\phi V_0&lt;/math&gt; that satisfy five properties (see Def. 8.2.1). Note that we say that Multiresolution analysis is the main tool that leads to wavelets but it has no function &lt;math&gt;\psi&lt;/math&gt; but a function &lt;math&gt;\phi&lt;/math&gt;. The key results in Prop. 8.2.6, Th. 8.2.7 and Prop. 8.2.8 establish the relationship between &lt;math&gt;\psi&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;. The relationship between those two functions is defined in terms of the [[Fourier transform]]. Specifically, those results show that a function &lt;math&gt;\psi&lt;/math&gt; is a wavelet if 

 &lt;math&gt;\hat{\psi}(2\gamma):=H_1(\gamma)\hat{\phi}(\gamma)&lt;/math&gt;

for a function &lt;math&gt;H_1(\gamma)&lt;/math&gt;, which is defined in terms of another function &lt;math&gt;H_{0}(\gamma)&lt;/math&gt; as &lt;math&gt;H_1(\gamma):=\overline{H_0(\gamma + \frac{1}{2})}e^{-2\pi i \gamma}&lt;/math&gt;. Moreover, the function &lt;math&gt;H_0(\gamma)&lt;/math&gt; must satisfy the relationship &lt;math&gt;\hat{\phi}(2\gamma) = H_0(\gamma) \hat{\phi}(\gamma)&lt;/math&gt;. Note that we traded one problem for another: we didn't know how to determine &lt;math&gt;\psi&lt;/math&gt;; now we know this but don't know how to determine &lt;math&gt;\phi&lt;/math&gt;.

== References ==</text>
      <sha1>nkzknlykxi4kml8qug9929hddm6mojj</sha1>
    </revision>
    <revision>
      <id>467</id>
      <parentid>466</parentid>
      <timestamp>2019-07-07T14:00:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2520">A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More precisely and following the book definition (Def. 8.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;), a wavelet is defined as below.

Let &lt;math&gt;\psi(x) \in L^2(\mathbb R)&lt;/math&gt;. For &lt;math&gt;j,k \in \mathbb Z&lt;/math&gt; define the function &lt;math&gt;\psi_{j,k}&lt;/math&gt; by

 &lt;math&gt;\psi_{j,k}(x):=2^{j/2} = D^j T_k \psi = \psi (2^jx-k),\,\,x\in \mathbb R&lt;/math&gt;

where &lt;math&gt;D^j:=D_{2^{-j}}&lt;/math&gt; is the [[dilation operator]] and &lt;math&gt;T_k&lt;/math&gt; is the [[translation operator]]. Then, the function &lt;math&gt;\psi&lt;/math&gt; is called a ''wavelet'' if the functions &lt;math&gt;\{\psi\}_{j,k\in \mathbb Z}&lt;/math&gt; define an [[orthonormal basis]] for &lt;math&gt;L^(\mathbb R)&lt;/math&gt;.

=== How to obtain a wavelet function? ===

It is by no means clear how to find a function that satisfies the property above. The main tool to establish that a function is a wavelet is a Multiresolution analysis (Def. 8.2.1&lt;ref name=ole/&gt;), which is a combination of sequence of closed subspaces &lt;math&gt;\{V_j\}_{j\in \mathbb Z}&lt;/math&gt; and a function &lt;math&gt;\phi V_0&lt;/math&gt; that satisfy five properties (see Def. 8.2.1). Note that we say that Multiresolution analysis is the main tool that leads to wavelets but it has no function &lt;math&gt;\psi&lt;/math&gt; but a function &lt;math&gt;\phi&lt;/math&gt;. The key results in Prop. 8.2.6, Th. 8.2.7 and Prop. 8.2.8 establish the relationship between &lt;math&gt;\psi&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;. The relationship between those two functions is defined in terms of the [[Fourier transform]]. Specifically, those results show that a function &lt;math&gt;\psi&lt;/math&gt; is a wavelet if 

 &lt;math&gt;\hat{\psi}(2\gamma):=H_1(\gamma)\hat{\phi}(\gamma)&lt;/math&gt;

for a function &lt;math&gt;H_1(\gamma)&lt;/math&gt;, which is defined in terms of another function &lt;math&gt;H_{0}(\gamma)&lt;/math&gt; as &lt;math&gt;H_1(\gamma):=\overline{H_0(\gamma + \frac{1}{2})}e^{-2\pi i \gamma}&lt;/math&gt;. Moreover, the function &lt;math&gt;H_0(\gamma)&lt;/math&gt; must satisfy the relationship &lt;math&gt;\hat{\phi}(2\gamma) = H_0(\gamma) \hat{\phi}(\gamma)&lt;/math&gt;. Note that we traded one problem for another: we didn't know how to determine &lt;math&gt;\psi&lt;/math&gt;; now we know this but don't know how to determine &lt;math&gt;\phi&lt;/math&gt;. The key result in Th. 8.2.11 states that the function &lt;math&gt;\phi&lt;/math&gt; that generates a multiresolution analysis can be identified by checking if it satisfies three properties. 

== References ==</text>
      <sha1>qpwpofph1wxbwd7oj8r5jpj3y49si5f</sha1>
    </revision>
    <revision>
      <id>468</id>
      <parentid>467</parentid>
      <timestamp>2019-07-07T14:01:48Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2810">A function &lt;math&gt;\psi(x)&lt;/math&gt; is called a ''wavelet'' if it is capable of becoming the base function of what I call a ''wavelet system'' in &lt;math&gt;L^(\mathbb R)&lt;/math&gt;. More precisely and following the book definition (Def. 8.1.1&lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;), a wavelet is defined as below.

Let &lt;math&gt;\psi(x) \in L^2(\mathbb R)&lt;/math&gt;. For &lt;math&gt;j,k \in \mathbb Z&lt;/math&gt; define the function &lt;math&gt;\psi_{j,k}&lt;/math&gt; by

 &lt;math&gt;\psi_{j,k}(x):=2^{j/2} = D^j T_k \psi = \psi (2^jx-k),\,\,x\in \mathbb R&lt;/math&gt;

where &lt;math&gt;D^j:=D_{2^{-j}}&lt;/math&gt; is the [[dilation operator]] and &lt;math&gt;T_k&lt;/math&gt; is the [[translation operator]]. Then, the function &lt;math&gt;\psi&lt;/math&gt; is called a ''wavelet'' if the functions &lt;math&gt;\{\psi\}_{j,k\in \mathbb Z}&lt;/math&gt; define an [[orthonormal basis]] for &lt;math&gt;L^(\mathbb R)&lt;/math&gt;.

=== How to obtain a wavelet function? ===

It is by no means clear how to find a function that satisfies the property above. The main tool to establish that a function is a wavelet is a Multiresolution analysis (Def. 8.2.1&lt;ref name=ole/&gt;), which is a combination of sequence of closed subspaces &lt;math&gt;\{V_j\}_{j\in \mathbb Z}&lt;/math&gt; and a function &lt;math&gt;\phi V_0&lt;/math&gt; that satisfy five properties (see Def. 8.2.1). Note that we say that Multiresolution analysis is the main tool that leads to wavelets but it has no function &lt;math&gt;\psi&lt;/math&gt; but a function &lt;math&gt;\phi&lt;/math&gt;. The key results in Prop. 8.2.6, Th. 8.2.7 and Prop. 8.2.8 establish the relationship between &lt;math&gt;\psi&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;. The relationship between those two functions is defined in terms of the [[Fourier transform]]. Specifically, those results show that a function &lt;math&gt;\psi&lt;/math&gt; is a wavelet if 

 &lt;math&gt;\hat{\psi}(2\gamma):=H_1(\gamma)\hat{\phi}(\gamma)&lt;/math&gt;

for a function &lt;math&gt;H_1(\gamma)&lt;/math&gt;, which is defined in terms of another function &lt;math&gt;H_{0}(\gamma)&lt;/math&gt; as &lt;math&gt;H_1(\gamma):=\overline{H_0(\gamma + \frac{1}{2})}e^{-2\pi i \gamma}&lt;/math&gt;. Moreover, the function &lt;math&gt;H_0(\gamma)&lt;/math&gt; must satisfy the relationship &lt;math&gt;\hat{\phi}(2\gamma) = H_0(\gamma) \hat{\phi}(\gamma)&lt;/math&gt;. Note that we traded one problem for another: we didn't know how to determine &lt;math&gt;\psi&lt;/math&gt;; now we know this but don't know how to determine &lt;math&gt;\phi&lt;/math&gt;. The key result in Th. 8.2.11 states that the function &lt;math&gt;\phi&lt;/math&gt; that generates a multiresolution analysis can be identified by checking if it satisfies three properties. Thus, given a function &lt;math&gt;\phi&lt;/math&gt;, we can check if it's feasible if it satisfies those three properties. Moreover, the functions &lt;math&gt;H_0, H_1&lt;/math&gt; are generally just trigonometric polynomials (see Prop. 8.2.6 and Prop. 8.2.8), therefore one just needs to find their coefficients.

== References ==</text>
      <sha1>j0jtlcmnftbdu9228y6qku29j2mw1l0</sha1>
    </revision>
  </page>
  <page>
    <title>MAT4 (Ole Christensen) Review</title>
    <ns>0</ns>
    <id>153</id>
    <revision>
      <id>470</id>
      <timestamp>2019-07-13T13:16:54Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1934">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[specific Banach/Hilbert spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[asis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications.


The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)


== References =</text>
      <sha1>5td4sx0rveffofy0f208j6eum87tcov</sha1>
    </revision>
    <revision>
      <id>471</id>
      <parentid>470</parentid>
      <timestamp>2019-07-13T13:19:14Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2113">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications.


The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]



== References =</text>
      <sha1>9bvh7lcoebvtwquzc7wbv7qll2kpama</sha1>
    </revision>
    <revision>
      <id>473</id>
      <parentid>471</parentid>
      <timestamp>2019-07-13T13:30:32Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2761">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]





== References =</text>
      <sha1>bn3l12w943ap2rsyduknriwspsls9ht</sha1>
    </revision>
    <revision>
      <id>474</id>
      <parentid>473</parentid>
      <timestamp>2019-07-13T13:55:37Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3594">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[Shauder basis]] (D.2.5.4)

== References =</text>
      <sha1>djdzl5us72o54dzkc10cxgxl5em8oat</sha1>
    </revision>
    <revision>
      <id>475</id>
      <parentid>474</parentid>
      <timestamp>2019-07-13T14:14:11Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4869">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some propreties of 



== References ==</text>
      <sha1>mdk6z68qhyjz28dvzi27zcuaojdu7n6</sha1>
    </revision>
    <revision>
      <id>476</id>
      <parentid>475</parentid>
      <timestamp>2019-07-13T14:16:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5083">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some properties of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; and &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;
* [[Translation operator]], [[dilation operator]], [[modulation operator]].
* L.6.2.2: The operators above are well-defined, linear, bounded and unitary.




== References ==</text>
      <sha1>p302kya6s97xqf20y23uvr42zk9k1ow</sha1>
    </revision>
    <revision>
      <id>477</id>
      <parentid>476</parentid>
      <timestamp>2019-07-13T14:31:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5739">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some properties of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; and &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;
* [[Translation operator]], [[dilation operator]], [[modulation operator]].
* L.6.2.2: The operators above are well-defined, linear, bounded and unitary.

=== Week 7 ===
* Composition of operators. Starting from the outer operator is generally better.
* L.6.2.3. On the compositions of translation, modulation and dilation operators.
* Orthonormal system definition (D.4.3.1)
* Orthonormal basis definition (D.4.7.1)
* '''T.4.7.2''': One of the central theorems of the book: Characterization of Orthonormal basis in 5 different ways.
* &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt; definition
* An orthonormal basis for &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt;
* Fourier series converges ''but not necessarily pointwise''. I.e., convergence happens in terms of norm, which uses an integral, which ''ignores'' individual points.

=== Week 8 ===

== References ==</text>
      <sha1>k1yinnhda9wfjzvojhel3sb22t331ua</sha1>
    </revision>
    <revision>
      <id>478</id>
      <parentid>477</parentid>
      <timestamp>2019-07-13T14:38:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6364">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some properties of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; and &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;
* [[Translation operator]], [[dilation operator]], [[modulation operator]].
* L.6.2.2: The operators above are well-defined, linear, bounded and unitary.

=== Week 7 ===
* Composition of operators. Starting from the outer operator is generally better.
* L.6.2.3. On the compositions of translation, modulation and dilation operators.
* Orthonormal system definition (D.4.3.1)
* Orthonormal basis definition (D.4.7.1)
* '''T.4.7.2''': One of the central theorems of the book: Characterization of Orthonormal basis in 5 different ways.
* &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt; definition
* An orthonormal basis for &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt;
* Fourier series converges ''but not necessarily pointwise''. I.e., convergence happens in terms of norm, which uses an integral, which ''ignores'' individual points.

=== Week 8 ===
* Beginning Approximation theory.
* Definition of [[dense subset]]s.
* L.3.2.4: spaces that contain sequences with ''finite non-zero elements'' are dense in &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; (which are infinite spaces) for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt;
* &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is dense in &lt;math&gt;L^p(\mathbb R)&lt;/math&gt;.
* [[Weierstrass' theorem]]: Any continuous function can be approximated arbitrarily well with polynomials (i.e., the set of polynomials is dense in the set of cont. functions).
* [[Operator extension|Extension of bounded operators]] to allow operation on vectors who are not in their original domain.


== References ==</text>
      <sha1>cxv8mzj53gyzarhg9cy8odrtwom93bb</sha1>
    </revision>
    <revision>
      <id>479</id>
      <parentid>478</parentid>
      <timestamp>2019-07-13T14:43:15Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7111">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some properties of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; and &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;
* [[Translation operator]], [[dilation operator]], [[modulation operator]].
* L.6.2.2: The operators above are well-defined, linear, bounded and unitary.

=== Week 7 ===
* Composition of operators. Starting from the outer operator is generally better.
* L.6.2.3. On the compositions of translation, modulation and dilation operators.
* Orthonormal system definition (D.4.3.1)
* Orthonormal basis definition (D.4.7.1)
* '''T.4.7.2''': One of the central theorems of the book: Characterization of Orthonormal basis in 5 different ways.
* &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt; definition
* An orthonormal basis for &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt;
* Fourier series converges ''but not necessarily pointwise''. I.e., convergence happens in terms of norm, which uses an integral, which ''ignores'' individual points.

=== Week 8 ===
* Beginning Approximation theory.
* Definition of [[dense subset]]s.
* L.3.2.4: spaces that contain sequences with ''finite non-zero elements'' are dense in &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; (which are infinite spaces) for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt;
* &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is dense in &lt;math&gt;L^p(\mathbb R)&lt;/math&gt;.
* [[Weierstrass' theorem]]: Any continuous function can be approximated arbitrarily well with polynomials (i.e., the set of polynomials is dense in the set of cont. functions).
* [[Operator extension|Extension of bounded operators]] to allow operation on vectors who are not in their original domain.

=== Week 9 ===
* [[Fourier transform]]
* Range of Fourier transform: i.e., which functions can be represented with a Fourier transform (see [[Fourier transform]])?
* Commutation of dilation and translation operators
* Function of [[Fourier transform]]: it measures the frequencies in a given functin
* [[Fourier transform]] is bounded
* L.7.2.1: The norm of a function is equal to the norm of its Fourier transform
* Calculation of Fourier transform in special cases:
** When the function is even
** When the function is odd
** When the function is composed with [[translation operator]]
** When the function is composed with [[modulation operator]]
** When the function is composed with differentiation
* Unitarity of Fourier transform (T.7.2.2)

== References ==</text>
      <sha1>5m8pzc66rcaqwyj99dejpq8q2ld5chr</sha1>
    </revision>
    <revision>
      <id>480</id>
      <parentid>479</parentid>
      <timestamp>2019-07-13T14:46:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7408">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some properties of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; and &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;
* [[Translation operator]], [[dilation operator]], [[modulation operator]].
* L.6.2.2: The operators above are well-defined, linear, bounded and unitary.

=== Week 7 ===
* Composition of operators. Starting from the outer operator is generally better.
* L.6.2.3. On the compositions of translation, modulation and dilation operators.
* Orthonormal system definition (D.4.3.1)
* Orthonormal basis definition (D.4.7.1)
* '''T.4.7.2''': One of the central theorems of the book: Characterization of Orthonormal basis in 5 different ways.
* &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt; definition
* An orthonormal basis for &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt;
* Fourier series converges ''but not necessarily pointwise''. I.e., convergence happens in terms of norm, which uses an integral, which ''ignores'' individual points.

=== Week 8 ===
* Beginning Approximation theory.
* Definition of [[dense subset]]s.
* L.3.2.4: spaces that contain sequences with ''finite non-zero elements'' are dense in &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; (which are infinite spaces) for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt;
* &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is dense in &lt;math&gt;L^p(\mathbb R)&lt;/math&gt;.
* [[Weierstrass' theorem]]: Any continuous function can be approximated arbitrarily well with polynomials (i.e., the set of polynomials is dense in the set of cont. functions).
* [[Operator extension|Extension of bounded operators]] to allow operation on vectors who are not in their original domain.

=== Week 9 ===
* [[Fourier transform]]
* Range of Fourier transform: i.e., which functions can be represented with a Fourier transform (see [[Fourier transform]])?
* Commutation of dilation and translation operators
* Function of [[Fourier transform]]: it measures the frequencies in a given functin
* [[Fourier transform]] is bounded
* L.7.2.1: The norm of a function is equal to the norm of its Fourier transform
* Calculation of Fourier transform in special cases:
** When the function is even
** When the function is odd
** When the function is composed with [[translation operator]]
** When the function is composed with [[modulation operator]]
** When the function is composed with differentiation
* Unitarity of Fourier transform (T.7.2.2)

=== Week 10 ===
* [[Paley-Wiener space]]: An important subspace of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt;
* [[Shannon's sampling theorem]] (T.7.4.5): 
* Definition of convolution operator
* Fourier convolution theorem: Convolution in the function's domain is equivalent to multiplication in Fourier domain.

== References ==</text>
      <sha1>cikl2e5tl5skrw7ibyc0uj4k8icbuzf</sha1>
    </revision>
    <revision>
      <id>481</id>
      <parentid>480</parentid>
      <timestamp>2019-07-13T15:22:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8651">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some properties of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; and &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;
* [[Translation operator]], [[dilation operator]], [[modulation operator]].
* L.6.2.2: The operators above are well-defined, linear, bounded and unitary.

=== Week 7 ===
* Composition of operators. Starting from the outer operator is generally better.
* L.6.2.3. On the compositions of translation, modulation and dilation operators.
* Orthonormal system definition (D.4.3.1)
* Orthonormal basis definition (D.4.7.1)
* '''T.4.7.2''': One of the central theorems of the book: Characterization of Orthonormal basis in 5 different ways.
* &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt; definition
* An orthonormal basis for &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt;
* Fourier series converges ''but not necessarily pointwise''. I.e., convergence happens in terms of norm, which uses an integral, which ''ignores'' individual points.

=== Week 8 ===
* Beginning Approximation theory.
* Definition of [[dense subset]]s.
* L.3.2.4: spaces that contain sequences with ''finite non-zero elements'' are dense in &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; (which are infinite spaces) for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt;
* &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is dense in &lt;math&gt;L^p(\mathbb R)&lt;/math&gt;.
* [[Weierstrass' theorem]]: Any continuous function can be approximated arbitrarily well with polynomials (i.e., the set of polynomials is dense in the set of cont. functions).
* [[Operator extension|Extension of bounded operators]] to allow operation on vectors who are not in their original domain.

=== Week 9 ===
* [[Fourier transform]]
* Range of Fourier transform: i.e., which functions can be represented with a Fourier transform (see [[Fourier transform]])?
* Commutation of dilation and translation operators
* Function of [[Fourier transform]]: it measures the frequencies in a given functin
* [[Fourier transform]] is bounded
* L.7.2.1: The norm of a function is equal to the norm of its Fourier transform
* Calculation of Fourier transform in special cases:
** When the function is even
** When the function is odd
** When the function is composed with [[translation operator]]
** When the function is composed with [[modulation operator]]
** When the function is composed with differentiation
* Unitarity of Fourier transform (T.7.2.2)

=== Week 10 ===
* [[Paley-Wiener space]]: An important subspace of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt;
* [[Shannon's sampling theorem]] (T.7.4.5): 
* Definition of convolution operator
* Fourier convolution theorem: Convolution in the function's domain is equivalent to multiplication in Fourier domain.
* Introduction to [[wavelet]]s

=== Week 11 &amp; Week 12 ===
* These are the weeks where the Wavelet theory is studied. It is shown how to consturct a wavelet system. This process is summarized in the article [[Wavelet]].
* The concept of Vanishing moments is introduced (D.8.3.2). This is good for compression (see T.8.3.3).
* The theorem T.8.3.3 is a ''breakthrough'' moment as it theoretically guarantees that certain wavelet coefficients will vanish and guides the choice of good wavelet functions.
* Applications of Wavelets:
** FBI stores fingerprints with wavelets (a technique that had won a competition)
** JPEG 2000, MP3 use wavelets
** Noise reduction. It was apparently used to be able to discover the music that was recorded with a simple technique by Brahms; without noise reduction the scores were not comprehensible.
* Splines and B-splines are introduced (D.10.1.2 for the latter)
* Many interesting properties are of B-splines are listed in T.10.1.3 and C10.1.7
* A great advantage of B-splines is that they lend themselves very well for computer-based computations:
** They have compact support
** All computations can be reduced to computer-friendly calculations of +/-/x/% as B-splines are piece-wise polynomials (C.10.1.7).

== References ==</text>
      <sha1>396l0341rb6lde8a26b3z2vl8elfo9w</sha1>
    </revision>
    <revision>
      <id>560</id>
      <parentid>481</parentid>
      <timestamp>2020-03-22T14:09:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8652">The core concept of this class is approximation. The class provides the fundamental mathematical tools for approximation. Suppose that we have a function &lt;math&gt;f \in L^2({\mathbb R})&lt;/math&gt; where &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; is a ][Special vector spaces|fairly generic space]] that covers a broad range of functions. The course uses the concept of [[dense subset|denseness]] to show that any function in &lt;math&gt;L^(\mathbb R)&lt;/math&gt; can be approximated with a continuous function &lt;math&gt;g&lt;/math&gt; that has compact support (i.e., &lt;math&gt;g \in C_c(\mathbb{R})&lt;/math&gt;). This statement requires the introduction of core concepts such as [[Banach spaces]], [[Subset closure]], [[Norm]], [[Operator norm]], [[Bounded linear operator]] etc.

The course starts from abstract and goes toward more concrete and applied concepts. Specifically, it starts with the more general topics of convergence in normed spaces, inner vector spaces, Banach spaces Hilbert spaces, bounded linear operators on Banach/Hilbert spaces etc. It then goes to [[special vector spaces|specific Banach/Hilbert spaces]] such as &lt;math&gt;L^2(\mathbb R), L^1(\mathbb R), C_c(\mathbb R), C_0(\mathbb R)&lt;/math&gt;.  The concept of [[basis]] in finite and infinite spaces is introduced. The course defines specific critical Bounded linear operations on &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; such as the [[Fourier transform]] or more basic operators such as [[translation operator]], [[modulation operator]] and [[dilation operator]]. All those operators then lead to the very important [[Wavelet]] theory, which is of great utility for real life applications. The course introduces important inequalities that are used to prove the main results, namely [[Inequalities in sequences|Minkowsky and Holder inequalities]] and [[Cauchy-Schwartz inequality]].

The definitions and theorems etc. referred to below are from the book &lt;ref name=ole&gt;O. Christensen, Functions, Spaces and Expansions, 2010&lt;/ref&gt;.

=== Week 1 ===
* Introduces norm
* Motivates usage of normed vector spoces
* Proves negative triangle inequality
* Definition of sequence convergence (Def. 2.1.5.)
* Definition of [[subspace]]
* Definition of supremum and "bounded above"
* T.1.6.3: Continuous functions on a bounded interval attain their supremum

=== Week 2 ===
* [[Banach spaces]]
* [[Cauchy sequence]]s (Def. 3.1.1), and their importance; that is, we don't need to guess to what the sequence converges too (see [[Cauchy sequence]]).
* Convergent sequence =&gt; Cauchy sequence (L3.1.3.). In many spaces, the opposite holds too; we call those [[Banach spaces]].
* &lt;math&gt;C[a,b]&lt;/math&gt; is a Banach space (T3.1.6).
* &lt;math&gt;\ell^1(\mathbb N)&lt;/math&gt; and &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces.
* [[Bounded linear operator]]
* [[Operator Norm]]

=== Week 3 ===
* [[Inner product|Inner product]] definition
* [[Cauchy Schwarz]] inequality (T.4.1.2)
* Defining norms from inner products (L.4.1.3)
* Not all norms come from an inner product (T.4.1.4 helps us identify when a norm comes from inner product).
* [[Hilbert spaces]]: The normed vector spaces whose norm originates from an inner product
* &lt;math&gt;\ell^2(\mathbb N)&lt;/math&gt; is a Hilbert space (but not other &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; spaces!)
* [[Functional]] definition
* Riesz' representation theorem. In a Hilbert space, any bounded functional can be shown to be the outcome of an inner product. 

=== Week 4 ===
* [[Adjoint]] operator.
* Properties of the adjoint operator
* [[Self-joint]] and [[unitary]] functions (Def. 4.5.4.); those are generalizations of transpose and orthogonality.
* [[basis|Shauder basis]] (D.2.5.4)

=== Week 5 ===
* Definition &lt;math&gt;L^1(\mathbb R):=\{f:\mathbb R \to \mathbb C : \int_{-\infty}^{\infty} |f(x)| dx &lt; \infty \}&lt;/math&gt;.
* How to establish membership to &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;. 
** That is, &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; ''always makes sense''; it can be finite or infinite but either makes sense.  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; may or may not make sense ; for example  &lt;math&gt;\int_{-\infty}^{\infty} \sin(x) dx&lt;/math&gt; does not make sense -- it's not clear what it means.
** L1.7.2: If  &lt;math&gt;\int_{-\infty}^{\infty} |f(x)| dx&lt;/math&gt; is finite, then  &lt;math&gt;\int_{-\infty}^{\infty} f(x) dx&lt;/math&gt; is well-defined (\i.e., it makes sense).
* [[Inequalities in sequences|Holder's Inequality]] (T.1.7.4)
* Support of a function (D.5.1.1)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_0&lt;/math&gt; are introduces
** &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; is a Banach space (L5.1.3)
** &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is a subspace of &lt;math&gt;C_0&lt;/math&gt;.
** Counter-intuitively, &lt;math&gt;C_c&lt;/math&gt; is NOT a Banach space (L5.1.4).

=== Week 6 ===
* &lt;math&gt;L^p(\mathbb R)&lt;/math&gt; spaces (see [[special vector spaces]]).
* Advantages and some properties of &lt;math&gt;C_0(\mathbb R)&lt;/math&gt; and &lt;math&gt;C_c(\mathbb R)&lt;/math&gt;
* Advantages and some properties of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt; and &lt;math&gt;L^1(\mathbb R)&lt;/math&gt;
* [[Translation operator]], [[dilation operator]], [[modulation operator]].
* L.6.2.2: The operators above are well-defined, linear, bounded and unitary.

=== Week 7 ===
* Composition of operators. Starting from the outer operator is generally better.
* L.6.2.3. On the compositions of translation, modulation and dilation operators.
* Orthonormal system definition (D.4.3.1)
* Orthonormal basis definition (D.4.7.1)
* '''T.4.7.2''': One of the central theorems of the book: Characterization of Orthonormal basis in 5 different ways.
* &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt; definition
* An orthonormal basis for &lt;math&gt;L^2(-\pi, \pi)&lt;/math&gt;
* Fourier series converges ''but not necessarily pointwise''. I.e., convergence happens in terms of norm, which uses an integral, which ''ignores'' individual points.

=== Week 8 ===
* Beginning Approximation theory.
* Definition of [[dense subset]]s.
* L.3.2.4: spaces that contain sequences with ''finite non-zero elements'' are dense in &lt;math&gt;\ell^p(\mathbb N)&lt;/math&gt; (which are infinite spaces) for any &lt;math&gt;p\in[1,\infty)&lt;/math&gt;
* &lt;math&gt;C_c(\mathbb R)&lt;/math&gt; is dense in &lt;math&gt;L^p(\mathbb R)&lt;/math&gt;.
* [[Weierstrass' theorem]]: Any continuous function can be approximated arbitrarily well with polynomials (i.e., the set of polynomials is dense in the set of cont. functions).
* [[Operator extension|Extension of bounded operators]] to allow operation on vectors who are not in their original domain.

=== Week 9 ===
* [[Fourier transform]]
* Range of Fourier transform: i.e., which functions can be represented with a Fourier transform (see [[Fourier transform]])?
* Commutation of dilation and translation operators
* Function of [[Fourier transform]]: it measures the frequencies in a given functin
* [[Fourier transform]] is bounded
* L.7.2.1: The norm of a function is equal to the norm of its Fourier transform
* Calculation of Fourier transform in special cases:
** When the function is even
** When the function is odd
** When the function is composed with [[translation operator]]
** When the function is composed with [[modulation operator]]
** When the function is composed with differentiation
* Unitarity of Fourier transform (T.7.2.2)

=== Week 10 ===
* [[Paley-Wiener space]]: An important subspace of &lt;math&gt;L^2(\mathbb R)&lt;/math&gt;
* [[Shannon's sampling theorem]] (T.7.4.5): 
* Definition of convolution operator
* Fourier convolution theorem: Convolution in the function's domain is equivalent to multiplication in Fourier domain.
* Introduction to [[wavelet]]s

=== Week 11 &amp; Week 12 ===
* These are the weeks where the Wavelet theory is studied. It is shown how to consturct a wavelet system. This process is summarized in the article [[Wavelet]].
* The concept of Vanishing moments is introduced (D.8.3.2). This is good for compression (see T.8.3.3).
* The theorem T.8.3.3 is a ''breakthrough'' moment as it theoretically guarantees that certain wavelet coefficients will vanish and guides the choice of good wavelet functions.
* Applications of Wavelets:
** FBI stores fingerprints with wavelets (a technique that had won a competition)
** JPEG 2000, MP3 use wavelets
** Noise reduction. It was apparently used to be able to discover the music that was recorded with a simple technique by Brahms; without noise reduction the scores were not comprehensible.
* Splines and B-splines are introduced (D.10.1.2 for the latter)
* Many interesting properties are of B-splines are listed in T.10.1.3 and C10.1.7
* A great advantage of B-splines is that they lend themselves very well for computer-based computations:
** They have compact support
** All computations can be reduced to computer-friendly calculations of +/-/x/% as B-splines are piece-wise polynomials (C.10.1.7).

== References ==</text>
      <sha1>ncnfhe5ks8j5nb8i6lcji9yolf2zbso</sha1>
    </revision>
  </page>
  <page>
    <title>LTI systems and convolution</title>
    <ns>0</ns>
    <id>154</id>
    <revision>
      <id>483</id>
      <timestamp>2019-08-04T14:22:46Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="627">Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost as an axiom, the reason of whi it holds is seldom mentioned, even though it actually has a very simple explanation.

Let &lt;math&gt;\mathcal L\{\cdot\}&lt;/math&gt; be a linear system, and &lt;math&gt;x[n]&lt;/math&gt; a discrete sequence. Let &lt;math&gt;y[n]:=\mathcal{L}\{x[n]\}&lt;/math&gt; be the output of the system. Clearly, the input &lt;math&gt;x[n]&lt;/math&gt; can be written as an infinite sum of Kronecker delta functions:

&lt;math&gt;x[n] = \sum_{k=-\infty}^{infty} x[k] \delta[n-k]&lt;/math&gt;</text>
      <sha1>9d8hosykzcm8yu604dqaico5kojj32z</sha1>
    </revision>
    <revision>
      <id>484</id>
      <parentid>483</parentid>
      <timestamp>2019-08-04T14:26:22Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1114">Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost as an axiom, the reason of whi it holds is seldom mentioned, even though it actually has a very simple explanation.

Let &lt;math&gt;\mathcal L\{\cdot\}&lt;/math&gt; be a linear system, and &lt;math&gt;x[n]&lt;/math&gt; a discrete sequence. Let &lt;math&gt;y[n]:=\mathcal{L}\{x[n]\}&lt;/math&gt; be the output of the system. Clearly, the input &lt;math&gt;x[n]&lt;/math&gt; can be written as an infinite sum of Kronecker delta functions:

&lt;math&gt;x[n] = \sum_{k=-\infty}^{\infty} x[k] \delta[n-k]&lt;/math&gt;.

Therefore, the output can be written as:

&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \mathcal{L}\{\sum_{k=-\infty}^{\infty} x[k] \delta[n-k]\} = \sum_{k=-\infty}^{\infty}x[k] \mathcal{L}\{\delta[n-k]\} =\sum_{k=-\infty}^{\infty}x[k] h[n-k]\}  &lt;/math&gt;

where &lt;math&gt;h[n]:=\mathcal{L}\{\delta[n]&lt;/math&gt; is the famous ''impulse response'' of the system. Thus, the output of a linear system to a given input can be obtained by convolving the input sequence with the impulse response of the system.</text>
      <sha1>ru6ki23c9i6189s9a0ayx8ucofv49d1</sha1>
    </revision>
    <revision>
      <id>489</id>
      <parentid>484</parentid>
      <timestamp>2019-08-04T14:40:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1646">Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost as an axiom, the reason of whi it holds is seldom mentioned, even though it actually has a very simple explanation.

Let &lt;math&gt;\mathcal L\{\cdot\}&lt;/math&gt; be a linear system, and &lt;math&gt;x[n]&lt;/math&gt; a discrete sequence. Let &lt;math&gt;y[n]:=\mathcal{L}\{x[n]\}&lt;/math&gt; be the output of the system. Clearly, the input &lt;math&gt;x[n]&lt;/math&gt; can be written as an infinite sum of Kronecker delta functions:

&lt;math&gt;x[n] = \sum_{k=-\infty}^{\infty} x[k] \delta[n-k]&lt;/math&gt;.

Therefore, the output can be written as:

&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \mathcal{L}\{\sum_{k=-\infty}^{\infty} x[k] \delta[n-k]\} = \sum_{k=-\infty}^{\infty}x[k] \mathcal{L}\{\delta[n-k]\} =\sum_{k=-\infty}^{\infty}x[k] h[n-k]\}  &lt;/math&gt;

where &lt;math&gt;h[n]:=\mathcal{L}\{\delta[n]&lt;/math&gt; is the famous ''impulse response'' of the system. Thus, the output of a linear system to a given input can be obtained by convolving the input sequence with the impulse response of the system:


&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \sum_{k=-\infty}^{\infty}x[k] h[n-k]\}  &lt;/math&gt;

=== Relation to system function (or transfer function) ===
It is also somewhat of a mystery why the transfer function is so much studied. For discrete systems, transfer function is obtained through &lt;math&gt;z&lt;/math&gt;-transformation. Since &lt;math&gt;z&lt;/math&gt; transform shares the convolution property with Fourier transform (i.e., convolution in time domain is equal to multiplication in &lt;math&gt;z&lt;/math&gt;-domain), it holds that

&lt;math&gt;Y(z) = X(z)H(z)&lt;/math&gt;</text>
      <sha1>l0t3th458pb7pzrl7foj4tur6nys11u</sha1>
    </revision>
    <revision>
      <id>490</id>
      <parentid>489</parentid>
      <timestamp>2019-08-04T14:41:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1787">Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost as an axiom, the reason of whi it holds is seldom mentioned, even though it actually has a very simple explanation.

Let &lt;math&gt;\mathcal L\{\cdot\}&lt;/math&gt; be a linear system, and &lt;math&gt;x[n]&lt;/math&gt; a discrete sequence. Let &lt;math&gt;y[n]:=\mathcal{L}\{x[n]\}&lt;/math&gt; be the output of the system. Clearly, the input &lt;math&gt;x[n]&lt;/math&gt; can be written as an infinite sum of Kronecker delta functions:

&lt;math&gt;x[n] = \sum_{k=-\infty}^{\infty} x[k] \delta[n-k]&lt;/math&gt;.

Therefore, the output can be written as:

&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \mathcal{L}\{\sum_{k=-\infty}^{\infty} x[k] \delta[n-k]\} = \sum_{k=-\infty}^{\infty}x[k] \mathcal{L}\{\delta[n-k]\} =\sum_{k=-\infty}^{\infty}x[k] h[n-k]\}  &lt;/math&gt;

where &lt;math&gt;h[n]:=\mathcal{L}\{\delta[n]&lt;/math&gt; is the famous ''impulse response'' of the system. Thus, the output of a linear system to a given input can be obtained by convolving the input sequence with the impulse response of the system:


&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \sum_{k=-\infty}^{\infty}x[k] h[n-k]\}  &lt;/math&gt;

=== Relation to system function (or transfer function) ===
It is also somewhat of a mystery why the transfer function is so much studied. For discrete systems, transfer function is obtained through &lt;math&gt;z&lt;/math&gt;-transformation. Since &lt;math&gt;z&lt;/math&gt; transform shares the convolution property with Fourier transform (i.e., convolution in time domain is equal to multiplication in &lt;math&gt;z&lt;/math&gt;-domain), it holds that

&lt;math&gt;Y(z) = X(z)H(z)&lt;/math&gt;

This justifies the usage of the famous transfer function &lt;math&gt;H(z)&lt;/math&gt;,

&lt;math&gt;H(z)=\frac{Y(z)}{X(z)}&lt;/math&gt;,

to characterize a system</text>
      <sha1>7a4qgmhtrxemg2awlwkdiwjiqitkint</sha1>
    </revision>
    <revision>
      <id>513</id>
      <parentid>490</parentid>
      <timestamp>2019-11-22T16:12:04Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1787">Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost as an axiom, the reason of why it holds is seldom mentioned, even though it actually has a very simple explanation.

Let &lt;math&gt;\mathcal L\{\cdot\}&lt;/math&gt; be a linear system, and &lt;math&gt;x[n]&lt;/math&gt; a discrete sequence. Let &lt;math&gt;y[n]:=\mathcal{L}\{x[n]\}&lt;/math&gt; be the output of the system. Clearly, the input &lt;math&gt;x[n]&lt;/math&gt; can be written as an infinite sum of Kronecker delta functions:

&lt;math&gt;x[n] = \sum_{k=-\infty}^{\infty} x[k] \delta[n-k]&lt;/math&gt;.

Therefore, the output can be written as:

&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \mathcal{L}\{\sum_{k=-\infty}^{\infty} x[k] \delta[n-k]\} = \sum_{k=-\infty}^{\infty}x[k] \mathcal{L}\{\delta[n-k]\} =\sum_{k=-\infty}^{\infty}x[k] h[n-k]\}  &lt;/math&gt;

where &lt;math&gt;h[n]:=\mathcal{L}\{\delta[n]&lt;/math&gt; is the famous ''impulse response'' of the system. Thus, the output of a linear system to a given input can be obtained by convolving the input sequence with the impulse response of the system:


&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \sum_{k=-\infty}^{\infty}x[k] h[n-k]\}  &lt;/math&gt;

=== Relation to system function (or transfer function) ===
It is also somewhat of a mystery why the transfer function is so much studied. For discrete systems, transfer function is obtained through &lt;math&gt;z&lt;/math&gt;-transformation. Since &lt;math&gt;z&lt;/math&gt; transform shares the convolution property with Fourier transform (i.e., convolution in time domain is equal to multiplication in &lt;math&gt;z&lt;/math&gt;-domain), it holds that

&lt;math&gt;Y(z) = X(z)H(z)&lt;/math&gt;

This justifies the usage of the famous transfer function &lt;math&gt;H(z)&lt;/math&gt;,

&lt;math&gt;H(z)=\frac{Y(z)}{X(z)}&lt;/math&gt;,

to characterize a system</text>
      <sha1>c30r2oakafbms8ufliy3b2vaw61sfm3</sha1>
    </revision>
    <revision>
      <id>514</id>
      <parentid>513</parentid>
      <timestamp>2019-11-22T16:13:40Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1784">Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost as an axiom, the reason of why it holds is seldom mentioned, even though it actually has a very simple explanation.

Let &lt;math&gt;\mathcal L\{\cdot\}&lt;/math&gt; be a linear system, and &lt;math&gt;x[n]&lt;/math&gt; a discrete sequence. Let &lt;math&gt;y[n]:=\mathcal{L}\{x[n]\}&lt;/math&gt; be the output of the system. Clearly, the input &lt;math&gt;x[n]&lt;/math&gt; can be written as an infinite sum of Kronecker delta functions:

&lt;math&gt;x[n] = \sum_{k=-\infty}^{\infty} x[k] \delta[n-k]&lt;/math&gt;.

Therefore, the output can be written as:

&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \mathcal{L}\{\sum_{k=-\infty}^{\infty} x[k] \delta[n-k]\} = \sum_{k=-\infty}^{\infty}x[k] \mathcal{L}\{\delta[n-k]\} =\sum_{k=-\infty}^{\infty}x[k] h[n-k] &lt;/math&gt;

where &lt;math&gt;h[n]:=\mathcal{L}\{\delta[n]\}&lt;/math&gt; is the famous ''impulse response'' of the system. Thus, the output of a linear system to a given input can be obtained by convolving the input sequence with the impulse response of the system:


&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \sum_{k=-\infty}^{\infty}x[k] h[n-k]  &lt;/math&gt;

=== Relation to system function (or transfer function) ===
It is also somewhat of a mystery why the transfer function is so much studied. For discrete systems, transfer function is obtained through &lt;math&gt;z&lt;/math&gt;-transformation. Since &lt;math&gt;z&lt;/math&gt; transform shares the convolution property with Fourier transform (i.e., convolution in time domain is equal to multiplication in &lt;math&gt;z&lt;/math&gt;-domain), it holds that

&lt;math&gt;Y(z) = X(z)H(z)&lt;/math&gt;

This justifies the usage of the famous transfer function &lt;math&gt;H(z)&lt;/math&gt;,

&lt;math&gt;H(z)=\frac{Y(z)}{X(z)}&lt;/math&gt;,

to characterize a system</text>
      <sha1>daiaqkn62erzzs1hzpeufttr5muid1v</sha1>
    </revision>
    <revision>
      <id>515</id>
      <parentid>514</parentid>
      <timestamp>2019-11-22T16:23:13Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1785">Professors in engineering schools often mention that the output of an LTI (Linear-Time Invariant) system can be obtained via convolution. While this statement is taken almost as an axiom, the reason of why it holds is seldom mentioned, even though it actually has a very simple explanation.

Let &lt;math&gt;\mathcal L\{\cdot\}&lt;/math&gt; be a linear system, and &lt;math&gt;x[n]&lt;/math&gt; a discrete sequence. Let &lt;math&gt;y[n]:=\mathcal{L}\{x[n]\}&lt;/math&gt; be the output of the system. Clearly, the input &lt;math&gt;x[n]&lt;/math&gt; can be written as an infinite sum of Kronecker delta functions:

&lt;math&gt;x[n] = \sum_{k=-\infty}^{\infty} x[k] \delta[n-k]&lt;/math&gt;.

Therefore, the output can be written as:

&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \mathcal{L}\{\sum_{k=-\infty}^{\infty} x[k] \delta[n-k]\} = \sum_{k=-\infty}^{\infty}x[k] \mathcal{L}\{\delta[n-k]\} =\sum_{k=-\infty}^{\infty}x[k] h[n-k] &lt;/math&gt;

where &lt;math&gt;h[n]:=\mathcal{L}\{\delta[n]\}&lt;/math&gt; is the famous ''impulse response'' of the system. Thus, the output of a linear system to a given input can be obtained by convolving the input sequence with the impulse response of the system:


&lt;math&gt;y[n] = \mathcal{L}\{x[n]\}= \sum_{k=-\infty}^{\infty}x[k] h[n-k]  &lt;/math&gt;

=== Relation to system function (or transfer function) ===
It is also somewhat of a mystery why the transfer function is so much studied. For discrete systems, transfer function is obtained through &lt;math&gt;z&lt;/math&gt;-transformation. Since &lt;math&gt;z&lt;/math&gt; transform shares the convolution property with Fourier transform (i.e., convolution in time domain is equal to multiplication in &lt;math&gt;z&lt;/math&gt;-domain), it holds that

&lt;math&gt;Y(z) = X(z)H(z)&lt;/math&gt;

This justifies the usage of the famous transfer function &lt;math&gt;H(z)&lt;/math&gt;,

&lt;math&gt;H(z)=\frac{Y(z)}{X(z)}&lt;/math&gt;,

to characterize a system.</text>
      <sha1>a0jk0sd6obldbq3bxah8a5g5w9dp6kw</sha1>
    </revision>
  </page>
  <page>
    <title>Linear time-invariant systems</title>
    <ns>0</ns>
    <id>155</id>
    <revision>
      <id>486</id>
      <timestamp>2019-08-04T14:30:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Linear time-invariant (LTI) systems are a class of systems that are well-studied because their analytically well-understood.   === Characterization of LTI systems === *    ==R..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="356">Linear time-invariant (LTI) systems are a class of systems that are well-studied because their analytically well-understood. 

=== Characterization of LTI systems ===
* 


==References ==
A linear time-invariant system can be characterized equivalently by its&lt;ref&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB (Appendix D), 2006&lt;/ref&gt;</text>
      <sha1>1kg0ksfljgz6dwcmkr6g0m62qibo03x</sha1>
    </revision>
    <revision>
      <id>487</id>
      <parentid>486</parentid>
      <timestamp>2019-08-04T14:34:24Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="531">Linear time-invariant (LTI) systems are a class of systems that are well-studied because their analytically well-understood. 

=== Characterization of LTI systems ===
A linear time-invariant system can be characterized equivalently by its&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB (Appendix D), 2006&lt;/ref&gt;

* Impulse response (see [[LTI systems and convolution]])
* Frequency response
* System function (or transfer function)
* Difference equations (page 802&lt;ref name=kay/&gt;).

==References ==</text>
      <sha1>r8a2p3mp8ffryd44p1gygfsslsh7luc</sha1>
    </revision>
    <revision>
      <id>488</id>
      <parentid>487</parentid>
      <timestamp>2019-08-04T14:35:07Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="618">Linear time-invariant (LTI) systems are a class of systems that are well-studied because their analytically well-understood. 

=== Characterization of LTI systems ===
''Characterizing'' an LTI system means to be able to compute its output for any input. A linear time-invariant system can be characterized equivalently by its&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB (Appendix D), 2006&lt;/ref&gt;

* Impulse response (see [[LTI systems and convolution]])
* Frequency response
* System function (or transfer function)
* Difference equations (page 802&lt;ref name=kay/&gt;).

==References ==</text>
      <sha1>3k81ldspruwuu5asghyr3l6s95xe6ew</sha1>
    </revision>
  </page>
  <page>
    <title>Random process</title>
    <ns>0</ns>
    <id>156</id>
    <revision>
      <id>491</id>
      <timestamp>2019-08-04T14:42:36Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "[[Stochastic Process]]"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22">[[Stochastic Process]]</text>
      <sha1>f2xjqwj9468d7nufbzx9ke9vtfrhyio</sha1>
    </revision>
  </page>
  <page>
    <title>Random variable</title>
    <ns>0</ns>
    <id>157</id>
    <revision>
      <id>492</id>
      <timestamp>2019-08-04T14:49:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A random variable is simply a mapping between all possible experimental outcomes and a numerical value. For example, in coin tossing all experimental outcomes are &lt;math&gt;H, T&lt;/..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="617">A random variable is simply a mapping between all possible experimental outcomes and a numerical value. For example, in coin tossing all experimental outcomes are &lt;math&gt;H, T&lt;/math&gt; and the numerical values are generally &lt;math&gt;0, 1&lt;/math&gt;; the random variable is nothing but a function that does this mapping. The name ''random variable'' is somewhat disappointing as there is nothing random about this function, it just maps &lt;math&gt;H&lt;/math&gt; to &lt;math&gt;0&lt;/math&gt; and &lt;math&gt;T&lt;/math&gt; to 1, pretty deterministicly. What is random is the outcome of the experiment; we don't know if we'll have &lt;math&gt;H&lt;/math&gt; or &lt;math&gt;T&lt;/math&gt;.</text>
      <sha1>tv0guq8lbf03w9mro3ut4cgjoxjrl9n</sha1>
    </revision>
    <revision>
      <id>573</id>
      <parentid>492</parentid>
      <timestamp>2020-04-26T15:29:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2061">A random variable is simply a mapping between all possible experimental outcomes and a numerical value. For example, in coin tossing all experimental outcomes are &lt;math&gt;H, T&lt;/math&gt; and the numerical values are generally &lt;math&gt;0, 1&lt;/math&gt;; the random variable is nothing but a function that does this mapping. The name ''random variable'' is somewhat disappointing as there is nothing random about this function, it just maps &lt;math&gt;H&lt;/math&gt; to &lt;math&gt;0&lt;/math&gt; and &lt;math&gt;T&lt;/math&gt; to 1, pretty deterministicly. What is random is the outcome of the experiment; we don't know if we'll have &lt;math&gt;H&lt;/math&gt; or &lt;math&gt;T&lt;/math&gt;.


My notes from Papoulis:

'''Definition of Random Variable.''' An RV &lt;math&gt;\mathbf x&lt;/math&gt; is a process of assigning a ''number'' &lt;math&gt;\mathbf x(\zeta)&lt;/math&gt; to every outcome &lt;math&gt;\zeta&lt;/math&gt;.

''Example 1.'' In the coin tossing experiment,  if &lt;math&gt;\zeta_1 = head &lt;/math&gt; and &lt;math&gt;\zeta_2 = tails&lt;/math&gt;, one may construct the RV &lt;math&gt;\mathbf x&lt;/math&gt; such that &lt;math&gt;\mathbf x(\zeta_1) = 0&lt;/math&gt; and &lt;math&gt;\mathbf x(\zeta_2) = 1&lt;/math&gt;.

''Example 2.'' Suppose that our RV &lt;math&gt;\mathbf x&lt;/math&gt; will measure temperature. Then, one can construct the RV &lt;math&gt;\mathbf x&lt;/math&gt; simply as  &lt;math&gt;\mathbf x(\zeta_i) = \zeta_i&lt;/math&gt; (\eg when &lt;math&gt;\zeta&lt;/math&gt; is a continuous quantity such as temperature). In this case, the variable &lt;math&gt;\zeta&lt;/math&gt; has a double meaning (see p77): It is the outcome of the experiment and the corresponding value &lt;math&gt;\mathbf x(t)&lt;/math&gt; of the RV &lt;math&gt;\mathbf x&lt;/math&gt;.

The construction of RV is important for accurately constructing the intended probability distribution. For example, in the experiment of tossing a coin twice, if we don't care about the order of the outcomes, one can construct the RV &lt;math&gt;\mathbf x&lt;/math&gt; as: &lt;math&gt;\mathbf x(HH) = 0, \,\,\,\mathbf x(HT)=1, \,\,\,\mathbf x(TT)=2.&lt;/math&gt;
But if we do care about the order, then we can construct &lt;math&gt;\mathbf x&lt;/math&gt; as:
&lt;math&gt;\mathbf x(HH) = 0, \,\,\,\mathbf x(HT)=1, \,\,\,\mathbf x(TH)=2, \,\,\,\mathbf x(TT)=3.&lt;/math&gt;</text>
      <sha1>gm0wyl2rwstyhdgwzj69cp0d7oa8rtu</sha1>
    </revision>
  </page>
  <page>
    <title>Stationarity</title>
    <ns>0</ns>
    <id>158</id>
    <revision>
      <id>498</id>
      <timestamp>2019-08-04T15:16:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "===Stationary processes===  A random process is called stationary if it's FDD (finite-dimensional distribution) does not change with the time origin. Recall that a random proc..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1284">===Stationary processes===

A random process is called stationary if it's FDD (finite-dimensional distribution) does not change with the time origin. Recall that a random process is infinite dimensional, and it's finite dimensional distribution is simply defined as a probability distribution (page 523&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes Using MATLAB, 2006&lt;/ref&gt;):

&lt;math&gt;P_{X[n_1], X[n_2], \dots, X_[n_N]}.&lt;/math&gt;

Then, it is called stationary if this distribution does not change with time origin, i.e., if

&lt;math&gt;P_{X[n_1], X[n_2], \dots, X_[n_N]} = P_{X[n_0+n_1], X[n_0+n_2], \dots, X_[n_0+n_N]}&lt;/math&gt;

for any time offset &lt;math&gt;n_0&lt;/math&gt;

===Wide-sense stationary processes===

The condition of stationarity is quite strong. A somewhat more relaxed condition is wide-sense stationarity (WSS). WSS processes are defined in terms of their mean and their covariance (and not their distribution). A random process is called WSS if (page 550&lt;ref name=kay/&gt;)
# It has constant mean, \ie &lt;math&gt;\mu_X[n] = \mu&lt;/math&gt;
# If it's covariance depends only  on the time difference, \ie &lt;math&gt;c_X[n_1,n_2] = g(|n_2-n_1|)&lt;/math&gt;

for some function &lt;math&gt;g&lt;/math&gt;.

Important: For Gaussian random processes, WSS is equivalent to stationarity.

== References ==</text>
      <sha1>4ndvri4ksgo0ng2jxmju0vrf6u5tosm</sha1>
    </revision>
    <revision>
      <id>499</id>
      <parentid>498</parentid>
      <timestamp>2019-08-04T15:50:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1290">===Stationary processes===

A random process is called stationary if it's FDD (finite-dimensional distribution) does not change with the time origin. Recall that a random process is infinite dimensional, and it's finite dimensional distribution is simply defined as a probability distribution (page 523&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes Using MATLAB, 2006&lt;/ref&gt;):

&lt;math&gt;P_{X[n_1], X[n_2], \dots, X_[n_N]}.&lt;/math&gt;

Then, it is called stationary if this distribution does not change with time origin, i.e., if

&lt;math&gt;P_{X[n_1], X[n_2], \dots, X_[n_N]} = P_{X[n_0+n_1], X[n_0+n_2], \dots, X_[n_0+n_N]}&lt;/math&gt;

for any time offset &lt;math&gt;n_0&lt;/math&gt;

===Wide-sense stationary processes===

The condition of stationarity is quite strong. A somewhat more relaxed condition is wide-sense stationarity (WSS). WSS processes are defined in terms of their mean and their covariance (and not their distribution). A random process is called WSS if (page 550&lt;ref name=kay/&gt;)
# It has constant mean, \ie &lt;math&gt;\mu_X[n] = \mu&lt;/math&gt;
# If it's covariance depends only  on the time difference, \ie &lt;math&gt;c_X[n_1,n_2] = g(|n_2-n_1|)&lt;/math&gt;

for some function &lt;math&gt;g&lt;/math&gt;.

'''Important:''' For Gaussian random processes, WSS is equivalent to stationarity.

== References ==</text>
      <sha1>a2rzrd3zeqj4p1u1x2v08ccu7411hed</sha1>
    </revision>
  </page>
  <page>
    <title>Autocorrelation</title>
    <ns>0</ns>
    <id>159</id>
    <revision>
      <id>500</id>
      <timestamp>2019-08-04T15:55:49Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Autocorrelation is a type of function that is defined for [[Stationarity|wide sense stationary]] (WSS) [[random process|random processes]]. It makes sense only for WSS because..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="582">Autocorrelation is a type of function that is defined for [[Stationarity|wide sense stationary]] (WSS) [[random process|random processes]]. It makes sense only for WSS because it's a function of the difference of two timepoints, and for WSS the covariance of the random process depends only on the difference.

Autocorrelation is defined as the joint moment

&lt;math&gt;r_X[k] = E[X[n] X[n+k]]&lt;/math&gt;.

This function makes sense only if we can choose &lt;math&gt;n&lt;/math&gt; arbitrarily, which we can do for a WSS random process (as its covariance depends only on the difference, &lt;math&gt;k&lt;/math&gt;).</text>
      <sha1>r1nsh0igkzb54rqx0rj021kehkivd80</sha1>
    </revision>
    <revision>
      <id>501</id>
      <parentid>500</parentid>
      <timestamp>2019-08-04T16:02:00Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1031">Autocorrelation is a type of function that is defined for [[Stationarity|wide sense stationary]] (WSS) [[random process|random processes]]. It makes sense only for WSS because it's a function of the difference of two timepoints, and for WSS the covariance of the random process depends only on the difference.

Autocorrelation is defined as the joint moment (page 552&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB, 2006&lt;/ref&gt;)

&lt;math&gt;r_X[k] = E[X[n] X[n+k]]&lt;/math&gt;.

This function makes sense only if we can choose &lt;math&gt;n&lt;/math&gt; arbitrarily, which we can do for a WSS random process (as its covariance depends only on the difference, &lt;math&gt;k&lt;/math&gt;).

=== What's the point of autocorrelation?===
Autocorrelation function gives some idea about what the outcome of a RP will be near a given time point. For example, if a RP &lt;math&gt;X[n]&lt;/math&gt; depends mostly on the values that are close to &lt;math&gt;n&lt;/math&gt;, then the autocorrelation will quickly die out as &lt;math&gt;k&lt;/math&gt; increases.

== References ==</text>
      <sha1>rs0c1xtd32h5ih40oxej2v95a55u89f</sha1>
    </revision>
  </page>
  <page>
    <title>Power spectral density</title>
    <ns>0</ns>
    <id>160</id>
    <revision>
      <id>502</id>
      <timestamp>2019-08-04T16:09:06Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Power spectral density (PSD) and [[autocorrelation]] are like two sides of the same coin (autocorrelation can be recovered from PSD -- see page 575&lt;ref name=kay&gt;Steven Kay, In..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="701">Power spectral density (PSD) and [[autocorrelation]] are like two sides of the same coin (autocorrelation can be recovered from PSD -- see page 575&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB, 2006&lt;/ref&gt;). PSD aims at explaining the amount of fluctuation in a random process &lt;math&gt;X[n]&lt;/math&gt;. PSD is like the equivalent of Fourier transform for probabilistic sequences: Just like the Fourier transform explains the fluctuations in a deterministic sequence, so does PSD describe the fluctuations in a random process. If a random process fluctuates a lot, then its PSD will die out slowly, whereas if it depends a lot on the past, then its PSD will die out quickly.</text>
      <sha1>kdrwcpedtjzhidtbd0yuh9066v32kui</sha1>
    </revision>
    <revision>
      <id>503</id>
      <parentid>502</parentid>
      <timestamp>2019-08-04T16:09:20Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="719">Power spectral density (PSD) and [[autocorrelation]] are like two sides of the same coin (autocorrelation can be recovered from PSD -- see page 575&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB, 2006&lt;/ref&gt;). PSD aims at explaining the amount of fluctuation in a random process &lt;math&gt;X[n]&lt;/math&gt;. PSD is like the equivalent of Fourier transform for probabilistic sequences: Just like the Fourier transform explains the fluctuations in a deterministic sequence, so does PSD describe the fluctuations in a random process. If a random process fluctuates a lot, then its PSD will die out slowly, whereas if it depends a lot on the past, then its PSD will die out quickly.

== References ==</text>
      <sha1>ps75d6qkypl6wwm7bi04abl1dqo8l24</sha1>
    </revision>
  </page>
  <page>
    <title>Ergodicity</title>
    <ns>0</ns>
    <id>161</id>
    <revision>
      <id>504</id>
      <timestamp>2019-08-04T16:21:28Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "In a word, ergodicity means that averaging over time and averaging over ensembles is the same thing. That is, consider (i) a single realization of a random process, &lt;math&gt;x_i[..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="596">In a word, ergodicity means that averaging over time and averaging over ensembles is the same thing. That is, consider (i) a single realization of a random process, &lt;math&gt;x_i[n]&lt;/math&gt;, and (ii) the random process &lt;math&gt;x_1[n_0],x_2[n_0], \dots&lt;/math&gt; that contains an infinite number of random variables obtained by picking the timepoint &lt;math&gt;n_0&lt;/math&gt; fro all the realizations &lt;math&gt;x_1, x_2, \dots&lt;/math&gt;. Then, ergodicity says that the two aforementioned means will be the same, i.e.,

&lt;math&gt;\lim_{M\to \infty}\frac{1}{M}\sum_{m=1}^M x_m[n_0] = \lim_{N\to \infty} \sum_{n=1}^N x_i[n]&lt;/math&gt;</text>
      <sha1>ndrzp42snrsv66e5si5zx4p88jgh25a</sha1>
    </revision>
    <revision>
      <id>505</id>
      <parentid>504</parentid>
      <timestamp>2019-08-04T16:29:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1554">In a word, ergodicity means that averaging over time and averaging over ensembles is the same thing (page 562&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB, 2006&lt;/ref&gt;). That is, consider (i) a single realization of a random process, &lt;math&gt;x_i[n]&lt;/math&gt;, and (ii) the random process &lt;math&gt;x_1[n_0],x_2[n_0], \dots&lt;/math&gt; that contains an infinite number of random variables obtained by picking the timepoint &lt;math&gt;n_0&lt;/math&gt; for all the realizations &lt;math&gt;x_1, x_2, \dots&lt;/math&gt;. Then, ergodicity says that the two aforementioned means will be the same, i.e.,

&lt;math&gt;\lim_{M\to \infty}\frac{1}{M}\sum_{m=1}^M x_m[n_0] = \lim_{N\to \infty} \sum_{n=1}^N x_i[n]&lt;/math&gt;

for any realization &lt;math&gt;x_i&lt;/math&gt;.  In short, we can say that for an ergodic random process a single realization is representative of an ensemble of realizations.

Ergodicity does not apply only for the mean,but also for other moments. E.g., we can say ergodic in the covariance etc.

* Fig. 17.8 gives nice example of non-ergodic random process

=== What's the point of ergodicity? ===

Ergodicity is a critical property as it allows us to infer properties of a random process using a single realization. In practice we never have infinite observations, but if we have a long sequence of observations, ergodicity tells us that we can divide this sequence into chunks and treat each chunk as a separate realization. This allows us, for example, to obtain better estimates of the [[Power spectral density|PSD]] function (see page 579&lt;ref name=kay/&gt;).</text>
      <sha1>3guuybap0lmdghvkvqzxj0puldbjip6</sha1>
    </revision>
    <revision>
      <id>506</id>
      <parentid>505</parentid>
      <timestamp>2019-08-04T16:29:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1572">In a word, ergodicity means that averaging over time and averaging over ensembles is the same thing (page 562&lt;ref name=kay&gt;Steven Kay, Intuitive Probability and Random Processes using MATLAB, 2006&lt;/ref&gt;). That is, consider (i) a single realization of a random process, &lt;math&gt;x_i[n]&lt;/math&gt;, and (ii) the random process &lt;math&gt;x_1[n_0],x_2[n_0], \dots&lt;/math&gt; that contains an infinite number of random variables obtained by picking the timepoint &lt;math&gt;n_0&lt;/math&gt; for all the realizations &lt;math&gt;x_1, x_2, \dots&lt;/math&gt;. Then, ergodicity says that the two aforementioned means will be the same, i.e.,

&lt;math&gt;\lim_{M\to \infty}\frac{1}{M}\sum_{m=1}^M x_m[n_0] = \lim_{N\to \infty} \sum_{n=1}^N x_i[n]&lt;/math&gt;

for any realization &lt;math&gt;x_i&lt;/math&gt;.  In short, we can say that for an ergodic random process a single realization is representative of an ensemble of realizations.

Ergodicity does not apply only for the mean,but also for other moments. E.g., we can say ergodic in the covariance etc.

* Fig. 17.8 gives nice example of non-ergodic random process

=== What's the point of ergodicity? ===

Ergodicity is a critical property as it allows us to infer properties of a random process using a single realization. In practice we never have infinite observations, but if we have a long sequence of observations, ergodicity tells us that we can divide this sequence into chunks and treat each chunk as a separate realization. This allows us, for example, to obtain better estimates of the [[Power spectral density|PSD]] function (see page 579&lt;ref name=kay/&gt;).

== References ==</text>
      <sha1>37ztsuj0wrglmgqcu58oghidcgfaw9x</sha1>
    </revision>
  </page>
  <page>
    <title>Convex set</title>
    <ns>0</ns>
    <id>162</id>
    <revision>
      <id>530</id>
      <timestamp>2020-03-21T14:24:58Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A set &lt;math&gt;C&lt;/math&gt; is ''convex''&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. if the line segment between two points in &lt;math&gt;C&lt;/math&gt; lies in &lt;math&gt;C&lt;/mat..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="478">A set &lt;math&gt;C&lt;/math&gt; is ''convex''&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. if the line segment between two points in &lt;math&gt;C&lt;/math&gt; lies in &lt;math&gt;C&lt;/math&gt;, i.e., if &lt;math&gt;\theta x_1 + (1-\theta) x_2 \in C&lt;/math&gt; and &lt;math&gt;0 \le \theta \le 1&lt;/math&gt;.

=== Convex Combination ===
A linear combination of points &lt;math&gt;\sum_{i} \theta_i x_i&lt;/math&gt; where &lt;math&gt;\sum_i \theta_i = 1&lt;/math&gt; and &lt;math&gt;\theta_i \ge 0&lt;/math&gt; is a convex combination

== References ==</text>
      <sha1>a3i9mx7tinvkv8qssf8n4exuvzr5u96</sha1>
    </revision>
    <revision>
      <id>561</id>
      <parentid>530</parentid>
      <timestamp>2020-03-22T14:15:12Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2145">A set &lt;math&gt;C&lt;/math&gt; is ''convex''&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. if the line segment between two points in &lt;math&gt;C&lt;/math&gt; lies in &lt;math&gt;C&lt;/math&gt;, i.e., if &lt;math&gt;\theta x_1 + (1-\theta) x_2 \in C&lt;/math&gt; and &lt;math&gt;0 \le \theta \le 1&lt;/math&gt;.

=== Convex Combination ===
A linear combination of points &lt;math&gt;\sum_{i} \theta_i x_i&lt;/math&gt; where &lt;math&gt;\sum_i \theta_i = 1&lt;/math&gt; and &lt;math&gt;\theta_i \ge 0&lt;/math&gt; is a convex combination

=== Operations that preserve set convexity ===

* '''Intersection}: Convexity is preserved under intersection; the intersection of even infinite convex sets is convex.
* '''Affine functions.''' Let &lt;math&gt;f&lt;/math&gt; be an affine function, \ie &lt;math&gt;f(x)=Ax+b&lt;/math&gt;. Then the image of &lt;math&gt;S&lt;/math&gt; under &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;f(S) = \{f(x) \mid x \in S\}&lt;/math&gt;
and the inverse image of &lt;math&gt;S&lt;/math&gt; under &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;f^{-1}(S) = \{x \mid f(x) \in S \}&lt;/math&gt;
are both convex if &lt;math&gt;S&lt;/math&gt; is convex.

* ''Cartesian product''. Define &lt;math&gt;S:=S_1\times S_2&lt;/math&gt; for two convex sets &lt;math&gt;S_1, S_2&lt;/math&gt;. Then, &lt;math&gt;S&lt;/math&gt; is convex.
* ''Sum'' The sum &lt;math&gt;S&lt;/math&gt; of two convex sets &lt;math&gt;S_1, S_2&lt;/math&gt;, &lt;math&gt;S=\{x_1 + x_2 : x_1 \in S_1, S_2 \in x_2\}&lt;/math&gt; is convex.

* '''Linear-fractional and perspective functions.'''

	* ''Perspective function'' is the &lt;math&gt;R^{n+1} \to \mathbf R^n &lt;/math&gt; function  &lt;math&gt;P(x,t) = x/t&lt;/math&gt; with domain &lt;math&gt;\textbf{dom}{P} = \mathbf R ^{n+1} \times \mathbf R_{++}&lt;/math&gt;. That is, the perspective function normalizes the input vector so the last element is one, and then drops this last element.

	If a set &lt;math&gt;C \subseteq \setop{dom}{P}&lt;/math&gt; is convex, then its image under &lt;math&gt;P&lt;/math&gt; is also convex.
	* ''Linear-fractional function'' is the composition &lt;math&gt;P \circ g&lt;/math&gt; of a perspective function &lt;math&gt;P&lt;/math&gt; with an affine function &lt;math&gt;g&lt;/math&gt;.

	It is easy to show that linear-fraction functions preserve convexity: If &lt;math&gt;S&lt;/math&gt; is convex, then  its image &lt;math&gt;g(S)&lt;/math&gt; under &lt;math&gt;g&lt;/math&gt; will be convex, then its image under perspective will also be convex.



== References ==</text>
      <sha1>6gj9ldukdft688r9ihw0la1uigvfohr</sha1>
    </revision>
    <revision>
      <id>562</id>
      <parentid>561</parentid>
      <timestamp>2020-03-22T14:19:47Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2140">A set &lt;math&gt;C&lt;/math&gt; is ''convex''&lt;ref&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;. if the line segment between two points in &lt;math&gt;C&lt;/math&gt; lies in &lt;math&gt;C&lt;/math&gt;, i.e., if &lt;math&gt;\theta x_1 + (1-\theta) x_2 \in C&lt;/math&gt; and &lt;math&gt;0 \le \theta \le 1&lt;/math&gt;.

=== Convex Combination ===
A linear combination of points &lt;math&gt;\sum_{i} \theta_i x_i&lt;/math&gt; where &lt;math&gt;\sum_i \theta_i = 1&lt;/math&gt; and &lt;math&gt;\theta_i \ge 0&lt;/math&gt; is a convex combination

== Operations that preserve set convexity ==

=== Intersection===
Convexity is preserved under intersection; the intersection of even infinite convex sets is convex.

=== Affine functions===

Let &lt;math&gt;f&lt;/math&gt; be an affine function, \ie &lt;math&gt;f(x)=Ax+b&lt;/math&gt;. Then the image of &lt;math&gt;S&lt;/math&gt; under &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;f(S) = \{f(x) \mid x \in S\}&lt;/math&gt;
and the inverse image of &lt;math&gt;S&lt;/math&gt; under &lt;math&gt;f&lt;/math&gt;, &lt;math&gt;f^{-1}(S) = \{x \mid f(x) \in S \}&lt;/math&gt;
are both convex if &lt;math&gt;S&lt;/math&gt; is convex.

* ''Cartesian product''. Define &lt;math&gt;S:=S_1\times S_2&lt;/math&gt; for two convex sets &lt;math&gt;S_1, S_2&lt;/math&gt;. Then, &lt;math&gt;S&lt;/math&gt; is convex.
* ''Sum'' The sum &lt;math&gt;S&lt;/math&gt; of two convex sets &lt;math&gt;S_1, S_2&lt;/math&gt;, &lt;math&gt;S=\{x_1 + x_2 : x_1 \in S_1, S_2 \in x_2\}&lt;/math&gt; is convex.

=== Linear-fractional and perspective functions===

* ''Perspective function'' is the &lt;math&gt;R^{n+1} \to \mathbf R^n &lt;/math&gt; function  &lt;math&gt;P(x,t) = x/t&lt;/math&gt; with domain &lt;math&gt;\textbf{dom}{P} = \mathbf R ^{n+1} \times \mathbf R_{++}&lt;/math&gt;. That is, the perspective function normalizes the input vector so the last element is one, and then drops this last element.

	If a set &lt;math&gt;C \subseteq \textbf{dom}{P}&lt;/math&gt; is convex, then its image under &lt;math&gt;P&lt;/math&gt; is also convex.

* ''Linear-fractional function'' is the composition &lt;math&gt;P \circ g&lt;/math&gt; of a perspective function &lt;math&gt;P&lt;/math&gt; with an affine function &lt;math&gt;g&lt;/math&gt;.
	It is easy to show that linear-fraction functions preserve convexity: If &lt;math&gt;S&lt;/math&gt; is convex, then  its image &lt;math&gt;g(S)&lt;/math&gt; under &lt;math&gt;g&lt;/math&gt; will be convex, then its image under perspective will also be convex.



== References ==</text>
      <sha1>kxsrnc52n67o7d1kdc279244dnce0ro</sha1>
    </revision>
  </page>
  <page>
    <title>Cone</title>
    <ns>0</ns>
    <id>163</id>
    <revision>
      <id>533</id>
      <timestamp>2020-03-21T14:55:52Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A set &lt;math&gt;C&lt;/math&gt; is called a ''cone'' if for every &lt;math&gt;x \in C&lt;/math&gt; and &lt;math&gt;\theta \ge 0&lt;/math&gt; we have &lt;math&gt;\theta x \in C&lt;/math&gt;. Cones are not necessarily conv..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1265">A set &lt;math&gt;C&lt;/math&gt; is called a ''cone'' if for every &lt;math&gt;x \in C&lt;/math&gt; and &lt;math&gt;\theta \ge 0&lt;/math&gt; we have &lt;math&gt;\theta x \in C&lt;/math&gt;. Cones are not necessarily [[convex set|convex]]. Cones can have very unintuitive shapes. For example, a line that passes from the origin is a cone. Also you can rotate the line, and the area it swipes will also form a cone. See some cone shapes in Fig. 35-41&lt;ref&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;.

=== Proper Cone ===

''Proper cone''s are a subset of cones that have more intuitive shapes. Formally, a ''proper cone'' &lt;math&gt;K \subseteq \mathbf{R}^n&lt;/math&gt; satisfies the following propeties&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;
* Is convex
* Is closed
* Is solid (i.e., it has nonempty interior)
* is pointed, which means that it contains no line (or, equivalently, &lt;math&gt;x \in K&lt;/math&gt;, &lt;math&gt;-x \in K \implies x = 0&lt;/math&gt;)


== What's the point of cones? ==

The concept of cones is central for defining generalized inequalities. For example, the componentwise inequality for two vectors &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;, namely &lt;math&gt;x \preceq y&lt;/math&gt;, implies that the vector &lt;math&gt;y-x&lt;/math&gt; belongs to the (proper) cone 

== References ==</text>
      <sha1>qlt9etk7dlkimexgkn1hui1vp901n6v</sha1>
    </revision>
    <revision>
      <id>534</id>
      <parentid>533</parentid>
      <timestamp>2020-03-21T14:58:19Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1462">A set &lt;math&gt;C&lt;/math&gt; is called a ''cone'' if for every &lt;math&gt;x \in C&lt;/math&gt; and &lt;math&gt;\theta \ge 0&lt;/math&gt; we have &lt;math&gt;\theta x \in C&lt;/math&gt;. Cones are not necessarily [[convex set|convex]]. Cones can have very unintuitive shapes. For example, a line that passes from the origin is a cone. Also you can rotate the line, and the area it swipes will also form a cone. See some cone shapes in Fig. 35-41&lt;ref&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;.

=== Proper Cone ===

''Proper cone''s are a subset of cones that have more intuitive shapes. Formally, a ''proper cone'' &lt;math&gt;K \subseteq \mathbf{R}^n&lt;/math&gt; satisfies the following propeties&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;
* Is convex
* Is closed
* Is solid (i.e., it has nonempty interior)
* is pointed, which means that it contains no line (or, equivalently, &lt;math&gt;x \in K&lt;/math&gt;, &lt;math&gt;-x \in K \implies x = 0&lt;/math&gt;)


== What's the point of cones? ==

The concept of cones is central for defining generalized inequalities. For example, the componentwise inequality for two vectors &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;, namely &lt;math&gt;x \preceq y&lt;/math&gt;, implies that the vector &lt;math&gt;y-x&lt;/math&gt; belongs to nonnegative orthant, namely &lt;math&gt;\mathbf{R}^n_+&lt;/math&gt;&lt;ref&gt;This is simply the space defined by nonnegative coefficients, e.g. for &lt;math&gt;\mathbf R^2&lt;/math&gt; it's the top right quadrant.&lt;/ref&gt;, which is a proper cone.

== References ==</text>
      <sha1>m14jt3lsh0s6lb25izqrc5cejss22tv</sha1>
    </revision>
    <revision>
      <id>535</id>
      <parentid>534</parentid>
      <timestamp>2020-03-21T14:58:43Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1461">A set &lt;math&gt;C&lt;/math&gt; is called a ''cone'' if for every &lt;math&gt;x \in C&lt;/math&gt; and &lt;math&gt;\theta \ge 0&lt;/math&gt; we have &lt;math&gt;\theta x \in C&lt;/math&gt;. Cones are not necessarily [[convex set|convex]]. Cones can have very unintuitive shapes. For example, a line that passes from the origin is a cone. Also you can rotate the line, and the area it swipes will also form a cone. See some cone shapes in Fig. 35-41&lt;ref&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;.

=== Proper Cone ===

''Proper cone''s are a subset of cones that have more intuitive shapes. Formally, a ''proper cone'' &lt;math&gt;K \subseteq \mathbf{R}^n&lt;/math&gt; satisfies the following propeties&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;
* Is convex
* Is closed
* Is solid (i.e., it has nonempty interior)
* is pointed, which means that it contains no line (or, equivalently, &lt;math&gt;x \in K&lt;/math&gt;, &lt;math&gt;-x \in K \implies x = 0&lt;/math&gt;)

== What's the point of cones? ==

The concept of cones is central for defining generalized inequalities. For example, the componentwise inequality for two vectors &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;, namely &lt;math&gt;x \preceq y&lt;/math&gt;, implies that the vector &lt;math&gt;y-x&lt;/math&gt; belongs to nonnegative orthant, namely &lt;math&gt;\mathbf{R}^n_+&lt;/math&gt;&lt;ref&gt;This is simply the space defined by nonnegative coefficients, e.g. for &lt;math&gt;\mathbf R^2&lt;/math&gt; it's the top right quadrant.&lt;/ref&gt;, which is a proper cone.

== References ==</text>
      <sha1>jwhuqt6xo28xqzx68wbt0ucqhb1toqk</sha1>
    </revision>
  </page>
  <page>
    <title>Proper cone</title>
    <ns>0</ns>
    <id>164</id>
    <redirect title="Cone" />
    <revision>
      <id>536</id>
      <timestamp>2020-03-21T15:01:59Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Cone#Proper cone]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30">#REDIRECT [[Cone#Proper cone]]</text>
      <sha1>k630fj1jvrm9wcebil4z3sezziqxwx1</sha1>
    </revision>
  </page>
  <page>
    <title>Interior</title>
    <ns>0</ns>
    <id>165</id>
    <revision>
      <id>537</id>
      <timestamp>2020-03-21T15:34:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "Imprecisely speaking, the ''interior'' of a set contains all the points of the set except its bounds. The interior of some sets is empty. For example, a line in &lt;math&gt;\mathbf..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="587">Imprecisely speaking, the ''interior'' of a set contains all the points of the set except its bounds. The interior of some sets is empty. For example, a line in &lt;math&gt;\mathbf R^2&lt;/math&gt; has empty interior. Similarly, any point has zero interior. A set that has nonempty interior in one [[ambient space]] can have empty interior in another. For example, a 2D disk has nonempty interior in &lt;math&gt;\mathbf R^2&lt;/math&gt; but empty interior in &lt;math&gt;\mathbf R^3.&lt;/math&gt;

Sometimes, we want to expand sets so that their interior is nonempty. To this end, we define the &lt;math&gt;\textbf{relint}&lt;/math&gt;</text>
      <sha1>rj9b0s2x3mrzxxdek4lyfbdxr9ahzdh</sha1>
    </revision>
    <revision>
      <id>538</id>
      <parentid>537</parentid>
      <timestamp>2020-03-21T15:37:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="915">Imprecisely speaking, the ''interior'' of a set &lt;math&gt;C&lt;/math&gt;, namely &lt;math&gt;\textbf{int} C&lt;/math&gt; contains all the points of the set except its bounds. The interior of some sets is empty. For example, a line in &lt;math&gt;\mathbf R^2&lt;/math&gt; has empty interior. Similarly, any point has zero interior. A set that has nonempty interior in one [[ambient space]] can have empty interior in another. For example, a 2D disk has nonempty interior in &lt;math&gt;\mathbf R^2&lt;/math&gt; but empty interior in &lt;math&gt;\mathbf R^3.&lt;/math&gt;

Sometimes, we want to expand sets so that their interior is nonempty. To this end, we define the &lt;math&gt;\textbf{relint}&lt;/math&gt; operator. Intuitively, this operator "fattens" a set so that its bound is nonzero. It performs this through intersection with a Euclidean ball:

&lt;math&gt;\textbf{relint}\, C = \{\setop{relint}{C} = \{x \in C | B(x,r) \cap \setop{aff}{C} \subseteq C\text{ for some }r&gt;0\}\}&lt;/math&gt;</text>
      <sha1>qw9176pemuic3krbz813jmh7uvlt95m</sha1>
    </revision>
    <revision>
      <id>539</id>
      <parentid>538</parentid>
      <timestamp>2020-03-21T15:39:44Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="995">Imprecisely speaking, the ''interior'' of a set &lt;math&gt;C&lt;/math&gt;, namely &lt;math&gt;\textbf{int} C&lt;/math&gt; contains all the points of the set except its bounds. The interior of some sets is empty. For example, a line in &lt;math&gt;\mathbf R^2&lt;/math&gt; has empty interior. Similarly, any point has zero interior. A set that has nonempty interior in one [[ambient space]] can have empty interior in another. For example, a 2D disk has nonempty interior in &lt;math&gt;\mathbf R^2&lt;/math&gt; but empty interior in &lt;math&gt;\mathbf R^3.&lt;/math&gt;

Sometimes, we want to expand sets so that their interior is nonempty. To this end, we define the &lt;math&gt;\textbf{relint}&lt;/math&gt; operator. Intuitively, this operator "fattens" a set so that its bound is nonzero. It performs this through intersection with a Euclidean ball:

&lt;math&gt;\textbf{relint}\, C = \{\textbf{relint}{C} = \{x \in C | B(x,r) \cap \textbf{aff}{C} \subseteq C\text{ for some }r&gt;0\}\},&lt;/math&gt;

where &lt;math&gt;\textbf{aff} C&lt;/math&gt; is the [[affine hull]] of &lt;math&gt;C&lt;/math&gt;.</text>
      <sha1>6l4cksked0c6alkhwpv0wkjr4i9j5il</sha1>
    </revision>
    <revision>
      <id>542</id>
      <parentid>539</parentid>
      <timestamp>2020-03-21T15:45:48Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="969">Imprecisely speaking, the ''interior'' of a set &lt;math&gt;C&lt;/math&gt;, namely &lt;math&gt;\textbf{int} C&lt;/math&gt; contains all the points of the set except its bounds. The interior of some sets is empty. For example, a line in &lt;math&gt;\mathbf R^2&lt;/math&gt; has empty interior. Similarly, any point has zero interior. A set that has nonempty interior in one [[ambient space]] can have empty interior in another. For example, a 2D disk has nonempty interior in &lt;math&gt;\mathbf R^2&lt;/math&gt; but empty interior in &lt;math&gt;\mathbf R^3.&lt;/math&gt;

Sometimes, we want to expand sets so that their interior is nonempty. To this end, we define the &lt;math&gt;\textbf{relint}&lt;/math&gt; operator. Intuitively, this operator "fattens" a set so that its bound is nonzero. It performs this through intersection with a Euclidean ball:

&lt;math&gt;\textbf{relint}{C} = \{x \in C | B(x,r) \cap \textbf{aff}{C} \subseteq C\text{ for some }r&gt;0\},&lt;/math&gt;

where &lt;math&gt;\textbf{aff} C&lt;/math&gt; is the [[affine hull]] of &lt;math&gt;C&lt;/math&gt;.</text>
      <sha1>pww8avf8dowbjwchm52ykefc3tnwhuu</sha1>
    </revision>
  </page>
  <page>
    <title>Affine set</title>
    <ns>0</ns>
    <id>166</id>
    <revision>
      <id>540</id>
      <timestamp>2020-03-21T15:44:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A set &lt;math&gt;C&lt;/math&gt; is called an affine set if any affine combination of the points in the set belongs to the set. Affine combination is just a linear combination where the c..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="307">A set &lt;math&gt;C&lt;/math&gt; is called an affine set if any affine combination of the points in the set belongs to the set. Affine combination is just a linear combination where the coefficients of the combination sum to one.

Affine sets are "very big". For example, they are lines, planes, cubes, hyperplanes etc.</text>
      <sha1>lxsv2ghdkuizc1f3lgrr915wste72vu</sha1>
    </revision>
    <revision>
      <id>564</id>
      <parentid>540</parentid>
      <timestamp>2020-03-22T14:28:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="411">A set &lt;math&gt;C&lt;/math&gt; is called an affine set if any affine combination of the points in the set belongs to the set. Affine combination is just a linear combination where the coefficients of the combination sum to one.

Affine sets are "very big". For example, they are lines, planes, subspaces, cubes, hyperplanes etc. (see Section 2.2&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;).</text>
      <sha1>ewirfwx3anfs7iwbxxplm1yte8wapyj</sha1>
    </revision>
  </page>
  <page>
    <title>Affine hull</title>
    <ns>0</ns>
    <id>167</id>
    <revision>
      <id>541</id>
      <timestamp>2020-03-21T15:45:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The affine hull of a set &lt;math&gt;C&lt;/math&gt;, namely &lt;math&gt;\textbf{aff} C&lt;/math&gt;, is the set of all affine combinations of all the points in &lt;math&gt;C&lt;/math&gt;. Thus, the affine hull i..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="349">The affine hull of a set &lt;math&gt;C&lt;/math&gt;, namely &lt;math&gt;\textbf{aff} C&lt;/math&gt;, is the set of all affine combinations of all the points in &lt;math&gt;C&lt;/math&gt;. Thus, the affine hull is the smallest affine set that contains all the points in &lt;math&gt;C&lt;/math&gt;. Affine hulls are "very big", because [[affine set|affine sets]] are "very big" (see [[affine set]]).</text>
      <sha1>djuo9f5eawxnllv0odu60p2guo727j6</sha1>
    </revision>
  </page>
  <page>
    <title>Hyperplane</title>
    <ns>0</ns>
    <id>168</id>
    <revision>
      <id>543</id>
      <timestamp>2020-03-21T15:57:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A ''hyperplane'' is a set of the form  &lt;math&gt;\{x | a^T x = b\}&lt;/math&gt;  for some row vector &lt;math&gt;a^T&lt;/math&gt; and a scalar &lt;math&gt;b&lt;/math&gt;. This set has several intuitive interpr..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="794">A ''hyperplane'' is a set of the form

&lt;math&gt;\{x | a^T x = b\}&lt;/math&gt;

for some row vector &lt;math&gt;a^T&lt;/math&gt; and a scalar &lt;math&gt;b&lt;/math&gt;. This set has several intuitive interpretations
* It is the hyperplane with a normal vector &lt;math&gt;A&lt;/math&gt; and an offset distance of &lt;math&gt;b&lt;/math&gt; from the origin
* &lt;math&gt;\{x | a^T x = b\} = \{x | a^T (x-x_0)\} = x_0 + a^\perp&lt;/math&gt; where &lt;math&gt;x_0&lt;/math&gt;is any point belonging to the hyperspace, and &lt;math&gt;a^\perp&lt;/math&gt; is the set that contains all vectors that are orthogonal to &lt;math&gt;a&lt;/math&gt;. Clearly, the latter sum operation is overloaded (its arguments are a set and a vector), and the definition of this overloading is straightforward (i.e., it's the set obtain by summing all the vectors in &lt;math&gt;a^\perp&lt;/math&gt; with the vector &lt;math&gt;x_0&lt;/math&gt;).</text>
      <sha1>2lkn1d0h8tldbl0ycvovduzbg26wbav</sha1>
    </revision>
    <revision>
      <id>545</id>
      <parentid>543</parentid>
      <timestamp>2020-03-21T16:18:27Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1448">A ''hyperplane'' is a set of the form

&lt;math&gt;\{x | a^T x = b\}&lt;/math&gt;

for some row vector &lt;math&gt;a^T&lt;/math&gt; and a scalar &lt;math&gt;b&lt;/math&gt;. This set has several intuitive interpretations
* It is the hyperplane with a normal vector &lt;math&gt;A&lt;/math&gt; and an offset distance of &lt;math&gt;b&lt;/math&gt; from the origin
* &lt;math&gt;\{x | a^T x = b\} = \{x | a^T (x-x_0)\} = x_0 + a^\perp&lt;/math&gt; where &lt;math&gt;x_0&lt;/math&gt;is any point belonging to the hyperspace, and &lt;math&gt;a^\perp&lt;/math&gt; is the set that contains all vectors that are orthogonal to &lt;math&gt;a&lt;/math&gt;. Clearly, the latter sum operation is overloaded (its arguments are a set and a vector), and the definition of this overloading is straightforward (i.e., it's the set obtain by summing all the vectors in &lt;math&gt;a^\perp&lt;/math&gt; with the vector &lt;math&gt;x_0&lt;/math&gt;).

=== Halfspace ===
Each hyperplane divides the space in which it lives into two ''halfspaces''. A halfspace is formally defined as

&lt;math&gt;\{x | a^T x \le b\}&lt;/math&gt;.

== What's the point of hyperplanes? ==
Hyperplanes are fundamental constructs in complex optimization. For example, the [[separating hyperplane theorem]] dictates that any two disjoint convex sets are separated by at least one hyperplane. Moreover, there is also the supporting hyperplane theorem, which essentially says that a set is convex, there is a supporting hyperplane on each point on its boundary (i.e., a hyperplane that does not "touch" any point on the interior of the set).</text>
      <sha1>6lm84ou53imt9xq1srocauulf8szdkx</sha1>
    </revision>
  </page>
  <page>
    <title>Separating hyperplane theorem</title>
    <ns>0</ns>
    <id>169</id>
    <revision>
      <id>544</id>
      <timestamp>2020-03-21T16:02:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "The ''separating hyperplane theorem'' says that any two disjoint convex sets are separated by at least one [[hyperplane]]&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Opti..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="196">The ''separating hyperplane theorem'' says that any two disjoint convex sets are separated by at least one [[hyperplane]]&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.</text>
      <sha1>dcpsyrh14zrdh073xd6ag7y789t2llk</sha1>
    </revision>
    <revision>
      <id>546</id>
      <parentid>544</parentid>
      <timestamp>2020-03-21T16:29:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="214">The ''separating hyperplane theorem'' says that any two disjoint convex sets are separated by at least one [[hyperplane]]&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

== References ==</text>
      <sha1>5zbgxy79b2t7jisv5r08t2sx0cntcae</sha1>
    </revision>
  </page>
  <page>
    <title>Generalized inequality</title>
    <ns>0</ns>
    <id>170</id>
    <revision>
      <id>547</id>
      <timestamp>2020-03-21T16:32:25Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "A generalized inequality is defined in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="385">A generalized inequality is defined in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;.</text>
      <sha1>ccgyyhqipgk3cbn223fdbjpd2215bfe</sha1>
    </revision>
    <revision>
      <id>548</id>
      <parentid>547</parentid>
      <timestamp>2020-03-21T16:33:10Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="469">A generalized inequality is defined in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;,

where &lt;math&gt;\textbf{int}\, K&lt;/math&gt; is the [[interior]] of the set &lt;math&gt;K&lt;/math&gt;.</text>
      <sha1>jwq2khqfb3n1i8ydn1ci9stush3n11q</sha1>
    </revision>
    <revision>
      <id>549</id>
      <parentid>548</parentid>
      <timestamp>2020-03-21T16:36:30Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1445">A generalized inequality is defined in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;,

where &lt;math&gt;\textbf{int}\, K&lt;/math&gt; is the [[interior]] of the set &lt;math&gt;K&lt;/math&gt;.

=== Properties of generalized inequalities ===

A generalized inequality has the following interesting properties
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under addition: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;u \preceq_K y&lt;/math&gt;, then &lt;math&gt;x+u \preceq_K y+v&lt;/math&gt;
# &lt;math&gt;\preceq_K&lt;/math&gt; is transitive: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K z&lt;/math&gt;, then &lt;math&gt;x \preceq_K z&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under nonnegative scaling: If &lt;math&gt;x\preceq_K y&lt;/math&gt; and &lt;math&gt;\alpha &gt; 0&lt;/math&gt;, then &lt;math&gt;\alpha  x \preceq_K \alpha y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is reflexive: If &lt;math&gt;x \preceq_K x&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is antisymmetric: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K x&lt;/math&gt;, then &lt;math&gt;x = y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under limits: If &lt;math&gt;x_i \preceq_K y_i&lt;/math&gt; for &lt;math&gt;i=1,2,\dots&lt;/math&gt; and &lt;math&gt;x_i \to x&lt;/math&gt; and &lt;math&gt;y_i \to i&lt;/math&gt;, then &lt;math&gt;x \preceq_K y&lt;/math&gt;.</text>
      <sha1>jcye4fli98s8io8zrl09m34c1a4h1wv</sha1>
    </revision>
    <revision>
      <id>550</id>
      <parentid>549</parentid>
      <timestamp>2020-03-21T16:36:56Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1537">A generalized inequality is defined&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;,

where &lt;math&gt;\textbf{int}\, K&lt;/math&gt; is the [[interior]] of the set &lt;math&gt;K&lt;/math&gt;.

=== Properties of generalized inequalities ===

A generalized inequality has the following interesting properties
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under addition: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;u \preceq_K y&lt;/math&gt;, then &lt;math&gt;x+u \preceq_K y+v&lt;/math&gt;
# &lt;math&gt;\preceq_K&lt;/math&gt; is transitive: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K z&lt;/math&gt;, then &lt;math&gt;x \preceq_K z&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under nonnegative scaling: If &lt;math&gt;x\preceq_K y&lt;/math&gt; and &lt;math&gt;\alpha &gt; 0&lt;/math&gt;, then &lt;math&gt;\alpha  x \preceq_K \alpha y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is reflexive: If &lt;math&gt;x \preceq_K x&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is antisymmetric: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K x&lt;/math&gt;, then &lt;math&gt;x = y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under limits: If &lt;math&gt;x_i \preceq_K y_i&lt;/math&gt; for &lt;math&gt;i=1,2,\dots&lt;/math&gt; and &lt;math&gt;x_i \to x&lt;/math&gt; and &lt;math&gt;y_i \to i&lt;/math&gt;, then &lt;math&gt;x \preceq_K y&lt;/math&gt;.

== References ==</text>
      <sha1>fprkeyg6047jjmrd76g3xdl524zfx5p</sha1>
    </revision>
    <revision>
      <id>554</id>
      <parentid>550</parentid>
      <timestamp>2020-03-21T21:09:38Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3993">A generalized inequality is defined&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;,

where &lt;math&gt;\textbf{int}\, K&lt;/math&gt; is the [[interior]] of the set &lt;math&gt;K&lt;/math&gt;.

=== Properties of generalized inequalities ===

A generalized inequality has the following interesting properties
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under addition: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;u \preceq_K y&lt;/math&gt;, then &lt;math&gt;x+u \preceq_K y+v&lt;/math&gt;
# &lt;math&gt;\preceq_K&lt;/math&gt; is transitive: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K z&lt;/math&gt;, then &lt;math&gt;x \preceq_K z&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under nonnegative scaling: If &lt;math&gt;x\preceq_K y&lt;/math&gt; and &lt;math&gt;\alpha &gt; 0&lt;/math&gt;, then &lt;math&gt;\alpha  x \preceq_K \alpha y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is reflexive: If &lt;math&gt;x \preceq_K x&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is antisymmetric: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K x&lt;/math&gt;, then &lt;math&gt;x = y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under limits: If &lt;math&gt;x_i \preceq_K y_i&lt;/math&gt; for &lt;math&gt;i=1,2,\dots&lt;/math&gt; and &lt;math&gt;x_i \to x&lt;/math&gt; and &lt;math&gt;y_i \to i&lt;/math&gt;, then &lt;math&gt;x \preceq_K y&lt;/math&gt;.

== Minimum and minimal elements ==

Generalized inequalities allow us to generalize the concept of a minimum element to the space of vectors. 

An essential difference between a regular inequality and a generalized one is that ''not all points are comparable''. That is, one of the two inequalities $x \le y$ or $x \le y$ has to hold. This is not the case for generalized inequality. 

'''Example.''' Consider the proper cone &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt;, and points &lt;math&gt;x=(3,3)&lt;/math&gt;, &lt;math&gt;y=(5,5)&lt;/math&gt; and &lt;math&gt;z=(4,2)&lt;/math&gt;. Clearly, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; are comparable and &lt;math&gt;x\preceq_K y&lt;/math&gt;. Similarly, &lt;math&gt;y&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are comparable and &lt;math&gt;z \preceq_K y&lt;/math&gt; . However, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are not comparable.

Also, a natural concept to define is that of the ''minimal'' point. The formal definitions of minimum and minimal points are as below. 

''Minimum element''. We say that &lt;math&gt;x \in S&lt;/math&gt; is the minimum element of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for every &lt;math&gt;y \in S&lt;/math&gt; we have &lt;math&gt;x \preceq_K y&lt;/math&gt;, which happens if and only if &lt;math&gt;S \subseteq x+K&lt;/math&gt;
	where &lt;math&gt;x+K&lt;/math&gt; is the set of all the points that are (i) comparable to &lt;math&gt;x&lt;/math&gt; and (ii) greater than or equal to &lt;math&gt;x&lt;/math&gt; (confer Fig. 2.17 of &lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; or Fig. 43 of &lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;).

There can be at most ''one'' minimum point.
''Minimal element''. First of all, a minimum point is also a minimal point. But a minimal point can exist even if there is no minimum. There can be more than one minimal points.

We say that &lt;math&gt;x\in S&lt;/math&gt; is the minimal point of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for any &lt;math&gt;y\in S&lt;/math&gt;, &lt;math&gt;y\preceq_K&lt;/math&gt; holds only if &lt;math&gt;y=x&lt;/math&gt;. Or, equivalently,

	&lt;math&gt; (x-K) \cap S = \{x\} &lt;/math&gt;

	where &lt;math&gt;x-K&lt;/math&gt; denotes the set of all points that are comparable to &lt;math&gt;x&lt;/math&gt; and are less then or equal to &lt;math&gt;x&lt;/math&gt; w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;.

Confer Fig. 2.17 of &lt;ref name=cvbook/&gt; or Fig. 43 of &lt;ref name=dattoro/&gt;. Note that in Fig. 43b it's impossible to draw the cone &lt;math&gt;K&lt;/math&gt; (centered on any of the minimal points) that would contain the entire &lt;math&gt;\mathcal C_2&lt;/math&gt;, therefore &lt;math&gt;\mathcal{C}_2&lt;/math&gt; has no minimum.


== References ==</text>
      <sha1>46ar9q1i6kzcvkfbrxnxl9r88ch46md</sha1>
    </revision>
    <revision>
      <id>555</id>
      <parentid>554</parentid>
      <timestamp>2020-03-21T21:12:39Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4015">A generalized inequality is defined&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;,

where &lt;math&gt;\textbf{int}\, K&lt;/math&gt; is the [[interior]] of the set &lt;math&gt;K&lt;/math&gt;.

=== Properties of generalized inequalities ===

A generalized inequality has the following interesting properties
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under addition: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;u \preceq_K y&lt;/math&gt;, then &lt;math&gt;x+u \preceq_K y+v&lt;/math&gt;
# &lt;math&gt;\preceq_K&lt;/math&gt; is transitive: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K z&lt;/math&gt;, then &lt;math&gt;x \preceq_K z&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under nonnegative scaling: If &lt;math&gt;x\preceq_K y&lt;/math&gt; and &lt;math&gt;\alpha &gt; 0&lt;/math&gt;, then &lt;math&gt;\alpha  x \preceq_K \alpha y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is reflexive: If &lt;math&gt;x \preceq_K x&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is antisymmetric: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K x&lt;/math&gt;, then &lt;math&gt;x = y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under limits: If &lt;math&gt;x_i \preceq_K y_i&lt;/math&gt; for &lt;math&gt;i=1,2,\dots&lt;/math&gt; and &lt;math&gt;x_i \to x&lt;/math&gt; and &lt;math&gt;y_i \to i&lt;/math&gt;, then &lt;math&gt;x \preceq_K y&lt;/math&gt;.

== Minimum and minimal elements ==

Generalized inequalities allow us to generalize the concept of a minimum element to the space of vectors. 

An essential difference between a regular inequality and a generalized one is that ''not all points are comparable''. That is, one of the two inequalities &lt;math&gt;x \le y&lt;/math&gt; or &lt;math&gt;x \le y&lt;/math&gt; has to hold. This is not the case for generalized inequality. 

'''Example.''' Consider the proper cone &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt;, and points &lt;math&gt;x=(3,3)&lt;/math&gt;, &lt;math&gt;y=(5,5)&lt;/math&gt; and &lt;math&gt;z=(4,2)&lt;/math&gt;. Clearly, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; are comparable and &lt;math&gt;x\preceq_K y&lt;/math&gt;. Similarly, &lt;math&gt;y&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are comparable and &lt;math&gt;z \preceq_K y&lt;/math&gt; . However, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are not comparable.

Also, a natural concept to define is that of the ''minimal'' point. The formal definitions of minimum and minimal points are as below. 

''Minimum element''. We say that &lt;math&gt;x \in S&lt;/math&gt; is the minimum element of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for every &lt;math&gt;y \in S&lt;/math&gt; we have &lt;math&gt;x \preceq_K y&lt;/math&gt;, which happens if and only if &lt;math&gt;S \subseteq x+K&lt;/math&gt;
	where &lt;math&gt;x+K&lt;/math&gt; is the set of all the points that are (i) comparable to &lt;math&gt;x&lt;/math&gt; and (ii) greater than or equal to &lt;math&gt;x&lt;/math&gt; (confer Fig. 2.17 of &lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; or Fig. 43 of &lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;).

There can be at most ''one'' minimum point.
''Minimal element''. First of all, a minimum point is also a minimal point. But a minimal point can exist even if there is no minimum. There can be more than one minimal points.

We say that &lt;math&gt;x\in S&lt;/math&gt; is the minimal point of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for any &lt;math&gt;y\in S&lt;/math&gt;, &lt;math&gt;y\preceq_K&lt;/math&gt; holds only if &lt;math&gt;y=x&lt;/math&gt;. Or, equivalently,

	&lt;math&gt; (x-K) \cap S = \{x\} &lt;/math&gt;

	where &lt;math&gt;x-K&lt;/math&gt; denotes the set of all points that are comparable to &lt;math&gt;x&lt;/math&gt; and are less then or equal to &lt;math&gt;x&lt;/math&gt; w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;.

Confer Fig. 2.17 of &lt;ref name=cvbook/&gt; or Fig. 43 of &lt;ref name=dattoro/&gt;. Note that in Fig. 43b it's impossible to draw the cone &lt;math&gt;K&lt;/math&gt; (centered on any of the minimal points) that would contain the entire &lt;math&gt;\mathcal C_2&lt;/math&gt;, therefore &lt;math&gt;\mathcal{C}_2&lt;/math&gt; has no minimum.


== References ==</text>
      <sha1>qgj4juh9kk1fuxbcogvkx0skdslindn</sha1>
    </revision>
    <revision>
      <id>556</id>
      <parentid>555</parentid>
      <timestamp>2020-03-21T21:15:26Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4018">A generalized inequality is defined&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;,

where &lt;math&gt;\textbf{int}\, K&lt;/math&gt; is the [[interior]] of the set &lt;math&gt;K&lt;/math&gt;.

=== Properties of generalized inequalities ===

A generalized inequality has the following interesting properties
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under addition: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;u \preceq_K v&lt;/math&gt;, then &lt;math&gt;x+u \preceq_K y+v&lt;/math&gt;
# &lt;math&gt;\preceq_K&lt;/math&gt; is transitive: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K z&lt;/math&gt;, then &lt;math&gt;x \preceq_K z&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under nonnegative scaling: If &lt;math&gt;x\preceq_K y&lt;/math&gt; and &lt;math&gt;\alpha &gt; 0&lt;/math&gt;, then &lt;math&gt;\alpha  x \preceq_K \alpha y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is reflexive: If &lt;math&gt;x \preceq_K x&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is antisymmetric: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K x&lt;/math&gt;, then &lt;math&gt;x = y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under limits: If &lt;math&gt;x_i \preceq_K y_i&lt;/math&gt; for &lt;math&gt;i=1,2,\dots&lt;/math&gt; and &lt;math&gt;x_i \to x&lt;/math&gt; and &lt;math&gt;y_i \to i&lt;/math&gt;, then &lt;math&gt;x \preceq_K y&lt;/math&gt;.

== Minimum and minimal elements ==

Generalized inequalities allow us to generalize the concept of a minimum element to the space of vectors. 

An essential difference between a regular inequality and a generalized one is that ''not all points are comparable''. That is, one of the two inequalities &lt;math&gt;x \le y&lt;/math&gt; or &lt;math&gt;x \le y&lt;/math&gt; has to hold. This is not the case for generalized inequality. 

''Example.'' Consider the proper cone &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt;, and points &lt;math&gt;x=(3,3)&lt;/math&gt;, &lt;math&gt;y=(5,5)&lt;/math&gt; and &lt;math&gt;z=(4,2)&lt;/math&gt;. Clearly, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; are comparable and &lt;math&gt;x\preceq_K y&lt;/math&gt;. Similarly, &lt;math&gt;y&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are comparable and &lt;math&gt;z \preceq_K y&lt;/math&gt; . However, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are not comparable.

Also, a natural concept to define is that of the ''minimal'' point. The formal definitions of minimum and minimal points are as below. 

'''Minimum element'''. We say that &lt;math&gt;x \in S&lt;/math&gt; is the minimum element of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for every &lt;math&gt;y \in S&lt;/math&gt; we have &lt;math&gt;x \preceq_K y&lt;/math&gt;, which happens if and only if &lt;math&gt;S \subseteq x+K&lt;/math&gt;
	where &lt;math&gt;x+K&lt;/math&gt; is the set of all the points that are (i) comparable to &lt;math&gt;x&lt;/math&gt; and (ii) greater than or equal to &lt;math&gt;x&lt;/math&gt; (confer Fig. 2.17 of &lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; or Fig. 43 of &lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;).

There can be at most ''one'' minimum point.

'''Minimal element'''. First of all, a minimum point is also a minimal point. But a minimal point can exist even if there is no minimum. There can be more than one minimal points.

We say that &lt;math&gt;x\in S&lt;/math&gt; is the minimal point of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for any &lt;math&gt;y\in S&lt;/math&gt;, &lt;math&gt;y\preceq_K&lt;/math&gt; holds only if &lt;math&gt;y=x&lt;/math&gt;. Or, equivalently,

	&lt;math&gt; (x-K) \cap S = \{x\} &lt;/math&gt;

	where &lt;math&gt;x-K&lt;/math&gt; denotes the set of all points that are comparable to &lt;math&gt;x&lt;/math&gt; and are less then or equal to &lt;math&gt;x&lt;/math&gt; w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;.

Confer Fig. 2.17 of &lt;ref name=cvbook/&gt; or Fig. 43 of &lt;ref name=dattoro/&gt;. Note that in Fig. 43b it's impossible to draw the cone &lt;math&gt;K&lt;/math&gt; (centered on any of the minimal points) that would contain the entire &lt;math&gt;\mathcal C_2&lt;/math&gt;, therefore &lt;math&gt;\mathcal{C}_2&lt;/math&gt; has no minimum.


== References ==</text>
      <sha1>9mbftbq78gg976ih7d80o4modawwx4m</sha1>
    </revision>
    <revision>
      <id>557</id>
      <parentid>556</parentid>
      <timestamp>2020-03-21T22:36:01Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4857">A generalized inequality is defined&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; in terms of a  [[proper cone]] &lt;math&gt;K&lt;/math&gt; and is symbolized with the notation &lt;math&gt;\preceq_K&lt;/math&gt;. The generalized inequality &lt;math&gt;\preceq_K&lt;/math&gt; is defined as 

&lt;math&gt;x \preceq_K y \implies y-x \in K&lt;/math&gt;.

A strict generalized inequality  &lt;math&gt;\prec_K&lt;/math&gt; is defined as 

&lt;math&gt;x \prec_K y \implies y-x \in \textbf{int}\, K&lt;/math&gt;,

where &lt;math&gt;\textbf{int}\, K&lt;/math&gt; is the [[interior]] of the set &lt;math&gt;K&lt;/math&gt;.

=== Properties of generalized inequalities ===

A generalized inequality has the following interesting properties
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under addition: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;u \preceq_K v&lt;/math&gt;, then &lt;math&gt;x+u \preceq_K y+v&lt;/math&gt;
# &lt;math&gt;\preceq_K&lt;/math&gt; is transitive: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K z&lt;/math&gt;, then &lt;math&gt;x \preceq_K z&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under nonnegative scaling: If &lt;math&gt;x\preceq_K y&lt;/math&gt; and &lt;math&gt;\alpha &gt; 0&lt;/math&gt;, then &lt;math&gt;\alpha  x \preceq_K \alpha y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is reflexive: If &lt;math&gt;x \preceq_K x&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is antisymmetric: If &lt;math&gt;x \preceq_K y&lt;/math&gt; and &lt;math&gt;y \preceq_K x&lt;/math&gt;, then &lt;math&gt;x = y&lt;/math&gt;.
# &lt;math&gt;\preceq_K&lt;/math&gt; is preserved under limits: If &lt;math&gt;x_i \preceq_K y_i&lt;/math&gt; for &lt;math&gt;i=1,2,\dots&lt;/math&gt; and &lt;math&gt;x_i \to x&lt;/math&gt; and &lt;math&gt;y_i \to i&lt;/math&gt;, then &lt;math&gt;x \preceq_K y&lt;/math&gt;.

== Minimum and minimal elements ==

Generalized inequalities allow us to generalize the concept of a minimum element to the space of vectors. 

An essential difference between a regular inequality and a generalized one is that ''not all points are comparable''. That is, one of the two inequalities &lt;math&gt;x \le y&lt;/math&gt; or &lt;math&gt;x \le y&lt;/math&gt; has to hold. This is not the case for generalized inequality. 

''Example.'' Consider the proper cone &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt;, and points &lt;math&gt;x=(3,3)&lt;/math&gt;, &lt;math&gt;y=(5,5)&lt;/math&gt; and &lt;math&gt;z=(4,2)&lt;/math&gt;. Clearly, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; are comparable and &lt;math&gt;x\preceq_K y&lt;/math&gt;. Similarly, &lt;math&gt;y&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are comparable and &lt;math&gt;z \preceq_K y&lt;/math&gt; . However, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; are not comparable.

Also, a natural concept to define is that of the ''minimal'' point. The formal definitions of minimum and minimal points are as below. 

'''Minimum element'''. We say that &lt;math&gt;x \in S&lt;/math&gt; is the minimum element of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for every &lt;math&gt;y \in S&lt;/math&gt; we have &lt;math&gt;x \preceq_K y&lt;/math&gt;, which happens if and only if &lt;math&gt;S \subseteq x+K&lt;/math&gt;
	where &lt;math&gt;x+K&lt;/math&gt; is the set of all the points that are (i) comparable to &lt;math&gt;x&lt;/math&gt; and (ii) greater than or equal to &lt;math&gt;x&lt;/math&gt; (confer Fig. 2.17 of &lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt; or Fig. 43 of &lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;).

There can be at most ''one'' minimum point.

'''Minimal element'''. First of all, a minimum point is also a minimal point. But a minimal point can exist even if there is no minimum. There can be more than one minimal points.

We say that &lt;math&gt;x\in S&lt;/math&gt; is the minimal point of &lt;math&gt;S&lt;/math&gt; (w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;) if for any &lt;math&gt;y\in S&lt;/math&gt;, &lt;math&gt;y\preceq_K&lt;/math&gt; holds only if &lt;math&gt;y=x&lt;/math&gt;. Or, equivalently,

	&lt;math&gt; (x-K) \cap S = \{x\} &lt;/math&gt;

	where &lt;math&gt;x-K&lt;/math&gt; denotes the set of all points that are comparable to &lt;math&gt;x&lt;/math&gt; and are less then or equal to &lt;math&gt;x&lt;/math&gt; w.r.t. &lt;math&gt;\preceq_K&lt;/math&gt;.

Confer Fig. 2.17 of &lt;ref name=cvbook/&gt; or Fig. 43 of &lt;ref name=dattoro/&gt;. Note that in Fig. 43b it's impossible to draw the cone &lt;math&gt;K&lt;/math&gt; (centered on any of the minimal points) that would contain the entire &lt;math&gt;\mathcal C_2&lt;/math&gt;, therefore &lt;math&gt;\mathcal{C}_2&lt;/math&gt; has no minimum.

== What's the point of generalized inequalities? ==
Generalized inequalities allow us to tackle some complicated, multi-dimensional optimization problems in principled ways. For example, we can  questions like "what's the minimum/minimal [[ellipsoid]] that contains a set of points". In this example, the generalized inequality is the [[matrix inequality]], and the proper cone that it is induced from is the cone of positive-definite matrices (i.e., &lt;math&gt;\mathbf S^n_{++}&lt;/math&gt;, see Example 2.18&lt;ref name=cvbook/&gt;).

== What's the point of minimal points? ==
The concept of minimal points is directly related to Pareto optimal points, which, put simply and coarsely, is the set of points that are, as far as the objective function of an optimization problem is concerned, equivalent (see page 71 and Section 4.7.4 &lt;ref name=cvbook/&gt;).


== References ==</text>
      <sha1>7wiug13wiyandyx36pfezmfuaua96jj</sha1>
    </revision>
  </page>
  <page>
    <title>Dual cone</title>
    <ns>0</ns>
    <id>171</id>
    <revision>
      <id>551</id>
      <timestamp>2020-03-21T20:48:29Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "\textbf{Dual cone}. Let &lt;math&gt;K&lt;/math&gt; be a cone. Then, the set &lt;math&gt;K^* = \{y : x^T y \ge 0 \text{ for all } x \in K\}&lt;/math&gt; is called the dual cone of &lt;math&gt;K&lt;/math&gt;.   An..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1685">\textbf{Dual cone}. Let &lt;math&gt;K&lt;/math&gt; be a cone. Then, the set &lt;math&gt;K^* = \{y : x^T y \ge 0 \text{ for all } x \in K\}&lt;/math&gt; is called the dual cone of &lt;math&gt;K&lt;/math&gt;. 

An intuitive way of grasping the meaning a dual cone is shown in Fig. 369 by Dattoro&lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;: Draw the two hyperplanes that are orthogonal to the two vertices that define the original cone &lt;math&gt;\mathcal K&lt;/math&gt; in the figure, and then their intersection (i.e., the purple area) is the dual cone (the intersection is necessary because the definition of the dual cone says that any vector in the dual cone should form an acute angle with all points of the original cone).

Some properties of dual cone &lt;math&gt;K^*&lt;/math&gt; (p51 and p53):
* A dual cone &lt;math&gt;K^*&lt;/math&gt; is a ... cone.
* &lt;math&gt;K^*&lt;/math&gt; is always ''convex''.
* &lt;math&gt;K^*&lt;/math&gt; is always closed.
* &lt;math&gt;K_1 \subseteq K_2 \implies K_2^* \subseteq K_1^*&lt;/math&gt;.
* If &lt;math&gt;K&lt;/math&gt; has nonempty interior, then &lt;math&gt;K^*&lt;/math&gt; is pointed.
* If the closure of &lt;math&gt;K&lt;/math&gt; is pointed, than &lt;math&gt;K^*&lt;/math&gt; has nonempty interior.
* &lt;math&gt;K^{**}&lt;/math&gt; is the closure of the convex hull of &lt;math&gt;K&lt;/math&gt; (hence, if &lt;math&gt;K&lt;/math&gt; is convex and closed, &lt;math&gt;K^{**}=K&lt;/math&gt;).
* If &lt;math&gt;K&lt;/math&gt; is a proper cone, then &lt;math&gt;K^*&lt;/math&gt; is also a proper cone.
* The dual cone of &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt; is itself.
* The dual cone of a line in space is its orthogonal complement.
* More generally, the dual cone of a subspace &lt;math&gt;V \subseteq \mathbf R^n&lt;/math&gt; is its orthogonal complement &lt;math&gt;\{ y : y^Tv  = 0 \text{ for all } v \in V \}&lt;/math&gt;

== References ==</text>
      <sha1>mfkx84q7deyq7suy5eehucrrgh5u40t</sha1>
    </revision>
    <revision>
      <id>552</id>
      <parentid>551</parentid>
      <timestamp>2020-03-21T20:49:45Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1794">\textbf{Dual cone}. Let &lt;math&gt;K&lt;/math&gt; be a cone. Then, the set &lt;math&gt;K^* = \{y : x^T y \ge 0 \text{ for all } x \in K\}&lt;/math&gt; is called the dual cone of &lt;math&gt;K&lt;/math&gt;. 

An intuitive way of grasping the meaning a dual cone is shown in Fig. 369 by Dattoro&lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;: Draw the two hyperplanes that are orthogonal to the two vertices that define the original cone &lt;math&gt;\mathcal K&lt;/math&gt; in the figure, and then their intersection (i.e., the purple area) is the dual cone (the intersection is necessary because the definition of the dual cone says that any vector in the dual cone should form an acute angle with all points of the original cone). As the figure shows, the smaller the original cone is the larger the dual cone. (See also property 4 below).

Some properties of dual cone &lt;math&gt;K^*&lt;/math&gt; (p51 and p53):
# A dual cone &lt;math&gt;K^*&lt;/math&gt; is a ... cone.
# &lt;math&gt;K^*&lt;/math&gt; is always ''convex''.
# &lt;math&gt;K^*&lt;/math&gt; is always closed.
# &lt;math&gt;K_1 \subseteq K_2 \implies K_2^* \subseteq K_1^*&lt;/math&gt;.
# If &lt;math&gt;K&lt;/math&gt; has nonempty interior, then &lt;math&gt;K^*&lt;/math&gt; is pointed.
# If the closure of &lt;math&gt;K&lt;/math&gt; is pointed, than &lt;math&gt;K^*&lt;/math&gt; has nonempty interior.
# &lt;math&gt;K^{**}&lt;/math&gt; is the closure of the convex hull of &lt;math&gt;K&lt;/math&gt; (hence, if &lt;math&gt;K&lt;/math&gt; is convex and closed, &lt;math&gt;K^{**}=K&lt;/math&gt;).
# If &lt;math&gt;K&lt;/math&gt; is a proper cone, then &lt;math&gt;K^*&lt;/math&gt; is also a proper cone.
# The dual cone of &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt; is itself.
# The dual cone of a line in space is its orthogonal complement.
# More generally, the dual cone of a subspace &lt;math&gt;V \subseteq \mathbf R^n&lt;/math&gt; is its orthogonal complement &lt;math&gt;\{ y : y^Tv  = 0 \text{ for all } v \in V \}&lt;/math&gt;

== References ==</text>
      <sha1>0wq57x6tmtkek3l6h4e61xnxgujdhr0</sha1>
    </revision>
    <revision>
      <id>553</id>
      <parentid>552</parentid>
      <timestamp>2020-03-21T20:50:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1868">\textbf{Dual cone}. Let &lt;math&gt;K&lt;/math&gt; be a cone. Then, the set &lt;math&gt;K^* = \{y : x^T y \ge 0 \text{ for all } x \in K\}&lt;/math&gt; is called the dual cone of &lt;math&gt;K&lt;/math&gt;. 

An intuitive way of grasping the meaning a dual cone is shown in Fig. 369 by Dattoro&lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;: Draw the two hyperplanes that are orthogonal to the two vertices that define the original cone &lt;math&gt;\mathcal K&lt;/math&gt; in the figure, and then their intersection (i.e., the purple area) is the dual cone (the intersection is necessary because the definition of the dual cone says that any vector in the dual cone should form an acute angle with all points of the original cone). As the figure shows, the smaller the original cone is the larger the dual cone. (See also property 4 below)&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Some properties of dual cone &lt;math&gt;K^*&lt;/math&gt; (p51 and p53):
# A dual cone &lt;math&gt;K^*&lt;/math&gt; is a ... cone.
# &lt;math&gt;K^*&lt;/math&gt; is always ''convex''.
# &lt;math&gt;K^*&lt;/math&gt; is always closed.
# &lt;math&gt;K_1 \subseteq K_2 \implies K_2^* \subseteq K_1^*&lt;/math&gt;.
# If &lt;math&gt;K&lt;/math&gt; has nonempty interior, then &lt;math&gt;K^*&lt;/math&gt; is pointed.
# If the closure of &lt;math&gt;K&lt;/math&gt; is pointed, than &lt;math&gt;K^*&lt;/math&gt; has nonempty interior.
# &lt;math&gt;K^{**}&lt;/math&gt; is the closure of the convex hull of &lt;math&gt;K&lt;/math&gt; (hence, if &lt;math&gt;K&lt;/math&gt; is convex and closed, &lt;math&gt;K^{**}=K&lt;/math&gt;).
# If &lt;math&gt;K&lt;/math&gt; is a proper cone, then &lt;math&gt;K^*&lt;/math&gt; is also a proper cone.
# The dual cone of &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt; is itself.
# The dual cone of a line in space is its orthogonal complement.
# More generally, the dual cone of a subspace &lt;math&gt;V \subseteq \mathbf R^n&lt;/math&gt; is its orthogonal complement &lt;math&gt;\{ y : y^Tv  = 0 \text{ for all } v \in V \}&lt;/math&gt;

== References ==</text>
      <sha1>hbv59pv0fvc7rx211hjxpn81smqmdjq</sha1>
    </revision>
    <revision>
      <id>565</id>
      <parentid>553</parentid>
      <timestamp>2020-03-22T14:34:18Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1944">\textbf{Dual cone}. Let &lt;math&gt;K&lt;/math&gt; be a cone. Then, the set &lt;math&gt;K^* = \{y : x^T y \ge 0 \text{ for all } x \in K\}&lt;/math&gt; is called the dual cone of &lt;math&gt;K&lt;/math&gt;. Very interestingly, a dual cone is convex even if the original cone is not. 

An intuitive way of grasping the meaning a dual cone is shown in Fig. 369 by Dattoro&lt;ref name=dattoro&gt;Dattoro, Convex Optimization  Euclidean Distance Geometry 2&lt;/ref&gt;: Draw the two hyperplanes that are orthogonal to the two vertices that define the original cone &lt;math&gt;\mathcal K&lt;/math&gt; in the figure, and then their intersection (i.e., the purple area) is the dual cone (the intersection is necessary because the definition of the dual cone says that any vector in the dual cone should form an acute angle with all points of the original cone). As the figure shows, the smaller the original cone is the larger the dual cone. (See also property 4 below)&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;.

Some properties of dual cone &lt;math&gt;K^*&lt;/math&gt; (p51 and p53):
# A dual cone &lt;math&gt;K^*&lt;/math&gt; is a ... cone.
# &lt;math&gt;K^*&lt;/math&gt; is always ''convex''.
# &lt;math&gt;K^*&lt;/math&gt; is always closed.
# &lt;math&gt;K_1 \subseteq K_2 \implies K_2^* \subseteq K_1^*&lt;/math&gt;.
# If &lt;math&gt;K&lt;/math&gt; has nonempty interior, then &lt;math&gt;K^*&lt;/math&gt; is pointed.
# If the closure of &lt;math&gt;K&lt;/math&gt; is pointed, than &lt;math&gt;K^*&lt;/math&gt; has nonempty interior.
# &lt;math&gt;K^{**}&lt;/math&gt; is the closure of the convex hull of &lt;math&gt;K&lt;/math&gt; (hence, if &lt;math&gt;K&lt;/math&gt; is convex and closed, &lt;math&gt;K^{**}=K&lt;/math&gt;).
# If &lt;math&gt;K&lt;/math&gt; is a proper cone, then &lt;math&gt;K^*&lt;/math&gt; is also a proper cone.
# The dual cone of &lt;math&gt;K=\mathbf R^n_+&lt;/math&gt; is itself.
# The dual cone of a line in space is its orthogonal complement.
# More generally, the dual cone of a subspace &lt;math&gt;V \subseteq \mathbf R^n&lt;/math&gt; is its orthogonal complement &lt;math&gt;\{ y : y^Tv  = 0 \text{ for all } v \in V \}&lt;/math&gt;

== References ==</text>
      <sha1>m8r0mt9iswi7axng56l1oab27k4w7j8</sha1>
    </revision>
  </page>
  <page>
    <title>Minimal points</title>
    <ns>0</ns>
    <id>172</id>
    <redirect title="Generalized inequality" />
    <revision>
      <id>558</id>
      <timestamp>2020-03-21T22:39:33Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Redirected page to [[Generalized inequality#Minimum and minimal elements]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="65">#REDIRECT [[Generalized inequality#Minimum and minimal elements]]</text>
      <sha1>azcdvau662ymh9uis1sws8peze1tlgu</sha1>
    </revision>
  </page>
  <page>
    <title>EE364a Overview</title>
    <ns>0</ns>
    <id>173</id>
    <revision>
      <id>566</id>
      <timestamp>2020-04-01T19:10:23Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "* TIP to quick-check a convex function: is every function that is obtained by constraining the function to a line convex? If you find one that is not, then the function is not..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="397">* TIP to quick-check a convex function: is every function that is obtained by constraining the function to a line convex? If you find one that is not, then the function is not convex. 
* Try to use "Convex calculus" when testing convexity of a function; i.e. see if the function can be obtain from simpler atoms, using [[Convex function#Establishing convexity|operations that preserve convexity]].</text>
      <sha1>axxez5w109ba8stw8fdpoyyws5ntcke</sha1>
    </revision>
  </page>
  <page>
    <title>Inequalities</title>
    <ns>0</ns>
    <id>174</id>
    <revision>
      <id>567</id>
      <timestamp>2020-04-01T19:52:45Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <comment>Created page with "* &lt;math&gt;\sqrt{ab} \le (a+b)/2&lt;/math&gt; * &lt;math&gt;a^\theta b^{1-\theta} \le \theta a+ (1-\theta)b&lt;/math&gt;  == References ==  (see Section 3.1.9&lt;ref name=cvbook&gt;S. Boyd, L. Vandenber..."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="212">* &lt;math&gt;\sqrt{ab} \le (a+b)/2&lt;/math&gt;
* &lt;math&gt;a^\theta b^{1-\theta} \le \theta a+ (1-\theta)b&lt;/math&gt;

== References == 
(see Section 3.1.9&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;)</text>
      <sha1>2nm41e1olzdano0kdqla96uhqkru4gs</sha1>
    </revision>
    <revision>
      <id>568</id>
      <parentid>567</parentid>
      <timestamp>2020-04-01T19:53:35Z</timestamp>
      <contributor>
        <username>Evangelos</username>
        <id>1</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="229">The following are from Section 3.1.9&lt;ref name=cvbook&gt;S. Boyd, L. Vandenberghe, Convex Optimization, 2009&lt;/ref&gt;

* &lt;math&gt;\sqrt{ab} \le (a+b)/2&lt;/math&gt;
* &lt;math&gt;a^\theta b^{1-\theta} \le \theta a+ (1-\theta)b&lt;/math&gt;

== References ==</text>
      <sha1>cexgpqta9t1go08zk48j7qkj0342t1y</sha1>
    </revision>
  </page>
</mediawiki>
