\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage{color,graphicx,amsmath,hyphenat}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.25in}
\usepackage{xcolor}
\setlength{\textheight}{9in}
\setcounter{MaxMatrixCols}{20}
\pagestyle{empty}

% some shortcuts
\newcommand{\ea}{\textit{et al. }} 
\newcommand{\eg}{\textit{e.g. }} 
\newcommand{\ie}{\textit{i.e. }} 
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\cg}{\color{gray}}
\newcommand{\tr}{\text{tr}}
\newcommand{\rank}{\text{rank}}
\newcommand{\adj}{\text{adj}}
\newcommand{\diag}{\text{diag}}
\newcommand{\fs}{\footnotesize}
\newcommand{\mb}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\newcommand{\lnote}[1]{#1\reversemarginpar\marginpar{{\textit{#1}}}}
\newcommand{\bnote}[1]{#1\reversemarginpar\marginpar{{\textit{{\textcolor{blue}{#1}}}}}}
\newcommand{\rnote}[1]{#1\reversemarginpar\marginpar{{\textit{{\textcolor{red}{#1}}}}}}
\usepackage{marginnote}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=50mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
 }
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\setcounter{section}{3}



\begin{document}
\pagestyle{empty}
  

\section{Linear Models for Classification}
Discusses linear models and \textit{generalised linear models} (GLM). GLM means that even if the prediction functions are non-linear, the decision surfaces are linear.

\subsection{Discriminant functions}
\subsubsection{Two Classes}
Describes the geometry of a discriminant function $y(\mb{x}) = \mathbf{w}^T\mb{x}+w_0$. That is, $\mb{w}$'s are orthogonal to decision surface and $|w_0|/||\mb{w}||$ describes dislocation from origin.

\subsubsection{Multiple Classes}
Discuss the limitation of one-vs-rest and one-vs-one classifiers, introduce the benfits of multi-class linear discriminant.

\subsubsection{Least-squares Classification}
Least squares classification has one extra limitation wrt. limitation of least squares regression: The target vector $\mb{t}$ are of 1-of-$K$ type.

\subsubsection{Fisher's Linear Discriminant}
Perform a dimensionality reduction and then discrimination. $J(\mb{w})$ is a function that does this and can be minimised via (4.2.9).

\subsubsection{Relation to Least-squares}
By changing the target variable representation for the 2-class problem, it's possible to relate Fisher and least-squares.

\subsubsection{Fisher's Discriminant for multiclass}
Consider generalisation to $K>2$ classes. The extension is similar to 2-class. Now there are multiple possible choices of (Fisher) criterion.

\subsubsection{The Perceptron Algorithm}
Construct GLM $y(\mb{x})=f(\mb{w}^T\phi(\mb{x}))$ where $f(a) = \begin{cases}+1 & a\ge 0 \\ -1& a<0 \end{cases}$. Patterns in $C_1$ become +1 and for $x_n \in C_1$ we want $\mb{w}^T\phi(\mb{x})>0$ and for $x\in C_2$ we want it to be $<0$. Both can be summarised as $t\mb{w}^T\phi(\mb{x})>0$.

The perceptron criterion minimises error only on misclassified patterns. The weight update algorDenemeithm operates for each sample $n$:
%
\begin{equation}
\mb{w}^{(\tau+1)}=\mb{w}^{(\tau)}-\eta \nabla E_p(\mb{w}) = \mb{w}^(\tau)+\eta \phi_n t_n
\end{equation}
%
where $\eta$ is the \textit{learning rate}. The update $(\tau+1)$ happens in the \textit{direction of misclassification} and \textit{guarantees} the error on misclassified sample to be reduced. Of course it doesn't guarantee anything on \textit{all} training samples.

\subsection{Probabilistic Generative Models}
Construct posterior $p(C_k|\mb{x})$ and represent via \textit{logistic sigmoid}:

\begin{equation}
p(C_1|\mb{x}) = \frac{p(\mb{x}|C_1)p(C_1)}{\sum\limits_{j\in \{1,2\}} p(\mb{x}|C_j)p(C_j)}=\frac{1}{1+\exp(-\alpha)}=\sigma(\alpha)
\end{equation}
%
%
where $\alpha = \ln \frac{p(\mb{x}|C_1)p(C_1)}{p(\mb{x}|C_2)p(C_2)}$ and $\sigma(\alpha)$ is the logistic sigmoid function.

\textit{We are interested in} situations where $\alpha(\mb{x})$ is linear and therefore creates posteriors governed by GLMs.

\subsubsection{Continuous Inputs}
We sgtart by assuming that all classes $C_k$ share same cov matrix $\Sigma$.

For $K$ classes $\alpha_k$ becomes $\alpha_k(\mb{x}) = \mb{w}^T\mb{x}+w_{k0}$ where $\mb{w}_k=\Sigma^{-1}\boldsymbol{\mu}_k$ and $w_{k0}$ is as in (4.70). 

That is, $\alpha_k$ is linear in $\mb{x}$. Decision boundaries (which correspond to misclassification rate) wioll be again linear in $\mb{x}$ so again we have GLM.

If we relax the ``shared covariance matrix'' assumption, then we'll have \textit{quadratic discriminant} rather than GLM.

\subsubsection{Maximum likelihood solution}
Once $p(\mb{x}|C_k)$ defined, we can determine values of its parameters and parameters of $p(C_k)$ via \textit{maximum likelihood}. Construct maximum function:
%
\begin{equation}
p(\mb{T,X} | \pi, \bs{\mu}_1, \bs{\mu}_2, \Sigma) = \prod\limits_n [\pi \mathcal{N}(x_n|\bs{\mu}_1, \Sigma)]^{t_n} [(1-\pi) \mathcal{N}(x_n|\bs{\mu}_2, \Sigma)]^{(1-t_n)}
\end{equation}
In the ML solution we get $\bs{\mu} = \frac{1}{N_1}\sum\limits_{n} t_n \mb{x}_n$ and $\bs{\mu} = \frac{1}{N_2}\sum\limits_{n} (1-t_n) \mb{x}_n$.

For covariance $\Sigma$, define $\mb{S}_1,\mb{S}_2, \mb{S}$ as:
%
\begin{align}
\mb{S}_1=&\frac{1}{N_1}\sum\limits_{n\in C_1}(\mb{x}_n-\bs{\mu}_1)(\mb{x}_n-\bs{\mu}_1)^T\\
\mb{S}_2=&\frac{1}{N_2}\sum\limits_{n\in C_2}(\mb{x}_n-\bs{\mu}_2)(\mb{x}_n-\bs{\mu}_2)^T\\
\mb{S}=&\frac{N_1}{N}\mb{S}_1+\frac{N_2}{N}\mb{S}_2
\end{align}

Overall, process not robust to outliers because ML is not. 

\subsubsection{Discrete Features}
\subsubsection{Exponential Family}
We manage to get GLMs for the above types too.
\subsection{Probabilistic Discriminative Models}
Advantage: There are less parameters to discover and usually leads to improved performance. 
\subsubsection{Fixed Basis Functions}
\subsubsection{Logistic Regression}
Here we set $M$ params whereas in generative modelling we set $(M+5)/2+1$ params.

Consider implementing a discriminative function directly as a via logistic sigmoid function:
\begin{equation}
p(C_1|\mb{\phi})=y(\phi)=\sigma(\mb{w}^T\phi)
\end{equation}
and naturally $p(C_2|\phi)=1-p(C_1|\phi)$. We can set params via ML. We start by seeing that $\frac{d\sigma}{d\alpha}=\sigma(1-\sigma)$ (exercise 4.12). Likelihood can be written as:
\begin{equation}
p(\mb{T}|\mb{w}) = \prod\limits_n y_n^{t_n}(1-y_n)^{1-t_n}
\end{equation}
%
where $y_n=p(C_1|\phi_n)$. Error function here is also called \lnote{cross entropy error}:
%
\begin{equation}
E(\mb{w})=-\ln p(\mb{T}|\mb{w}) = -\sum\limits_{n}[t_n \ln y_n + (1-t_n)\ln(1-y_n)]
\end{equation}
%
Taking the gradient wrt $\mb{w}$:
%
\begin{equation}
\nabla E(\mb{w})=\sum_n(y_n-t_n)\phi_n
\end{equation}

\subsubsection{Iterative Reweighted Least Squares}
We no longer have closed-form solution (as we did for regression). Fortunately the error function is still convex there is the (iterative) \lnote{Newton-Raphson or iterative reweighted least squares} algorithm:
%
\begin{equation}
\mb{w}^{(new)}=\mb{w}^{(old)}=\mb{H}^{-1}\nabla E(\mb{w})
\end{equation}
%
where $\mb{H}$ is the hassian matrix whose elements comprise the second derivs of $E(\mb{w})$ wrt components of $\mb{w}$.
%
\begin{align}
\nabla E(\mb{w})=&\sum_{n}(y_n-t_n)\phi_n=\mb{\Phi}^T(\mb{Y-T})\\
\mb{H}=&\nabla\nabla E(\mb{w})=\sum\limits_{n}y_n(1-y_n)\phi_n\phi_n^T=\mb{\Phi}^T\mb{R\Phi}
\end{align}
where $\mb{\Phi}$ is the $N\times M$ \lnote{design matrix} whose $n$th row is given by $\phi_n^T$ and $\mb{R}$ is the $N\times N$ diagonal matrix with elements $\mb{R}_{nn}=y_n(1-y_n)$.

\subsubsection{Multiclass logistic regression}
The formalism is similar to 2-class logistic regression. Instead of sigmoid we use the \textit{softmax} function. Again we have \textit{cross-entropy} function as error function. The multiclass version of cross-entropy is:
\begin{equation}
E(\mb{w}_1,\hdots, \mb{w}_K) = -ln p(\mb{T}| \mb{w}_1, \hdots, \mb{w}_K) = -\sum\limits_n\sum\limits_k t_{nk}-\ln y_{nk}.
\end{equation}
Again we can use \textit{iterative reweighted least squares} (\#210).

\subsubsection{Probit regression}
The inverse probit function (or the similar $\text{erf}$ function) are similar to sigmoid in shape but have more plausible analytical properties. Will be discussed in Sec. 4.5.

\subsubsection{Canonical link function}
This is one of the most frequently-referred sections of the book. The choices of sigmoid/softmax in earlier sections were not arbitrary \---- they were chosen to convert the error function to a simple form that involves $y_n-t_n$. This is a general result of assuming a conditional distribution for the activation function known as the \lnote{canonical link function}.

A GLM is a model for which $y$ is a nonlinear function of a linear combination of input variables: 
\begin{equation}
y=f(\mb{w}^T\phi)
\end{equation}
%
where $f(\cdot)$ is the \textit{activation function} and $f^{-1}(\cdot)$ is known as the \textit{link function}.

Let the conditional distro be $p(\mb{T}|\eta, s)$. We formulate its derivative in the following form:
%
\begin{equation}
\nabla_{\mb{w}} \ln p(\mb{T}|\eta,s)= \hdots \text{(see \#213)} = \sum\limits_n\frac{1}{s}\psi'(y_n)f'(y_n)\phi_n
\end{equation}.
The canonical link function chosen as $f^{-1}(y)=\psi(y)$ provides a great simplification:
\begin{equation}
\nabla E(\mb{w})= \frac{1}{s}\sum\limits_n(y_n-t_n)\phi_n
\end{equation}

\subsection{The Laplace Approximation}
To perform closed-form analysis for Bayesian logistic regression, we'll need to do approximation. The Laplace approx. is used for this purpose. Approximation is performed by matching the \textit{mode} of the target distribution with the mode of a Gaussian via Taylor expansion (where the first-order term disappears as expansion is made around a local maximum). Let $\mathbf{z}_0$ be the mode of the target distribution. The 2$^{nd}$ order Taylor expansion around $\mb{z}_0$ is:
\begin{equation}
f(\mb{z})\approx\ln f(\mb{z}_0)-\frac{1}{2}(\mb{z-z}_0)^{T}\mb{A}(\mb{z-z}_0).
\end{equation}
%
This will enable us to compute the approximated distribtion $q(\mb{z})$ directly as $q(\mb{z})=\mathcal{N}(\mb{z}|\mb{z}_0, \mb{A})$.

\bnote{Better methods will be explored in Chapter 10}.

\subsubsection{Model comparison and BIC}
We can use the approximation above for model comparison, which will lead to Bayesian Information Criterion (BIC). Start with the normalisation term: 
\begin{equation}
Z\approx f(\mb{z}_0) \int \exp \left[ -\frac{1}{2}(\mb{z-z}_0)^T \mb{A}(\mb{z-z}_0) \right] d\mb{z}=f(\mb{z}_0)\frac{(2\pi)^{M/2}}{|\mb{A}|^{1/2}}.
\end{equation}

Consider data set $\mathcal{D}$ and models $\{\mathcal{M}_i\}$ with parameters $\{\bs{\theta}_i\}$. For each model we define a likelihood function $p(\mathcal{D}|\bs{\theta}_i, \mathcal{M}_i)$ \---- or shortly, $p(\mathcal{D}|\bs{\theta}_i)$.

Defining $f(\bs{\theta})=p(\mathcal{D}|\bs{\theta})p(\theta)$ and identifying that $Z=p(\mathcal{D})$, we can apply the result above to get:
%
\begin{equation}
\ln p(\mathcal{D})\approx \ln p(\mathcal{D}|\bs{\theta}_{\text{MAP}})+\overbrace{\ln p(\bs{\theta}_{\text{MAP}})+\frac{M}{2}\ln(2\pi)-\frac{1}{2}\ln |\mb{A}|}^{\text{Occam factor}}.
\end{equation}

With further simplifications via (not necessarily realistic) assumptions (see \#217) we get the BIC:
\begin{equation}
\ln p(\mathcal{D}) \approx \ln p(\mathcal{D}|\bs{\theta}_{\text{MAP}})-\frac{1}{2}M \ln N
\end{equation}
Essentially this is an information criterion that penalizes model complexity


\subsection{Bayesian Logistic Regression}
Again, exact inference for logistic regression is intractable, due to normalisation (which involves likelihood computation, which is a product of sigmoids (one for each data point)). We apply Laplace approximation for tractability.

\subsubsection{Laplace Approximation}
Because Laplace involves Gaussian approx, we start with Gaussian prior $p(\mb{w}) = \mathcal{N}(\mb{w}|\mb{m}_0,\mb{S}_0)$. Posterior is $p(\mb{w}|\mb{T})\propto p(\mb{w})p(\mb{T|w})$. To compute the approx of this posterior, $q(\mb{w})$, we first express it in closed form (4.142) and then find the MAP solution, $\mb{w}_{\text{MAP}}$. Then we compute the approximation around $\mb{w}_{\text{MAP}}$; that is, we find the cov. matrix $S_N$ by using (4.132) \---- the mean is already $\mb{w}_{\text{MAP}}$ \--- and obtain approx as: $p(\mb{w|T})\approx q(\mb{w}) = \mathcal{N}(\mb{w}|\mb{w}_{\text{MAP}}, \mb{S}_N)$.

Now we'll use this for prediction by computing the \textit{predictive distribution}.

\subsubsection{Predictive Distribution}
Recall that for 2-class NN the prob of a sample being $C_1$ is the output of the NN, which is $p(C_1|\bs{\phi},\mb{w})=\sigma(\mb{w}^T\phi)q(\mb{w})$. Predictive distribution involves the following marginalization over $\mb{w}$:
%
\begin{equation}
p(C_1|\bs{\phi},\mb{T})=\int p(C_1|\bs{\phi},\mb{w}) p (\mb{w}|\mb{T}) \approx \int \sigma (\mb{w}^{T}\bs{\phi})q(\mb{w})d\mb{w}.
\label{eq:pred_distr}
\end{equation}
%
The problem here is that $\sigma(\cdot)$ is non-linear and again not tractable. Putting this aside for a moment, the section first computes the marginalization of $q(\mb{w})$, $\int q(\mb{w})d\mb{w}$. Then, it computes the overall integral in (\ref{eq:pred_distr}) by approximating the sigma function with the inverse probit function, $\Phi(\cdot)$ \---- see \#219 for details.

 
\clearpage
\newpage

\section{Neural Networks}
This is a critical chapter because many commonly-used techniques are motivated and described in detail. These include the \textit{gradient-descent optimization} and also the \textit{backpropagation} technique which is used for many purposes including \textit{Hessian computation} and \textit{Jacobian computation}. 

\subsection{Feed-forward Neural Networks}
\subsection{Network Training}
Again we'll minimize an error function. We can directly minimize a sum-of-squares error such as $E(\mb{w}) = \frac{1}{2}\sum||\mb{y}(\mb{x}_n-\mb{t}_n)||^2$. But we'll give rise to a probabilistic interpretation by considering likelihood maximization. This will be beneficial for many purposes.

The rest of the section derives the energy function and cross-entropy error function in the context of NNs.

\subsubsection{Parameter optimisation}
Clearly, there is no hope to find an analytical solution to optimum $\mb{w}$. We'll therefore resort to iterative algorithms of the following form:
%
\begin{equation}
\mb{w}^{(\tau+1)}=\mb{w}^{(\tau)}+\Delta \mb{w}^{(\tau)}.
\end{equation}
%
There are many algos of this form, and they differ by their choice of $\Delta\mb{w}^{(\tau)}$. Most use \textit{gradient information} due to reasons discussed in following section.

\subsubsection{Local quadratic approximation}
Consider second-order Taylor approx of $E(\mb{w})$ around some $\mb{\widehat{w}}$:
%
\begin{equation}
E(\mb{w})\approx E(\mb{\widehat{w}})+(\mb{w-\widehat{w}})^T\mb{b}+\frac{1}{2}(\mb{w-\widehat{w}})^T \mb{H} \mb{(w-\widehat{w}})
\end{equation}
%
where $\mb{b}=\nabla E_{\mb{w}}(\mb{w})|_{\mb{w=\widehat{w}}}$ and $\mb{H}$ is the Hessian matrix with elements $(\mb{H})_{ij}=\frac{\partial^2 E}{\partial w_i\partial w_j}|_{\mb{w=\widehat{w}}}$. If we pick $\mb{\widehat{w}}$ to be a minimum, say $\mb{w}^{\star}$ the second term above vanishes.
%
\begin{equation}
E(\mb{w})\approx E(\mb{{w}^{\star}})+\frac{1}{2}(\mb{w-{w}^{\star}})^T \mb{H} \mb{(w-{w}^{\star}})
\end{equation}
%
We analyse $\mb{H}$ by considering its eigenvectors, and we see that constant-error contours $E(\mb{w})=C$ are ellipses whose axes are aligned with the eigenvectors of $\mb{H}$, $\mb{u}_i$, and the length of these axes are inverse proportional to the corresponding eigenvectors, $\lambda_i$ (see \#238-239 and Exercise 5.10).

\subsubsection{Use of gradient information}
Gradient allows us to compute evaluate $E$ in $O(W^2)$ instead of $O(W^3)$.

\subsubsection{Gradient descent optimization}
Standard (\ie batch) gradient descent ($\mb{w}^{(\tau+1)}=\mb{w}^{(\tau)}-\eta \nabla E(\mb{w}).$) leads to poor performance. However we can perform gradient-descent for each sample in dataset separately, which is known as on-line gradient descent (or stochastic GD or sequential GD), which proved to be much better.  

But for batch optimization there are much more efficient methods such as \textit{conjugate gradients} and \textit{quasi-Newton} methods.

\subsection{Error Backpropagation}
The main goal is to find efficient methods to compute the gradient of $E(\mb{w})$, $\nabla E(\mb{w})$. The importance of backpropagation lies in that it can be used beyond the scope of NNs and gradient descent, such as other derivatives and graphical models etc.

There are 2 steps at each iteration of a backpropt technique:
\begin{enumerate}
\setlength{\parindent}{0mm}
\item Evaluate $\nabla E(\mb{w})$ (this is where backpropagation comes to play)
\setlength{\parindent}{0mm}
\item Update $\mb{w}^{\star}$ based on step 1.
\setlength{\parindent}{0mm}
\end{enumerate}

\subsection{Evaluation of error-function derivatives}
We will analyse the error of a single sample, $E_n$, and the total error is simply the sum over $N$. Let $\delta_j$ be defined as
\begin{equation}
\delta_j=\frac{\partial E_n}{\partial a_j},
\end{equation}
where $\delta$'s are typically referred to as errors. In the last layer, due to our choice of activation function, we have (see 5.18):
%
\begin{equation}
\delta_k = \frac{\partial E_n}{\partial a_k} = y_k-t_k
\end{equation}
%
The ultimate goal is to compute derivatives $\frac{\partial E_n}{\partial w_{ji}}$ and the $\delta$'s will be the messages that are propagated backwards.

We can express the desired derivative by means of the output and label as:
%
\begin{equation}
\frac{\partial E_n}{\partial w_{ji}} = (y_{nj}-t_{nj})x_{ni}
\end{equation}
%
Note that by feed-forward NN definition we have $a_j=\sum\limits_i w_{ji}z_i$ and $z_j=h(a_j)$. We can therefore express the derivative above through the chain rule as:
%
\begin{equation}
\frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j}\frac{\partial a_j}{\partial w_{ji}}=\delta_j \frac{\partial a_j}{\partial w_{ji}}
\end{equation}
%
%
Note that the first derivative on the rhs corresponds to our $d_j$ definition.

Now we can start from the last (output) layer and propagate backwards:
\begin{equation}
\delta_j=\frac{\partial E_n}{\partial a_j}=\sum\limits_k \frac{\partial E_n}{\partial a_k}\frac{\partial a_k}{\partial a_j}=\sum\limits_k\delta_k w_{kj}
\label{eq:bp_output}
\end{equation}
where the units $k$ are those to which $j$ sends connections. Here $\frac{\partial a_k}{\partial a_j}=w_{kj}$ holds because the $k$th layers are output layers, and we are considering the regression problem where the activation function ($h(\cdot)$) is simply the unity function and therefore:
%
\begin{equation}
a_k=\sum\limits_i w_{ki}a_{i}.
\end{equation}
%

Now let us consider a node $j$ that lies in one layer before where the activation function is not unity and therefore:
\begin{equation}
a_j=\sum\limits_i w_{ji} h(a_{i})
\end{equation}
%
%
and therefore we have 
\begin{equation}
\delta_j=h'(a_j)\sum_{k}w_{kj}\delta_k.
\label{eq:bp_hidden}
\end{equation}

We can summarise backpropagation as follows:
\begin{enumerate}
\setlength{\parindent}{0mm}
\item Apply an input vector $\mb{x}_n$ to the network and propagate forwards
\item Evaluate $\delta_k$ for output units through (5.54) (or (\ref{eq:bp_output} above)
\item Evaluate $\delta_j$ for all hidden units in between input and output layers (or (\ref{eq:bp_hidden}) above).
\item Finally obtain the derivative $\frac{\partial E_n}{\partial w_{ji}}$ through (5.53), which is $\frac{\partial E_n}{\partial w_{ji}}=\delta_j z_i$.
\end{enumerate}
%
%
\subsubsection{Efficiency of backpropagation}
Backpropagation's main advantage is efficiency. Derivatives can be computed with central differences method as in (5.69), however, that is too costly. But the central differences method can be used to check the correctness of a backpropagation method.

\subsubsection{The Jacobian Matrix}
In addition to the derivatives of the error function, backprop can also be used for the Jacobian:
%
\begin{equation}
J_{ki} = \frac{\partial y_k}{\partial x_i}
\end{equation}
%
We similarly will try to derive a message passing algorithm. Jacobian can be written out as:
%
\begin{equation}
J_{ki} = \frac{\partial y_k}{\partial x_i}=\sum\limits_j\frac{\partial y_k}{\partial a_j}\frac{\partial a_j}{\partial x_i}=\sum\limits_j w_{ji} \frac{\partial y_k}{\partial a_j}
\end{equation}
%
where $j$'s represent the nodes to which node $i$ sends connection. To compute the derivative above, we compute the second term in the rhs as:
%
\begin{equation}
\frac{\partial y_k}{\partial a_j}=\sum\limits_l\frac{\partial y_k}{\partial a_l}\frac{\partial a_l}{\partial a_j}=h'(a_j)\sum\limits_l w_{lj} \frac{\partial y_k}{\partial a_l}
\end{equation}
%
Again we start at output units, see \#248 for further details.

\subsection{The Hessian Matrix}
This section is critical as it discusses in detail various types of Hessian computation, including Hessian computation via backprop. The different methods for Hessian computations are due to the different assumptions which lead to different approximations. Exact Hessian computation (\ie no assumptions) is also possible via backprop. Among other things, Hessian is useful for:
%
\begin{enumerate}
\setlength{\parindent}{0mm}
\item Finding non-significant weights
\item Computing Laplace approximation for Bayesian Networks
\item Non-linear optimization techniques
\item Efficient method for re-training network
\end{enumerate}

\subsubsection{Diagonal Approximation}
The first way is to discard non-diagonal elements in which case we have a quite simple computation for Hessian. However Hessian can be strongly non-diagonal therefore these assumptions must be treated with care.

\subsubsection{Outer product approximation (Levenberg-Marquardt approximation)} 
For sum-of-squares Error function, the Hessian can be written out as:
%
\begin{equation}
\mb{H}=\nabla\nabla E = \sum\limits_{n=1}^N \nabla y_n(\nabla y_n)^T+\sum_(y_n-t_n)\nabla\nabla y_n.
\end{equation}
%
The second term is likely to be very small and can be neglected, which leads to \textit{outer product} or \lnote{Levenberg-Marquard approximation}:
%
\begin{equation}
\mb{H}\approx \sum_{n=1}^N\nabla a_n(\nabla a_n)^T=\sum\limits_{n=1}^N \mb{b}_n\mb{b}_n^T.
\end{equation}
Similarly, for cross-entropy error function we obtain:
%
\begin{equation}
\mb{H}\approx \sum\limits_{n=1}^N y_n(1-y_n) \mb{b}_n\mb{b}_n^T
\end{equation}



\subsubsection{Inverse Hessian}
The outer product approx above leads also to efficient methods for computing the inverse of Hessian. The Hessian can be updated sequentially for each point as:
%
\begin{equation}
\mb{H}_{L+1} = \mb{H}_L+\mb{b}_{L+1}\mb{b}_{L+1}^T
\end{equation}
and using the Woodbury identity (C.7) the inverse can be computed as 5.89.

\subsubsection{Finite differences}
The correctness of the Hessian and its inverse can again be checked by comparing it to the output of finite differences method.

\subsubsection{Exact evaluation of the Hessian}
The backprop procedure can be used for Hessian as well. Again we derive a message passing scheme by defining some $\delta$'s \---- see \#253-254.

\subsubsection{Fast multiplication by the Hessian} 
Many times we are interested in computing a multiplication of the Hessian with a vector $\mb{Hv}$ rather than computing the Hessian $\mb{H}$ itself. This is also possible via the backprop algo \---- see \#254-256.

\subsection{Regularization in Neural Networks}
\subsubsection{Consistent Gaussian Priors}
In Neural nets, different sets of priors can be equivalent to each other due to the bias parameter. A well-defined regularizer should be aware of this and not favour one equivalent solution to the other. We can achieve this via the consistent Gaussian priors discussed here. The section will be used for \bnote{automatic relevance determination in RVMs}.


\subsubsection{Early Stopping}
An alternative is to prevent overfitting is early stopping \--- stop the training as soon as the validation error starts getting larger.

\subsubsection{Invariances}
There are four methods to implement invariance for certain transformations:
\begin{enumerate}
\item Augment training set with replicas
\item Tangent propagation (manifold learning)
\item Invariance during feature extraction
\item Shared weights (for limited types of invariances)
\end{enumerate}
%
%
%

\subsubsection{Tangent propagation}
Provided that a transformation is continuous, then the transformed pattern will sweep out a manifold $\mathcal{M}$ in a $D$-dimensional space. Assume that this transformation acting on a vector $\mb{x}_n$ is given via the function $\mb{s(x}_n,\xi)$ where $\xi$ controls the amount of transformation.

Under a transformation, the output of the network will usually change \---- but we don't want it to. So we punish changes on the output.

A related technique is \lnote{tangent distance} where invariance is built into the distance metric (Simard \ea, 1993).

\subsubsection{Training with transformed data}
Training with transformed data is closely related to tangent  propagation. The section discusses this analytically.

\subsubsection{Convolutional networks}
Discuss the weight-sharing of CNNs.

\subsubsection{Soft weight sharing}
CNNs share weights by making them identical. This is a softer version of sharing weights. This section model soft weights as GMM.

\subsubsection{Mixture Density Networks}
We can create a very flexible output by having a Gaussian mixture of NNs. \rnote{Potentially useful} technique for many purposes \---- it can model arbitrary distributions. The conditional mode of a distribution is likely to be an important property (see end of section and Fig. 5.21d).


\subsection{Bayesian Neural Networks}
So far we focused on ML for finding the weight parameters. Now we consider Bayesian methods. We'll ultimately be interested in making predictions. That is, as far as Bayesian is concerned, computing the predictive distribution. 

The conditional distro for one sample is:
%
\begin{equation}
p(t|\mb{x,w},\beta) = \mathcal{N}(t|y(\mb{x,w}, \beta^{-1})).
\end{equation}
%
Then we have the prior:
%
\begin{equation}
p(\mb{w}|\alpha) = \mathcal{N}(\mb{w}|\mb{0}, \alpha^{-1}\mb{I})
\end{equation}
%
We compute likelihood for dataset $\mathcal{D}$ as $p(\mathcal{D}|\mb{w}, \beta)$. Then posterior is
\begin{equation}
p(\mb{w}|\mathcal{D}, \alpha, \beta) \propto p(\mathcal{D}|\mb{w}, \beta)p(\mb{w}, \alpha).
\end{equation}
We'll ultimately be interested in computing the predictive distro: $p(t|\mb{x}, \mathcal{D},\alpha, \beta)$. We'll also be optimizing $\alpha, \beta$ and evantually will have $p(t|\mb{x}, \mathcal{D})$.

We work with Laplace approximation due to intractability of exact analytical solutions.

First we make Laplace approx for the posterior $p(\mb{w}|\mathcal{D})$ \---- the approx is denoted with $q(\mb{w}|\mathcal{D})$. But even with this approx the following integral for predictive distro is intractable (see \#279):
%
\begin{equation}
p(t|\mb{w},D) = \int p(t|\mb{x,w})q(\mb{w}|\mathcal{D})d\mb{w}.
\end{equation}
%
To make progress we'll approximate the output function $\mb{y(x,w)}$ around $\mb{w}_{\text{MAP}}$ via Taylor expansion. Then we'll have two Gaussians and we know how to compute the marginal of two Gaussians (see end of \#279 or 2.115).

\subsubsection{Hyperparameter Optimization}
In the formalism so far we assumed that $\alpha, \beta$ are known and fixed. In this section we see how to optimize them. 


\clearpage
\newpage

\section{Kernel Methods}
Problems that is formulated as empirical risk minimisation can be converted into a dual representation where an explicit \textit{kernel representation} comes out. 

\subsection{Dual Representations}

Consider that we wish to minimise the sum-of-squares error function below:
%
\begin{equation}
J(\mb{w}) = \frac{1}{2}\sum\limits_{n}\{\mb{w}^T\phi(\mb{x}_n)-t_n\}^2+\frac{\lambda}{2}\mb{w}^T\mb{w}
\end{equation}
%
By defining $\mb{a}=(a_1,\hdots,a_N)^T$ where $a_n$ is:
%
\begin{equation}
a_n=-\frac{1}{\lambda}\{\mb{w}^T \phi(\mb{x}_n)-t_n\}
\end{equation}
%
we obtain the dual representation of the error function:
%
\begin{equation}
J(\mb{a})=\frac{1}{2}\mb{a}^T\mb{\Phi\Phi}^T\mb{\Phi\Phi}^T\mb{a}-\mb{a}^T\mb{\Phi\Phi}^T\mb{T}+\frac{1}{2}\mb{T}^T\mb{T}+\frac{\lambda}{2}\mb{a}^T\mb{\Phi\Phi}^T\mb{a}.
\end{equation}
%
%
We define \textit{Gram matrix} $\mb{K}=\mb{\Phi\Phi}^T$ and obtain the following more compact representation:
%
%
\begin{equation}
J(\mb{a}) = \frac{1}{2}\mb{KK}^T-\mb{a}^T\mb{K}\mb{T}+\frac{1}{2}\mb{T}^T\mb{T}+\frac{\lambda}{2}\mb{a}^T\mb{Ka}.
\end{equation}
Solving for $\mb{a}$ we obtain:
%
\begin{equation}
\mb{a} = (\mb{K}+\lambda\mb{I}_N)^{-1}\mb{T}
\end{equation}
which we can plug back to linear regression function to express the prediction in terms of training samples:
%
\begin{equation}
y(\mb{x})= \mb{k}(\mb{x})^T(\mb{K}+\lambda\mb{I}_N)^{-1}\mb{T}
\end{equation}


\subsection{Constructing Kernels}

The section discusses how to construct ``valid'' kernels. Valid is a kernel for which the Gram matrix $\mb{K}$ is positive semi-definite for all possible choices of the set $\{\mb{x}_n\}$. Page \#296 lists some rules to construct sophisticated kernels from simple kernels.

Important discussion in Section \#297: \rnote{getting the best of two (discriminative and generative) worlds} worlds by kernels from probability functions.

\subsection{Radial Basis Function Networks}
Historically RBFs were first considered for the interpolation problem, and they were used by adding one kernel centred on each data point.

But in ML we don't want to fit data exactly due to noise. Noise can exists i) in the target values $t_n$ ii) in the training samples $\mb{x}_n$. The latter will give rise to \textit{Nadaraya-Watson} model in Section 6.3.1.

\subsubsection{Nadaraya-Watson model} 
A common kernel method that assigns more weight to data points that are closer to a given point.

\subsection{Gaussian Processes}
\subsubsection{Linear regression revisited}
\subsubsection{Gaussian processes for regression}
We are given a training set $\mb{X}=\{\mb{x}_n\}$, $\mb{T}=\{t_n\}$. We want to make predictions based on these. More specifically, we want to obtain a \textit{predictive distribution} which in the context of Gaussian processes is a conditional distribution $p(t_{N+1}|\mb{x}_{N+1},\mb{T},\mb{X})$. 

We have a model $y(\mb{x})=\mb{w}^T\mb{\phi}(\mb{x})$. Consider a prior over $\mb{w}$:
%
\begin{equation}
p(\mb{w}) = \mathcal{N}(\mb{w}|\mb{0}, \alpha^{-1}\mb{I})
\end{equation}
%
%
Define $\mb{Y}=(y_1,\hdots,y_n)$. Using eq. above we note that this vector is given by $\mb{Y}=\mb{\Phi w}$. We'll use the following \textit{joint distribution} while estimating via GPs: 
%
\begin{align}
\mathbb{E}[\mb{Y}] &= 0 \\
\text{cov}[\mb{Y}] &= \mb{K}
\end{align}
%
where $K_{nm} = \frac{1}{\alpha}\phi{\mb{x}_n}^T\phi{\mb{x}_m}$.

To obtain $p(t_{N+1}|\mb{x}_{N+1},\mb{T},\mb{X})$, we'll first obtain $p(\mb{T}|\mb{x}_{N+1})$. We'll obtain the latter by marginalisation using the distributions $p\mb{(\mb{Y}|\mb{X}})$ and $p\mb{(T|Y,X)}$. Using (2.115) we get:
%
\begin{equation}
p(\mb{T}) = \int p(\mb{T|Y})p(\mb{Y})d\mb{Y} = \mathcal{N}(\mb{T}|\mb{0,C})
\end{equation}
%
%
where $C_{nm}=C(\mb{x}_n,\mb{x}_m)=k(\mb{x}_n,\mb{x}_m)+\beta^{-1} \delta_{nm}$. Here we can use any valid kernel function. A frequently used one is given via (6.63) and has four parameters $\theta_0,\theta_1,\theta_2,\theta_3$:
%
\begin{equation}
k(\mb{x}_n,\mb{x}_m) = \theta_0 \exp\left\{-\frac{\theta_1}{2} ||\mb{x}_n-\mb{x}_m||^2 \right\}+\theta_2+\theta_3\mb{x}_n^T\mb{x}_m.
\end{equation}


To obtain ultimate predictive distro for a $\mb{x}_{N+1}$ we first compute a new joint distro $P(\mb{T}_{N+1})$, which, similarly as above, is given as $p(\mb{T}_{N+1})=\mathcal{N}(\mb{T}_{N+1}|\mb{0}, \mb{C}_{N+1})$. 
Finally, using (2.81) and (2.82), we can obtain the predictive distribution as:
%
\begin{equation}
p(t_{N+1}|\mb{T,X})=\mathcal{N}(\mb{x}_{N+1},\sigma^2(\mb{x}_{N+1}))=\mathcal{N}(\mb{k}^T\mb{C}_{N}^{-1},\mb{T},c-\mb{k}^T \mb{C}_N^{-1},\mb{k})
\label{eq:GP}
\end{equation}
%
The critical point here is that both the mean and covariance depend on $\mb{x}_{N+1}$.

There are various extensions of GPs. GTM is also an extension that models the \rnote{distribution over low-dimensional manifolds for unsupervised learning}.


\subsubsection{Learning the hyperparameters}
The goal here is to learn the parameters $\bs{\theta}=(\theta_0,\theta_1,\theta_2,\theta_3)$. The straightforward way is to maximize the likelihood $p(\mb{T}|\bs{\theta})$.
 
\subsubsection{Gaussian processes for classification}
We'll derive GP expression for classification. Again we'll use the sigmoid function. We consider the two-class case $t\in\{0,1\}$. 

Assume we have a function $a(\mb{x})$ that takes arbitrary values. For regression we defined a GP for $y(\mb{x})$ but now we'll define a GP over $a(\mb{x})$. Instead of computing the predictive distro via $p(\mb{T})$, we'll compute it via $p(\mb{a}_N)$:
%
\begin{equation}
p(\mb{a}_N) = \mathcal{N}(\mb{a}_{N+1}|\mb{0}, \mb{C}_{N+1})
\label{eq:GP_cls}
\end{equation}
%
This is a non-Gaussian process due to the non-linear sigmoid function that is used to obtain it, $p(t|a)=\sigma(a)^t (1-\sigma(a))^{1-t}$.

\subsubsection{Laplace approximation}
Now we'll use the Laplace approx machinery to approximate the non-Gaussian process above to a Gaussian process.

Again our ultimate goal will be to derive the predictive distro 
\begin{equation}
p(t_{N+1}=1|\mb{T}_N)=\int p(t_{N+1}=1|a_{N+1})p(a_{N+1}\mb{T}_N)d a_{N+1}.
\end{equation}

Again, we'll first derive the posterior $p(a_{N+1}|\mb{T})$ which boils down to the following expression:
%
\begin{equation}
p(a_{N+1}|\mb{T}_N) = \int p(a_{N+1}|\mb{a}_N)p(\mb{a}_N|\mb{T}_N) d\mb{a}_N 
\label{eq:GP_post_cls}
\end{equation}
%
The first term in RHS above is computed via (\ref{eq:GP}) \---- or (6.66) and (6.67) in the book. The tricky part where we need to do approximation is the posterior $p(\mb{a}_{N}|\mb{T}_{N})$.

To compute $p(\mb{a}_{N}|\mb{T}_{N})$ we need two terms: the prior over $\mb{a}_N$, $p(\mb{a}_{N})$, and the data term $p(\mb{T}_{N}|\mb{a}_{N})$. The first is a zero-mean GP given by (\ref{eq:GP_cls}). The latter term is the \textit{data term} (resembles likelihood), assuming iid:
%
\begin{equation}
p(\mb{T}_N|\mb{a}_N) = \prod\limits_n \sigma(a_n)^{t_n}(1-\sigma(a_n))^{1-tn}
\end{equation}
%
To obtain $p(\mb{a}_N|\mb{T}_N)\propto p(\mb{a}_N)p(\mb{T}_N|\mb{a}_N)$ we'll compute the following approx:
%
\begin{equation}
\Psi(\mb{a}_N) = \ln p(\mb{a}_N) + \ln p(\mb{T}_N|\mb{a}_N)
\end{equation}
%
To compute the approx we find the mode of the above function finding the solution of $\nabla \Psi(\mb{a}_n)=0$, which will be done via the IRLS algorithm because we can't find a closed form solution (\#316). IRLS requires the second derivatives $\nabla\nabla\Psi(\mb{a}_N)$.

The approximation is denoted as $q(\mb{a}_N) = \mathcal{N}(\mb{a}_N|\mb{a}_N^\star, \nabla\nabla\Psi(\mb{a}_N))$. Now we can approximate $p(a_{N+1}|\mb{T}_N)$ in (\ref{eq:GP_post_cls}). Because $q(\mb{a}_N)$ is Gaussian, $p(a_{N+1}|\mb{T}_N)$ is also a Gaussian with second order statistics given by (6.87) and (6.88). The predictive distro $p(t_{N+1}|\mb{T}_N)$ is finally obtain by the inverse Probit approximation given in (4.153).

The section concludes on a similar approximation strategy that aims at determining the parameters $\bs{\theta}$.




\section{Sparse Kernel Machines}

\subsection{Maximum Margin Classifiers}
\subsection{Relevance Vector Machines}
\subsubsection{RVM for regression}
As usual, we'll eventually have a linear model in the form $y(\mb{x}) = \sum\limits_{i=1}^{M}w_i\phi_i(\mb{x}) = \mb{w}^T\bs{\phi}(\mb{x})$. Following the concept of ``Kernel Machine'', the model will be in the following form:
%
\begin{equation}
y(\mb{x}) = \sum\limits_{n=1}^N w_n k(\mb{x}, \mb{x}_n)+b.
\end{equation}
%
That is, the output is produced in terms of training samples. We are after probabilistic predictions, and because of their advantages in closed form analysis, we use Gaussian RVs. For a given sample $\mb{x}$, we produce the following conditional distribution:
%
\begin{equation}
p(t|\mb{x}, \mb{w}, \beta) = \mathcal{N}(t|y(\mb{x}), \beta^{-1})
\end{equation}
%
The parameters $\mb{w},\beta$ will be learnt in the training process.	More critically, we'll define a prior $\alpha_n$ for each training sample $\mb{x}_n$. Sparsity will be achieved by setting some priors $\alpha_i=\infty$.

Let us denote the training samples with the matrix $X$ whose $n$th row is $\mb{x}_n^T$ and the target values with $\mb{T}=(t_1,\hdots, t_N)$. In training we'll consider two terms. First, the likelihood function, $p(\mb{T}|\mb{X},\mb{w},\beta)$, and second the prior $p(\mb{w}|\bs{\alpha})$. Likelihood is given by:
%
\begin{equation}
p(\mb{T}|\mb{X}, \mb{w}, \beta) = \prod\limits_n^{N} p(t_n|\mb{x}_n, \mb{w}, \beta).
\end{equation}
%
The prior term is defined as follows (we assume independence between priors of different training samples):
%
\begin{equation}
p(\mb{w}|\bs{\alpha}) = \prod\limits_{i=1}^M \mathcal{N}(w_i|0, \alpha_i^{-1})
\end{equation}
%
%
%
%
%
We'll follow the typical full-Bayesion solution, according to which we'll first derive the posterior distribution $p(\mb{w}|\mb{T},\mb{X}, \bs{\alpha}, \beta)$ and then will marginalize over this distribution to obtain a predictive distribution.

The posterior is derived rather straightforwardly because both the prior and likelihood are Gaussians:
%
\begin{equation}
p(\mb{w}|\mb{T,X},\bs{\alpha}, \beta) = \mathcal{N}(\mb{w|m,\Sigma})=\mathcal{N}(\mb{w}|\beta \mb{\Sigma \Phi}^T\mb{T}, (\mb{A}+\beta\mb{\Phi}^T\mb{\Phi})^{-1})
\end{equation}



\section{Graphical Models}
\section{Mixture Models and EM}






\newpage
\clearpage

\section{Approximate Inference}
Too often we our task boils down to approximating an intractable distribution. EM was one way to achieve this.

Two further classes of methods are i) Stochastic approx via Markov chain Monte Carlo (will be discussed in Chapter 11) and variational inference techniques (discussed here). The former is in fact able to model a distro \textit{exactly}, provided that there exist infinite computational resources. The approximation there occurs due to the usage of a finite amount of processor time. The approximation in the latter set of techniques comes from the particular choice of distribution classes that are used while approximating distros (\eg exponential distributions) \---- after we specify the distro class there is nothing intrinsically approximate (see \#463 par 2).

\subsection{Variational Inference}
Overall, inference will try discover some functions: the form of the functions as well as their parameters. These parameters are typically the parameters that describe the distribution (\eg mean and cov. of Gaussians), hyperparameters that define the priors of the distributions or the values of latent variables assumed while modelling the distribution. 	

The theoretical framework of variational framework resembles EM optimisation. Our task will finding an approximation for a \textit{complete-data} distribution $p(\mb{X,Z})$. Similarly to EM we decompose the distribution $p(\mb{X})$ as:
%
\begin{align}
\ln p(\mb{X}) &= \mathcal{L}(q)+\text{KL}(p||q) \label{eq:decomp}\\
 &= \int q(\mb{Z})\ln \left\{\frac{p(\mb{X,Z})}{q(\mb{Z})}\right\}d\mb{Z} - 
 \int q(\mb{Z})\ln \left\{\frac{p(\mb{Z|X})}{q(\mb{Z})}\right\}d\mb{Z}
\end{align}


\subsubsection{Factorized Distributions}
So we are after approximating a distribution with another distribution $q(\mb{Z})$ of our choice. In our choice we'll have to make an assumption that ensures the tractability of our solution. 

This section investigates what happens for a particular type of assumption: the assumption that the distribution $q(\mb{Z})$ can be factorized such as:
%
\begin{equation}
q(\mb{Z}) = \prod\limits_{i=1}^M q_i(\mb{Z}_i)
\end{equation}
%
%
Importantly, there will be \textbf{no other assumptions}. The form of the distribution (\eg Gaussian etc.) will be discovered automatically.

To understand how a good approximation will be made, we first must realize that (\ref{eq:decomp}) will hold \textit{no matter which $q(\mb{Z})$ we choose} \---- the r.h.s. terms will always sum up to the l.h.s. What separates a good approx from a bad one is how small the KL divergence in the 2nd term is. This is equivalent to saying how large the 1st term on the r.h.s., \ie $\mathcal{L}(q)$, is. This is the most critical thing to understand about variational inference, because we are converting the problem of finding an approximate distribution to the problem of maximising the lower bound $\mathcal{L}(q)$.

Now let's discuss how to maximise $\mathcal{L}(q)$ when we assume that a distribution can be factorised. We'll discuss the maximisation for one factor $q_j(\mb{Z}_j)$, but the discussion will be applicable to any factor. To maximise $\mathcal{L}(q)$, we'll ignore any term that does not depend on $q_j(\mb{Z}_j)$ by simply absorbing them into some constants \---- these constants are denoted in (10.6) with $\text{const}$. Then $\mathcal{L}(q)$ can be written as: 
%
%
\begin{equation}
\mathcal{L}(q) = \int q_j \ln \tilde{p}(\mb{X},\mb{Z}_j) d\mb{Z}_j-\int q_j \ln q_j d\mb{Z}_j+\text{const} \label{eq:lower_bound}
\end{equation}
where $\ln \tilde{p}(\mb{X},\mb{Z}_j) = \mathbb{E}_{i\neq j} [\ln p(\mb{X,Z})]$. Note that (\ref{eq:lower_bound}) now became another (negative) KL divergence, therefore maximising $\mathcal{L}(q)$ is achieved when the two distributions that appear in (\ref{eq:lower_bound}) are identical. This gives the solution of our problem as:
%
\begin{equation}
\ln q_j^{*}(\mb{Z}_j) = \mathbb{E}_{i\neq j} [\ln p(\mb{X,Z})].
\end{equation}





\subsubsection{Properties of Factorised Approximations}
The order in KL distance affects the type of approximation made: the distances $\text{KL}(p||q)$ and $\text{KL}(q||p)$ have quite different characteristics. See \#468.

\subsubsection{Example: The univariate Gaussian}

\subsection{Illustration: Variational Mixture of Gaussians}
We have: Dataset.
We have: likelihood function with \textit{unknown} mean, covariance and prior.
We have: an unknown latent variable associated with each sample.

Conditional distro of \textit{observed} data vectors \textit{given} latent variables and component parameters is:
%
\begin{equation}
p(\mb{X}|\mb{Z,\mu,\Lambda}) = \prod_n \prod_k \mathcal{N}(\mb{x}_n|\bs{\mu}_k,\bs{\Lambda}_k^{-1})^{z_{nk}}
\end{equation}
%
%
We introduce priors over parameters $\bs{\mu, \Lambda, \pi}$ \---- We pick conjugate priors:
%
%
\begin{equation}
p(\bs{\pi}) = \text{Dir}(\bs{\pi|\alpha}_0) = C(\bs{\alpha}_0)\prod_k \pi_k^{\alpha_0-1}
\end{equation}
%
%
Similarly we introduce a Gaussian-Wishart distro over the component parameters:
%
%
\begin{equation}
p(\bs{\mu,\Lambda}) =\mathcal{N}(\cdot)\mathcal{W}(\cdot)
\end{equation}

\subsubsection{Variational Distribution}
The \textit{actual} distribution decomposes as:
\begin{equation}
p(\mb{X,Z},\bs{\pi,\mu},\bs{\Lambda}) = p(\mb{X|Z},\bs{\mu,\Lambda})p(\mb{Z}|\bs{\pi})p(\bs{\mu|\Lambda})p(\bs{\Lambda})
\end{equation}

We impose the following decomposition on the \textit{variational} distribution \---- note how we dispense with the data after modelling:
\begin{equation}
q(\mb{Z},\bs{\pi,\mu,\Lambda}) = q(\mb{Z})q(\bs{\pi,\mu,\Lambda})
\end{equation}

Using the general conclusion in (10.9) we obtain the following distributions:

\begin{equation}
\ln q^{\star}(\mb{Z}) = \mathbb{E}
\end{equation}













\subsection{Illustration: Variational Mixture of Gaussians}

To get the big picture of variational inference the best is to build an analogy with EM. See table below:
%
\begin{table}
\centering
\begin{tabular}{lcc} \hline
 & EM & Variational Inf. \\\hline
 Responsibilities (E-step) & $\gamma(z_{nk})$ & $r_{nk}$ \\ 
 Update eqs (M-step) & $\bs{\mu}_k^{new}, \bs{\Sigma}_k^{new}, \pi_k^{new}$ \--- see \#439 & $\alpha_k, \beta_k, \mb{m}_k, \mb{W}_k, \nu_k$ \--- see \#478 \\ \hline
\end{tabular}
\end{table}

\subsubsection{Variatonal lower bound}
Lower bound $\mathcal{L}(q)$ enables the sanity-check of the software implementation. The lower bound should not decrease at each step.

\subsubsection{Determining the number of components}
\subsubsection{Induced factorizations} 
Induced factorizations are (unintended) factorizations that arise automatically from the interplay between the data and our design. These factorizations should be exploited during software implementation \---- \eg we shouldn't keep a full covariance matrix.
\subsection{Variational linear regression}
An example similar to mixture of Gaussians.

\subsection{Exponential Family Distributions}
The section separates the variables into two groups:\\
\textbf{Latent (or extensive) variables}: Grow with the number of training data.\\
\textbf{Parameters (or intensive) variables}: Fixed size.


\subsection{Local Variational Methods}
\subsection{Variational Logistic Regression}
\subsection{Expectation Propagation}
















\clearpage
\newpage



\section*{Miscellaneous}
\paragraph*{Model Comparison} The more rigorous section is Sec. 3.4 (and 3.5) with a proper treatment of a theoretically plausible model selection approach. AIC (see 1.73) and BIC (Sec 4.4.1, \#217) offer simpler model comparison criteria.





\bibliographystyle{IEEEtran}
\bibliography{report}


\end{document}










