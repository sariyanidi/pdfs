\documentclass[a4paper]{article}
\usepackage[english]{babel}
\usepackage{color,graphicx,amsmath,hyphenat}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.25in}
\usepackage{xcolor}
\setlength{\textheight}{9in}
\setcounter{MaxMatrixCols}{20}
\pagestyle{empty}

% some shortcuts
\newcommand{\ea}{\textit{et al. }} 
\newcommand{\eg}{\textit{e.g. }} 
\newcommand{\ie}{\textit{i.e. }} 
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\cg}{\color{gray}}
\newcommand{\tr}{\text{tr}}
\newcommand{\rank}{\text{rank}}
\newcommand{\adj}{\text{adj}}
\newcommand{\diag}{\text{diag}}
\newcommand{\fs}{\footnotesize}
\newcommand{\mb}{\mathbf}
\newcommand{\bs}{\boldsymbol}
\newcommand{\lnote}[1]{#1\reversemarginpar\marginpar{{\textit{#1}}}}
\newcommand{\bnote}[1]{#1\reversemarginpar\marginpar{{\textit{{\textcolor{blue}{#1}}}}}}
\newcommand{\rnote}[1]{#1\reversemarginpar\marginpar{{\textit{{\textcolor{red}{#1}}}}}}
\usepackage{marginnote}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={210mm,297mm},
 left=50mm,
 right=20mm,
 top=20mm,
 bottom=20mm,
 }
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\setcounter{section}{3}



\begin{document}
\pagestyle{empty}
  

\section{Linear Models for Classification}
Discusses linear models and \textit{generalised linear models} (GLM). GLM means that even if the prediction functions are non-linear, the decision surfaces are linear.

\subsection{Discriminant functions}
\subsubsection{Two Classes}
Describes the geometry of a discriminant function $y(\mb{x}) = \mathbf{w}^T\mb{x}+w_0$. That is, $\mb{w}$'s are orthogonal to decision surface and $|w_0|/||\mb{w}||$ describes dislocation from origin.

\subsubsection{Multiple Classes}
Discuss the limitation of one-vs-rest and one-vs-one classifiers, introduce the benfits of multi-class linear discriminant.

\subsubsection{Least-squares Classification}
Least squares classification has one extra limitation wrt. limitation of least squares regression: The target vector $\mb{t}$ are of 1-of-$K$ type.

\subsubsection{Fisher's Linear Discriminant}
Perform a dimensionality reduction and then discrimination. $J(\mb{w})$ is a function that does this and can be minimised via (4.2.9).

\subsubsection{Relation to Least-squares}
By changing the target variable representation for the 2-class problem, it's possible to relate Fisher and least-squares.

\subsubsection{Fisher's Discriminant for multiclass}
Consider generalisation to $K>2$ classes. The extension is similar to 2-class. Now there are multiple possible choices of (Fisher) criterion.

\subsubsection{The Perceptron Algorithm}
Construct GLM $y(\mb{x})=f(\mb{w}^T\phi(\mb{x}))$ where $f(a) = \begin{cases}+1 & a\ge 0 \\ -1& a<0 \end{cases}$. Patterns in $C_1$ become +1 and for $x_n \in C_1$ we want $\mb{w}^T\phi(\mb{x})>0$ and for $x\in C_2$ we want it to be $<0$. Both can be summarised as $t\mb{w}^T\phi(\mb{x})>0$.

The perceptron criterion minimises error only on misclassified patterns. The weight update algorDenemeithm operates for each sample $n$:
%
\begin{equation}
\mb{w}^{(\tau+1)}=\mb{w}^{(\tau)}-\eta \nabla E_p(\mb{w}) = \mb{w}^(\tau)+\eta \phi_n t_n
\end{equation}
%
where $\eta$ is the \textit{learning rate}. The update $(\tau+1)$ happens in the \textit{direction of misclassification} and \textit{guarantees} the error on misclassified sample to be reduced. Of course it doesn't guarantee anything on \textit{all} training samples.

\subsection{Probabilistic Generative Models}
Construct posterior $p(C_k|\mb{x})$ and represent via \textit{logistic sigmoid}:

\begin{equation}
p(C_1|\mb{x}) = \frac{p(\mb{x}|C_1)p(C_1)}{\sum\limits_{j\in \{1,2\}} p(\mb{x}|C_j)p(C_j)}=\frac{1}{1+\exp(-\alpha)}=\sigma(\alpha)
\end{equation}
%
%
where $\alpha = \ln \frac{p(\mb{x}|C_1)p(C_1)}{p(\mb{x}|C_2)p(C_2)}$ and $\sigma(\alpha)$ is the logistic sigmoid function.

\textit{We are interested in} situations where $\alpha(\mb{x})$ is linear and therefore creates posteriors governed by GLMs.

\subsubsection{Continuous Inputs}
We sgtart by assuming that all classes $C_k$ share same cov matrix $\Sigma$.

For $K$ classes $\alpha_k$ becomes $\alpha_k(\mb{x}) = \mb{w}^T\mb{x}+w_{k0}$ where $\mb{w}_k=\Sigma^{-1}\boldsymbol{\mu}_k$ and $w_{k0}$ is as in (4.70). 

That is, $\alpha_k$ is linear in $\mb{x}$. Decision boundaries (which correspond to misclassification rate) wioll be again linear in $\mb{x}$ so again we have GLM.

If we relax the ``shared covariance matrix'' assumption, then we'll have \textit{quadratic discriminant} rather than GLM.

\subsubsection{Maximum likelihood solution}
Once $p(\mb{x}|C_k)$ defined, we can determine values of its parameters and parameters of $p(C_k)$ via \textit{maximum likelihood}. Construct maximum function:
%
\begin{equation}
p(\mb{T,X} | \pi, \bs{\mu}_1, \bs{\mu}_2, \Sigma) = \prod\limits_n [\pi \mathcal{N}(x_n|\bs{\mu}_1, \Sigma)]^{t_n} [(1-\pi) \mathcal{N}(x_n|\bs{\mu}_2, \Sigma)]^{(1-t_n)}
\end{equation}
In the ML solution we get $\bs{\mu} = \frac{1}{N_1}\sum\limits_{n} t_n \mb{x}_n$ and $\bs{\mu} = \frac{1}{N_2}\sum\limits_{n} (1-t_n) \mb{x}_n$.

For covariance $\Sigma$, define $\mb{S}_1,\mb{S}_2, \mb{S}$ as:
%
\begin{align}
\mb{S}_1=&\frac{1}{N_1}\sum\limits_{n\in C_1}(\mb{x}_n-\bs{\mu}_1)(\mb{x}_n-\bs{\mu}_1)^T\\
\mb{S}_2=&\frac{1}{N_2}\sum\limits_{n\in C_2}(\mb{x}_n-\bs{\mu}_2)(\mb{x}_n-\bs{\mu}_2)^T\\
\mb{S}=&\frac{N_1}{N}\mb{S}_1+\frac{N_2}{N}\mb{S}_2
\end{align}

Overall, process not robust to outliers because ML is not. 

\subsubsection{Discrete Features}
\subsubsection{Exponential Family}
We manage to get GLMs for the above types too.
\subsection{Probabilistic Discriminative Models}
Advantage: There are less parameters to discover and usually leads to improved performance. 
\subsubsection{Fixed Basis Functions}
\subsubsection{Logistic Regression}
Here we set $M$ params whereas in generative modelling we set $(M+5)/2+1$ params.

Consider implementing a discriminative function directly as a via logistic sigmoid function:
\begin{equation}
p(C_1|\mb{\phi})=y(\phi)=\sigma(\mb{w}^T\phi)
\end{equation}
and naturally $p(C_2|\phi)=1-p(C_1|\phi)$. We can set params via ML. We start by seeing that $\frac{d\sigma}{d\alpha}=\sigma(1-\sigma)$ (exercise 4.12). Likelihood can be written as:
\begin{equation}
p(\mb{T}|\mb{w}) = \prod\limits_n y_n^{t_n}(1-y_n)^{1-t_n}
\end{equation}
%
where $y_n=p(C_1|\phi_n)$. Error function here is also called \lnote{cross entropy error}:
%
\begin{equation}
E(\mb{w})=-\ln p(\mb{T}|\mb{w}) = -\sum\limits_{n}[t_n \ln y_n + (1-t_n)\ln(1-y_n)]
\end{equation}
%
Taking the gradient wrt $\mb{w}$:
%
\begin{equation}
\nabla E(\mb{w})=\sum_n(y_n-t_n)\phi_n
\end{equation}

\subsubsection{Iterative Reweighted Least Squares}
We no longer have closed-form solution (as we did for regression). Fortunately the error function is still convex there is the (iterative) \lnote{Newton-Raphson or iterative reweighted least squares} algorithm:
%
\begin{equation}
\mb{w}^{(new)}=\mb{w}^{(old)}=\mb{H}^{-1}\nabla E(\mb{w})
\end{equation}
%
where $\mb{H}$ is the hassian matrix whose elements comprise the second derivs of $E(\mb{w})$ wrt components of $\mb{w}$.
%
\begin{align}
\nabla E(\mb{w})=&\sum_{n}(y_n-t_n)\phi_n=\mb{\Phi}^T(\mb{Y-T})\\
\mb{H}=&\nabla\nabla E(\mb{w})=\sum\limits_{n}y_n(1-y_n)\phi_n\phi_n^T=\mb{\Phi}^T\mb{R\Phi}
\end{align}
where $\mb{\Phi}$ is the $N\times M$ \lnote{design matrix} whose $n$th row is given by $\phi_n^T$ and $\mb{R}$ is the $N\times N$ diagonal matrix with elements $\mb{R}_{nn}=y_n(1-y_n)$.

\subsubsection{Multiclass logistic regression}
The formalism is similar to 2-class logistic regression. Instead of sigmoid we use the \textit{softmax} function. Again we have \textit{cross-entropy} function as error function. The multiclass version of cross-entropy is:
\begin{equation}
E(\mb{w}_1,\hdots, \mb{w}_K) = -ln p(\mb{T}| \mb{w}_1, \hdots, \mb{w}_K) = -\sum\limits_n\sum\limits_k t_{nk}-\ln y_{nk}.
\end{equation}
Again we can use \textit{iterative reweighted least squares} (\#210).

\subsubsection{Probit regression}
The inverse probit function (or the similar $\text{erf}$ function) are similar to sigmoid in shape but have more plausible analytical properties. Will be discussed in Sec. 4.5.

\subsubsection{Canonical link function}
This is one of the most frequently-referred sections of the book. The choices of sigmoid/softmax in earlier sections were not arbitrary \---- they were chosen to convert the error function to a simple form that involves $y_n-t_n$. This is a general result of assuming a conditional distribution for the activation function known as the \lnote{canonical link function}.

A GLM is a model for which $y$ is a nonlinear function of a linear combination of input variables: 
\begin{equation}
y=f(\mb{w}^T\phi)
\end{equation}
%
where $f(\cdot)$ is the \textit{activation function} and $f^{-1}(\cdot)$ is known as the \textit{link function}.

Let the conditional distro be $p(\mb{T}|\eta, s)$. We formulate its derivative in the following form:
%
\begin{equation}
\nabla_{\mb{w}} \ln p(\mb{T}|\eta,s)= \hdots \text{(see \#213)} = \sum\limits_n\frac{1}{s}\psi'(y_n)f'(y_n)\phi_n
\end{equation}.
The canonical link function chosen as $f^{-1}(y)=\psi(y)$ provides a great simplification:
\begin{equation}
\nabla E(\mb{w})= \frac{1}{s}\sum\limits_n(y_n-t_n)\phi_n
\end{equation}

\subsection{The Laplace Approximation}
To perform closed-form analysis for Bayesian logistic regression, we'll need to do approximation. The Laplace approx. is used for this purpose. Approximation is performed by matching the \textit{mode} of the target distribution with the mode of a Gaussian via Taylor expansion (where the first-order term disappears as expansion is made around a local maximum). Let $\mathbf{z}_0$ be the mode of the target distribution. The 2$^{nd}$ order Taylor expansion around $\mb{z}_0$ is:
\begin{equation}
f(\mb{z})\approx\ln f(\mb{z}_0)-\frac{1}{2}(\mb{z-z}_0)^{T}\mb{A}(\mb{z-z}_0).
\end{equation}
%
This will enable us to compute the approximated distribtion $q(\mb{z})$ directly as $q(\mb{z})=\mathcal{N}(\mb{z}|\mb{z}_0, \mb{A})$.

\bnote{Better methods will be explored in Chapter 10}.

\subsubsection{Model comparison and BIC}
We can use the approximation above for model comparison, which will lead to Bayesian Information Criterion (BIC). Start with the normalisation term: 
\begin{equation}
Z\approx f(\mb{z}_0) \int \exp \left[ -\frac{1}{2}(\mb{z-z}_0)^T \mb{A}(\mb{z-z}_0) \right] d\mb{z}=f(\mb{z}_0)\frac{(2\pi)^{M/2}}{|\mb{A}|^{1/2}}.
\end{equation}

Consider data set $\mathcal{D}$ and models $\{\mathcal{M}_i\}$ with parameters $\{\bs{\theta}_i\}$. For each model we define a likelihood function $p(\mathcal{D}|\bs{\theta}_i, \mathcal{M}_i)$ \---- or shortly, $p(\mathcal{D}|\bs{\theta}_i)$.

Defining $f(\bs{\theta})=p(\mathcal{D}|\bs{\theta})p(\theta)$ and identifying that $Z=p(\mathcal{D})$, we can apply the result above to get:
%
\begin{equation}
\ln p(\mathcal{D})\approx \ln p(\mathcal{D}|\bs{\theta}_{\text{MAP}})+\overbrace{\ln p(\bs{\theta}_{\text{MAP}})+\frac{M}{2}\ln(2\pi)-\frac{1}{2}\ln |\mb{A}|}^{\text{Occam factor}}.
\end{equation}

With further simplifications via (not necessarily realistic) assumptions (see \#217) we get the BIC:
\begin{equation}
\ln p(\mathcal{D}) \approx \ln p(\mathcal{D}|\bs{\theta}_{\text{MAP}})-\frac{1}{2}M \ln N
\end{equation}
Essentially this is an information criterion that penalizes model complexity


\subsection{Bayesian Logistic Regression}
Again, exact inference for logistic regression is intractable, due to normalisation (which involves likelihood computation, which is a product of sigmoids (one for each data point)). We apply Laplace approximation for tractability.

\subsubsection{Laplace Approximation}
Because Laplace involves Gaussian approx, we start with Gaussian prior $p(\mb{w}) = \mathcal{N}(\mb{w}|\mb{m}_0,\mb{S}_0)$. Posterior is $p(\mb{w}|\mb{T})\propto p(\mb{w})p(\mb{T|w})$. To compute the approx of this posterior, $q(\mb{w})$, we first express it in closed form (4.142) and then find the MAP solution, $\mb{w}_{\text{MAP}}$. Then we compute the approximation around $\mb{w}_{\text{MAP}}$; that is, we find the cov. matrix $S_N$ by using (4.132) \---- the mean is already $\mb{w}_{\text{MAP}}$ \--- and obtain approx as: $p(\mb{w|T})\approx q(\mb{w}) = \mathcal{N}(\mb{w}|\mb{w}_{\text{MAP}}, \mb{S}_N)$.

Now we'll use this for prediction by computing the \textit{predictive distribution}.

\subsubsection{Predictive Distribution}
Recall that for 2-class NN the prob of a sample being $C_1$ is the output of the NN, which is $p(C_1|\bs{\phi},\mb{w})=\sigma(\mb{w}^T\phi)q(\mb{w})$. Predictive distribution involves the following marginalization over $\mb{w}$:
%
\begin{equation}
p(C_1|\bs{\phi},\mb{T})=\int p(C_1|\bs{\phi},\mb{w}) p (\mb{w}|\mb{T}) \approx \int \sigma (\mb{w}^{T}\bs{\phi})q(\mb{w})d\mb{w}.
\label{eq:pred_distr}
\end{equation}
%
The problem here is that $\sigma(\cdot)$ is non-linear and again not tractable. Putting this aside for a moment, the section first computes the marginalization of $q(\mb{w})$, $\int q(\mb{w})d\mb{w}$. Then, it computes the overall integral in (\ref{eq:pred_distr}) by approximating the sigma function with the inverse probit function, $\Phi(\cdot)$ \---- see \#219 for details.

 
\clearpage
\newpage

\section{Neural Networks}
This is a critical chapter because many commonly-used techniques are motivated and described in detail. These include the \textit{gradient-descent optimization} and also the \textit{backpropagation} technique which is used for many purposes including \textit{Hessian computation} and \textit{Jacobian computation}. 

\subsection{Feed-forward Neural Networks}
\subsection{Network Training}
Again we'll minimize an error function. We can directly minimize a sum-of-squares error such as $E(\mb{w}) = \frac{1}{2}\sum||\mb{y}(\mb{x}_n-\mb{t}_n)||^2$. But we'll give rise to a probabilistic interpretation by considering likelihood maximization. This will be beneficial for many purposes.

The rest of the section derives the energy function and cross-entropy error function in the context of NNs.

\subsubsection{Parameter optimisation}
Clearly, there is no hope to find an analytical solution to optimum $\mb{w}$. We'll therefore resort to iterative algorithms of the following form:
%
\begin{equation}
\mb{w}^{(\tau+1)}=\mb{w}^{(\tau)}+\Delta \mb{w}^{(\tau)}.
\end{equation}
%
There are many algos of this form, and they differ by their choice of $\Delta\mb{w}^{(\tau)}$. Most use \textit{gradient information} due to reasons discussed in following section.

\subsubsection{Local quadratic approximation}
Consider second-order Taylor approx of $E(\mb{w})$ around some $\mb{\widehat{w}}$:
%
\begin{equation}
E(\mb{w})\approx E(\mb{\widehat{w}})+(\mb{w-\widehat{w}})^T\mb{b}+\frac{1}{2}(\mb{w-\widehat{w}})^T \mb{H} \mb{(w-\widehat{w}})
\end{equation}
%
where $\mb{b}=\nabla E_{\mb{w}}(\mb{w})|_{\mb{w=\widehat{w}}}$ and $\mb{H}$ is the Hessian matrix with elements $(\mb{H})_{ij}=\frac{\partial^2 E}{\partial w_i\partial w_j}|_{\mb{w=\widehat{w}}}$. If we pick $\mb{\widehat{w}}$ to be a minimum, say $\mb{w}^{\star}$ the second term above vanishes.
%
\begin{equation}
E(\mb{w})\approx E(\mb{{w}^{\star}})+\frac{1}{2}(\mb{w-{w}^{\star}})^T \mb{H} \mb{(w-{w}^{\star}})
\end{equation}
%
We analyse $\mb{H}$ by considering its eigenvectors, and we see that constant-error contours $E(\mb{w})=C$ are ellipses whose axes are aligned with the eigenvectors of $\mb{H}$, $\mb{u}_i$, and the length of these axes are inverse proportional to the corresponding eigenvectors, $\lambda_i$ (see \#238-239 and Exercise 5.10).

\subsubsection{Use of gradient information}
Gradient allows us to compute evaluate $E$ in $O(W^2)$ instead of $O(W^3)$.

\subsubsection{Gradient descent optimization}
Standard (\ie batch) gradient descent ($\mb{w}^{(\tau+1)}=\mb{w}^{(\tau)}-\eta \nabla E(\mb{w}).$) leads to poor performance. However we can perform gradient-descent for each sample in dataset separately, which is known as on-line gradient descent (or stochastic GD or sequential GD), which proved to be much better.  

But for batch optimization there are much more efficient methods such as \textit{conjugate gradients} and \textit{quasi-Newton} methods.

\subsection{Error Backpropagation}
The main goal is to find efficient methods to compute the gradient of $E(\mb{w})$, $\nabla E(\mb{w})$. The importance of backpropagation lies in that it can be used beyond the scope of NNs and gradient descent, such as other derivatives and graphical models etc.

There are 2 steps at each iteration of a backpropt technique:
\begin{enumerate}
\setlength{\parindent}{0mm}
\item Evaluate $\nabla E(\mb{w})$ (this is where backpropagation comes to play)
\setlength{\parindent}{0mm}
\item Update $\mb{w}^{\star}$ based on step 1.
\setlength{\parindent}{0mm}
\end{enumerate}

\subsection{Evaluation of error-function derivatives}
We will analyse the error of a single sample, $E_n$, and the total error is simply the sum over $N$. Let $\delta_j$ be defined as
\begin{equation}
\delta_j=\frac{\partial E_n}{\partial a_j},
\end{equation}
where $\delta$'s are typically referred to as errors. In the last layer, due to our choice of activation function, we have (see 5.18):
%
\begin{equation}
\delta_k = \frac{\partial E_n}{\partial a_k} = y_k-t_k
\end{equation}
%
The ultimate goal is to compute derivatives $\frac{\partial E_n}{\partial w_{ji}}$ and the $\delta$'s will be the messages that are propagated backwards.

We can express the desired derivative by means of the output and label as:
%
\begin{equation}
\frac{\partial E_n}{\partial w_{ji}} = (y_{nj}-t_{nj})x_{ni}
\end{equation}
%
Note that by feed-forward NN definition we have $a_j=\sum\limits_i w_{ji}z_i$ and $z_j=h(a_j)$. We can therefore express the derivative above through the chain rule as:
%
\begin{equation}
\frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j}\frac{\partial a_j}{\partial w_{ji}}=\delta_j \frac{\partial a_j}{\partial w_{ji}}
\end{equation}
%
%
Note that the first derivative on the rhs corresponds to our $d_j$ definition.

Now we can start from the last (output) layer and propagate backwards:
\begin{equation}
\delta_j=\frac{\partial E_n}{\partial a_j}=\sum\limits_k \frac{\partial E_n}{\partial a_k}\frac{\partial a_k}{\partial a_j}=\sum\limits_k\delta_k w_{kj}
\label{eq:bp_output}
\end{equation}
where the units $k$ are those to which $j$ sends connections. Here $\frac{\partial a_k}{\partial a_j}=w_{kj}$ holds because the $k$th layers are output layers, and we are considering the regression problem where the activation function ($h(\cdot)$) is simply the unity function and therefore:
%
\begin{equation}
a_k=\sum\limits_i w_{ki}a_{i}.
\end{equation}
%

Now let us consider a node $j$ that lies in one layer before where the activation function is not unity and therefore:
\begin{equation}
a_j=\sum\limits_i w_{ji} h(a_{i})
\end{equation}
%
%
and therefore we have 
\begin{equation}
\delta_j=h'(a_j)\sum_{k}w_{kj}\delta_k.
\label{eq:bp_hidden}
\end{equation}

We can summarise backpropagation as follows:
\begin{enumerate}
\setlength{\parindent}{0mm}
\item Apply an input vector $\mb{x}_n$ to the network and propagate forwards
\item Evaluate $\delta_k$ for output units through (5.54) (or (\ref{eq:bp_output} above)
\item Evaluate $\delta_j$ for all hidden units in between input and output layers (or (\ref{eq:bp_hidden}) above).
\item Finally obtain the derivative $\frac{\partial E_n}{\partial w_{ji}}$ through (5.53), which is $\frac{\partial E_n}{\partial w_{ji}}=\delta_j z_i$.
\end{enumerate}
%
%
\subsubsection{Efficiency of backpropagation}
Backpropagation's main advantage is efficiency. Derivatives can be computed with central differences method as in (5.69), however, that is too costly. But the central differences method can be used to check the correctness of a backpropagation method.

\subsubsection{The Jacobian Matrix}
In addition to the derivatives of the error function, backprop can also be used for the Jacobian:
%
\begin{equation}
J_{ki} = \frac{\partial y_k}{\partial x_i}
\end{equation}
%
We similarly will try to derive a message passing algorithm. Jacobian can be written out as:
%
\begin{equation}
J_{ki} = \frac{\partial y_k}{\partial x_i}=\sum\limits_j\frac{\partial y_k}{\partial a_j}\frac{\partial a_j}{\partial x_i}=\sum\limits_j w_{ji} \frac{\partial y_k}{\partial a_j}
\end{equation}
%
where $j$'s represent the nodes to which node $i$ sends connection. To compute the derivative above, we compute the second term in the rhs as:
%
\begin{equation}
\frac{\partial y_k}{\partial a_j}=\sum\limits_l\frac{\partial y_k}{\partial a_l}\frac{\partial a_l}{\partial a_j}=h'(a_j)\sum\limits_l w_{lj} \frac{\partial y_k}{\partial a_l}
\end{equation}
%
Again we start at output units, see \#248 for further details.

\subsection{The Hessian Matrix}
This section is critical as it discusses in detail various types of Hessian computation, including Hessian computation via backprop. The different methods for Hessian computations are due to the different assumptions which lead to different approximations. Exact Hessian computation (\ie no assumptions) is also possible via backprop. Among other things, Hessian is useful for:
%
\begin{enumerate}
\setlength{\parindent}{0mm}
\item Finding non-significant weights
\item Computing Laplace approximation for Bayesian Networks
\item Non-linear optimization techniques
\item Efficient method for re-training network
\end{enumerate}

\subsubsection{Diagonal Approximation}
The first way is to discard non-diagonal elements in which case we have a quite simple computation for Hessian. However Hessian can be strongly non-diagonal therefore these assumptions must be treated with care.

\subsubsection{Outer product approximation (Levenberg-Marquardt approximation)} 
For sum-of-squares Error function, the Hessian can be written out as:
%
\begin{equation}
\mb{H}=\nabla\nabla E = \sum\limits_{n=1}^N \nabla y_n(\nabla y_n)^T+\sum_(y_n-t_n)\nabla\nabla y_n.
\end{equation}
%
The second term is likely to be very small and can be neglected, which leads to \textit{outer product} or \lnote{Levenberg-Marquard approximation}:
%
\begin{equation}
\mb{H}\approx \sum_{n=1}^N\nabla a_n(\nabla a_n)^T=\sum\limits_{n=1}^N \mb{b}_n\mb{b}_n^T.
\end{equation}
Similarly, for cross-entropy error function we obtain:
%
\begin{equation}
\mb{H}\approx \sum\limits_{n=1}^N y_n(1-y_n) \mb{b}_n\mb{b}_n^T
\end{equation}



\subsubsection{Inverse Hessian}
The outer product approx above leads also to efficient methods for computing the inverse of Hessian. The Hessian can be updated sequentially for each point as:
%
\begin{equation}
\mb{H}_{L+1} = \mb{H}_L+\mb{b}_{L+1}\mb{b}_{L+1}^T
\end{equation}
and using the Woodbury identity (C.7) the inverse can be computed as 5.89.

\subsubsection{Finite differences}
The correctness of the Hessian and its inverse can again be checked by comparing it to the output of finite differences method.

\subsubsection{Exact evaluation of the Hessian}
The backprop procedure can be used for Hessian as well. Again we derive a message passing scheme by defining some $\delta$'s \---- see \#253-254.

\subsubsection{Fast multiplication by the Hessian} 
Many times we are interested in computing a multiplication of the Hessian with a vector $\mb{Hv}$ rather than computing the Hessian $\mb{H}$ itself. This is also possible via the backprop algo \---- see \#254-256.

\subsection{Regularization in Neural Networks}
\subsubsection{Consistent Gaussian Priors}
In Neural nets, different sets of priors can be equivalent to each other due to the bias parameter. A well-defined regularizer should be aware of this and not favour one equivalent solution to the other. We can achieve this via the consistent Gaussian priors discussed here. The section will be used for \bnote{automatic relevance determination in RVMs}.


\subsubsection{Early Stopping}
An alternative is to prevent overfitting is early stopping \--- stop the training as soon as the validation error starts getting larger.

\subsubsection{Invariances}
There are four methods to implement invariance for certain transformations:
\begin{enumerate}
\item Augment training set with replicas
\item Tangent propagation (manifold learning)
\item Invariance during feature extraction
\item Shared weights (for limited types of invariances)
\end{enumerate}
%
%
%

\subsubsection{Tangent propagation}
Provided that a transformation is continuous, then the transformed pattern will sweep out a manifold $\mathcal{M}$ in a $D$-dimensional space. Assume that this transformation acting on a vector $\mb{x}_n$ is given via the function $\mb{s(x}_n,\xi)$ where $\xi$ controls the amount of transformation.

Under a transformation, the output of the network will usually change \---- but we don't want it to. So we punish changes on the output.

A related technique is \lnote{tangent distance} where invariance is built into the distance metric (Simard \ea, 1993).

\subsubsection{Training with transformed data}
Training with transformed data is closely related to tangent  propagation. The section discusses this analytically.

\subsubsection{Convolutional networks}
Discuss the weight-sharing of CNNs.

\subsubsection{Soft weight sharing}
CNNs share weights by making them identical. This is a softer version of sharing weights. This section model soft weights as GMM.

\subsubsection{Mixture Density Networks}
We can create a very flexible output by having a Gaussian mixture of NNs. \rnote{Potentially useful} technique for many purposes \---- it can model arbitrary distributions. The conditional mode of a distribution is likely to be an important property (see end of section and Fig. 5.21d).


\subsection{Bayesian Neural Networks}
So far we focused on ML for finding the weight parameters. Now we consider Bayesian methods. We'll ultimately be interested in making predictions. That is, as far as Bayesian is concerned, computing the predictive distribution. 

The conditional distro for one sample is:
%
\begin{equation}
p(t|\mb{x,w},\beta) = \mathcal{N}(t|y(\mb{x,w}, \beta^{-1})).
\end{equation}
%
Then we have the prior:
%
\begin{equation}
p(\mb{w}|\alpha) = \mathcal{N}(\mb{w}|\mb{0}, \alpha^{-1}\mb{I})
\end{equation}
%
We compute likelihood for dataset $\mathcal{D}$ as $p(\mathcal{D}|\mb{w}, \beta)$. Then posterior is
\begin{equation}
p(\mb{w}|\mathcal{D}, \alpha, \beta) \propto p(\mathcal{D}|\mb{w}, \beta)p(\mb{w}, \alpha).
\end{equation}
We'll ultimately be interested in computing the predictive distro: $p(t|\mb{x}, \mathcal{D},\alpha, \beta)$. We'll also be optimizing $\alpha, \beta$ and evantually will have $p(t|\mb{x}, \mathcal{D})$.

We work with Laplace approximation due to intractability of exact analytical solutions.

First we make Laplace approx for the posterior $p(\mb{w}|\mathcal{D})$ \---- the approx is denoted with $q(\mb{w}|\mathcal{D})$. But even with this approx the following integral for predictive distro is intractable (see \#279):
%
\begin{equation}
p(t|\mb{w},D) = \int p(t|\mb{x,w})q(\mb{w}|\mathcal{D})d\mb{w}.
\end{equation}
%
To make progress we'll approximate the output function $\mb{y(x,w)}$ around $\mb{w}_{\text{MAP}}$ via Taylor expansion. Then we'll have two Gaussians and we know how to compute the marginal of two Gaussians (see end of \#279 or 2.115).

\subsubsection{Hyperparameter Optimization}
In the formalism so far we assumed that $\alpha, \beta$ are known and fixed. In this section we see how to optimize them. 


















\clearpage
\newpage



\section*{Miscellaneous}
\paragraph*{Model Comparison} The more rigorous section is Sec. 3.4 (and 3.5) with a proper treatment of a theoretically plausible model selection approach. AIC (see 1.73) and BIC (Sec 4.4.1, \#217) offer simpler model comparison criteria.





\bibliographystyle{IEEEtran}
\bibliography{report}


\end{document}










