\documentclass[a4paper, one	side]{book}

\input{../preliminaries}
\usepackage{pstricks, pst-node, pst-tree}
\begin{document}

\chapter*{Introduction}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Estimation Theory}

This chapter deals with how to estimate model parameters. 

\section{Desirable properties of estimators}
\begin{itemize}
\item Unbiasedness 
\item Consistency
\item Efficiency
\item Robustness
\end{itemize}

\section{Methods}
\begin{itemize}
\item \textbf{Method of moments}. This is an old method that is justified theoretically only partly. The basic idea is to use sample estimates of the (higher-order) moments; \eg, to estimate the $j$th order moment $\alpha_j:=E\{x^j|\bs{\theta}\}$ with the corresponding sample moment:%
$$ d_j=\frac{1}{T}\sum\limits_{i=1}^T (x_i)^j  .$$
\item \textbf{Least squares.}  The most standard approach to deterministic parameter estimator. Let $\mb{x}_T=(x_1,\hdots,x_T)$ be the target values to estimate and $\mb{H}$ the matrix of ``features'' such that $\mb{x}_T = \mb{H}\bs{\theta} + \mb{v}_T $, where $\mb{v}_T$ represents measurement errors. Least-squares aims to find the optimal model parameters $\bs \theta$ for reducing the squared error $\mathcal{E}_{LS}=\frac{1}{2}(\mb{x}_T-\mb H \bs \theta)^T(\mb{x}_T-\mb H \bs \theta)$, which turn out to be:%
 $$\bs{\hat{\theta}}_{LS}=\left(\mb{H}^T\mb H\right)^{-1}\mb{H}^T\mb{x}_T$$ 
\item \textbf{Generalized least squares}. Very similar to least squares, but allows a weighting matrix $\mb{W}$ such that $\mathcal{E}_{WLS} = (\mb{x}_T-\mb H \bs \theta)^T\mb W (\mb{x}_T-\mb H \bs \theta)$. The resulting estimator is $$\hat{\bs \theta}_{WLS}=(\mb H^T\mb{C}_v^{-1}\mb H)^{-1} \mb{H}^T \mb{C}_v^{-1} \mb{x}_T.$$

The estimator above is often referred to as \textit{best linear unbiased estimator (BLUE)} or \textit{Gauss-Markov estimator}.
\item \textbf{Maximum Likelihood method}. This method is applied by treating the data as if it comes from probability distributions. The ML method has many plausible properties (see my notes from Papoulis). The ML estimate is obtained by setting the derivatives of the log-likelihood equation to zero: %
$$\left.\frac{\p}{\p \theta_i}\ln p(\mb{x}_T | \bs{\theta})\right|_{\bs{\theta}=\hat{\bs{\theta}}_{ML}}=0$$
\end{itemize}

\section{Bayesian methods}
\begin{itemize}
\item \textbf{Minimum mean-square error estimator.} Since it is Bayesian, this method assumes a prior distribution over the model parameters and therefore a joint distribution $p_{\bs{\theta},\mb{x}}(\bs{\theta},\mb{x}_T)$. The goal is to find estimation of parameters $\bs{\theta}$.%
$$ \hat{\bs{\theta}}_{MSE} = E\{\bs{\theta}|\mb{x}_T\} .$$
%
This estimator is theoretically very significant due to its conceptual simplicity and generality; the result above holds for all distributions for which $p_{\bs{\theta},\mb{x}}(\bs{\theta},\mb{x}_T)$ exists. However, computing it is difficult due to the integrals inside the expectation operator.
%
The computations are simplified tremendously when the estimator is constrained to be linear. If the densities are gaussian, linear is optimal anyway. (see p.96)
%
\item \textbf{Maximum a posteriori estimator}. Similar to ML estimator but the prior distro about the parameters to estimate acts like a regularizer. \textit{The MAP estimator is a compromise between the general minimum mean-square error estimator and the ML estimator (p98).} The MAP has the advantage over ML that it takes into account (possibly available) information about the parameters $\bs{\theta}$, but it is computationally somewhat more difficult. Both ML and MAP estimators have the advantage over the minimum mean-square estimator that they avoid some difficult integration operations. 
\end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Information Theory}

\section{Entropy}
\begin{itemize}
\item \textbf{Standard entropy} (\ie on discrete-valued RV, $X$ taking values $\{a_i\}_i$): $H(X)=-\sum_i P(X=a_i)\log P(X=a_i)$.

This entropy is never negative.
\item \textbf{Differential entropy} (for continuous RVs, $x$): $H(x) = -\int p(\xi) \log p(\xi) d\xi$. This can be negative.

\item \textbf{Entropy of a transformation}. The entropy of a transformed RV, \eg $\mb{y}=\mb{f(x)}$, is larger than that of the original RV, $\mb{x}$. \textit{Entropy is not scale-invariant}; the entropy of a linearly scaled RV, $\mb{y}=\mb{Mx}$, is $H(\mb y) = H(\mb x) + \log |\det \mb M|$.

\item \textbf{Gaussian has maximal entropy} (within a subset of distros). Among the RVs that can take all real values and have zero mean and fixed variance, \textit{gaussian is the one that has the maximal entropy}. This is very significant as leads to using entropy as a measure of non-gaussianity. 

\item \textbf{Negentropy}. The property above leads to the definition of \textit{negentropy}: $J(\mb{x})=H(\mb x_{\text{gauss}})-H(\mb x)$. This type of entropy is useful for measuring non-gaussianity, and it has additional properties not shared by the differential (\ie continuous) entropy:  It is (i) always non-negative (ii) zero iff $\mb x$ is gaussian (iii) \textit{invariant to linear transformations}.

\item \textbf{Approximating entropy}. Negentropy is nice but difficult to compute as we don't have access to the pdf of $\mb x$ and simple estimations of it would lead to undesirable results (\eg sensitivity to outliers). Instead, we approximate negentropy. 
	\begin{itemize}
	\item \textit{Polynomial density expansions.} A Taylor-like expansion of the pdf via the Chebyshev-Hermite polynomials $H_i$: $$p_x(\xi)\approx \hat{p}_x(\xi)=\phi(\xi)\left(1+\kappa_3(\xi) \frac{H_3(\xi)}{3!} + \kappa_4(x)\frac{H_4(\xi)}{4!}\right)$$
	leads to the following approximation of the negentropy of a standardized RV: $$J(x) \approx \frac{1}{12}E\{x^3\}^2 + \frac{1}{48}\text{kurt}(x)^2$$ 
	\item \textit{Approximating the \underline{maximum} entropy}. This approach has the theoretical appeal of approximating the maximum (\ie upper bound) of entropy. The idea is that the finite observations we have may stem from infinite distributions; we transform this ill-posed problem to a well-defined one by constraining the distro to be the one that has maximal entropy. We compute the approximate the maximum entropy density $\hat{p}(\xi)$ as: $$ \hat{p}(\xi) = \phi(\xi)\left( 1+ \sum\limits_{i=1}^n  c_i F^i (\xi) \right)$$
	
	where $F^i$ are some (non-polynomial) functions.
	\end{itemize}

\end{itemize}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Principal Component Analysis and Whitening}

\section{PCA and some variants}

\begin{itemize}
\item \textbf{PCA}. The standard PCA is a distribution free technique. It was originally formulated as the problem of minimizing the squared reconstruction error by Pearson, although later it was also formulated as the problem of  variance maximization. 
\item \textbf{Online PCA}. It is possible to compute PCA in an online fashion too. It was shown that the online solution converges to the true eigenvectors (\ie the batch solution). An advantage of online PCA is that it can be used in the presence of incoming data stream and is particularly useful if the data is nonstationary. In the latter case, a constant learning would be preferred over an adaptive one (p133).
\item \textbf{Factor Analysis.} PCA can be derived from a generative model too. Assume $$\mb{x = Ay + n}$$ 
where $\mb y$ is standardized gaussian RV. The maximum likelihood solution to the rows of $\mb A$ is scaled eigenvectors of $\mb{C_x}$ in the limiting case when noise $\mb n$ tends to zero.

The formulation above can be used to derive an FA model too. Unlike PCA, FA was developed mainly for explaining observed results. The major difference is that $\mb n$ is not assumed white or infinitely small, leading to the major differences between PCA and FA (see below). Let the noise covariance be $\mb{Q}=E\mb\{\mb{nn}^T\}$. Assuming $\mb{Q}$ is known or can be estimated, one can solve $\mb A$ from $\mb{AA}^T=\mb{C_x-Q}$. However, there is no unique solution in this case. Various solution techniques that impose various criterion exist, such as varimax, quartimax and oblimin (see p139).
\item \textbf{PCA vs. Factor Analysis}. In PCA observations can be recovered simply with a linear transformation; this is not the case with FA as the term $\mb n$ (called noise in PCA but specific factor in FA) can have a significant contribution in FA. Solutions in FA do not in general correspond to (scaled) eigenvectors of $\mb{C_x}$. 

\item \textbf{Whitening.} PCA  can be used for whitening. Essentially, whitening aims to find a linar transformation $\mb{z=Vx}$ such that $\mb z$ is white (or, equivalently, sphered). The diagonal matrix of eigenvalues $\mb{D}=\diag(d_1,\hdots,d_n)$ and the matrix of eigenvectors $\mb{E}=(\mb{e}_1,\hdots,\mb{e}_n)$ can be used to obtain $\mb V$ as (p140): $$\mb{V=D}^{-1/2}\mb{E}^T.$$

\item \textbf{Orthogonalization}. While in practice PCA and ICA yields orthogonal/orthonormal bassi vectors, in practice the iterative algorithms may be unable to produce orthogonality. In this case, we may need to manually orthogonalize after each iteration or at specific intervals (p141). Orthogonalization can be performed via the Gram-Schmidt or symmetric orthonormalization methods (Householder transformation?); see p142.
\end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\chapter{What is Independent Component Analysis?}

\begin{itemize}
\item \textbf{Restrictions in ICA}.
	\begin{itemize}
	\item Independent components are assumed to be statistically \textit{independent}.
	\item Independent components must have \textit{nongaussian} distributions.
	\item For simplicity, we assume that the unknown mixing matrix is square (although this assumption can be relaxed).
	\end{itemize}
	\item \textbf{We always center the variables}. Without loss of generality, we always assume that both the mixture variables and the independent components have zero mean.
\item \textbf{Ambiguities in ICA}
	\begin{itemize}
	\item We cannot determine the variances (energies) of the independent components; the mixing can be shown as $$\mb{x} = \sum\limits_i \left( \frac{1}{\alpha_1}\mb{a}_i \right)(s_i\alpha_i)$$
	and we clearly cannot distinguish if the variance would be $E\{s_i^2\}$ or $E\{(s_i\alpha_i)^2\}$. (Variables are zero-mean \---- see above.) It is typical to fix the variances to be $E\{s_i^2\}=1$. But the \textit{ambiguity of sign} of the component remains \---- we don't know if it's -1 or 1.
	\item We cannot determine the order of independent components (differently from, say, PCA).
	\end{itemize}
\item \textbf{Whitening for pre-processing}. Whitening simplifies the problem of ICA considerably. Part of the reason is that, after whitening, the new mixing matrix is \textit{orthogonal}. Knowing this, we can restrict the search of the mixing matrix to orthogonal matrices. Whitening involves a linear transformation of the RV:%
$$\mb{z=VAs=\tilde{A}s}.$$
The whitening matrix, $\mb V$, can be obtained via PCA or via \textit{eigenvalue decomposition} (p159). The latter is: $$\mb{V=ED}^{-1/2}\mb{E}^T,$$ 
where $\mb E$ is the orthogonal matrix of eigenvectors of $E\{\mb{xx}^T\}$ and $\mb D^{-1/2} = \diag\left(d_1^{-1/2},\hdots,d_n^{-1/2}\right)$ where $d_i$ are the eigenvalues.

Whitening has other advantages too; see, \eg
	\begin{itemize}
	\item The notes under \textit{Method 1: Maximizing nongaussianity via kurtosis} below.
	\item Estimating multiple components becomes easy through the orthogonality constraint (see Notes on \textit{Estimating multiple components} below).
	\end{itemize}
\end{itemize}


\chapter{ICA by Maximization of Nongaussianity}


\begin{quotation}
\em It is usually argued that the gaussian distribution is the least interesting one, and that the most interesting directions are those that show the least gaussian distribution. [p197]
\end{quotation}

\begin{itemize}
\item \textbf{How does nongaussianity lead to independence?} The elegant conclusion is reached via the \textit{Central limit theorem} (CLM). According to CLM, the sum of two \iid RVs is more gaussian than the RVs themselves. In other words, the original RVs are less gaussian. Therefore, by finding the least gaussian linear combination we can recover the independent components. Below the idea is elaborated a bit further.

Let us assume that we try to recover one component. Assume that $$\mb{x=As},$$ where $\mb{s}=(s_1,\hdots,s_n)$ are the sources. If we find the mixing matrix $\mb A$, then we can invert it to get the ICs: $$\mb{s=A}^{-1/2}\mb{x}.$$
Put differently, each IC is a linear combination of $\mb{x}$. Let us denote this with $y=\mb b^T \mb x$, where $\mb b$ is one row of $\mb A^{-1}$. The question is, how can we use the CLM to find $\mb b$? Note that, $y$ is an IC, so that it can be denoted as $y =\mb{q}^T \mb s$ where $\mb q$ is a vector with only one nonzero element. The latter condition is satisfied with the maximization of nongaussianity, as described above. Therefore, our problem of finding the ICs becomes equivalent to maximizing the nongaussianity in $\mb{q}^T \mb s$ or, equivalently, in $\mb b ^T \mb x$. (We have to operate over $\mb b^T \mb x$ since we don't have access to $\mb s$.)

The rest of the chapter shows two ways to maximize nongaussianity.

\begin{itemize}

\item \textbf{Method 1: Maximizing nongaussianity via kurtosis.} One idea is to maximize $\text{kurt}(y).$ Assume that there are 2 ICs, then $$\text{kurt}(y)=\text{kurt}(q_1 s_1)+\text{kurt}(q_2 s_2) = q_1^4\text{kurt}( s_1)+q_2 ^4\text{kurt}(s_2). $$ 
(the latter follows from property of kurtosis). Since we assume that the ICs are of unit variance (see notes on Chapter 7), we are effectively constraining $\mb q$ to lie on the 2D unit circle: $E\{y^2\}=q_1^2+q_2^2=||\mb q||^2=1$.

The equality above can also be expressed in terms of the RV $\mb x$:%
$$\text{kurt}(y)=\text{kurt}(b_1 x_1)+\text{kurt}(b_2 x_2) = b_1^4\text{kurt}(x_1)+b_2 ^4\text{kurt}(x_2).$$

Note that, we can operate the optimization only on this latter equality as we have no access to $s_i$, whereas our constraint is defined in terms of the $q_i$'s. Here whitening provides an elegant bridge between the two. For the whitened data, $\mb{z=VAs}$, we seek a linear combination $\mb w^T \mb z$ that maximizes nongaussianity. Since we have $\mb{q=(VA)}^T \mb{w}$, $||q||^2$ can be expressed as 
$$||\mb q||^2 = (\mb w^T \mb{VA})(\mb A^T \mb V^T \mb w)=||\mb w||^2.$$ Thus, the constraint $||\mb q||^2=1$ becomes equivalent to constraint $||\mb w||^2=1$.

The method above can be implemented via the gradient algorithm (p175) or a fixed-point algorithm (p178).

\item \textbf{Method 2: Maximizing nongaussianity via negentropy}. Kurtosis has the drawback of outlier sensitivity.

In Chapter 5 negentropy, $J(y)$,  was presented as another measure of nongaussianity. $J(y)$ needs to be approximated. But if the simple, polynomial based approximation 
$$J(y)\approx \frac{1}{12}E\{y^3\}^2+\frac{1}{48}\text{kurt}(y)^2$$
is adopted, we can't gain much more than kurtosis as the first term above is zero in (approx.) symmetric distributions and the second term is ... kurtosis. 

Another and more robust approximation was presented in Section 5.6. This approximation is like a generalization of one above:
$$J(y)\approx k_1(E\{G^1(y)\})^2 + k_2(E\{G^2(y)\}-E\{G^2(v)\})^2$$
where $v$ is a standard gaussian RV.
The main difference is that the functions $G^i$ are not polynomials. If $y$ has symmetric distribution, this approximation is simplified as 
$$J(y) \propto (E\{G(y)\}-E\{G(v)\})^2.$$
Wise choices of $G$ make this expression robust to outliers. Two such choices are:

$$G_1(y) = \frac{1}{a_1}\log\cosh a_1 y$$
$$G_2(y) = -\exp (-y^2/2)$$

where $1\le a_1 \le 2$ is some suitable constant, often taken as 1.
The method above can be implemented via the gradient algorithm (p185) or a fixed-point algorithm (p188).
\end{itemize}

\item \textbf{Estimating multiple components}. The summary above deals with a single component. The algorithms (gradient or fixed-point) described for the material above are called ``one-unit''*** algorithms as they consider a single component. Thanks to whitening, estimating multiple components becomes quite easy. There are two ways to do it:
	\begin{itemize}
	\item \textbf{Deflationary orthogonalization}. We first estimate the first IC, $\mb w_1$, and then one by one determine $p-1$ more components with the criterion of being orthogonal to the existing components. The drawback is that errors in the first IC can be accumulated.
	\item \textbf{Symmetric orthogonalization}. Contrary to the one above, this approach is parallel estimation of all $p$ components. We initialize all components $\mb{w}_1,\hdots, \mb{w}_p$ randomly and then iteratively converge to the solution. At each iteration, we (i) do \textit{one} iteration of the one-unit algorithms, (ii) form the matrix $\mb W$ by concatenating horizontally the vectors $\mb w_i$ and (iii) do a symmetric orthogonalization of $\mb W$ via $$\mb{W} \gets (\mb{WW}^T)^{-1/2}\mb W\hspace{0.65cm}\text{where}\hspace{0.2cm} \mb{A}^{-1/2} = \mb E\, \diag(d_1^{-1/2},\hdots,d_n^{-1/2}) \mb E^T$$ with $\mb E$ and $d_i$ being the matrix of eigenvectors and the eigenvalues, respectively  (see p195). Symmetric orthogonalization discussed in Chapter 6 (Section 6.5). 
	\end{itemize}
	
	
\end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\chapter{ICA by Maximum Likelihood Estimation}

An alternative way to ICA is maximizing (log)likelihood. This can be achieved using the densities of the ICs. However, this is not strictly necessary, as shown in this chapter. One may find the ML solution by finding the family of the density in question: The space of probability densities is divided (wisely) into two halves; if one manages to correctly identify the half, then the ML solution is consistent \---- no need for more specific information!


\begin{itemize}
\item \textbf{The (log) likelihood equation.} We aim to recover the inverse of the mixing matrix $\mb {x = As}$; let us denote this matrix as $\mb{B} = \mb A^{-1}$. After calculations (p203-204), this equation is: %
$$\frac{1}{T} \log L(\mb B) = E\left\{ \sum\limits_{i=1}^n \log p_i (\mb b_i^T \mb x) \right\} + \log |\det \mb B| $$  
\item \textbf{Approaches to solve the ML problem.} The problem in the equation above is that we don't know the densities $p_i$ as well as $\mb B$. One can overcome this problem in two ways.
	\begin{itemize}
	\item \textit{Method 1: When the densities are known} in advance we just use them.
	\item \textit{Method 2: Finding the density family}. If we don't know the densities $p_i$ in advance, their estimation from finite data is always very difficult. But this chapter shows that finding the density is not strictly necessary, and the elegant alternative rests on Theorem 9.1. This alternative is to divide the space of probabilities into two (the division is also done through a condition found via Theorem 9.1), and then to find the only the family that the density in question belongs to. Theorem 9.1 shows that under mild conditions the ML solution is consistent as long as the family is correctly identified. The division is done as supergaussian and subgaussian densities (p206). The identification of the correct half is done at each iteration of the optimization (see Table 9.1 in p210). If a priori information is available, it can be used too.
	\end{itemize}
\item \textbf{Optimization}.
	\begin{itemize}
	\item \textit{The natural gradient algorithm}. This is better than (p208) standard gradient algorithm, but it has some pitfalls compared to the fixed-point algorithm. See Table 9.1 in p210 for algorithm.
	\item \textbf{Fixed-point algorithm} is like a computationally optimized version of the natural gradient algorithm; the $\alpha_i, \beta_i$ parameters act as adaptive learning rates that make convergence faster. See p211 for other advantages and Table 9.2 (p212) for summary of algorithm.
	\end{itemize}
\item \textbf{Infomax $\equiv$ Likelihood Maximization } (p213).
\end{itemize}















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{ICA by minimization of mutual information}
Mutual information provides theoretical justification for the heuristic that maximizing nongaussianity leads to independent components. Mutual information (MI) is defined in terms of entropy; more specifically, the entropy of the individual RVs and that of the joint random vector: %
$$ I(y_1,y_2,\hdots,y_m) = \sum\limits_{i=1}^{m} H(y_i) - H(\mb y) $$

The general idea is that: Independent components should have minimal mutual information, so we need to minimize mutual information. The goal becomes to find the linear transformation $\mb{y=Bx}$ that minimizes mutual information. Using results on Chapter 5, we can rewrite MI as:%
$$ I(y_1, y_2, \hdots, y_m) = \sum_i H(y_i) - H(\mb x) - \log |\det \mb B| $$
%
The restrictions that we assume about the $y_i$'s allow us to rewrite MI as:
$$ I(y_1, y_2, \hdots, y_m) = \text{const.} - \sum_i J(y_i)  $$
where $J(\cdot)$ is the negentropy. 

The whole chapter basically serves as a bridge between heuristics their theoretical justification. The application of MI minimization is nearly equivalent either to nongaussianity maximization or to likelihood maximization, depending on how we approximate entropy (see Section 10.4, p224).
\\

\noindent In practice, there are at least two differences between MI minimization and other nongaussianity maximization principles (p223)
\begin{enumerate}
\item Negentropy and other measures of nongaussianity enable deflationary, \ie one-by-one estimation of the ICs. This is not possible for MI minimization or likelihood maximization principles. 
\item For nongaussianity we force the estimates of the ICs to be uncorrelated, thus reduce the optimization space. This is not necessary when using MI. 
\end{enumerate}





















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\chapter{ICA by Nonlinear Decorrelation and Nonlinear PCA}

For gaussian variables, uncorrelatedness is a sufficient (and necessary) condition for independence. However, this is not necessarily the case for other variables; there do exist variables for which uncorrelatedness doesn't imply independence.

This section investigates what happens if we look at \textit{nonlinear correlations}.  That is, if two variables are nonlinearly uncorrelated can we infer that they are independent too? Nonlinear uncorrelatedness for two RVs $y_1, y_2$ is defined as 
%
\begin{equation}
E\{f(y_1)g(y_2)\} = 0
\label{eq:nonlinear_decorrelation}
\end{equation}
where at least one  of the functions $f, g$ is nonlinear. The second half of this section is devoted to nonlinear PCA and to showing some equivalences between nonlinear PCA and ICA; the former is praised as a \textit{versatile} technique too (p261).  

\section{Nonlinear Decorrelation}

\begin{itemize}
\item \textbf{What does nonlinear uncorrelatedness have to do with independence?} Taylor expansion around the origin allows us to write $E\{f(y_1)g(y_2)\}$ as (p241):
$$ E\{f(y_1)g(y_2)\} = \sum_i\sum_j f_i g_j E\{y_1^i y_2^j\}, $$
where $f_i,g_j$ are the coefficients from the Taylor expansion. Here it becomes clear that the RHS is zero if $E\{y_1^i y_2^j\}$. Ensuing discussion shows that this condition can be satisfied if (i) $y_1$ and $y_2$ are independent AND $E\{y_1^i\}=0$ or $E\{y_2^i\}=0$ for all odd $i$ values and $f(y_1)$ (or $g(y_2)$) is an odd function. Note that here independence (+ some additional conditions) is only a sufficient condition and NOT a necessary condition. In other words, nonlinear uncorrelatedness doesn't warrant independence, but I understand that for practical purposes we can assume that it does warrant it. 

This provides the justification for using nonlinear decorrelation to obtain ICs.

\item \textbf{Methods.} There are many methods to reach the condition \eqref{eq:nonlinear_decorrelation}. 
	\begin{itemize}
	\item \textit{The Herault-Jutten algorithm.} For two ICs, the mixed model can be represented as
	\begin{align*}
	x_1 = a_{11}s_1 + a_{12} s_2 \\
	x_2 = a_{21}s_1 + a_{22} s_2
	\end{align*}
	
	The authors defined a feedback circuit which recovers the estimated ICs $y_1, y_2$:
	\begin{align*}
	y_1 = x_1 - m_{12} y_2 \\
	y_2 = x_2 - m_{21} y_1.
	\end{align*}
	If the algorithm manages to find the ICs, then $\mb y=\mb{(I+M)}^{-1}\mb x = \mb{A x}$ where $\mb M$ is the matrix with zero as diagonal elements and $m_{12}, m_{21}$ as off-diagonals. The authors propose the update rules: 
	\begin{align*}
	\Delta m_{12} = \mu f(y_1) g(y_2) \\ 
	\Delta m_{21} = \mu f(y_2) g(y_1) \\ 
	\end{align*} If the algorithm converges, then by definition of convergence the RHS of those equations must be zero and the nonlinear decorrelatedness in \eqref{eq:nonlinear_decorrelation} is attained. 	In practice, the algorithm suffers from the requirement of inverting the matrix $(\mb{I+M})$ as well as other limitations (p243).
	\item \textit{The Cichocki-Unbehauen algorithm} doesn't suffer from the aforelisted shortcoming as it avoids matrix inversion via a feedforward network. It estimates the ICs via a model $\mb{y = Bx}$ where the goal is to estimate $\mb B$ iteratively. The learning algorithm is: 
	$$\Delta \mb B = \mu [\bs \Lambda - f(\mb y)g(\mb y^T)]\mb B$$
	where $\bs \Lambda$ is a diagonal matrix.
	
	If the algorithm converges, then by definition of convergence it must hold that (trivial solution $\mb B = \mb 0$ aside):
	$$ \bs \Lambda - E\{f(\mb y) g(\mb y^T)\} = \mb 0 $$
	Since $\bs \Lambda$ is diagonal, the result above suggests that convergence implies that the nonlinear uncorrelatedness condition in \eqref{eq:nonlinear_decorrelation} is attained.
	\end{itemize}
	
	\item \textbf{Generalization via Estimating functions} \---- and derivation of the two methods above as special cases. Amari and Cardoso (p246) proved that one can always find an equally performing or better (\ie more efficient) \textit{estimating function} $\mb F(\mb{x, B})$ than any given function in the form: 	
	$$\mb{F(x,B)} = \bs \Lambda -f(\mb{Bx}) (\mb{Bx})^T.$$
	Interestingly, the Herault-Jutten and Cichocki-Unbehauen algorithms follow this general form. Importantly, the result also shows that \textit{only one nonlinear function} is sufficient. 
\end{itemize}


\section{Nonlinear PCA}
The standard linear PCA tries to maximize (in one formulation) the mean-square:
\begin{equation}
\text{minimize}\,\, E\left\{ ||\mb x - \sum\limits_{i=1}^n (\mb w_i^T \mb x) \mb w_i||^2 \right\}
\end{equation}
The rest of the chapter relies on a simple nonlinearization of this criterion:
\begin{equation}
\text{minimize}\,\, E\left\{ ||\mb x - \sum\limits_{i=1}^n g_i(\mb w_i^T \mb x) \mb w_i||^2 \right\}
\label{eq:nl_pca}
\end{equation}
where $g(\cdot)$ is a nonlinear function. Clearly, the choice of the nonlinear algorithm will dictate the outcomes of nonlinear PCA \---- and there are many choices. The chapter shows that some choices will lead to discovering ICs.

Specifically, for prewhitened data, through \eqref{eq:nl_pca} we can obtain formulations equivalent to ICA. In p252 it is shown that \eqref{eq:nl_pca} is equivalent to minimizing:  
\begin{equation}
J_{NLPCA}(\mb W) := \sum_{i=1}^n E\{[y_i-g_i(y_i)]^2\}.
\label{eq:J_nlpca}czardas vittorio monti
\end{equation}
where $\mb y = (y_1,\hdots,y_n) = \mb{Wz}$.

\begin{itemize}
\item \textbf{Connection to ICA by kurtosis} By choosing $g_i(y)$ for all $i$ as 
$$g_i(y) = \begin{cases} y^2 + y & \text{if}\,\, y \ge 0 \\ -y^2 + y & \text{if}\,\, y<0 \end{cases},$$
we can obtain the kurtosis-based formulation of ICA.
\item \textbf{Connection to ML-based ICA solution.} By choosing $g_i(y_i)$ for all $i$ as %
$$g_i(y_i) = -\frac{p_i'(y_i)}{p_i(y_i)}$$
where $p_i(\cdot)$ is the density of the RV $y_i$, we attain the ML-based solution of ICA. \end{itemize}
The details of a good algorithm for the optimization of nonlinear PCA is given in p259, but it may be necessary to go to ref \#347 in the book. Section 2.3 of \cite{landqvist05} also seems to be a good source.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\chapter{Practical Considerations}

This chapter gives a few suggestions for applying ICA. 
\begin{itemize}
\item Filter your data (if time series). ICA works \textit{much better with data that doesn't have much noise} (p265). The type of filtering depends on the application. One can do
	\begin{itemize}
	\item Low-pass filtering
	\item High-pass filtering
	\item Band pass filtering.
	\end{itemize}

In any case it's difficult to decide the frequencies to keep; there are no guidelines, it's trial and error.

\item Reduce dimensionality through PCA. This is useful to:
	\begin{itemize}
	\item Make the mixing matrix square (which is needed for Basic ICA) in cases where the number of mixtures is smaller than the number of ICs.
	\item Reduce noise
	\item Prevent overlearning, which typically yields ``spiky'' signals. 
	\end{itemize}
\item Choice of learning algorithm
	\begin{itemize}
	\item Estimating all components parallel or one-by-one? Parallel is suggested.
	\item The non-linearity choice: $\tanh(\cdot)$ seems to be a good choice in many cases, although it's useful to try other nonlinearities too.
	\item On-line or batch? Choose batch and FastICA if possible; if the problem necessitates online learning go for stochastic gradient descent. 
	\end{itemize}
\end{itemize}

An advice actually from the next chapter is that

\begin{itemize}
\item Do the whitening in a robust way (p278).
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\chapter{Overview and comparison of Basic ICA methods}

The Basic ICA methods are compared theoretically and experimentally. Theoretical comparison includes (i) statistical efficiency (\ie variance) and (ii) robustness. Experimental comparison includes (i) accuracy (\ie matrix recovery performance) and (ii) computational efficiency. 

\begin{itemize}
\item \textbf{Differences between estimation principles} (p275)
	\begin{itemize}
	\item Some principles are able to estimate single independent components whereas others need to estimate them simultaneously
	\item Some objective functions use nonpolynomial functions based on the assumed probability density functions of the ICs, whereas others use polynomial functions related to cumulants. This leads to different nonquadratic functions in the objective functions. 
	\item In many estimation principles, uncorrelatedness is required. This is strictly not necessary (although I don't get it how) and reduces the optimization space.  However, practically it doesn't cause much loss. 
	\end{itemize}
\item \textbf{Theoretical comparisons}.
	\begin{itemize}
	\item \textit{Comparison of asymptotic variance.} Since we have a finite sample, the expectations in the theoretical definitions of the objective functions are replaced with sample averages. We want to know how much problem can be caused by this. A classic measure of such errors is asymptotic (co)variance. Theorem 14.1 shows how this variance can be computed, and Theorem 14.2 shows the family of functions that minimizes it:%
	$$ G_{opt}(y) = c_1 \log p_i(y) + c_2 y^2 + c_3 $$
	Note that $\log p_i(y)$ also belongs to the family above. \underline{This shows that negentropy is an optimal measure}  of nongaussianity.
	\item \textit{Comparison of robustness}. Robustness is defined through \textit{B-robustness}. An estimator is called B-robust if the influence of outliers is limited (see p278 for more precise definition). 
	\end{itemize}
\end{itemize}
































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\bibliographystyle{IEEEtran}
\bibliography{../bibliography}

\printindex

\end{document}
