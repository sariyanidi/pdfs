\documentclass[a4paper]{book}

\input{../preliminaries}
\usepackage{pstricks, pst-node, pst-tree}
\begin{document}

\chapter*{Introduction}

My notes from the legendary book of Papoulis and Pillai \cite{papoulis02}. The book assumes that the reader does already have fairly strong background in calculus and linear algebra. I used two books \cite{Gradshteyn80, Abramowitz70} as assistant for evaluating some of the mathematical identities or integrals. 

This book made me realize that a good technical book is not necessarily one that's easy to follow -- one that shows you very easily how each identity or result is arrived at. Some of the results in the book are presented without much explanation, and this required me to show a lot of effort to understand them; this effort seems to lead to a staying power. 


\chapter{Sequences of Random Variables}


\section{Notes}
\begin{itemize}
\item Multivariate Transformation
\item How to integrate some RVs from a multi-variate distro to obtain ... (Sec 7.2)
\item How to compute the mean conditioned on a subset of RVs
\item Characteristic function leads to so much interesting applications, such as:
%\begin{itemize}
	\item Computing the PDF of Bernoulli from Binomials (\#256)
	\item Computing the PDF of Poisson from Binomials (\#256)
	\item The sum of jointly normal RVs is normal (\#257)
	\item The sum of the squares of independent normal variables is chi-square (\#259)
	\item The sum of two chi-square distros is also chi-square (\#260)
%	\end{itemize}
\item The optimal single-value estimation (in the MS sense), $c$, of a future value of a RV $\mb{y}$ is $c=E(\mb{y})$.
\item The optimal functional estimation of $\mb{y}$ \ito a (dependent) RV $\mb{x}$ is $c(x)=E(\mb{y}|x)$. In general, this estimation is \textit{non-linear}.
\item The optimal linear f. estimation of $\mb{y}$ \ito $\mb{x}$ is $c(x) = Ax + B$ where $B = \eta_y - A\eta_x$ and $A = r \sigma_x/\sigma_y$. 
\item \textit{For Gaussian RVs, linear and non-linear MS estimators are identical (\#264)}.
\item \textit{The orthogonality principle}: The error between of linear estimator $\mb{\hat{y}}=A\mb{x}+B$ of $\mb{y}$ is orthogonal to the data: $E\{(\mb{y}-\mb{\hat{y}})\mb{x}\}=0$
%	\begin{itemize}
	\item Generalizes to the linear estimate of $\mb{s}$ \ito multi RVs $\mb{x}_1, \hdots, \mb{x}_n$: $E\{(\mb{s}-\mb{\hat{s}})\mb{x}_i\}$ for $i=1,\hdots,n$.
	\item Generalizes to \textit{non-linear} estimation: $E\{[\mb{s}-g(\mb{X})]w(\mb{X})\}$, where $g(\mb{X})$ is the non-linear MS estimator and $w(\mb{X})$ \textit{any} function of data $\mb{X}=[\mb{x}_1,\hdots,\mb{x}_n]$. (\#269)
%	\end{itemize}
\item Computing the linear MS estimator is \textit{much easier} if the RVs $\mb{x}_i$ are orthogonal to one another (\ie $R_{ij}=0$ for $i\neq j$).  That is why it is often we perform \textit{whitening} (see \#271-272).
\item Convergence modes for a random sequence (RS) $\mb{x}_1, \hdots, \mb{x}_n, \hdots$
\begin{itemize}
\item Everywhere (e), almost everywhere (a.e.), in the MS sense (MS), in prob (p), in distro (d).
\item Ordered in relaxedness (except last two): d $>$ p $>$ (a.e. $|$ MS)
\item Cauchy criterion (CC): We typically think of convergence as converging into a sequence $x$. With CC, we can eliminate this need. That is, we ask that $|x_{n+m}-x_n|\to	0$ as $n\to	\infty$.
\end{itemize}
\item All the below are technically applications of RS convergence:
	\begin{itemize}
	\item The law of large numbers: If $p$ is the prob of event $A$ in a single experiment and $k$ is number of successes in $n$ trials, then we can show that $p$ tends to $k/n$ in \textit{probability}.
	\item Strong law of large numbers: We can even show that $p$ tends to $k/n$ almost everywhere (but proof is more complicated).
	\end{itemize}
	\item The ones below are more specifically applications for estimating $E\{\mb{\bar{x}}_n\}$ (and optionally $\bar{\sigma}^2_n$): the sample mean of a RS of $n$ RVs $\mb{\bar{x}}_n=\frac{\mb{x}_1+\hdots+\mb{x}_n}{n}$ (the variance of the sample mean)
	\begin{itemize}
	\item Markov's thm: If the RVs $\mb{x}_i$ are s.t. the mean of $\bar{\eta}_n$ of $\bar{\mb{x}}_n$ tends to a limit $\eta$ and its variance $\bar{\sigma}_n$ tends to $0$ as $n\to \infty$, then $E\{(\mb{\bar{x}}_n-\eta)^2\}\to 0$ as $n\to \infty$. Convergence in MS sense.\\
	(Note that $\mb{x}_i$ do not have to be uncorrelated or independent)
	\item Corollary: if $\mb{x}_i$ are uncorrelated and $\frac{\sigma_1^2+\hdots+\sigma_n^2}{n}\to 0$ as $n\to \infty$, then $\mb{\bar{x}}_n\to \eta=\lim_{n\to\infty} \frac{1}{n}\sum\limits_{i=1}^n E\{\mb{x}_n\}$ as $n\to \infty$. Convergence in MS sense. \\
	(Note that now we do require uncorrelated RVs but in exchange, compared to Markov's thm, the condition on the mean is removed and we \textit{do} have a way of computing the mean $\eta$.)
	\item Khinchin's thm: the above two required us to know \textit{something} about the variance. According to Khinchin, if $\mb{x}_i$ are \iid (stricter condition), then we $\mb{\bar{x}}_n$ tends to $\eta$ even if we know nothing about the variance of $\mb{x}_i$'s. However, now we have convergence in probability only.
	\end{itemize}
\item The Central Limit Theorem (CLT) is also application of RS conv.\----conv. in \textit{distribution}:
	\begin{itemize}
	\item Given $n$ independent RVs $\mb{x}_i$, we form their sum (not sample mean!) $\mb{x}=\mb{x}_1+ \hdots+ \mb{x}_n$. This is an RV with mean $\eta$ and $\sigma$. CLT states that the distro $F(x)$ of $\mb{x}$ approaches a \textit{normal distro} with the same mean and variance: $F(x)\approx G(\frac{x-\eta}{\sigma})$.
	\item If the RVs are continuous, then $f(x)$, the \textit{density} of $x$, also approaches a normal density.
	\item The approximations become \textit{exact} asymptotically (\ie as $n\to \infty$).  
	\item Good $n$ values: if $\mb{x}_i$ are \iid, then $n=30$ is adequate for most applications. If the functions $f_{i}(x)$ are smooth, even $n=5$ can be enough.
	\item Error corr: The errors of CLT can be corrected by matching higher order moments using Hermite polynomials (see\#281 and also Section~\ref{sec:error_correction}).
	\item CLT for products $\mb{y}=\mb{x}_1\mb{x}_2\hdots\mb{x}_n$ (assuming $\mb{x}_i>0$) is \textit{lognormal}. (Easily seen by letting $\mb{z} = \ln{\mb{y}} = \ln \mb{x}_1+\hdots+\ln\mb{x}_n$ and $\mb{y} = e^{\mb{z}}$.
	\end{itemize}
\item Sampling random sequences for a target distribution $F(x)$
\item The general idea is to generate a uniform RS $u_1, \hdots, u_n$, and then use some method to convert it to a RS $x_1, \hdots, x_{n'}$ (where $n' \leq n$) that matches the distribution of $F(x)$.
\item There are several methods for sampling
	\begin{itemize}
	\item Percentile Transformation Method: This is a straightforward method ($x_i=F^{-1}_x(u_i)$) but we can apply it only if we can compute the inverse of the target distro, $F^{-1}_x(u)$. We can use this also to transform a sequence $y_i$ with a non-uniform distribution: $x_i=F_x^{-1}\left(F_y(y_i)\right)$ (see \#292).
	\item Rejection method: Allows sampling without $F^{-1}_x$. The idea is to find an event $M$ such that $f_x(x|M) = f_y(x)$. Turns out that this event is $M=\{\mb{u}\leq r(\mb{x})\}$ where $r(\mb{x}) = a f_y(x)/f_x(x)$. To implement this, one needs to find the upper bound (i.e. $a$; see Example 7-23). 
	\item Mixing method: $f_x(x) = p_1 f_1(x)+\hdots+p_m f_m(x)$. See proof in \#294 for explanation. 
	\item Some transformations:
		\begin{itemize}
		\item Binomial RV: Sum of $m$ \iid Bernoulli's equals Binomial (\#294).
		\item Erlang: Sum of $m$ \iid exponentials equals Erlang
		\item Chi-square: Sum an appropriate Erlang and a Normal
		\item Student-t: Use a normal and a chi-square
		\item Log-normal: Use a normal
		\end{itemize}
	\item Generating Normal RSs
		\begin{itemize}
		\item Clearly, we can use CLT and sum some uniform RSs
		\item Rejection and mixing: We can mix two Truncated normal RVs obtained via rejection
		\item Polar coordinates: Use a Rayleigh and a uniform to obtain two independent normals
		\item Use two uniforms to obtain two normals (see Example 6-15 to follow the equation right after 7-172)
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{Interesting identities/lemmas/theorems}

\begin{itemize}
\item The unbiased linear estimator with minimum variance is the one shown in (7-17).
\item Theorem 7.1: The correlation matrix is nonnegative definite
\item Sum of jointly normal RVs is normal
\item Sample mean and variance of $n$ RVs $\mb{x}_1, \hdots, \mb{x}_n$ are $\mb{\bar{x}} = \frac{1}{n}\sum\limits_{i=1}^n$ and $\bar{\mb{v}} = \frac{1}{n-1}\sum\limits_{i=1}^n(\mb{x}-\bar{\mb{x}})^2$.
\item Goodman's theorem: The statistics of a real zero-mean $n$-dimensional normal RV are completely determined i.t.o. the $n^2$ parameters\----the elements of its covariance matrix. However, a similar \textit{complex} RV requires $2n^2+n$ elements. Goodman's theorem (\#259) gives a special class of normal complex RVs that are determined completely with $n^2$ parameters only.
\item Berry-Esseen Theorem: Gives an upper bound for the approx. error of CLT, given that the third order of the unknown distro is finite and the variables $\mb{x}_i$ are \iid
\end{itemize}

\section{Important concepts}

\begin{itemize}
\item Group independence
\item Correlation and covariance matrices
\item The orthogonality principle
\item Mean square estimation (Linear and non-linear)
\end{itemize}

\section{Some terminology}
\begin{itemize}
\item Homogeneous linear estimation:  Linear estimation without a bias term
\item Nonhomogeneous linear estimation: Linear estimation with a bias term

\end{itemize}
\section{Redo in future}
\begin{itemize}
\item Poisson
\item Chi square 
\item Show why sample variance is divided by $n-1$.
\item Prove why the generalized orthogonality principle (7-92) leads to optimal MS estimators (linear or non-linear).
\item Order convergence modes
\item Prove the law of large numbers
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\chapter{Statistics}

\section{Definitions}
\begin{itemize}
\item \textit{Statistic.} Any function of the sample RV vector, $\mb{X}=[\mb{x}_1,\hdots,\mb{x}_n]$, is called a \textit{statistic}. (This definition applies only for this chapter! See footnote in p306)
\item \textit{Unbiased estimator.} An estimator $T(\mb{X})$ is an unbiased estimator for $\theta$ if $E\{T(\mb{X})\}=\theta$.
\item \textit{Point estimator} is an RV $\bs{\hat{\theta}}=g(\mb{X})$  that estimates a parameter $\theta$.
\item \textit{Point estimate} $\hat{\theta}$ is a function of the \textit{observation vector} $X=[x_1, \hdots, x_n]$ and the corresponding RV; \ie $\hat{\theta}=g(X)$
\item \textit{Consistent estimator.} Let $\bs{\hat{\theta}}_n=g_n(\mb{X})$ be estimator of parameter $\theta$ based on a sample $\mb{X}$ with $n$ observations. $\bs{\hat{\theta}}$ is a consistent estimator if its error, $E\{(\bs{\hat{\theta}}_n-\theta)^2\}$, tends to zero as $n\to \infty$.
\item \textit{Best estimator.} Let $\bs{\hat{\theta}}=g_n(\mb{X})$ be estimator of parameter $\theta$. $\bs{\hat{\theta}}$ is best estimator if it minimizes the error $E\{(\bs{\hat{\theta}}_n-\theta)^2\}$.
\item \textit{Sufficient statistic.} A function $T(\mb{X})$ is a sufficient statistic for a parameter $\theta$, if $T(\mb{X})$ contains all information about $\theta$ in the data set $\mb{X}$. That is, given the PDF $P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta\}$, if $P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta | T(\mb{X}\}$ \textit{does not} depend on $\theta$, then $T(\mb{X})$ is sufficient statistic for $\theta$. (p322-323)
\item \textit{Efficient estimator.} An estimator is called efficient if it has the lowest possible variance. More specifically, we have first-order efficiency and second-order efficiency. The former means that an estimator is efficient w.r.t. the Cramer-Rao bound (see below), because the CR bound uses only first-order derivatives; the latter means that an estimator is efficient w.r.t. Bhattacharya bound, which is computed using the second-order derivative.
\end{itemize}


\section{Parameter Estimation}
Very important section with concepts directly applicable to machine learning. Specifically, it is shown why the Maximum Likelihood (ML) is so important, and from which mathematical framework it comes from. How can one obtain an unbiased estimator \textit{with minimum variance}, or how one can measure the variance of a given unbiased estimator. It looks like, if one wants to use non-Gaussian RVs for machine learning, the tools presented in this section are fundamental to show how to find their parameters. 

The concepts of Sufficient Statistic and Uniformly Minimum Variance Unbiased Estimator (UMVUE) are introduced. The latter is a desirable and most used estimator type, and the former leads to finding it. The most important theorems along the way are: \textit{Factorization theorem} (helps finding sufficient statistic), the \textit{Cramer-Rao Bound} (finds lower bound for variance of unbiased estimator), the \textit{Bhattacharya Bound} (improves Cramer-Rao), and the \textit{Rao-Blackwell theorem} (the finale: finds the UMVUE from sufficient statistic).

\begin{figure}[!h]
\begin{center}
\begin{pspicture}[shift=*](-2,-0.5)(6,2)
\psmatrix[colsep=0.7cm, rowsep=0.4cm]
 &  & [mnode=r,name=AUE,colsep=2cm] \small \stc{\textit{Any} unbiased}{Estimator} &\\
[mnode=oval,name=LF]  {\stc{Likelihood}{Function}} & [name=FT]\psframebox{\stc{Factorization}{Theorem}} & [name=RBT,colsep=2cm]\psframebox{\stc{Rao-Blackwell}{Theorem}} & [mnode=oval,name=UMVUE] UMVUE
\endpsmatrix
\psset{arrows=->}
\ncline{LF}{FT}
\ncline{FT}{RBT} \taput{\rnode{C}{\small \stc{Sufficient}{Statistic}}}
\ncline{RBT}{UMVUE}
\ncline{AUE}{RBT}
\end{pspicture}
\end{center}
\caption{The relationship between the major concepts and theorems of this section (parameter estimation).}
\end{figure}


\subsection{Notes}


\begin{itemize}
\item \textbf{The Factorization Theorem} allows us to find suff. statistic (see also Examples 8-14, 8-15). Let $\mb{x}_1,\hdots,\mb{x}_n$ be a set of discrete RVs with PMF $P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta \}$. Then, $T(\mb{x}_1, \hdots, \mb{x}_n)$ is sufficient for $\theta$ iff 
\begin{equation}
P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta | T(\mb{X})\}=h(x_1,\dots,x_n)g_\theta(T\mb{x}=t)
\end{equation}	
where $h(\cdot)$ is a nonnegative function of $\mb{x}_1, \hdots, \mb{x}_n$ that does not depend on $\theta$, and $g(\cdot)$ is a nonnegative function of $\theta$ and $T$ only (and \textit{not} of $\mb{x}_1, \hdots, \mb{x}_n$ in any other manner.)
\item \textbf{Cramer-Rao (CR) Lower Bound} gives us the lower bound for the variance of an estimator $T(\mb{x})$. Let $T(\mb{x})$ be any unbiased estimator for $\theta$ under the joint PDF $f(\mb{x}_1=x_1,\hdots,\mb{x}_n; \theta)$, denoted by $f(\mb{x};\theta)$. Then,
\begin{equation}
\text{Var}(T\{\mb{x}\}) \ge \frac{1}{E\left\{ \left( \frac{\partial}{\partial \theta}\log f(\mb{x}; \theta) \right)^2 \right\}} = -\frac{1}{E\left\{ \frac{\partial^2}{\partial \theta^2}\log f(\mb{x}; \theta)  \right\}} := \sigma_{\text{CR}}^2
\end{equation}
provided that the following regularity conditions are satisfied
\begin{align}
\frac{\partial}{\partial \theta} \int f(\mb{x}; \theta) dx = \int \frac{\partial f(\mb{x};\theta)}{\partial \theta} dx = 0 \\
\frac{\partial}{\partial \theta} \int T(\mb{x}) f(\mb{x};\theta)dx = \int T(\mb{x}) \frac{\partial f(\mb{x;\theta})}{\partial \theta} dx.
\end{align}
If $\mb{x}_1, \hdots, \mb{x}_n$ are independent, then 
\begin{equation}
\sigma_{\text{CR}}^2 = \frac{1}{n E \left\{  \left( \frac{\partial \log f(\mb{x}_i; \theta) }{\partial \theta} \right)^2   \right\}    } = \frac{1}{n E \left\{  \frac{\partial^2 \log f(\mb{x}_i; \theta) }{\partial \theta^2}    \right\}    }
\end{equation}
\item \textbf{Cramer-Rao for Multiparameter Case}. Let $\underline{\theta} = (\theta_1, \hdots, \theta_m)$ be the parameter vector, and the corresponding estimator $\underline{T}(\mb{x})=[T_1(\mb{x}), \hdots, T_m(\mb{x})]$. Then, it holds\footnote{$A\ge B$ in the sense that $A-B$ is a nonnegative-definite matrix} $\text{Cov}\{\underline{T}(\mb{x})\}\ge J^{-1}(\underline{\theta})$, where $J(\underline{\theta})$ represents the $m\times m$ Fisher matrix; i.e., 
\begin{equation}
J_{ij} = E\left\{ \frac{\partial \log f(\mb{x}; \underline{\theta})}{\partial \theta_i}\frac{\partial \log f(\mb{x}; \underline{\theta})}{\partial \theta_j} \right\}
\end{equation}
The regularity conditions are similar\----the $\partial \theta$ is replaced with $\partial \theta_i$.
\item \textbf{First-order efficient}. An estimator whose variance is equal to $\sigma_{\text{CR}}^2$ is called \textit{first-order efficient} as the Cramer-Rao bound makes use of the first-order derivative.
\item The pdf of an efficient estimator is from the \textit{exponential family}.  (p330)
\item \textbf{Improvement on CR Bound: The Bhattacharya Bound}: The Bhattacharya Bound makes use of second order derivative. An estimator that attains this bound is called \textbf{second-order efficient}.
\item \textbf{UMVUE}. The unbiased estimator for a parameter $\theta$ that has the lowest possible variance is called uniformly minimum variance unbiased estimator (UMVUE); the "the" in the former sentence is justified as the UMVUE is unique (p334). UMVUEs depend only on the sufficient statistic, and no other information about $\theta$ contained in the data set $\mb{X}$. 
\item The \textbf{Maximum-Likelihood Estimator (MLE)} $\bs{\hat{\theta}}_{ML}$ has very interesting properties, particularly for large $n$ (p354).
	\begin{itemize}
	\item If an efficient estimator exists, then \textit{it is the MLE} (p330). 
	\item If MLE exists, then it is only a function of the sufficient statistic (see 8-283 and 8-284).
	\item Assume that $\psi(\theta)$ is an unknown parameter depending on $\theta$. Then, the MLE of $\psi$ is given by $\psi(\bs{\hat{\theta}}_{ML}$).
	\item If MLE is the unique solution to the likelihood equation, then under some additional restrictions and regularity conditions (see p354), we also have as $n\to \infty$: (i) $\bs{\hat{\theta}}_{ML}$ tends to a normal RV (ii) $\text{Var}{\bs{\hat{\theta}}} \to \sigma^2_{\text{CR}}$ (iii) $E(\bs{\hat{\theta}}_{ML}) \to \theta$; therefore $\frac{\bs{\hat{\theta}}_{ML}-\theta}{\sigma^2_{\text{CR}}}\to N(0,1)$. 
	\end{itemize}
\end{itemize}


\section{Confidence Interval Estimation}
This section (p.307-317) is more like for evaluating the reliability of a particular parameter choice. E.g., what is the probability that the mean $\mu$ is within a certain interval? Or what is the interval $(c_1, c_2)$ that contains $\mu$ with 95\% confidence? Therefore I find it less applicable to signal processing and machine learning applications, but more appropriate like for testing hypotheses etc. In short: it's less interesting for me.

\begin{itemize}
\item General property: The minimum-length CI is such that $f(c_1)=f(c_2)$ (see p305 and Prob. 8-6), but finding this may not be easy (p314) therefore we may settle for a larger interval.
\item \textbf{Mean}, $\eta$. The goal is estimating confidence interval (CI) for the mean $\eta$ of $\mb{x}$ from \textit{noisy} observations $x_1, ..., x_n$ drawn from RVs $\mb{x}_1, ..., \mb{x}_n$. The overall strategy is based on using the sample mean $\mb{\bar{x}}$ to find an \textit{interval estimator} which is a pair of RVs which are functions of $\mb{\bar{x}}$, $\left(g_1(\mb{\bar{x}}), g_2(\mb{\bar{x}})\right)$, s.t. $P\{g_1(\mb{\bar{x}}) < \eta < g_2(\mb{\bar{x}})\}=\gamma$, where $\gamma$ is \textit{confidence coefficient}. We get the \textit{interval estimation} by using the computed sample mean $\bar{x}$: $\left(g_1(\bar{x}), g_2(\bar{x})\right)$. This task is simplified when we assume the sample mean $\mb{\bar{x}}$ is Normal (which is true if $\mb{x}$ is Normal or approx true if $n$ is large).
	\begin{itemize}
	\item \textbf{Known Variance}, $\sigma^2$. $\mb{\bar{x}}$ is $\sim N(\eta,\sigma/\sqrt{n})$ in this case, therefore say that $\eta$ is in the range $\bar{x}\pm z_{1-\delta/2} \sigma/\sqrt{n}$  with confidence $\gamma$, where $\delta = 1-\gamma$ and $z_u$ is the standard Normal distro's $u$-percentile.
	\item \textbf{Unknown Variance}. We now use the (unbiased) sample variance estimator, $\mb{s}^2=\frac{1}{n-1}\sum_{i=1}^{n}\left(\mb{x}_i-\mb{\bar{x}}\right)^2$. We can be lazy and compute CI using the formula just above with $s$, \ie $\bar{x}\pm z_{1-\delta/2} s/\sqrt{n}$ (which is OK for large $n$). Or, we can do things more properly. The RV $\frac{\mb{\bar{x}}-\eta}{\mb{s}/\sqrt{n}}$ has Student $t$ distro with $n-1$ DoF. Denoting by $t_u$ its percentiles, the CI that we are after is $\bar{x}\pm t_{1-\delta/2} \sigma/\sqrt{n}$.
	\item \textbf{Nothing is known}. We use the elegant Tchebycheff Inequality, and our CI is $\bar{x}\pm \sigma/\sqrt{n\delta}$. (I assume that $\sigma$ is sample variance? Not clear from book).
	\end{itemize}
\item \textbf{Zero-one event probability}, $P(A)$. A zero-one event (e.g. coin tossing or Republicans winning) is $\sim\text{Bernoulli}(p)$ and we aim to find CI for $p$. Since the mean of Bernoulli is $p$, we are effectively trying to estimate the mean of the distro, therefore we can use the stuff listed above. Specifically, for large $n$, our Bernoulli is approximated by $N(p, \sqrt{pq/n})$, therefore our CI is $\bar{x}\pm z_{1-\delta/2} \sqrt{pq}/\sqrt{n}$ with $p=\bar{x}$ and $q=1-p$.
\item \textbf{Variance}, $\sigma^2$.
	\begin{itemize}
	\item \textbf{Known mean}, $\eta$. The RV $\mb{\hat{v}}=\frac{1}{n}\sum_{i=1}^n(\mb{x}_i-\eta)^2$ is an unbiased and consistent estimator of variance (p313), and $n\mb{\hat{v}}/\sigma^2$ is $\sim\sim\chi^2(n)$. We can then either build a CI by excluding equal tail prob. masses of $1-\delta/2$, \ie $\frac{n\hat{v}}{\chi^2_{1-\delta/2}(n)}<\sigma^2<\frac{n\hat{v}}{\chi^2_{1-\delta/2}(n)}$. This interval \textit{does not} have minimum length; but the determination of the latter is not simple (p313-314).
	\item \textbf{Unknown mean}. Very similar to the case above (p314). We use $\sigma^2$'s point estimator, $\mb{s}^2$, which is $\sim\chi^2(n-1)$. The CI is $\frac{(n-1)\hat{s}^2}{\chi^2_{1-\delta/2}(n-1)}<\sigma^2<\frac{(n-1)\hat{s}^2}{\chi^2_{\delta/2}(n-1)}$.
	\end{itemize}
	\item See p315-317 for estimating \textbf{Percentiles} (\ie, $x_u$ s.t. $F(x_u)=u$), \textbf{Distributions} ($\hat{F}(x)$) and interval estimates for a specific $x$ value of Distributions. 
\end{itemize}




\section{Hypothesis Testing}
Let $H_0$ be the null hypothesis that a parameter $\theta$ of a distribution equals $\theta_0$, \ie $H_0 : \theta = \theta_0$. The alternative hypothesis can be $H_1:\theta \neq \theta_0$, or (more specifically) $H_1:\theta > \theta_0$ or $H_1:\theta < \theta_0$. Hypothesis testing aims to establish whether there is sufficient evidence to reject $H_0$. The idea is to compute some quantity from the sample and test whether this quantity falls into the critical region, $D_c$. 

Testing is based on the allowed errors; specifically, the probability \textit{masses} $\alpha, \beta(\theta)$ (defs below) 
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c|} \hline
				& $H_0$ is really true 			& $H_0$ is really false \\ \hline	
We reject $H_0$	& Type I error (prob: $\alpha = P\{ \mb{X} \in D_c | H_0 \}$) & ;)) \\\hline
We accept $H_0$	& ;) & Type II error (prob: $\beta(\theta) = P\{ \mb{X} \notin D_c | H_1 \}$)  \\\hline
\end{tabular} 
\end{table}

\noindent \textbf{Overall strategy} to hypothesis testing is: 
\begin{enumerate}
\setlength\itemsep{0.05em}
\item Select a test statistic $\mb{q} = g(\mb{X})$ (e.g. )
\item Find the critical region $R_c$ where the density of $\mb{q}$ is negligible according to an $\alpha$ value.
\item Compute the value $q$ using the observation vector $X=[x_1,x_2,\hdots,x_n]$
\item Reject $H_0$ if $q \in R_c$ 
\end{enumerate}

\noindent \textbf{Example}. We want to test if the mean of $\mb{x}$ is $\eta_0$. The sample mean is $\mb{\bar{x}}$ and, under familiar assumptions, $\mb{\bar{x}}\sim N(\eta, \sigma/\sqrt{n})$
\begin{enumerate}
\setlength\itemsep{0.05em}
\item Select $\mb{q}$ as the standard (under $H_0$ assumption) NV: $\mb{q}=\frac{\mb{\bar{x}}-\eta_0}{\sigma/\sqrt{n}}$. Note that $\mb{q}\sim N(\eta_q, 1)$ where $\eta_q=\frac{\eta-\eta_0}{\sigma/\sqrt{n}}$
\item We set $\alpha=0.05$, in which case the critical region for $R_c = \{q : z_{0.05} < q < z_{1-0.05}\}$
\item We compute $q(X)$ using the observation vector $X$
\item We reject $H_0:\eta=\eta_0$ iff $q\in R_c$
\end{enumerate}

\noindent The general formula above works across a variety of scenarios and tests. The example was for testing mean but the same strategy works for testing variance, (event) probability, distribution fit



\begin{figure}[h!]
\SpecialCoor
\def\edgeCyan#1#2{\ncdiagg[angleA=180, arm=0pt,nodesep=2pt, linecolor=cyan]{#2}{#1}}
\def\edgeBlue#1#2{\ncdiagg[angleA=180, arm=0pt,nodesep=2pt, linecolor=cyan]{#2}{#1}}
\psset{nodesep=5pt}
\pstree[treemode=R,levelsep=*1cm]{\Tr{Hypothesis Testing}}{
\pstree[treemode=R,levelsep=*1cm]{\Tr{Mean\rnode{A}{}}}{%
	\Tr{Known Variance (Gaussian RV)}
	\Tr{Unknown Variance (Student $t$ RV)}
}
\pstree[treemode=R,levelsep=*1cm]{\Tr{Variance\rnode{B}{}}}{%
	\Tr{Known Mean ($\chi^2(n)$ RV)}
	\Tr{Unknown Mean ($\chi^2(n-1)$ RV)}
}
\pstree[treemode=R,levelsep=*1.2cm]{\Tr{Discrete variables\rnode{C}{}}}{%
	\Tr{Binary probability (Gaussian for large $n$)}
	\Tr{Multiple($m$)-event probability ($\chi^2(m-1)$)}
	\Tr{Test of statistical independence (also $\chi^2$\----p362)}
}
\pstree[treemode=R,levelsep=*1.2cm]{\Tr{Distribution $F(x)$\rnode{C}{}}}{%
	\Tr{Entire  $F(x)$ (Kolmogorov-Smirnov test p361)}
	\Tr{For specific $x$'s ($\chi^2$\----p363)}
}}
\caption{Summary (p358-364) of which tests are appropriate for the test of various quantities}
\end{figure}





















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter*{Towards a motivation guide}
\section{Why transformations are useful}
\begin{itemize}
\item By applying an orthonormal transformation (\aka whitening) to a set of RVs we can easily compute the optimal 
\item How to optimally estimate: We do all our calculations in the space of the RVs; there we find an optimal (in some sense) estimators that are based on the hypothetical observation RVs (typically iid), and once we have the estimator, we apply our observations to this estimator (\ie replace the RVs $\mb{x}_i$'s with the observed $x_i$'s). Doing the estimation in the space of RVs gives us some theoretical guarantees under certain assumptions.
\end{itemize}

\chapter*{Towards a cheatsheet}
\section{Interesting RV Transformations}
\begin{itemize}
\item If $\mb{x}_1, \hdots, \mb{x}_n$ are jointly normal, then $\mb{z} = \mb{x}_1 + \hdots \mb{x}_n$ is also normal (\#257)
\item If $\mb{x}_1, \hdots, \mb{x}_n$ are independent and each $\mb{x}_i$ is $N(0,1)$, then $\mb{z} = \mb{x}_1 + \hdots \mb{x}_n$ is $\chi^2(n)$ (\#259)
\item If $\mb{x}$ is $\chi^2(n)$ and $\mb{y}$ is $\chi^2(m)$ and $\mb{x,y}$ independent, then $\mb{z} = \mb{x+y}$ is $\chi^2(n+m)$ (\#260)
\end{itemize}

\section{Useful Identities}
\begin{itemize}
\item $\text{Var}(\mb{x}) = E\{\text{Var}(\mb{x|y})\} + \text{Var}(E\{\mb{x|y}\})$ (p337)
\end{itemize}



\chapter*{Sketches to solutions}
\section{Solutions to equations in the book}

\subsection{Error correction for CLT}
\label{sec:error_correction}
It is really not obvious how the error correction for CLT is arrived at in \#281. The general idea is to match the higher order moments of the approximation with that of the data. The approximation error between the (unknown) density $f(x)$ and the approximated normal density $f_n(x; 0, \sigma)$ is:
%
\begin{equation}
\epsilon(x) = f(x)-f_n(x; 0, \sigma).
\end{equation}
%
The idea is to write the error (and thus $f(x)$) \ito an orthogonal set of polynomials, viz. Hermite Polynomials $H_k(x)$ (see 7-126). Since they are orthogonal, they form an orthogonal set on the real line:
%
\begin{equation}
\int\limits_{-\infty}^\infty e^{-x^2/2} H_n(x) H_m(x) dx = \begin{cases}n!\sqrt{2\pi}&n=m\\0&n\neq m\end{cases}
\label{eq:h_orth}
\end{equation}
%
Using those polynomials, one can approximate any function (including $\epsilon(x)$ or $f(x)$) \ito an infinite series:
%
\begin{equation}
\epsilon(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-x^2/2\sigma^2}\sum\limits_{k=3}^\infty C_k H_k\left(\frac{x}{\sigma}\right)
\end{equation}
%
The sum starts from $3$, as the moments of $\epsilon(x)$ up to order 2 are zero (I guess because the Gaussian approx. is sufficient to make those moments zero?) The book limits itself up to $4th$ order approx.

The idea is to find the coefficients $C_3$ and $C_4$ by matching the $3^{rd}$ and $4^{th}$ order moments of the unknown distro with that of the data (*** Be more pecise here). The third order moment of $\mb{x}$ is:
%
\begin{equation}
E_{f(x)}\{x^3\} = \int x^3 f(x) dx
\end{equation}
%
The third order moment of the (approximated) normal density is zero (see 5-73), and therefore the error in terms of the third moments is:
\begin{equation}
E_{f(x)}\{x^3\} - E_{f_n(x)}\{x^3\} = \int x^3 f(x) dx - \int x^3 f_n(x; 0, \sigma) dx = \int x^3\left[f(x)-f_n(x; 0, \sigma)\right] dx.
\end{equation}
Since the content of the last brackets equals $\epsilon(x)$, and since $ E_{f_n(x)}\{x^3\}$ is zero, the above equals $m_3 = E_{f(x)}\{x^3\}$ identity can be written as:
%
\begin{equation}
m_3 = \frac{1}{\sigma \sqrt{2\pi}} \int x^3 e^{-x^2/2\sigma^2}\sum\limits_{k=3}^\infty C_k H_k\left(\frac{x}{\sigma}\right) dx \label{eq:mom3}
\end{equation}
%
Now here is the tricky part that is not obvious in the book. Our goal is to find the coefficients $C_k$ using the above equality. This is achieved by using the orthogonality of the Hermite polynomials. The third order Hermite Polynomial is $H_3(x) = x^3-3x$, and using \eqref{eq:h_orth} and doing a change of variables we see that:
%
\begin{align}
&\int e^{-x^2/2\sigma^2} \left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx = \\
&=\int e^{-x^2/2\sigma^2} \frac{x^3}{\sigma^3}\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx -3\int e^{-x^2/2\sigma^2} \frac{x}{\sigma}\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx \label{eq:int2}\\
&=3!\sigma \sqrt{2\pi}
\end{align}
%
The second integral in \eqref{eq:int2} equals zero because $H_1(x)\propto x$ and $H_3(x)\perp H_1(x)$, and the first integral in \eqref{eq:int2} is proportional to $m_3$. More specifically, using \eqref{eq:int2} and \eqref{eq:mom3}, we arrive at the obscure derivation in the book:
%
\begin{equation}
m_3 = 3! \sigma^3 C_3.
\end{equation}






\bibliographystyle{IEEEtran}
\bibliography{../bibliography}


\end{document}
