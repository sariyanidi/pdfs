\documentclass[a4paper, oneside]{book}

\input{../preliminaries}
\usepackage{pstricks, pst-node, pst-tree}
\begin{document}

\chapter*{Introduction}

My notes from the legendary book of Papoulis and Pillai \cite{papoulis02}. The book assumes that the reader does already have fairly strong background in calculus and linear algebra. I used two books \cite{Gradshteyn80, Abramowitz70} as assistant for evaluating some of the mathematical identities or integrals. 

This book made me realize that a good technical book is not necessarily one that's easy to follow -- one that shows you very easily how each identity or result is arrived at. Some of the results in the book are presented without much explanation, and this required me to show a lot of effort to understand them; this effort seems to lead to a staying power. 


\chapter{The meaning of probability}
\begin{center}
\textit{Probabilistic interpretations are [...] necessary because of our ignorance.}
\end{center}

\section{Notations and basic definitions}
\begin{itemize}
\item The \textit{certain event} $S$ is the event that occurs in every trial.
\item The union of two events $A,B$ is denoted with $A\cup B $ or $A+B$.
\item The intersection of two events $A,B$ is denoted with $A \cap B = AB$. 
\item Two sets $A,B$ are \textit{mutually exclusive} if they have no common elements, \ie $AB = \emptyset$.
\item \textit{Partition\index{Partition}} is a collection mutually exclusive subsets $A_1,\hdots,A_n$ whose union equals $S$, \ie $\bigcup\limits_{i=1}^n A_i = S$.
\end{itemize}




























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{The Axioms of Probability}
\section{The Axioms}
We assign to each event $A$ a number $P(A)$, which we call \textit{the probability of the event $A$}. This number is so chosen to satisfy three conditions:
\begin{enumerate}[I.]
\item $P(A)\ge 0$
\item $P(S) = 1$
\item if $AB = \emptyset $, then $P(A\cup B)=P(A)+P(B)$.
\end{enumerate}

\section{Definitions and some basic laws}
\begin{itemize}
\item De Morgan's Law: For two sets $A$ and $B$, $$\overline{A\cup B}=\overline{A}\,\,\overline{B}$$
$$\overline{AB}=\overline{A}\cup\overline{B}$$
\item Two events $A,B$ are \textit{equal with probability 1} (\ie both contain the same elements) iff $P(A)=P(B)=P(AB)$
\item \textit{Events} are subsets of $S$ to which we have assigned probabilities.
\item The class $\mb{F}$ of events is a nonempty class of sets such that if $A,B$ are events, then $A+B$ and $AB$ are also events:
\begin{center}If $A\in \mb{F}$ then $\overline{A}\in\mb{F}$\end{center}
\begin{center}If $A\in \mb{F}$ and $B\in \mb{F}$ then $A+B\in\mb{F}$ \end{center}
\item \textit{Field}. The two properties above give a minimum set of conditions for $\mb{F}$ to be a field. 
\item \textit{\index{Borel fields}Borel fields}. Suppose $A_1, \hdots, A_n, \hdots$ is an infinite sequence of sets in $\mb{F}$. If the union and intersection of these sets also belongs to $\mb{F}$ then $\mb{F}$ is called a Borel field.  
\item \textit{Axiomatic definition of an experiment.} In theory of probability, an experiment is specified in terms of the following concepts:
\begin{enumerate}[1.]
\item The set $S$ of all experimental outcomes
\item The Borel field of all events of $S$.
\item The probabilities of these events.
\end{enumerate}
%
\textbf{Example.} The (fair) die experiment has 6 experimental outcomes. Two of the events are \textit{even, odd}; each of those events is a subset of three experimental outcomes with probability $0.5$.
\item \textit{Conditional probability} of an event $A$ conditioned on another event $M$ is $$P(A|M)=\frac{P(AM)}{P(M)}$$
\item \textbf{Fundamental remark.} Conditional probabilities satisfy the three probability axioms: 
	\begin{enumerate}[I.]
	\item $P(A|M) = \frac{P(AM)}{P(M)}$
	\item $P(S|M)=1$
	\item If $A,B$ are mutually exclusive, then $P(A\cup B | M) = P(A|M)+P(B|M)$
	\end{enumerate}
\item \textit{Total probability theorem}. If $\mb U = [ A_1, \hdots, A_n]$ is a partition of $S$ and $B$ is an arbitrary event, then $$P(B)=P(B|A_1)P(A_1)+\hdots+P(B|A_n)P(A_n)$$.
\item \textit{Bayes' theorem} follows from the theorem above and the fact that $P(BA_i)=P(B|A_i)P(A_i)=P(A_i|B)P(B)$: $$P(A_i|B) = \frac{P(B|A_i)P(A_i)}{P(B|A_1)P(A_1) + \hdots P(B|A_n)P(A_n)}$$
\item \textit{\index{Independence}Independence}. The events $A,B$ are independent if $P(AB) = P(A)P(B)$
\item \textit{\index{Mutual independence}Mutual independence}. The events $A_k$ for $k\in K$ are independent iff $P(\bigcap_{m\in M})=\bigcap_{m\in M}P(A_m)$ for any $M\subset K$.

\textbf{Example}. For the four events $A,B,C,D$ to be independent, we must have:\\%
$P(ABCD)=P(A)P(B)P(C)P(D)$,  \\
$P(ABC)=P(A)P(B)P(C)$,  $P(ABD)=P(A)P(B)P(D)$, \\
$P(ACD)=P(A)P(C)P(D)$,  $P(BCD)=P(B)P(C)P(D)$, \\
$P(AB)=P(A)P(B)$, $P(AC)=P(A)P(C)$, $P(AD)=P(A)P(D)$, \\
$P(BC)=P(B)P(C)$, $P(BD)=P(B)P(D)$, $P(CD)=P(C)P(D)$.

\textbf{Example}. It is possible for events to be pairwise independent but not mutually independent.

\end{itemize}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\chapter{Repeated Trials}
Combining two experiments, such as tossing a coin and rolling a die, requires the definition of a new set $S$ of all experimental outcomes. This set is constructed via Cartesian product:
$$S=S_1\times S_,2$$
where $S_1$ ($S_2$) is set of experimental outcomes the first (second) experiment.

The same can be applied for repeated experiments. If we toss a coin $n$ times, the set of experimental outcomes becomes $$S = S_1\times \hdots \times S_n.$$

\section{Bernoulli Trials}

\begin{itemize}
\item \textit{Permutation} is a distinct arrangement of $n$ objects; \eg \textit{bac} is a permutation of \textit{a,b,c}.

Permutation applies also to $k<n$ objects. For example, \textit{ba} is a permutation of $k=2$ objects in \textit{a,b,c}.

\item The total number of permutations of $n$ objects taken $k$ at a time is $\frac{n!}{(n-k)!}$; \eg, \textit{ab, ac, bc, ba, ca, cb} are the permutations of \textit{a,b,c} for $k=1$.

\item The number above pays attention to distinct orderings; \ie two permutations that have the same objects in different orders count as two.

\item \textit{Combination} does not pay attention to the number of orderings. So the total combinations of $n$ objects taken $k$ at a time is $\binom nk:=\frac{n!}{(n-k)!k!}$. \textit{E.g.,} \textit{ab, ac, bc} are the combinations of \textit{a,b,c} for $k=2$.

\item \textbf{Fundamental theorem. } Let us repeat a binary experiment $n$ times. Then, %
$$p_n(k):=P\{\text{$A$ occurs $k$ times in any order}\}=\binom n k p^k q^{n-k}$$

\item \textbf{Bernoulli's Theorem} is fundamental as it serves as a bridge between the axiomatic and frequency definitions of probability. In other words, it provides justification for the relative frequency interpretation.

Let $A$ be an event whose probability of occurrence in a single trial is $p$. If $k$ denotes the number of occurrences of $A$ in $n$ independent trials, then% 
$$P\left(\left| \frac{k}{n}-p\right|>\epsilon\right)<\frac{pq}{n\epsilon^2}.$$

In other words, the probability that the frequency interpretation is \textit{not} justified can be made arbitrarily small [\ie $P(|k/n-p|>\epsilon)$ for any $\epsilon>0$] provided that the number of trial is above a certain limit. 
  
\end{itemize}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{The Concept of a Random Variable}


\section{Definitions and some basic properties}

\begin{itemize}
\item \textbf{Definition of Random Variable.} An RV $\mb x$ is a process of assigning a \underline{number} $\mb x(\zeta)$ to every outcome $\zeta$. 

\textit{Example 1.} In the coin tossing experiment,  if $\zeta_1 = head $ and $\zeta_2 = tails$, one may construct the RV $\mb x$ such that $\mb x(\zeta_1) = 0$ and $\mb x(\zeta_2) = 1$.

\textit{Example 2.} Suppose that our RV $\mb x$ will measure temperature. Then, one can construct the RV $\mb x$ simply as  $\mb x(\zeta_i) = \zeta_i$ (\eg when $\zeta$ is a continuous quantity such as temperature). In this case, the variable $\zeta$ has a double meaning (see p77): It is the outcome of the experiment and the corresponding value $\mb x(t)$ of the RV $\mb x$.

The construction of RV is important for accurately constructing the intended probability distribution. For example, in the experiment of tossing a coin twice, if we don't care about the order of the outcomes, one can construct the RV $\mb x$ as: $$\mb x(HH) = 0, \,\,\,\mb x(HT)=1, \,\,\,\mb x(TT)=2.$$
But if we do care about the order, then we can construct $\mb x$ as:
$$\mb x(HH) = 0, \,\,\,\mb x(HT)=1, \,\,\,\mb x(TH)=2, \,\,\,\mb x(TT)=3.$$

\item \textbf{Probability distribution function.} The probability $P\{\mb x \le x \}$ of the event $\{\mb x\le x\}$ is function of $x$, called the \textit{(cumulative) distribution function} of the RV $\mb x$. Some properties of distribution functions are:
	\begin{itemize}
	\item $F(+\infty) =1$ and $F(-\infty) = 0$.
	\item It is a nondecreasing function of $x$; \ie $x_1 < x_2 \implies F(x_1) \le F(x_2)$.
	\item If $F(x_0)=0$, then $F(x)=0$ for every $x\le x_0$.
	\item $P\{\mb x>x\} = 1-F(x)$.
	\item The function $F(x)$ is \underline{continuous from the right} $F(x^+)=\lim_{\epsilon\to 0}F(x+\epsilon)=F(x)$
	\item $P\{x_1 < \mb x \le x_2\} = F(x_2) - F(x_1)$.
	\item $P\{\mb{x}=x\} = F(x)-F(x^-)$ where $F(x^-) = \lim_{\epsilon\to 0} F(x-\epsilon)$
	
	\item $P\{x_1 \le \mb x \le x_2\} = F(x_2) - F(x_1)$.
	\end{itemize}
\item \textbf{Probability density function.} The derivative of the probability distribution function $F_x(x)$ is called the probability density function of $\mb x$. Thus $$f_x(x) := \frac{dF_x (x)}{dx}=\lim_{\Delta x \to 0} \frac{F_x(x+\Delta x) - F_x(x)}{\Delta x} \ge 0 .$$

\item \textbf{Conditional Distributions}. The conditional distribution $F(x|M)$ of an RV $\mb x$ assuming $M$ is: 
$$F(x|M)=P\{\mb x \le x |M \} = \frac{P\{\mb x \le x, M\}}{P(M)},$$
where $P\{\mb x \le x, M\}$ is the intersection of the events $\{\mb x \le x\}$ and $M$; that is, the event consisting of all outcomes $\zeta$ such that $\mb x(\zeta)\le x$ and $\zeta \in M$. 
	\begin{itemize}
	\item Suppose that $M = \{\mb x \le a\}$. Then, 
	$$F (x | M) = \begin{cases}1& \text{if}\,\, x\ge a \\
	 \frac{F(x)}{F(a)} & \text{if}\,\, x< a\end{cases}$$
	 
	 Moreover, $$f(x|M) = \begin{cases}\frac{f(x)}{F(a)} & \text{if}\,\, x<a \\ 0 & \text{otherwise}\end{cases}$$
	\item Suppose that $M = \{b < \mb x \le a\}$. In this case,
	$$
	F(x|M) = \begin{cases}
	1  & x>a  \\ 
	\frac{F(x)-F(b)}{F(a)-F(b)} & b\le x < a \\
	0 & x < b
	\end{cases}
	$$
	
	The corresponding Density is: $$f(x|M) = \begin{cases}\frac{f(x)}{F(a)-F(b)} & \text{if}\,\, x<a \\ 0 & \text{otherwise}\end{cases}$$
	\end{itemize}

\end{itemize}


\section{Some Continuous Random Variables}
\begin{itemize}
\item \textbf{Normal (Gaussian) Distribution}, $N(\mu, \sigma^2)$. Its density function is$$f_x(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}e^{-(x-\mu)^2)/2\sigma^2}.$$
Its distribution function is $F_x(x) = \int\limits_{-\infty}^x  f_x(x) := G(\frac{x-\mu}{\sigma})$ where the function $G(x)$%
$$G(x) := \int\limits_{-\infty}^x e^{-y^2/2}dy$$  is often available in tabulated form and is directly related to the $\text{erf}$ function; $\int\limits_{0}^x e^{-y^2/2}dy  = G(x) - \frac{1}{2}.$

\item \textbf{Exponential distribution.} $$f(x) = \begin{cases} \lambda e^{-\lambda x}  & x\ge 0 \\ 0 & \text{otherwise} \end{cases}$$
The exponential distribution has the \textit{memoryless} propert; that is, for $s,t\ge 0$, $P\{\mb x> t+s | \mb x >s\} =P \{\mb x >t\} $. In other words, the time that passed, $s$, doesn't change anything for the exponential distro. Memorylessness is also equivalent to the condition $f(x | \mb x > t) = f(t-t) =  f(0) = \lambda$ (see p229 and Fig. 6-32).
\end{itemize}

\section{Some Discrete Random Variables}
\begin{itemize}
\item \textbf{Binomial distribution.} $\mb y$ is said to be a Binomial RV with parameters $n$ and $p$ if $\mb y$ takes values $0,1,\hdots, n$ with %
$$ P\{\mb y = k\} = \binom nk p^k q^{n-k}\,\,\,\,\,\,\,\,\,\,\, p+q=1\,\,\,\,\,\,\,\,\,\,\,k=0,1,\hdots,n $$
\item \textbf{Poisson distribution} is similar to Binomial; it represents the number of occurrences of a \textit{rare} events in a large number of trials.  $$P\{\mb x = k\} = \lambda^k \frac{e^{-\lambda}}{k!}\,\,\,\,\,\, k=0,1,2,\hdots,\infty$$
\item \textbf{Geometric distribution} represents the probability of first success against the number of trials. $$P\{\mb x = k\} = P(\underbrace{\overline{A}\overline{A}\hdots\overline{A}}_{k-1}A) = P(\overline{A})P(\overline{A})\hdots P(\overline{A}) P(A) = (1-p)^{k-1}p =  pq^{k-1} $$
\item \textbf{Negative binomial distribution} generalizes geometric distribution; it's used to answer the question ``how many trials are needed for $r$'' successes. $$P\{y=k\} = \binom{k-1}{r-1}p^r q^{k-r}.$$
\end{itemize}

\section{Approximations to Binomial Distribution}
Despite its enormous importance, computing the binomial distribution can be involved particularly for large $n$. There are two widespread approximations of the binomial for two different situations.
\begin{itemize}
\item \textbf{The Normal Approximation (DeMoivre-Laplace Theorem)}. This is used when $k$ is in the $\sqrt{npq}$ neighbourhood of $np$; that is, when $k$ is near the peak of the distribution and $p$ is not too close to $0$ or $1$. In such cases; $$\binom nk p^k q^{n-k} \approx \frac{1}{\sqrt{2\pi npq}} e^{-(k-np)^2/2npq}\,\,\,\,\,\,\,\, p+q=1$$
%
and (with error correction \---- see p108) $$P\{k_1\le \mb x \le k_2\} \approx G\left(\frac{k_2+0.5-np}{\sqrt{npq}}\right)-G\left(\frac{k_1-0.5-np}{\sqrt{npq}}\right).$$ 

\textit{Generalization to $m$ events.}
$$\frac{n!}{k_1!\hdots k_{m}!}p_1^{k_1}\hdots p^{k_{m}}_{m} \approx \frac{\exp \{-\frac{1}{2}\left[ \frac{(k_1-np_1)^2}{np_1} +  \hdots \frac{(k_r-np_r)^2}{np_r} \right]\}}{\sqrt{(2\pi n)^{r-1}p_1\hdots p_r}} $$

\item \textbf{The Poisson Approximation.} The Gaussian approximation deteriorates one of the two events happens very rarely; \ie as $p\to 0$ or $p\to 1$. However, there are many events that fall in this category (\eg number of calls on a telephone line, claims in an insurance company etc). In this case, $$\binom nk p^k q^{n-k} \approx e^{-\lambda} \frac{\lambda^k}{k!}$$ and $$P\{k_1\le \mb x \le k_2\} \approx  e^{-np} \sum\limits_{k=k_1}^{k_2} \frac{(np)^k}{k!}$$.

\textit{Generalization to $m+1$ events.} Suppose that $A_1,\hdots,A_{m+1}$ are the $m+1$ events of a partition with $P{A_i}=p_i$. If $np_i\to a_i$ for $i\le m$ (note that the $(m+1)$th event is excluded), then 
$$\frac{n!}{k_1!\hdots k_{m+1}!}p_1^{k_1}\hdots p^{k_{m+1}}_{m+1} \approx \frac{e^{-a_1}a_1^{k_1}}{k_1!} \hdots \frac{e^{-a_m}a_m^{k_m}}{k_m!}$$
\end{itemize}






























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\chapter{Functions of one Random Variable}

This section is interested in computing the statistics of a transformed RV, namely $\mb y = g(\mb x)$ where $\mb x$ is the original RV and $g(\cdot)$ is the transformation function. The key idea is that $$F_y(y) = P\{\mb{y} \le y\} = P\{g(\mb{x}) \le y\} $$
%
\textbf{Fundamental Theorem.} To determine $f_y(y)$ for a \textit{specific} $y$, we solve the equation $y=g(x)$. Denoting its real roots by $x_n$, \ie $y=g(x_1) = g(x_2) = \hdots = g(x_n) = \hdots$, we can compute the pdf of the transformed variable as: %
$$f_y(y) = \frac{f_x(x_1)}{|g'(x_1)|} + \hdots + \frac{f_x(x_n)}{|g'(x_n)|} + \hdots$$
%
The equation above is valid for conditional densities too; all we need to do is replace the densities $f_y(y), f_x(x)$ with the conditional densities $f_y(y|M), f_x(x|M)$.


\section{Mean, variance and other moments}
\begin{itemize}
\item \textbf{Mean (or expected value):} $E\{x\} = \int\limits_{-\infty}^{\infty} x f(x) dx$
	\begin{itemize}
	\item The mean is linear, $E\{a g(x) + b h(x) \}$ = $a E\{g(x)\} + b E\{h(x)\}$.
	\item \textbf{Important property}: $E\{g(\mb x)\} = \int\limits_{-\infty}^\infty g(x) f(x) dx $. In other words, to compute the mean of the transformed variable \textit{we don't need} to obtain the pdf of the transformed RV (p142).
	\item \textbf{Approximating the mean.} If $\mb x$ is concentrated near its mean, then $E\{g(\mb x)\} \approx g(\mu) \int\limits_{\mu-\epsilon}^{\mu+\epsilon} f(x) d x \approx g(\mu)$. The estimate can be improved if $g(x)$ is approximated by a polynomial: $E\{g(\mb x)\} \approx g(\mu) + g''(\mu) \frac{\sigma^2}{2} + \hdots  + g^{(n)}(\mu) \frac{\mu_n}{n!}$

	\end{itemize}
\item \textbf{Variance:} $\sigma_x = E\{(x-\mu_x)^2\} = \int\limits_{-\infty}^{\infty} (x-\mu_x)^2 f(x) dx$
\item \textbf{Moments}
	\begin{itemize}
	\item \textit{Moments}: $\mu_n = E\{\mu^n\}$
	\item \textit{Central moments}: $\mu_n = E\{(x-\mu)^n\}$
	\item \textit{Absolute moments}: $E\{|x-\mu|^n\} \hspace{1truecm} E\{|x|^n\}$
	\item \textit{Generalized moments}: $E\{(x-a)^n\} \hspace{1truecm} E\{|x-a|^n\}$
	\end{itemize}
	
Special case: Normal Random Variables. All the moments of the standard RV can be specified from the mean and the standard deviation. I.e. Normal RVs have no unique information beyond the second order moment!
\end{itemize}


\section{Important inequalities}
\begin{itemize}
\item \textbf{Chebyshev (Tchebycheff) inequality}: For any $\epsilon >0$, $P\{|x-\mu| \ge \epsilon\} = \frac{\sigma^2}{\epsilon^2}$.
\item \textbf{Markov inequality} If $f(x)=0$ for $x<0$, then, for any $\alpha > 0$: $P\{ \mb x \ge \alpha \} \le \frac{\mu}{\alpha}$
\item \textbf{Bienayme inequality}. $P\{ |\mb x - a| \ge \epsilon\} \le \frac{E\{ |\mb x - a|^n \}}{\epsilon^n}$
\item \textbf{Lyapunov inequality}. Let $\beta_k = E\{|\mb x|^k\} < \infty$ be the absolute moments of $\mb x$. Then, for any $k$, $\beta^{1/(k-1)}_{k-1} \le \beta_k^{1/k}$
\end{itemize}


\section{Characteristic Functions, moment generating functions and cumulants}
\begin{itemize}
\item For continuous RVs.
	\begin{itemize}
	\item \textbf{(First) characteristic function.} $\Phi_x(\omega) := E\{e^{j\omega x}\} = \int_{-\infty}^{\infty} f(x) e^{j \omega x}dx$
	
	The characteristic function sometimes simplifies greatly the computation of a transformed RV, mainly thanks to the important property of the mean mentioned also above: %
	 $$\Phi_y(\omega) = E\{e^{j\omega \mb y}\} = E\{e^{j\omega g(\mb x)}\} = \int\limits_{-\infty}^{\infty} e^{j\omega g(x)} f(x) dx.$$ 
	Computation can be simplified because sometimes we compute $\Phi_y(\omega)$ for the transformed RV above and then try to match it with existing characteristing functions (\eg those in Table 5-2, p162). Some examples
	\begin{itemize}
	\item The DeMoivre-Laplace theorem is proven this way (see p156). 
	\item See Examples 5-34 and 5-35.
	\item Computing the PDF of Bernoulli from Binomials (p256)
	\item Computing the PDF of Poisson from Binomials (p256)
	\item The sum of jointly normal RVs is normal (p257)
	\item The sum of the squares of independent normal variables is chi-square (p259)
	\item The sum of two chi-square distros is also chi-square (p260)
\end{itemize}	 
	Characteristic functions can also be useful to ascertain the independence of two RVs (see p216)
	\item \textbf{Inversion} through characteristic function. $f(x) = \frac{1}{2\pi} \int_{-\infty}^\infty e^{-j\omega x} \Phi_x(\omega) d\omega$.
	\item \textbf{Moment generating function.} $\bs \Phi_x(s) = E\{e^{sx}\} = \int_{-\infty}^{\infty} e^{s x} f(x)dx$.
	
	As the name suggests, this function can simplify greatly the computation of the moments, as in the theorem below.
	\item \textbf{Moment theorem.} $\bs \Phi^{(n)}(s) = E\{\mb x^n e^{s \mb x}\}$; hence, $\bs \Phi^{(n)}(s)=E\{\mb x^n\} = m_n$
	\item \textbf{(Second) characteristic function.} $\Psi_x(\omega) = \ln \Phi_x{\omega} = \bs \Psi (j\omega)$
	\item \textbf{Cumulants.} The cumulants $\lambda_n$ of an RV $\mb x$ are by definition $\left.\frac{d^n\Psi(s)}{d s^n}\right|_{s=0}=\lambda_n$
	
	Cumulants and moments contain the same statistical information~\cite{hyvarinen04}, but sometimes it's preferable to work with cumulants as they have two properties not shared by moments \cite{hyvarinen04}: (i) If $\mb {x}$ and $\mb y$ are two independent RVs, then the cumulant of their sum $\mb z = \mb{x+y}$ is equal to the sum of the cumulants of $\mb x$ and $\mb y$. This property generalizes to more than two RVs. (ii) If the distribution of the RV $\mb x$ is multivariate gaussian, ten all its cumulants of order three and higher are identically zero. 
	\end{itemize}	
	
\item For discrete RVs.
	\begin{itemize}
	\item \textbf{(First) Characteristic Function}. $\Phi_x(\omega) = \sum_i p_i e^{i \omega x_i}$.
	
	However, if $\mb x$ takes only integer values, a definition in terms of the $z$ transform, $\bs \Gamma(\mb z) = \sum_{-\infty}^\infty p_n z^n$, is preferable: $\Phi_x(\omega) = \bs{\Gamma}(e^{j\omega}) = \sum_{-\infty}^{\infty} p_n e^{j\omega n} $.
	
	\item \textbf{(Second) Characteristic Function}. $\bs{\Psi}(s) = \ln \bs\Gamma (e^x)$
	\item \textbf{Moment theorem.} $\bs \Gamma^{(k)}(z) = E\{\mb x(\mb x -1)\hdots(\mb x-k+1)z^{\mb x-k}\}$. With $z=1$, we have $\bs \Gamma^{(k)}(1) = E\{\mb x(\mb x -1)\hdots(\mb x-k+1)\}$
	\end{itemize}
\end{itemize}






















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Two Random Variables} 

\section{Definitions and some properties}

\begin{itemize}
\item \textbf{Joint distribution}. $F(x,y) = P\{\mb x \le x, \mb y \le y\}$

Some properties.
	\begin{itemize}
	\item $F(-\infty,y) = 0 \hspace{0.7truecm} F(x,-\infty)=0 \hspace{0.7truecm} F(\infty, \infty)=1$
	\item $P\{x_1 \le \mb x \le x_2, \mb y \le y\} = F(x_2,y) - F(x_1, y)$
	\item $P\{\mb x \le x, y_1 \le \mb y \le y_2\} = F(x,y_2) - F(x, y_1)$
	\item $P\{x_1 \le \mb x \le x_2, y_1 \le \mb y \le y_2\} = F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x1, y_1)$
	\end{itemize}

\item \textbf{Joint density.} $$f(x,y) = \frac{\p^2 F(x,y)}{\p x \p y}.$$ Clearly, $F(x,y) = \int\limits_{-\infty}^x \int\limits_{-\infty}^y f(a,b)dadb$

\item \textbf{Marginal Statistics.}
	\begin{itemize}
	\item $F_x(x) = F(x,\infty) \hspace{0.7truecm} F_y(y) = F(\infty, y)$
	\item $f_x(x) = \int\limits_{-\infty}^{\infty} f(x,y) dy$
	\item $f_y(y) = \int\limits_{-\infty}^{\infty} f(x,y) dx$
	\end{itemize}
\item \textbf{Independence.} If two RVs are statistically independent then $f$
\item \textbf{Important theorem:} If the RVs $\mb x$ and $\mb y$ are independent, then the RVs $\mb z = g(\mb x)$ and $\mb w = h(\mb y)$ are also independent. 
\item \textbf{Joint normality. } $\mb x$ and $\mb y$ are jointly normal, denoted as $N(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, r)$,  if their joint density is:
$$f(x,y) =  A \exp\left\{ -\frac{1}{2(1-r^2)} \left( \frac{(x-\mu_1)^2}{\sigma_1^2} -2 r\frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2}  \right)  \right\}\hspace{0.5cm}\text{where}\hspace{0.5cm}A= \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1-r^2}}$$
\item \textbf{Circular symmetry. } We say that the joint density of two RVs $\mb{x,y}$ is circularly symmetric if it depend only on the distance from the origin $r$, \ie $f(x,y) = g(r)$
\end{itemize}

\section{RV Transformations}
\label{sec:two_rv_transformations}
\begin{itemize}
\item \textbf{One transformation of two RVs}: $\mb z = g(\mb{x,y})$. The book provides between p180-197 the following transformations:
	\begin{itemize}
	\item $\mb z = \mb{x+y}$; $f(z) = \int\limits_{-\infty}^\infty f_{xy}(z-y, y)dy$
	\item $\mb{x-y}$
	\item $\mb{x/y}$
	\item $\mb{xy}$
	\item $\max(\mb{x,y})$
	\item $\min(\mb{x,y})$
	\item $\frac{\min(\mb{x,y})}{\max(\mb{x,y})}$
	\item $\sqrt{\mb x^2 + \mb y^2}$
	\end{itemize}
\item \textbf{Two functions of two RVs}. Assume we have $\mb z = g(\mb{x,y})$ and $\mb w  = h(\mb{x,y})$. What is the \textit{joint} density of $\mb z$ and $\mb w$?

If both are continuous and differentiable functions, then we can develop a formula to obtain the joint pdf $f_{zw} (z,w)$ directly. Let us denote the solutions to $\mb z = g(\mb{x,y})$ and $\mb w  = h(\mb{x,y})$ with $(x_1,y_1), \hdots (x_n,y_n)$, \ie for all $i$%
$$g(x_i,y_i) = z\hspace{1truecm} h(x_i,y_i) = w.$$
%
Then, the joint density of the transformed variables, $f_{zw}(z,w)$, is obtained as:%
\begin{equation}
f_{zw}(z,w) = \sum_{i}|J(z,w)|f_{xy}(x_i,y_i) = \sum_{i}\frac{1}{|J(x_i,y_i)|}f_{xy}(x_i,y_i),
\label{eq:two_rv_transformation}
\end{equation}
where $J(\cdot)$ is the Jacobian that can be written either in terms of the inverse transformations\footnote{In p200 inv. transformation is denoted with $g_1,h_1$ but I found this notation counter-intuitive and prefer $g^{-1}, y^{-1}$}, $x_i = g^{-1}(z,w)$ and $y_i = h^{-1}(z,w)$:
\begin{equation}
J(z,w) = \left| \begin{matrix}
\frac{\p g^{-1}}{\p z} & \frac{\p g^{-1}}{\p w} \\
\frac{\p h^{-1}}{\p z} & \frac{\p h^{-1}}{\p w}
\end{matrix} \right|
\label{eq:j_inverse}
\end{equation}
%
or in terms of the direct transformation
%
\begin{equation}
J(x_i,y_i) = \left| \begin{matrix}
\frac{\p g}{\p x} & \frac{\p g}{\p y} \\
\frac{\p h}{\p x} & \frac{\p h}{\p y}
\end{matrix} \right|_{x=x_i,y=y_i},
\label{eq:j_direct}
\end{equation}
depending on whichever of the two leads to a simpler output.

\underline{\textit{Example}}. Say, we do the transformation $\mb r = g(\mb{x,y}) = \sqrt{\mb x^2+\mb y^2}$ and $\bs \theta = \tan^{-1}(\mb{y/x})$. Then, in the interval $(-\pi,\pi)$ we have one solution: $x_1 = r\cos \theta$ and $y_1 = r \sin \theta$. If we use \eqref{eq:j_inverse}, we have: %
$$J(r,\theta) = \left| \begin{matrix} \frac{\p x_1}{\p r} & \frac{\p x_1}{\p \theta} \\ 
\frac{\p x_1}{\p \theta} & \frac{\p y_1}{\p \theta} \end{matrix}\right|,$$
%
so that $|J(r,\theta)| = r$. We can also compute $J(x,y)$ using \eqref{eq:j_direct}: %
$$J(x,y) = \left| \begin{matrix}
\frac{x}{\sqrt{x^2+y^2}} & \frac{y}{\sqrt{x^2+y^2}} \\
\frac{-y}{x^2+y^2} & \frac{x}{x^2+y^2}
\end{matrix} \right| = \frac{1}{\sqrt{x^2+y^2}} = \frac{1}{r}.$$ 
%
\item \textbf{Auxiliary variables.}  Suppose that we want to compute the density of a RV such as $\mb z = g(\mb{x,y})$. It is sometimes easier to put use dummy variable such as $\mb{w=x}$ or $\mb{w=y}$, discover the joint density $f_{zw}(z,w)$ and then marginalize out $\mb w$. Some useful examples are Example 6-24 to 6-26.

\item \textbf{Mean}: Assume we form the RV $\mb z = g(\mb{x,y})$ and want to compute its mean, $E\{\mb z\}$. Theorem 6-4 (p209) says that we don't need to do the RV transfomration to compute this mean; we can directly compute it as $$E\{\mb z\} = E\{g(\mb{x,y})\} = \int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty} g(x,y) f(x,y) dx dy.$$
\item \textbf{Covariance}: The covariance $C$ or $C_{xy}$ is: $C_{xy} = E\{(\mb x - \mu_x)(\mb y - \mu_y)\}.$
\item \textbf{Correlation coefficient} $\rho$ is $\rho = \frac{C_{xy}}{\sigma_x \sigma_y}$.
\item \textbf{Uncorrelatedness} can be expressed in three forms: (1) $C_{xy} = 0$, (2) $\rho = 0$ or $E\{\mb{xy}\} = E\{\mb x\}E\{\mb y\}$
\item \textbf{Orthogonality}  is  when $E\{\mb{xy}\} = 0$ and denoted as $\mb x \perp \mb y$. \item \textbf{Joint moments} The mean $m_{kr} = E\{\mb x^k\ \mb y^r\}$ is called the joint moment
\item \textbf{Estimates of mean and var. of $\{g(\mb{x,y})\}$}: $$\eta_g \approx g + \frac{1}{2}\left( \frac{\p^2 g}{\p x^2}\sigma_x^2 + 2 \frac{\p^2g}{\p x \p y}\rho_{xy} \sigma_x  \sigma_y + \frac{\p^2 g}{\p y^2} \sigma_y^2  \right)$$
$$\sigma_g^2 \approx \left( \frac{\p g}{\p x} \right)^2 \sigma_x^2  + 2\left( \frac{\p g}{\p x} \right) \rho_{xy}\sigma_x \sigma_y  + \left( \frac{\p g}{\p y} \right)^2\sigma_y^2 $$
\end{itemize}


\section{Characteristic functions}
\begin{itemize}
\item \textbf{Joint characteristic function } is:%
$$\Phi(\omega_1,\omega_2) = E\{e^{j(\omega_1 x + \omega_2 y)}\} =  \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} e^{j(\omega_1 x + \omega_2 y)} f(x,y) dx dy.$$ 
\item \textbf{Inversion.} The density $f(x,y)$ can be obtained as: %
$$f(x,y) = \frac{1}{4\pi^2}\int\lii \int\lii \Phi(\omega_1,\omega_2) e^{-j(\omega_1 x + \omega_2 y)d\omega_1 d\omega_2}. $$
\item \textbf{Convolution theorem.} If the RVs $\mb{x,y}$ are independent, then $E\{\Phi(\omega_1, \omega_2)\} = E\{\Phi_x(\omega_1)\}E\{\Phi_y(\omega_2)\}$
\end{itemize}

\section{Notes}
\begin{itemize}
\item \textbf{Price's Theorem}. Given two jointly normal random variables $\mb x$ and $\mb y$, we form the mean %
$$I = E\{g(\mb{x,y})\} = \int\lii\int\lii g(x,y) f(x,y)dx dy.$$ This is a function of the covariance $c$ between the RVs $\mb x$ and $\mb y$. According to Price's theorem:
$$\frac{\p ^n I(c)}{\p c^n} = \int\lii \int\lii \frac{\p^{2n} g(x,y)}{\p x^n \p y^n} f(x,y) dx dy = E \left( \frac{\p^{2n} g(\mb {x,y})}{\p \mb x^n \p \mb y^n} \right),$$
given that $g(x,y)f(x,y) \to 0$ as $(x,y)\to \infty$. When used smartly, Price's theorem can greatly simplify the calculation of certain expectations and higher-order moments for normal RVs.

\textit{Example}. Computing $E\{\mb x^2 \mb y^2\}$ (for gaussian $\mb{x,y}$) is somewhat difficult (see p219). Using Price's theorem with $g(\mb{x,y}) = \mb{x}^2 \mb y^2$, for $n=1$ we note that
$$\frac{\p I(c)}{\p c} = E\left( \frac{\p^2 g(\mb{x,y})}{\p \mb x \p \mb y} \right) = 4 E\{\mb{xy}\}= 4c \implies I(c) = \frac{4c^2}{2} + I(0).$$
But $I(0)$ is when $\mb{x,y}$ are independent, hence $I(0) = E\{\mb{x}^2 \mb y^2\} = E\{\mb x^2\} E\{\mb y^2\}$, and we conclude that $E\{\mb{x}^2 \mb y^2\} = 2(E\{\mb x \mb y\})^2+E\{\mb x^2\} E\{\mb y^2\}$.\item \textbf{Conditional Distributions}. $$F(z,w|M) = \frac{F(z,w,M)}{P(M)}$$ %
Using a limit argument (see p221), we can define $$f(y|x) = \frac{f(x,y)}{f(x)}$$
\item \textbf{Markov Matrix}. Suppose the RVs $\mb{x,y}$ are discrete and let $p_{ik} = P\{\mb x = x_i,\mb y = y_k\}$, and $p_i = \sum_k p_{ik}$ and $q_k = \sum_k p_{ik} $. Let $P$ be the matrix whose $ik$th element is the conditional probability $\pi_{ik} = P\{\mb y = y_k | \mb x=x_i \}$.
\item \textbf{System reliability} and \textbf{time to failure}. The difference $R(t) = 1 - F(t) = P\{ \mb x > t\}$ is called time to failure. The mean time to failure is $E\{\mb x\} = \int_0^\infty xf(x)dx = \int_0^\infty R(t) dt$ (see also 5-52). The probability that a system that works until time $t$ fails prior to time $x$ is: $F(x|\mb{x} >t) = \frac{P\{t <\mb x < x\}}{1-F(t)} = \frac{F(x) - F(t)}{1-F(t)}$.
\item \textbf{Conditional expected values.} $E\{g(\mb y) | M\} = \int \lii g(y) f(y|M) dy$ and similarly $E\{g(\mb y) | x\} = \int \lii g(y) f(y|x) dy$. Specifically, the conditional mean and variance are $$\mu_{y|x} = E\{\mb y |x\} = \int\lii y f(y|x) dy \hspace{0.8truecm} \sigma^2_{y|x} = E\{ (\mb y - \mu_{y|x})^2 | x \} = \int\lii(y-\mu_{y|x})^2 f(y|x) dy$$.
\item \textbf{Some very useful identities} about the conditional means (p234):
	\begin{itemize}
	\item $E\{E\{\mb{y | x}\}\} = E\{\mb y\}$.
	\item $E\{ E\{ g(\mb{x,y}) |\mb x\} \} = E\{g(\mb{x,y})\}$
	\item $E\{ g_1(\mb x) g_2(\mb y) | x \} = E\{ g_1 (x) g_2(\mb y) \} = g_1(x) E\{g_2(\mb y)\}$.
	\item $E\{g_1(\mb x) g_2(\mb y)\} = E\{E\{ g_1( \mb x) g_2( \mb y) | \mb x  \}\} = E\{ g_1(\mb x) E\{ g_2 (\mb y) | \mb x \} \}$.
	\end{itemize}
 \end{itemize}





























%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









\chapter{Sequences of Random Variables}


\section{Definitions and some basic operations and properties}
\begin{itemize}
\item \textbf{Multivariate Transformation} is a direct extension of two-variable transformation in \eqref{eq:two_rv_transformation}
\item \textbf{Refining conditional distributions:} How to integrate RVs from a multi-variate distro to eliminate them from conditional probabilities (p253-254):
	\begin{itemize}
	\item To remove any variables from the left of the conditional line, we just integrate them out, \eg
	$$f(x_1|x_3) = \int\lii f(x_1, x_2 | x_3) dx_2$$
	\item To remove any variables from the right of the line, we multiply by their conditional density wrt. the \textit{remaining} variable on the right and we integrate the product, \eg
	$$f(x_1 | x_3) = \int\lii \int\lii f(x_1|x_2,x_3,x_4) f(x_2,x_3 | x_4) d x_2 dx_3$$
	\end{itemize}
\item \textbf{Group independence} The RV groups $\mb x_1, \hdots, \mb x_n$ and $\mb y_1, \hdots, \mb y_k$ are independent if $f(x_1,\hdots,x_n, y_1,\hdots,y_k) = f(x_1,\hdots,x_n)f(y_1,\hdots,y_k)$  
\item \textbf{Joint normality}. The RVs $\mb x_1, \hdots, \mb x_n$ are jointly normal iff the sum%
 $$a_1 \mb x_1 + \hdots a_n \mb x_n = A\mb X^t$$%
 is a normal RV for any $A$.
 \item \textbf{Homogeneous linear estimation}. Linear estimation without a bias term
\item \textbf{Nonhomogeneous linear estimation}. Linear estimation with a bias term

 
\end{itemize}


\section{Mean, Variance and Characteristic functions}
\begin{itemize} 
\item \textbf{Conditional mean}; \eg $E\{\mb x_1 | x_2,\hdots,x_n\} = \int\lii x_1 f(x_1| x_2,\hdots,x_n) d x_1$. To remove any variables from the right of the conditional line, we multiply by their conditional density wrt remaining variables on the right and integrate the product, \eg %
$$E\{\mb x_1 | x_3\} = \int\lii E\{\mb x_1 | x_2, x_3\} f(x_2|x_3) d x_2$$ 
\item Characteristic function leads to so much interesting applications (see notes of Chapter 5)
\item \textbf{Sample mean and variances}
$$\mb{\bar{x}} = \frac{1}{n} \sum\limits_{i=1}^n \mb x_i \hspace{1truecm} \mb{\bar{v}} = \frac{1}{n-1} \sum\limits_{i=1}^n (\mb x_i -\mb{\bar{x}})^2$$
\item \textbf{Minium-variance unbiased linear estimator of the mean} is shown at p258:
$$ \hat{\eta} = \frac{x_1/\sigma_1^2 + \hdots + x_n/\sigma_n^2}{1/\sigma_1^2 + \hdots + 1/\sigma_n^2} $$
Note that this equals the sample mean if we assume that RVs $\mb x_i$ have the same variance.  
\item \textbf{Correlation and Covariance matrices}. The $ij$th entry of the correlation matrix of RVs $\mb x_i$ is $R_{ij} = E\{ \mb x_i \mb x_j\}$ and the $ij$th entry of the covariance matrix is $C_{ij} = E\{(\mb x-\eta_x)(\mb y-\eta_y)\}$. Clearly, for $\mb X = [\mb x_1, \hdots, \mb x_n]$, 
$$R = E\{\mb X^t \mb X\}$$
\item \textbf{Linear independence} of RVs. The RVs $\mb{x}_i$ are called linearly independent if (p251) $$E\{|a_1 \mb x_1 + \hdots + a_n \mb x_n|\}>0$$
The determinant of the correlation matrix, $\Delta_n$, is zero if the RVs are linearly dependent. Otherwise, it is strictly and so are all its principal minors.
\item \textbf{Orthonormal data transformation: Whitening}. If the data $\mb x_i$ are orthogonal, then $R$ is a diagonal matrix. One can obtain an orthonormal matrix through the Gram-Schmidt method (p271). Whitening is very useful, especially for linear estimators (see also last note in the item list below).
\end{itemize}
 
\section{Mean square estimation}
\begin{itemize}
\item The optimal \textit{single-value} estimation (in the MS sense), $c$, of a future value of a RV $\mb{y}$ is $c=E(\mb{y})$.
\item The optimal functional estimation of $\mb{y}$ \ito a (dependent) RV $\mb{x}$ is $c(x)=E\{\mb{y}|x\}$. In general, this estimation is \textit{non-linear}.

If $\mb x$ and $\mb y$ are independent, then $c(x) = E\{\mb y | x\} = E\{\mb y\} = c$; \ie the knowledge of $\mb x$ has no effect on estimate of $\mb y$.
\item The optimal linear functional estimation of $\mb{y}$ \ito $\mb{x}$ is $c(x) = Ax + B$ where $B = \eta_y - A\eta_x$ and $A = r \sigma_x/\sigma_y$. 
\item \textit{For Gaussian RVs, linear and non-linear MS estimators are identical (\#264)}.
\item \textit{The orthogonality principle}: A linear estimator is optimal if its error $(\mb y - \mb{\hat{y}})$ is orthogonal to the data $\mb x$; \ie $E\{(\mb{y}-\mb{\hat{y}})\mb{x}\}=0$
%	\begin{itemize}
	\item Generalizes to the linear estimate $\mb{\hat{s}}$ of $\mb{s}$ \ito multi RVs $\mb{x}_1, \hdots, \mb{x}_n$: $E\{(\mb{s}-\mb{\hat{s}})\mb{x}_i\}=0$ for $i=1,\hdots,n$.
	\item Generalizes to \textit{non-linear} estimation: $E\{[\mb{s}-g(\mb{X})]w(\mb{X})\}=0$, where $g(\mb{X})$ is the optimal non-linear MS estimator and $w(\mb{X})$ \textit{any} function of data $\mb{X}=[\mb{x}_1,\hdots,\mb{x}_n]$. (\#269)
%	\end{itemize}
\item Computing the linear MS estimator is \textit{much easier} if the RVs $\mb{x}_i$ are orthogonal to one another (\ie $R_{ij}=0$ for $i\neq j$).  That is why we often perform \textit{whitening} (see \#271-272).
\end{itemize}

\section{Stochastic convergence and limit theorems}
\begin{itemize}
\item Convergence modes for a random sequence (RS) $\mb{x}_1, \hdots, \mb{x}_n, \hdots$
\begin{itemize}
\item Everywhere (e), almost everywhere (a.e.), in the MS sense (MS), in prob (p), in distro (d).
\item Ordered in relaxedness (except last two): d $>$ p $>$ (a.e. $|$ MS)
\item Cauchy criterion (CC): We typically think of convergence as converging into a sequence $x$. With CC, we can eliminate this need. That is, we ask that $|x_{n+m}-x_n|\to	0$ as $n\to	\infty$.
\end{itemize}
\end{itemize}

\section{Application of RS convergence}
\begin{itemize}
\item All the below are technically applications of RS convergence:
	\begin{itemize}
	\item The law of large numbers: If $p$ is the prob of event $A$ in a single experiment and $k$ is number of successes in $n$ trials, then we can show that $p$ tends to $k/n$ in \textit{probability}.
	\item Strong law of large numbers: We can even show that $p$ tends to $k/n$ almost everywhere (but proof is more complicated).
	\end{itemize}
	\item The ones below are more specifically applications for estimating $E\{\mb{\bar{x}}_n\}$ (and optionally $\bar{\sigma}^2_n$): the sample mean of a RS of $n$ RVs $\mb{\bar{x}}_n=\frac{\mb{x}_1+\hdots+\mb{x}_n}{n}$ (the variance of the sample mean)
	\begin{itemize}
	\item Markov's thm: If the RVs $\mb{x}_i$ are s.t. the mean $\bar{\eta}_n$ of $\bar{\mb{x}}_n$ tends to a limit $\eta$ and its variance $\bar{\sigma}_n$ tends to $0$ as $n\to \infty$, then $E\{(\mb{\bar{x}}_n-\eta)^2\}\to 0$ as $n\to \infty$. Convergence in MS sense.\\
	(Note that $\mb{x}_i$ do not have to be uncorrelated or independent)
	\item Corollary: if $\mb{x}_i$ are uncorrelated and $\frac{\sigma_1^2+\hdots+\sigma_n^2}{n}\to 0$ as $n\to \infty$, then $\mb{\bar{x}}_n\to \eta=\lim_{n\to\infty} \frac{1}{n}\sum\limits_{i=1}^n E\{\mb{x}_n\}$ as $n\to \infty$. Convergence in MS sense. \\
	(Note that now we do require uncorrelated RVs but in exchange, compared to Markov's thm, the condition on the mean is removed and we \textit{do} have a way of computing the mean $\eta$.)
	\item Khinchin's thm: the above two required us to know \textit{something} about the variance. According to Khinchin, if $\mb{x}_i$ are \iid (stricter condition), then we $\mb{\bar{x}}_n$ tends to $\eta$ even if we know nothing about the variance of $\mb{x}_i$'s. However, now we have convergence in probability only.
	\end{itemize}
\item The Central Limit Theorem (CLT) is also application of RS conv.\----conv. in \textit{distribution}:
	\begin{itemize}
	\item Given $n$ independent RVs $\mb{x}_i$, we form their sum (not sample mean!) $\mb{x}=\mb{x}_1+ \hdots+ \mb{x}_n$. This is an RV with mean $\eta$ and $\sigma$. CLT states that the distro $F(x)$ of $\mb{x}$ approaches a \textit{normal distro} with the same mean and variance: $F(x)\approx G(\frac{x-\eta}{\sigma})$.
	\item If the RVs are continuous, then $f(x)$, the \textit{density} of $x$, also approaches a normal density.
	\item The approximations become \textit{exact} asymptotically (\ie as $n\to \infty$).  
	\item Good $n$ values: if $\mb{x}_i$ are \iid, then $n=30$ is adequate for most applications. If the functions $f_{i}(x)$ are smooth, even $n=5$ can be enough.
	\item Error corr: The errors of CLT can be corrected by matching higher order moments using Hermite polynomials (see\#281 and also Section~\ref{sec:error_correction}).
	\item CLT for products $\mb{y}=\mb{x}_1\mb{x}_2\hdots\mb{x}_n$ (assuming $\mb{x}_i>0$) is \textit{lognormal}. (Easily seen by letting $\mb{z} = \ln{\mb{y}} = \ln \mb{x}_1+\hdots+\ln\mb{x}_n$ and $\mb{y} = e^{\mb{z}}$.
	\end{itemize}
	
\end{itemize}


\section{Sampling for matching a target distribution}
\begin{itemize}
\item Sampling random sequences for a target distribution $F(x)$
\item The general idea is to generate a uniform RS $u_1, \hdots, u_n$, and then use some method to convert it to a RS $x_1, \hdots, x_{n'}$ (where $n' \leq n$) that matches the distribution of $F(x)$.
\item There are several methods for sampling
	\begin{itemize}
	\item Percentile Transformation Method: This is a straightforward method ($x_i=F^{-1}_x(u_i)$) but we can apply it only if we can compute the inverse of the target distro, $F^{-1}_x(u)$. We can use this also to transform a sequence $y_i$ with a non-uniform distribution: $x_i=F_x^{-1}\left(F_y(y_i)\right)$ (see \#292).
	\item Rejection method: Allows sampling without $F^{-1}_x$. The idea is to find an event $M$ such that $f_x(x|M) = f_y(x)$. Turns out that this event is $M=\{\mb{u}\leq r(\mb{x})\}$ where $r(\mb{x}) = a f_y(x)/f_x(x)$. To implement this, one needs to find the upper bound (i.e. $a$; see Example 7-23). 
	\item Mixing method: $f_x(x) = p_1 f_1(x)+\hdots+p_m f_m(x)$. See proof in \#294 for explanation. 
	\item Some transformations:
		\begin{itemize}
		\item Binomial RV: Sum of $m$ \iid Bernoulli's equals Binomial (\#294).
		\item Erlang: Sum of $m$ \iid exponentials equals Erlang
		\item Chi-square: Sum an appropriate Erlang and a Normal
		\item Student-t: Use a normal and a chi-square
		\item Log-normal: Use a normal
		\end{itemize}
	\item Generating Normal RSs
		\begin{itemize}
		\item Clearly, we can use CLT and sum some uniform RSs
		\item Rejection and mixing: We can mix two Truncated normal RVs obtained via rejection
		\item Polar coordinates: Use a Rayleigh and a uniform to obtain two independent normals
		\item Use two uniforms to obtain two normals (see Example 6-15 to follow the equation right after 7-172)
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{Interesting identities/lemmas/theorems}

\begin{itemize}

\item Theorem 7.1: The correlation matrix is nonnegative definite
\item Sum of jointly normal RVs is normal
\item Sample mean and variance of $n$ RVs $\mb{x}_1, \hdots, \mb{x}_n$ are $\mb{\bar{x}} = \frac{1}{n}\sum\limits_{i=1}^n$ and $\bar{\mb{v}} = \frac{1}{n-1}\sum\limits_{i=1}^n(\mb{x}-\bar{\mb{x}})^2$.
\item Goodman's theorem: The statistics of a real zero-mean $n$-dimensional normal RV are completely determined i.t.o. the $n^2$ parameters\----the elements of its covariance matrix. However, a similar \textit{complex} RV requires $2n^2+n$ elements. Goodman's theorem (\#259) gives a special class of normal complex RVs that are determined completely with $n^2$ parameters only.
\item Berry-Esseen Theorem: Gives an upper bound for the approx. error of CLT, given that the third order of the unknown distro is finite and the variables $\mb{x}_i$ are \iid
\end{itemize}

\section{Important concepts}

\begin{itemize}
\item Group independence
\item Correlation and covariance matrices
\item The orthogonality principle
\item Mean square estimation (Linear and non-linear)
\end{itemize}

\section{Redo in future}
\begin{itemize}
\item Poisson
\item Chi square 
\item Show why sample variance is divided by $n-1$.
\item Prove why the generalized orthogonality principle (7-92) leads to optimal MS estimators (linear or non-linear).
\item Order convergence modes
\item Prove the law of large numbers
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\chapter{Statistics}

\section{Definitions}
\begin{itemize}
\item \textit{Statistic.} Any function of the sample RV vector, $\mb{X}=[\mb{x}_1,\hdots,\mb{x}_n]$, is called a \textit{statistic}. (This definition applies only for this chapter! See footnote in p306)
\item \textit{Unbiased estimator.} An estimator $T(\mb{X})$ is an unbiased estimator for $\theta$ if $E\{T(\mb{X})\}=\theta$.
\item \textit{Point estimator} is an RV $\bs{\hat{\theta}}=g(\mb{X})$  that estimates a parameter $\theta$.
\item \textit{Point estimate} $\hat{\theta}$ is a function of the \textit{observation vector} $X=[x_1, \hdots, x_n]$ and the corresponding RV; \ie $\hat{\theta}=g(X)$
\item \textit{Consistent estimator.} Let $\bs{\hat{\theta}}_n=g_n(\mb{X})$ be estimator of parameter $\theta$ based on a sample $\mb{X}$ with $n$ observations. $\bs{\hat{\theta}}$ is a consistent estimator if its error, $E\{(\bs{\hat{\theta}}_n-\theta)^2\}$, tends to zero as $n\to \infty$.
\item \textit{Best estimator.} Let $\bs{\hat{\theta}}=g_n(\mb{X})$ be estimator of parameter $\theta$. $\bs{\hat{\theta}}$ is best estimator if it minimizes the error $E\{(\bs{\hat{\theta}}_n-\theta)^2\}$.
\item \textit{Sufficient statistic.} A function $T(\mb{X})$ is a sufficient statistic for a parameter $\theta$, if $T(\mb{X})$ contains all information about $\theta$ in the data set $\mb{X}$. That is, given the PDF $P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta\}$, if $P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta | T(\mb{X}\}$ \textit{does not} depend on $\theta$, then $T(\mb{X})$ is sufficient statistic for $\theta$. (p322-323)
\item \textit{Efficient estimator.} An estimator is called efficient if it has the lowest possible variance. More specifically, we have first-order efficiency and second-order efficiency. The former means that an estimator is efficient w.r.t. the Cramer-Rao bound (see below), because the CR bound uses only first-order derivatives; the latter means that an estimator is efficient w.r.t. Bhattacharya bound, which is computed using the second-order derivative.
\end{itemize}


\section{Parameter Estimation}
Very important section with concepts directly applicable to machine learning. Specifically, it is shown why the Maximum Likelihood (ML) is so important, and from which mathematical framework it comes from. How can one obtain an unbiased estimator \textit{with minimum variance}, or how one can measure the variance of a given unbiased estimator. It looks like, if one wants to use non-Gaussian RVs for machine learning, the tools presented in this section are fundamental to show how to find their parameters. 

The concepts of Sufficient Statistic and Uniformly Minimum Variance Unbiased Estimator (UMVUE) are introduced. The latter is a desirable and most used estimator type, and the former leads to finding it. The most important theorems along the way are: \textit{Factorization theorem} (helps finding sufficient statistic), the \textit{Cramer-Rao Bound} (finds lower bound for variance of unbiased estimator), the \textit{Bhattacharya Bound} (improves Cramer-Rao), and the \textit{Rao-Blackwell theorem} (the finale: finds the UMVUE from sufficient statistic).

\begin{figure}[!h]
\begin{center}
\begin{pspicture}[shift=*](-2,-0.5)(6,2)
\psmatrix[colsep=0.7cm, rowsep=0.4cm]
 &  & [mnode=r,name=AUE,colsep=2cm] \small \stc{\textit{Any} unbiased}{Estimator} &\\
[mnode=oval,name=LF]  {\stc{Likelihood}{Function}} & [name=FT]\psframebox{\stc{Factorization}{Theorem}} & [name=RBT,colsep=2cm]\psframebox{\stc{Rao-Blackwell}{Theorem}} & [mnode=oval,name=UMVUE] UMVUE
\endpsmatrix
\psset{arrows=->}
\ncline{LF}{FT}
\ncline{FT}{RBT} \taput{\rnode{C}{\small \stc{Sufficient}{Statistic}}}
\ncline{RBT}{UMVUE}
\ncline{AUE}{RBT}
\end{pspicture}
\end{center}
\caption{The relationship between the major concepts and theorems of this section (parameter estimation).}
\end{figure}


\subsection{Notes}


\begin{itemize}
\item \textbf{The Factorization Theorem} allows us to find suff. statistic (see also Examples 8-14, 8-15). Let $\mb{x}_1,\hdots,\mb{x}_n$ be a set of discrete RVs with PMF $P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta \}$. Then, $T(\mb{x}_1, \hdots, \mb{x}_n)$ is sufficient for $\theta$ iff 
\begin{equation}
P\{\mb{x}_1=x_1,\hdots,\mb{x}_n=x_n; \theta | T(\mb{X})\}=h(x_1,\dots,x_n)g_\theta(T\mb{x}=t)
\end{equation}	
where $h(\cdot)$ is a nonnegative function of $\mb{x}_1, \hdots, \mb{x}_n$ that does not depend on $\theta$, and $g(\cdot)$ is a nonnegative function of $\theta$ and $T$ only (and \textit{not} of $\mb{x}_1, \hdots, \mb{x}_n$ in any other manner.)
\item \textbf{Cramer-Rao (CR) Lower Bound} gives us the lower bound for the variance of an estimator $T(\mb{x})$. Let $T(\mb{x})$ be any unbiased estimator for $\theta$ under the joint PDF $f(\mb{x}_1=x_1,\hdots,\mb{x}_n; \theta)$, denoted by $f(\mb{x};\theta)$. Then,
\begin{equation}
\text{Var}(T\{\mb{x}\}) \ge \frac{1}{E\left\{ \left( \frac{\partial}{\partial \theta}\log f(\mb{x}; \theta) \right)^2 \right\}} = -\frac{1}{E\left\{ \frac{\partial^2}{\partial \theta^2}\log f(\mb{x}; \theta)  \right\}} := \sigma_{\text{CR}}^2
\end{equation}
provided that the following regularity conditions are satisfied
\begin{align}
\frac{\partial}{\partial \theta} \int f(\mb{x}; \theta) dx = \int \frac{\partial f(\mb{x};\theta)}{\partial \theta} dx = 0 \\
\frac{\partial}{\partial \theta} \int T(\mb{x}) f(\mb{x};\theta)dx = \int T(\mb{x}) \frac{\partial f(\mb{x;\theta})}{\partial \theta} dx.
\end{align}
If $\mb{x}_1, \hdots, \mb{x}_n$ are independent, then 
\begin{equation}
\sigma_{\text{CR}}^2 = \frac{1}{n E \left\{  \left( \frac{\partial \log f(\mb{x}_i; \theta) }{\partial \theta} \right)^2   \right\}    } = \frac{1}{n E \left\{  \frac{\partial^2 \log f(\mb{x}_i; \theta) }{\partial \theta^2}    \right\}    }
\end{equation}
\item \textbf{Cramer-Rao for Multiparameter Case}. Let $\underline{\theta} = (\theta_1, \hdots, \theta_m)$ be the parameter vector, and the corresponding estimator $\underline{T}(\mb{x})=[T_1(\mb{x}), \hdots, T_m(\mb{x})]$. Then, it holds\footnote{$A\ge B$ in the sense that $A-B$ is a nonnegative-definite matrix} $\text{Cov}\{\underline{T}(\mb{x})\}\ge J^{-1}(\underline{\theta})$, where $J(\underline{\theta})$ represents the $m\times m$ Fisher matrix; i.e., 
\begin{equation}
J_{ij} = E\left\{ \frac{\partial \log f(\mb{x}; \underline{\theta})}{\partial \theta_i}\frac{\partial \log f(\mb{x}; \underline{\theta})}{\partial \theta_j} \right\}
\end{equation}
The regularity conditions are similar\----the $\partial \theta$ is replaced with $\partial \theta_i$.
\item \textbf{First-order efficient}. An estimator whose variance is equal to $\sigma_{\text{CR}}^2$ is called \textit{first-order efficient} as the Cramer-Rao bound makes use of the first-order derivative.
\item The pdf of an efficient estimator is from the \textit{exponential family}.  (p330)
\item \textbf{Improvement on CR Bound: The Bhattacharya Bound}: The Bhattacharya Bound makes use of second order derivative. An estimator that attains this bound is called \textbf{second-order efficient}.
\item \textbf{UMVUE}. The unbiased estimator for a parameter $\theta$ that has the lowest possible variance is called uniformly minimum variance unbiased estimator (UMVUE); the "the" in the former sentence is justified as the UMVUE is unique (p334). UMVUEs depend only on the sufficient statistic, and no other information about $\theta$ contained in the data set $\mb{X}$. 
\item The \textbf{Maximum-Likelihood Estimator (MLE)} $\bs{\hat{\theta}}_{ML}$ has very interesting properties, particularly for large $n$ (p354).
	\begin{itemize}
	\item If an efficient estimator exists, then \textit{it is the MLE} (p330). 
	\item If MLE exists, then it is only a function of the sufficient statistic (see 8-283 and 8-284).
	\item Assume that $\psi(\theta)$ is an unknown parameter depending on $\theta$. Then, the MLE of $\psi$ is given by $\psi(\bs{\hat{\theta}}_{ML}$).
	\item If MLE is the unique solution to the likelihood equation, then under some additional restrictions and regularity conditions (see p354), we also have as $n\to \infty$: (i) $\bs{\hat{\theta}}_{ML}$ tends to a normal RV (ii) $\text{Var}{\bs{\hat{\theta}}} \to \sigma^2_{\text{CR}}$ (iii) $E(\bs{\hat{\theta}}_{ML}) \to \theta$; therefore $\frac{\bs{\hat{\theta}}_{ML}-\theta}{\sigma^2_{\text{CR}}}\to N(0,1)$. 
	\end{itemize}
\end{itemize}


\section{Confidence Interval Estimation}
This section (p.307-317) is more like for evaluating the reliability of a particular parameter choice. E.g., what is the probability that the mean $\mu$ is within a certain interval? Or what is the interval $(c_1, c_2)$ that contains $\mu$ with 95\% confidence? Therefore I find it less applicable to signal processing and machine learning applications, but more appropriate like for testing hypotheses etc. In short: it's less interesting for me.

\begin{itemize}
\item General property: The minimum-length CI is such that $f(c_1)=f(c_2)$ (see p305 and Prob. 8-6), but finding this may not be easy (p314) therefore we may settle for a larger interval.
\item \textbf{Mean}, $\eta$. The goal is estimating confidence interval (CI) for the mean $\eta$ of $\mb{x}$ from \textit{noisy} observations $x_1, ..., x_n$ drawn from RVs $\mb{x}_1, ..., \mb{x}_n$. The overall strategy is based on using the sample mean $\mb{\bar{x}}$ to find an \textit{interval estimator} which is a pair of RVs which are functions of $\mb{\bar{x}}$, $\left(g_1(\mb{\bar{x}}), g_2(\mb{\bar{x}})\right)$, s.t. $P\{g_1(\mb{\bar{x}}) < \eta < g_2(\mb{\bar{x}})\}=\gamma$, where $\gamma$ is \textit{confidence coefficient}. We get the \textit{interval estimation} by using the computed sample mean $\bar{x}$: $\left(g_1(\bar{x}), g_2(\bar{x})\right)$. This task is simplified when we assume the sample mean $\mb{\bar{x}}$ is Normal (which is true if $\mb{x}$ is Normal or approx true if $n$ is large).
	\begin{itemize}
	\item \textbf{Known Variance}, $\sigma^2$. $\mb{\bar{x}}$ is $\sim N(\eta,\sigma/\sqrt{n})$ in this case, therefore say that $\eta$ is in the range $\bar{x}\pm z_{1-\delta/2} \sigma/\sqrt{n}$  with confidence $\gamma$, where $\delta = 1-\gamma$ and $z_u$ is the standard Normal distro's $u$-percentile.
	\item \textbf{Unknown Variance}. We now use the (unbiased) sample variance estimator, $\mb{s}^2=\frac{1}{n-1}\sum_{i=1}^{n}\left(\mb{x}_i-\mb{\bar{x}}\right)^2$. We can be lazy and compute CI using the formula just above with $s$, \ie $\bar{x}\pm z_{1-\delta/2} s/\sqrt{n}$ (which is OK for large $n$). Or, we can do things more properly. The RV $\frac{\mb{\bar{x}}-\eta}{\mb{s}/\sqrt{n}}$ has Student $t$ distro with $n-1$ DoF. Denoting by $t_u$ its percentiles, the CI that we are after is $\bar{x}\pm t_{1-\delta/2} \sigma/\sqrt{n}$.
	\item \textbf{Nothing is known}. We use the elegant Tchebycheff Inequality, and our CI is $\bar{x}\pm \sigma/\sqrt{n\delta}$. (I assume that $\sigma$ is sample variance? Not clear from book).
	\end{itemize}
\item \textbf{Zero-one event probability}, $P(A)$. A zero-one event (e.g. coin tossing or Republicans winning) is $\sim\text{Bernoulli}(p)$ and we aim to find CI for $p$. Since the mean of Bernoulli is $p$, we are effectively trying to estimate the mean of the distro, therefore we can use the stuff listed above. Specifically, for large $n$, our Bernoulli is approximated by $N(p, \sqrt{pq/n})$, therefore our CI is $\bar{x}\pm z_{1-\delta/2} \sqrt{pq}/\sqrt{n}$ with $p=\bar{x}$ and $q=1-p$.
\item \textbf{Variance}, $\sigma^2$.
	\begin{itemize}
	\item \textbf{Known mean}, $\eta$. The RV $\mb{\hat{v}}=\frac{1}{n}\sum_{i=1}^n(\mb{x}_i-\eta)^2$ is an unbiased and consistent estimator of variance (p313), and $n\mb{\hat{v}}/\sigma^2$ is $\sim\sim\chi^2(n)$. We can then either build a CI by excluding equal tail prob. masses of $1-\delta/2$, \ie $\frac{n\hat{v}}{\chi^2_{1-\delta/2}(n)}<\sigma^2<\frac{n\hat{v}}{\chi^2_{1-\delta/2}(n)}$. This interval \textit{does not} have minimum length; but the determination of the latter is not simple (p313-314).
	\item \textbf{Unknown mean}. Very similar to the case above (p314). We use $\sigma^2$'s point estimator, $\mb{s}^2$, which is $\sim\chi^2(n-1)$. The CI is $\frac{(n-1)\hat{s}^2}{\chi^2_{1-\delta/2}(n-1)}<\sigma^2<\frac{(n-1)\hat{s}^2}{\chi^2_{\delta/2}(n-1)}$.
	\end{itemize}
	\item See p315-317 for estimating \textbf{Percentiles} (\ie, $x_u$ s.t. $F(x_u)=u$), \textbf{Distributions} ($\hat{F}(x)$) and interval estimates for a specific $x$ value of Distributions. 
\end{itemize}




\section{Hypothesis Testing}
Let $H_0$ be the null hypothesis that a parameter $\theta$ of a distribution equals $\theta_0$, \ie $H_0 : \theta = \theta_0$. The alternative hypothesis can be $H_1:\theta \neq \theta_0$, or (more specifically) $H_1:\theta > \theta_0$ or $H_1:\theta < \theta_0$. Hypothesis testing aims to establish whether there is sufficient evidence to reject $H_0$. The idea is to compute some quantity from the sample and test whether this quantity falls into the critical region, $D_c$. 

Testing is based on the allowed errors; specifically, the probability \textit{masses} $\alpha, \beta(\theta)$ (defs below) 
\begin{table}[h!]
\centering
\begin{tabular}{|l | c | c|} \hline
				& $H_0$ is really true 			& $H_0$ is really false \\ \hline	
We reject $H_0$	& Type I error (prob: $\alpha = P\{ \mb{X} \in D_c | H_0 \}$) & ;)) \\\hline
We accept $H_0$	& ;) & Type II error (prob: $\beta(\theta) = P\{ \mb{X} \notin D_c | H_1 \}$)  \\\hline
\end{tabular} 
\end{table}

\noindent \textbf{Overall strategy} to hypothesis testing is: 
\begin{enumerate}
\setlength\itemsep{0.05em}
\item Select a test statistic $\mb{q} = g(\mb{X})$ (e.g. )
\item Find the critical region $R_c$ where the density of $\mb{q}$ is negligible according to an $\alpha$ value.
\item Compute the value $q$ using the observation vector $X=[x_1,x_2,\hdots,x_n]$
\item Reject $H_0$ if $q \in R_c$ 
\end{enumerate}

\noindent \textbf{Example}. We want to test if the mean of $\mb{x}$ is $\eta_0$. The sample mean is $\mb{\bar{x}}$ and, under familiar assumptions, $\mb{\bar{x}}\sim N(\eta, \sigma/\sqrt{n})$
\begin{enumerate}
\setlength\itemsep{0.05em}
\item Select $\mb{q}$ as the standard (under $H_0$ assumption) NV: $\mb{q}=\frac{\mb{\bar{x}}-\eta_0}{\sigma/\sqrt{n}}$. Note that $\mb{q}\sim N(\eta_q, 1)$ where $\eta_q=\frac{\eta-\eta_0}{\sigma/\sqrt{n}}$
\item We set $\alpha=0.05$, in which case the critical region for $R_c = \{q : z_{0.05} < q < z_{1-0.05}\}$
\item We compute $q(X)$ using the observation vector $X$
\item We reject $H_0:\eta=\eta_0$ iff $q\in R_c$
\end{enumerate}

\noindent The general formula above works across a variety of scenarios and tests. The example was for testing mean but the same strategy works for testing variance, (event) probability, distribution fit



\begin{figure}[h!]
\SpecialCoor
\def\edgeCyan#1#2{\ncdiagg[angleA=180, arm=0pt,nodesep=2pt, linecolor=cyan]{#2}{#1}}
\def\edgeBlue#1#2{\ncdiagg[angleA=180, arm=0pt,nodesep=2pt, linecolor=cyan]{#2}{#1}}
\psset{nodesep=5pt}
\pstree[treemode=R,levelsep=*1cm]{\Tr{Hypothesis Testing}}{
\pstree[treemode=R,levelsep=*1cm]{\Tr{Mean\rnode{A}{}}}{%
	\Tr{Known Variance (Gaussian RV)}
	\Tr{Unknown Variance (Student $t$ RV)}
}
\pstree[treemode=R,levelsep=*1cm]{\Tr{Variance\rnode{B}{}}}{%
	\Tr{Known Mean ($\chi^2(n)$ RV)}
	\Tr{Unknown Mean ($\chi^2(n-1)$ RV)}
}
\pstree[treemode=R,levelsep=*1.2cm]{\Tr{Discrete variables\rnode{C}{}}}{%
	\Tr{Binary probability (Gaussian for large $n$)}
	\Tr{Multiple($m$)-event probability ($\chi^2(m-1)$)}
	\Tr{Test of statistical independence (also $\chi^2$\----p362)}
}
\pstree[treemode=R,levelsep=*1.2cm]{\Tr{Distribution $F(x)$\rnode{C}{}}}{%
	\Tr{Entire  $F(x)$ (Kolmogorov-Smirnov test p361)}
	\Tr{For specific $x$'s ($\chi^2$\----p363)}
}}
\caption{Summary (p358-364) of which tests are appropriate for the test of various quantities}
\end{figure}





















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter*{Towards a motivation guide}
\section{Why transformations are useful}
\begin{itemize}
\item By applying an orthonormal transformation (\aka whitening) to a set of RVs we can easily compute the optimal 
\item How to optimally estimate: We do all our calculations in the space of the RVs; there we find an optimal (in some sense) estimators that are based on the hypothetical observation RVs (typically iid), and once we have the estimator, we apply our observations to this estimator (\ie replace the RVs $\mb{x}_i$'s with the observed $x_i$'s). Doing the estimation in the space of RVs gives us some theoretical guarantees under certain assumptions.
\end{itemize}

\chapter*{Towards a cheatsheet}

\section{Interesting facts}

\begin{itemize}
\item About normality.
	\begin{itemize}
	\item If two RVs are jointly normal, then they are also marginally normal (p176). The converse is not true (see p176 and also Example 6-3).
	\item If $\mb{x,y}$ are independent and their sum is normal, then $\mb{x,y}$ are also normal.
	
	\end{itemize}
\end{itemize}

\section{Interesting identities, inequalities}
\begin{itemize}
\item $E^2\{\mb{xy}\} \le E\{\mb x^2\} E\{\mb y^2\}$
\item The variance of the sum of two RVs $\mb{z = x+y }$ is $\sigma_z^2 = \sigma_x^2 + 2\rho_{xy} \sigma_x\sigma_y+ \sigma_y^2$. If $\mb{x,y}$ independent, then $\sigma_z^2 = \sigma_x^2 + \sigma_y^2$
\end{itemize}

\section{Interesting RV Transformations}
\begin{itemize}
\item If $\mb x$ is Cauchy (with parameter $\alpha$), $\mb y = \frac{1}{\mb x}$ is also Cauchy (with parameter $1/\alpha$ \---- p131).
\item If $\mb x$ is normal, then $\mb y = e^{\mb x}$ is lognormal.
\item If $\mb{x, y}$ are zero-mean jointly normal, $N(0,0,\sigma_1^2, \sigma_2^2, r)$, then $\mb z=\mb{x/y}$ has Cauchy density centered at $r \sigma_1/\sigma_2$ (p187).
\item If $\mb{x, y}$ are independent Gamma RVs, then $\mb z=\mb{x/(x+y)}$ has a beta density (p188).
\item If $\mb{x, y}$ are independent zero-mean Gaussians, then $\mb z=\mb{x}^2+\mb{y}^2$ is exponential RV with parameter $2\sigma^2$ (p190).
\item The magnitude and phase of a zero-mean complex Gaussian, $\mb{z} = \mb{x}+j \mb y$, have Rayleigh and Uniform distros, \textit{i.e.:}
	\begin{itemize}
	\item $\mb z = \sqrt{\mb x^2 + \mb y^2}$ is Rayleigh
	\item $\bs \theta = \tan^{-1}\left(\frac{\mb y}{\mb x}\right)$ is Uniform
	\end{itemize}
\item If $\mb x$ and $\mb y$ are independent exponential RVs with param $\lambda$, then $\min(\mb{x,y})$ is also exponential (w/ param $2 \lambda$)
\item If $\mb x \sim N(0,1)$ and $\mb y \sim \chi^2(n)$, and $\mb {x,y}$ are independent, then $\mb z =\frac{\mb x}{\sqrt{\mb y/n}}$ has a $t(n)$ (Student $t$) distribution (p207).
\item If $\mb{x,y}$ are independent Gamma RVs, then $\mb{z=x+y}$ is also Gamma (p206).
\item If $\mb{x,y}$ are independent, $\mb x\sim \chi^2(n)$ and $\mb y \sim \chi^2(m)$, then $\mb F = \frac{\mb x/m}{\mb y/n}$ is said to have an $F$ distribution with $(m,n)$ degrees of freedom.
\item If $\mb x$ is uniform in $(-\frac{\pi}{2}, \frac{\pi}{2})$, then $\mb y = \tan \mb x$ is Cauchy with param. 1 (p136).
\item Whatever $\mb x$ is, $\mb u = F_x(\mb x)$ is uniform in (0,1) (p139).
\item Assume $F_y(y)$ is any invertible distro function and $\mb u \sim U(0,1)$. Then, $\mb y = F_y^{-1} (\mb u)$ has distribution $F_y(y)$.
\item If $\mb x$ has chi-square density with $n$ DoF, then $\mb y = \sqrt{\mb x}$ has chi-density with $n$ DoF
\item If $\mb x$ is standardized normal $N(0,1)$, then $\mb y= \mb x^2$ is chi-square with 1 degree of freedom.
\item If $\mb{x}_1, \hdots, \mb{x}_n$ are jointly normal, then $\mb{z} = \mb{x}_1 + \hdots \mb{x}_n$ is also normal (p257)
\item If $\mb{x}_1, \hdots, \mb{x}_n$ are independent and each $\mb{x}_i$ is $N(0,1)$, then $\mb{z} = \mb{x}_1^2 + \hdots \mb{x}_n^2$ is $\chi^2(n)$ (p259)
\item If $\mb{x}$ is $\chi^2(n)$ and $\mb{y}$ is $\chi^2(m)$ and $\mb{x,y}$ independent, then $\mb{z} = \mb{x+y}$ is $\chi^2(n+m)$ (p260)
\end{itemize}

\section{Useful Identities}
\begin{itemize}
\item $\text{Var}(\mb{x}) = E\{\text{Var}(\mb{x|y})\} + \text{Var}(E\{\mb{x|y}\})$ (p337)
\end{itemize}
























\chapter*{Sketches to solutions}
\section{Solutions to equations in the book}

\subsection{Error correction for CLT}
\label{sec:error_correction}

It is really not obvious how the error correction for CLT is arrived at in \#281. The general idea is to match the higher order moments of the approximation with that of the data. The approximation error between the (unknown) density $f(x)$ and the approximated normal density $f_n(x; 0, \sigma)$ is:
%
\begin{equation}
\epsilon(x) = f(x)-f_n(x; 0, \sigma).
\end{equation}
%
The idea is to write the error (and thus $f(x)$) \ito an orthogonal set of polynomials, viz. Hermite Polynomials $H_k(x)$ (see 7-126). Since they are orthogonal, they form an orthogonal set on the real line:
%
\begin{equation}
\int\limits_{-\infty}^\infty e^{-x^2/2} H_n(x) H_m(x) dx = \begin{cases}n!\sqrt{2\pi}&n=m\\0&n\neq m\end{cases}
\label{eq:h_orth}
\end{equation}
%
Using those polynomials, one can approximate any function (including $\epsilon(x)$ or $f(x)$) \ito an infinite series:
%
\begin{equation}
\epsilon(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-x^2/2\sigma^2}\sum\limits_{k=3}^\infty C_k H_k\left(\frac{x}{\sigma}\right)
\end{equation}
%
The sum starts from $3$, as the moments of $\epsilon(x)$ up to order 2 are zero (I guess because the Gaussian approx. is sufficient to make those moments zero?) The book limits itself up to $4th$ order approx.

The idea is to find the coefficients $C_3$ and $C_4$ by matching the $3^{rd}$ and $4^{th}$ order moments of the unknown distro with that of the data (*** Be more pecise here). The third order moment of $\mb{x}$ is:
%
\begin{equation}
E_{f(x)}\{x^3\} = \int x^3 f(x) dx
\end{equation}
%
The third order moment of the (approximated) normal density is zero (see 5-73), and therefore the error in terms of the third moments is:
\begin{equation}
E_{f(x)}\{x^3\} - E_{f_n(x)}\{x^3\} = \int x^3 f(x) dx - \int x^3 f_n(x; 0, \sigma) dx = \int x^3\left[f(x)-f_n(x; 0, \sigma)\right] dx.
\end{equation}
Since the content of the last brackets equals $\epsilon(x)$, and since $ E_{f_n(x)}\{x^3\}$ is zero, the above equals $m_3 = E_{f(x)}\{x^3\}$ identity can be written as:
%
\begin{equation}
m_3 = \frac{1}{\sigma \sqrt{2\pi}} \int x^3 e^{-x^2/2\sigma^2}\sum\limits_{k=3}^\infty C_k H_k\left(\frac{x}{\sigma}\right) dx \label{eq:mom3}
\end{equation}
%
Now here is the tricky part that is not obvious in the book. Our goal is to find the coefficients $C_k$ using the above equality. This is achieved by using the orthogonality of the Hermite polynomials. The third order Hermite Polynomial is $H_3(x) = x^3-3x$, and using \eqref{eq:h_orth} and doing a change of variables we see that:
%
\begin{align}
&\int e^{-x^2/2\sigma^2} \left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx = \\
&=\int e^{-x^2/2\sigma^2} \frac{x^3}{\sigma^3}\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx -3\int e^{-x^2/2\sigma^2} \frac{x}{\sigma}\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx \label{eq:int2}\\
&=3!\sigma \sqrt{2\pi}
\end{align}
%
The second integral in \eqref{eq:int2} equals zero because $H_1(x)\propto x$ and $H_3(x)\perp H_1(x)$, and the first integral in \eqref{eq:int2} is proportional to $m_3$. More specifically, using \eqref{eq:int2} and \eqref{eq:mom3}, we arrive at the obscure derivation in the book:
%
\begin{equation}
m_3 = 3! \sigma^3 C_3.
\end{equation}

















\bibliographystyle{../IEEEtran}
\bibliography{../bibliography}

\printindex

\end{document}
