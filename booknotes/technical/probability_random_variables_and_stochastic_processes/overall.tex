\documentclass[a4paper]{book}

\input{../preliminaries}

\begin{document}

\chapter*{Introduction}

My notes from the legendary book of Papoulis and Pillai \cite{papoulis02}. The book assumes that the reader does already have fairly strong background in calculus and linear algebra. I used two books \cite{Gradshteyn80, Abramowitz70} as assistant for evaluating some of the mathematical identities or integrals. 

This book made me realize that a good technical book is not necessarily one that's easy to follow -- one that shows you very easily how each identity or result is arrived at. Some of the results in the book are presented without much explanation, and this required me to show a lot of effort to understand them; this effort seems to lead to a staying power. 


\chapter{Sequences of Random Variables}


\section{Notes}
\begin{itemize}
\item Multivariate Transformation
\item How to integrate some RVs from a multi-variate distro to obtain ... (Sec 7.2)
\item How to compute the mean conditioned on a subset of RVs
\item Characteristic function leads to so much interesting applications, such as:
%\begin{itemize}
	\item Computing the PDF of Bernoulli from Binomials (\#256)
	\item Computing the PDF of Poisson from Binomials (\#256)
	\item The sum of jointly normal RVs is normal (\#257)
	\item The sum of the squares of independent normal variables is chi-square (\#259)
	\item The sum of two chi-square distros is also chi-square (\#260)
%	\end{itemize}
\item The optimal single-value estimation (in the MS sense), $c$, of a future value of a RV $\mb{y}$ is $c=E(\mb{y})$.
\item The optimal functional estimation of $\mb{y}$ \ito a (dependent) RV $\mb{x}$ is $c(x)=E(\mb{y}|x)$. In general, this estimation is \textit{non-linear}.
\item The optimal linear f. estimation of $\mb{y}$ \ito $\mb{x}$ is $c(x) = Ax + B$ where $B = \eta_y - A\eta_x$ and $A = r \sigma_x/\sigma_y$. 
\item \textit{For Gaussian RVs, linear and non-linear MS estimators are identical (\#264)}.
\item \textit{The orthogonality principle}: The error between of linear estimator $\mb{\hat{y}}=A\mb{x}+B$ of $\mb{y}$ is orthogonal to the data: $E\{(\mb{y}-\mb{\hat{y}})\mb{x}\}=0$
%	\begin{itemize}
	\item Generalizes to the linear estimate of $\mb{s}$ \ito multi RVs $\mb{x}_1, \hdots, \mb{x}_n$: $E\{(\mb{s}-\mb{\hat{s}})\mb{x}_i\}$ for $i=1,\hdots,n$.
	\item Generalizes to \textit{non-linear} estimation: $E\{[\mb{s}-g(\mb{X})]w(\mb{X})\}$, where $g(\mb{X})$ is the non-linear MS estimator and $w(\mb{X})$ \textit{any} function of data $\mb{X}=[\mb{x}_1,\hdots,\mb{x}_n]$. (\#269)
%	\end{itemize}
\item Computing the linear MS estimator is \textit{much easier} if the RVs $\mb{x}_i$ are orthogonal to one another (\ie $R_{ij}=0$ for $i\neq j$).  That is why it is often we perform \textit{whitening} (see \#271-272).
\item Convergence modes for a random sequence (RS) $\mb{x}_1, \hdots, \mb{x}_n, \hdots$
\begin{itemize}
\item Everywhere (e), almost everywhere (a.e.), in the MS sense (MS), in prob (p), in distro (d).
\item Ordered in relaxedness (except last two): d $>$ p $>$ (a.e. $|$ MS)
\item Cauchy criterion (CC): We typically think of convergence as converging into a sequence $x$. With CC, we can eliminate this need. That is, we ask that $|x_{n+m}-x_n|\to	0$ as $n\to	\infty$.
\end{itemize}
\item All the below are technically applications of RS convergence:
	\begin{itemize}
	\item The law of large numbers: If $p$ is the prob of event $A$ in a single experiment and $k$ is number of successes in $n$ trials, then we can show that $p$ tends to $k/n$ in \textit{probability}.
	\item Strong law of large numbers: We can even show that $p$ tends to $k/n$ almost everywhere (but proof is more complicated).
	\end{itemize}
	\item The ones below are more specifically applications for estimating $E\{\mb{\bar{x}}_n\}$ (and optionally $\bar{\sigma}^2_n$): the sample mean of a RS of $n$ RVs $\mb{\bar{x}}_n=\frac{\mb{x}_1+\hdots+\mb{x}_n}{n}$ (the variance of the sample mean)
	\begin{itemize}
	\item Markov's thm: If the RVs $\mb{x}_i$ are s.t. the mean of $\bar{\eta}_n$ of $\bar{\mb{x}}_n$ tends to a limit $\eta$ and its variance $\bar{\sigma}_n$ tends to $0$ as $n\to \infty$, then $E\{(\mb{\bar{x}}_n-\eta)^2\}\to 0$ as $n\to \infty$. Convergence in MS sense.\\
	(Note that $\mb{x}_i$ do not have to be uncorrelated or independent)
	\item Corollary: if $\mb{x}_i$ are uncorrelated and $\frac{\sigma_1^2+\hdots+\sigma_n^2}{n}\to 0$ as $n\to \infty$, then $\mb{\bar{x}}_n\to \eta=\lim_{n\to\infty} \frac{1}{n}\sum\limits_{i=1}^n E\{\mb{x}_n\}$ as $n\to \infty$. Convergence in MS sense. \\
	(Note that now we do require uncorrelated RVs but in exchange, compared to Markov's thm, the condition on the mean is removed and we \textit{do} have a way of computing the mean $\eta$.)
	\item Khinchin's thm: the above two required us to know \textit{something} about the variance. According to Khinchin, if $\mb{x}_i$ are \iid (stricter condition), then we $\mb{\bar{x}}_n$ tends to $\eta$ even if we know nothing about the variance of $\mb{x}_i$'s. However, now we have convergence in probability only.
	\end{itemize}
\item The Central Limit Theorem (CLT) is also application of RS conv.\----conv. in \textit{distribution}:
	\begin{itemize}
	\item Given $n$ independent RVs $\mb{x}_i$, we form their sum (not sample mean!) $\mb{x}=\mb{x}_1+ \hdots+ \mb{x}_n$. This is an RV with mean $\eta$ and $\sigma$. CLT states that the distro $F(x)$ of $\mb{x}$ approaches a \textit{normal distro} with the same mean and variance: $F(x)\approx G(\frac{x-\eta}{\sigma})$.
	\item If the RVs are continuous, then $f(x)$, the \textit{density} of $x$, also approaches a normal density.
	\item The approximations become \textit{exact} asymptotically (\ie as $n\to \infty$).  
	\item Good $n$ values: if $\mb{x}_i$ are \iid, then $n=30$ is adequate for most applications. If the functions $f_{i}(x)$ are smooth, even $n=5$ can be enough.
	\item Error corr: The errors of CLT can be corrected by matching higher order moments using Hermite polynomials (see\#281 and also Section~\ref{sec:error_correction}).
	\item CLT for products $\mb{y}=\mb{x}_1\mb{x}_2\hdots\mb{x}_n$ (assuming $\mb{x}_i>0$) is \textit{lognormal}. (Easily seen by letting $\mb{z} = \ln{\mb{y}} = \ln \mb{x}_1+\hdots+\ln\mb{x}_n$ and $\mb{y} = e^{\mb{z}}$.
	\end{itemize}
\item Sampling random sequences for a target distribution $F(x)$
\item The general idea is to generate a uniform RS $u_1, \hdots, u_n$, and then use some method to convert it to a RS $x_1, \hdots, x_{n'}$ (where $n' \leq n$) that matches the distribution of $F(x)$.
\item There are several methods for sampling
	\begin{itemize}
	\item Percentile Transformation Method: This is a straightforward method ($x_i=F^{-1}_x(u_i)$) but we can apply it only if we can compute the inverse of the target distro, $F^{-1}_x(u)$. We can use this also to transform a sequence $y_i$ with a non-uniform distribution: $x_i=F_x^{-1}\left(F_y(y_i)\right)$ (see \#292).
	\item Rejection method: Allows sampling without $F^{-1}_x$. The idea is to find an event $M$ such that $f_x(x|M) = f_y(x)$. Turns out that this event is $M=\{\mb{u}\leq r(\mb{x})\}$ where $r(\mb{x}) = a f_y(x)/f_x(x)$. To implement this, one needs to find the upper bound (i.e. $a$; see Example 7-23). 
	\item Mixing method: $f_x(x) = p_1 f_1(x)+\hdots+p_m f_m(x)$. See proof in \#294 for explanation. 
	\item Some transformations:
		\begin{itemize}
		\item Binomial RV: Sum of $m$ \iid Bernoulli's equals Binomial (\#294).
		\item Erlang: Sum of $m$ \iid exponentials equals Erlang
		\item Chi-square: Sum an appropriate Erlang and a Normal
		\item Student-t: Use a normal and a chi-square
		\item Log-normal: Use a normal
		\end{itemize}
	\item Generating Normal RSs
		\begin{itemize}
		\item Clearly, we can use CLT and sum some uniform RSs
		\item Rejection and mixing: We can mix two Truncated normal RVs obtained via rejection
		\item Polar coordinates: Use a Rayleigh and a uniform to obtain two independent normals
		\item Use two uniforms to obtain two normals (see Example 6-15 to follow the equation right after 7-172)
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{Interesting identities/lemmas/theorems}

\begin{itemize}
\item The unbiased linear estimator with minimum variance is the one shown in (7-17).
\item Theorem 7.1: The correlation matrix is nonnegative definite
\item Sum of jointly normal RVs is normal
\item Sample mean and variance of $n$ RVs $\mb{x}_1, \hdots, \mb{x}_n$ are $\mb{\bar{x}} = \frac{1}{n}\sum\limits_{i=1}^n$ and $\bar{\mb{v}} = \frac{1}{n-1}\sum\limits_{i=1}^n(\mb{x}-\bar{\mb{x}})^2$.
\item Goodman's theorem: The statistics of a real zero-mean $n$-dimensional normal RV are completely determined i.t.o. the $n^2$ parameters\----the elements of its covariance matrix. However, a similar \textit{complex} RV requires $2n^2+n$ elements. Goodman's theorem (\#259) gives a special class of normal complex RVs that are determined completely with $n^2$ parameters only.
\item Berry-Esseen Theorem: Gives an upper bound for the approx. error of CLT, given that the third order of the unknown distro is finite and the variables $\mb{x}_i$ are \iid
\end{itemize}

\section{Important concepts}

\begin{itemize}
\item Group independence
\item Correlation and covariance matrices
\item The orthogonality principle
\item Mean square estimation (Linear and non-linear)
\end{itemize}

\section{Some terminology}
\begin{itemize}
\item Homogeneous linear estimation:  Linear estimation without a bias term
\item Nonhomogeneous linear estimation: Linear estimation with a bias term

\end{itemize}
\section{Redo in future}
\begin{itemize}
\item Poisson
\item Chi square 
\item Show why sample variance is divided by $n-1$.
\item Prove why the generalized orthogonality principle (7-92) leads to optimal MS estimators (linear or non-linear).
\item Order convergence modes
\item Prove the law of large numbers
\end{itemize}



\chapter*{Towards a motivation guide}
\section{Why transformations are useful}
\begin{itemize}
\item By applying an orthonormal transformation (\aka whitening) to a set of RVs we can easily compute the optimal 
\end{itemize}

\chapter*{Towards a cheatsheet}
\section{Interesting RV Transformations}
\begin{itemize}
\item If $\mb{x}_1, \hdots, \mb{x}_n$ are jointly normal, then $\mb{z} = \mb{x}_1 + \hdots \mb{x}_n$ is also normal (\#257)
\item If $\mb{x}_1, \hdots, \mb{x}_n$ are independent and each $\mb{x}_i$ is $N(0,1)$, then $\mb{z} = \mb{x}_1 + \hdots \mb{x}_n$ is $\chi^2(n)$ (\#259)
\item If $\mb{x}$ is $\chi^2(n)$ and $\mb{y}$ is $\chi^2(m)$ and $\mb{x,y}$ independent, then $\mb{z} = \mb{x+y}$ is $\chi^2(n+m)$ (\#260)
\end{itemize}


\chapter*{Sketches to solutions}
\section{Solutions to equations in the book}

\subsection{Error correction for CLT}
\label{sec:error_correction}
It is really not obvious how the error correction for CLT is arrived at in \#281. The general idea is to match the higher order moments of the approximation with that of the data. The approximation error between the (unknown) density $f(x)$ and the approximated normal density $f_n(x; 0, \sigma)$ is:
%
\begin{equation}
\epsilon(x) = f(x)-f_n(x; 0, \sigma).
\end{equation}
%
The idea is to write the error (and thus $f(x)$) \ito an orthogonal set of polynomials, viz. Hermite Polynomials $H_k(x)$ (see 7-126). Since they are orthogonal, they form an orthogonal set on the real line:
%
\begin{equation}
\int\limits_{-\infty}^\infty e^{-x^2/2} H_n(x) H_m(x) dx = \begin{cases}n!\sqrt{2\pi}&n=m\\0&n\neq m\end{cases}
\label{eq:h_orth}
\end{equation}
%
Using those polynomials, one can approximate any function (including $\epsilon(x)$ or $f(x)$) \ito an infinite series:
%
\begin{equation}
\epsilon(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-x^2/2\sigma^2}\sum\limits_{k=3}^\infty C_k H_k\left(\frac{x}{\sigma}\right)
\end{equation}
%
The sum starts from $3$, as the moments of $\epsilon(x)$ up to order 2 are zero (I guess because the Gaussian approx. is sufficient to make those moments zero?) The book limits itself up to $4th$ order approx.

The idea is to find the coefficients $C_3$ and $C_4$ by matching the $3^{rd}$ and $4^{th}$ order moments of the unknown distro with that of the data (*** Be more pecise here). The third order moment of $\mb{x}$ is:
%
\begin{equation}
E_{f(x)}\{x^3\} = \int x^3 f(x) dx
\end{equation}
%
The third order moment of the (approximated) normal density is zero (see 5-73), and therefore the error in terms of the third moments is:
\begin{equation}
E_{f(x)}\{x^3\} - E_{f_n(x)}\{x^3\} = \int x^3 f(x) dx - \int x^3 f_n(x; 0, \sigma) dx = \int x^3\left[f(x)-f_n(x; 0, \sigma)\right] dx.
\end{equation}
Since the content of the last brackets equals $\epsilon(x)$, and since $ E_{f_n(x)}\{x^3\}$ is zero, the above equals $m_3 = E_{f(x)}\{x^3\}$ identity can be written as:
%
\begin{equation}
m_3 = \frac{1}{\sigma \sqrt{2\pi}} \int x^3 e^{-x^2/2\sigma^2}\sum\limits_{k=3}^\infty C_k H_k\left(\frac{x}{\sigma}\right) dx \label{eq:mom3}
\end{equation}
%
Now here is the tricky part that is not obvious in the book. Our goal is to find the coefficients $C_k$ using the above equality. This is achieved by using the orthogonality of the Hermite polynomials. The third order Hermite Polynomial is $H_3(x) = x^3-3x$, and using \eqref{eq:h_orth} and doing a change of variables we see that:
%
\begin{align}
&\int e^{-x^2/2\sigma^2} \left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx = \\
&=\int e^{-x^2/2\sigma^2} \frac{x^3}{\sigma^3}\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx -3\int e^{-x^2/2\sigma^2} \frac{x}{\sigma}\left(\frac{x^3}{\sigma^3}-3\frac{x}{\sigma}\right)dx \label{eq:int2}\\
&=3!\sigma \sqrt{2\pi}
\end{align}
%
The second integral in \eqref{eq:int2} equals zero because $H_1(x)\propto x$ and $H_3(x)\perp H_1(x)$, and the first integral in \eqref{eq:int2} is proportional to $m_3$. More specifically, using \eqref{eq:int2} and \eqref{eq:mom3}, we arrive at the obscure derivation in the book:
%
\begin{equation}
m_3 = 3! \sigma^3 C_3.
\end{equation}

\newpage

\bibliographystyle{IEEEtran}
\bibliography{../bibliography}


\end{document}
