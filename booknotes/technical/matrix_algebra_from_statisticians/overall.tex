\documentclass[a4paper, oneside]{book}

%\topmargin=-0.95in    % Make letterhead sftart about 1 inch from top of page
%\textheight=9.60in    % text height can be bigger for a longer letter
%\oddsidemargin=-0.5in % leftmargin is 1 inch
%\textwidth=8.7in   % textwidth of 6.5in leaves 1 inch for right margin



\input{../preliminaries.tex}

\begin{document}

\chapter*{Introduction}

These are my notes from the book of Harville~\cite{harville97}. This are probably very esoteric notes, I noted whatever was important for me. That is, however basic, anything that I didn't know found a place here. It is also a summary of what I gained from this book. My coverage of chapters will probably be insanely unbalanced.

\chapter{Matrices}
Preliminaries and basics of matrix algebra are presented here. But one has to go through to become familiar with the notation that will follow (\eg $\mb{J}_{mn}$ is an $m\times n$ matrix of ones). Plus going through the entire text and solving the exercises helped me to start and become familiar with \textit{proving} linear algebra. Exercise 8 is particularly interesting.

Some notes:

\begin{itemize}
\item $\mb{J}_{mn} \mb{J}_{np} = n \mb{J}_{mp}$ 
\item \textbf{L1.3.1} If $\mb{A,B}$ are upper (lower) triangular, then (i) $\mb{AB}$ is upper (lower) triang. (ii) the $i$th diagonal of $\mb{AB}$ is $a_{ii} b_{ii}$
\end{itemize}

\subsection{Redo in future}
\begin{itemize}
\item Exercise 4.a--c
\item Exercise 8
\end{itemize}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Submatrices and Partitioned Matrices}
A basic topic that I wasn't sufficiently familiar is covered in this chapter (see title). 

\section{Definitions and some properties}
\begin{itemize}
\item \textit{Principle submatrix} is a submatrix of $\mb A$ that is obtained by striking out the rows and columns with the same indices.
\item \textit{Leading principal submatrix} is an $r\times r$ submatrix of $\mb A$ obtained by striking out the last $n-r$ columns and rows.
\item A \textit{partitioning} of a matrix or \textit{partitioned matrix} is obtained by putting horizontal/vertical lines between its various columns or rows. The only requirement is that the lines shouldn't be \textit{staggered} (see \#15). 
\end{itemize}

\section*{Notes}
\begin{itemize}
\item The fact that multiplication of partitioned matrices is similar to that of the regular matrices (see 2.7) is very intuitive. Proves useful when proving thing in future chapters.
\end{itemize}

\section*{Interesting identities/lemmas/theorems}

\begin{itemize}
\item (2.9): $\mb{AB} = \sum_{k=1}^c \mb{A}_k \mb{B}_k$, where $\mb{A,B}$ are appropriately defined partitionings (see \#19), i.e. $\mb{A} = \left(\mb{A}_1, \mb{A}_2, \hdots, \mb{A}_c \right)$, $B=\begin{bmatrix} \mb{B}_1\\ \mb{B}_2\\ \hdots\ \\ \mb{B}_c \end{bmatrix}$. Particularly interesting when $\mb{A}_i$ and $\mb{B}_j$ are cols and rows of $\mb A$ and $\mb B$ respectively, i.e. $\mb{C}=\mb{A}\mb{B}=\sum_{i=1}^c \mb{a}_i\mb{b}'_i$.
\item \textbf{L2.3.2} For any two $m\times n$ matrices $ \mb{A,B}$, $\mb A = \mb B \iff \mb{Ax = Bx}$ for every $n\times 1$ vector $\mb x$
\end{itemize}

\section*{Important concepts}
\begin{itemize}
\item Block-diagonal matrix
\item Block-triangular matrix
\end{itemize}

\subsection{Redo in future}
\begin{itemize}
\item Exercise 4
\end{itemize}














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Linear Dependence and Independence}
Very short chapter, but its lemmas are very useful, they can be used extensively in proofs that come later in the chapters. Lemma 3.2.1 and 3.2.2 are fundamental and they should be practically memorized. Lemma 3.2.4 is an elegant one, it seems like it can be the key to some complicated proofs. 

\section{Definitions}
\begin{itemize}
\item A nonempty finite set of matrices $\{\mb A_1, \hdots,  \mb A_k\}$ is called \textit{linearly dependent} if $\exists$ scalars $x_1,\hdots, x_k$ such that $$x_1 \mb A_1 + \hdots + x_k \mb A_k = \mb 0$$
\end{itemize}

\section{Notes}
\begin{itemize}
\item \textbf{L3.2.1} A set of $m\times n$ matrices  $\{\mb A_1, \hdots,  \mb A_k\}$  is linearly dependent iff there is at least one matrix that is expressible as the linear combination of others.
\item \textbf{L3.2.2} A set of $m\times n$ matrices  $\{\mb A_1, \hdots,  \mb A_k\}$, the first of which is nonnull, is linearly dependent fif at least one of the matrices is expressible as the linear combination of the preceding ones.
\item \textbf{L3.2.4} Let $\{\mb A_i\}_{i=1}^k$ and $\{\mb C_i\}_{i=1}^r$ be sets of matrices and $\mb x_1,\hdots,\mb x_r$ vectors such that $$\mb C_j = x_{1j} \mb A_1 + \hdots + x_{kj} \mb A_k,$$
where $\mb x_j = (x_{1j}, \hdots, x_{kj})$. If $\mb x_1, \hdots, \mb x_k$ and $\mb A_1, \hdots, \mb A_k$ are linearly independent, then $\mb C_1, \hdots, \mb C_r$ are linearly independent. If $\mb {x}_1, \hdots, \mb x_r$ are linearly dependent, then $\mb C_1, \hdots, \mb C_r$ are linearly dependent.z
\end{itemize}

\section{Redo in future}


\section*{Notes}
\begin{itemize}
\item Given $\mb{A}_1,\hdots,\mb{A}_k$. Let $\mb{C}_j=x_{1j}\mb{A}_1+\hdots x_{kj}\mb{A}_k$ for $j=1,\hdots,r$ and $\mb{x}_j=(x_{1j}, \hdots, x_{kj})$. If  $\mb{A}_1,\hdots,\mb{A}_k$ are linearly independent and $\mb{x}_1, \hdots, \mb{x}_r$ are linearly independent, then $\mb{C}_1,\hdots,\mb{C}_r$ are linearly independent. If $\mb{x}_1, \hdots, \mb{x}_r$ are linearly dependent, then $\mb{C}_1,\hdots,\mb{C}_r$ are linearly dependent.
\end{itemize}


\begin{itemize}
\item Lemma 3.2.1
\item Lemma 3.2.2
\item Lemma 3.2.4
\item Exercise 2
\end{itemize}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Linear Space: Row and Column Spaces}

Very useful chapter, it's title doesn't do justice. Fundamental concepts of rank, dimension, basis, linear space, subspace, column/row space are introduced and they are made concrete with many lemmas and theorems.

\section{Definitions and basic properties}
\begin{itemize}
\item \textit{Column space} of an $m\times n$ matrix, $\mathcal{C}(\mb A) $, is the set of all $m$-dimensional col vectors that can be expressed as the linear combination of columns of $\mb A$, \ie $\mathcal{C}(\mb A) = \left\{\sum_{i=1}^n x_i\mb{a}_i : \forall x_i \in \mathcal{R} \right\}$.
\item \textit{Row space} $\mathcal{R}(\mb A) $ is defined similarly as $\mathcal{R}(\mb A) = \left\{\sum_{i=1}^m x_i\mb{a'}_i : \forall x_i \in \mathcal{R} \right\}$.
\item Simple ways to ascertain a column space.
	\begin{itemize}
	\item \textbf{L4.1.1.} For any $m\times n$ $\mb A$, $\mb y \in \mc C (\mb A) \iff \mb y' \in \mc R(\mb A')$
	\item \textbf{L4.2.1.} Given $m\times n$ $\mb A$. For any subspace $\mc U$ of $\mc R^m$, $\mc C(\mb A) \in \mc U \iff $ every column of $\mb A$ belongs to $\mc V$. 
	
	Similarly, for any subspace $\mc V$ of $\mc R^n$, $\mc R(\mb A) \in \mc V \iff $ every row of $\mb A$ belongs to $\mc U$. Similarly, for any
	\end{itemize}
\item \textit{Linear space}. A nonempty set of $m\times n$ is a linear space if (1) $\mb{A,B} \in \mc{V} \implies \mb {A+B} \in \mc V$ and (2) $\mb A \in \mc V \implies k\mb A \in \mc V$.
\item \textit{Subspace}. A subset $\mc U$ of a linear space $\mc V$ is a \textit{subspace} if $\mc U$ itself is a linear space. 

\item \textit{Span.} The span of a set of matrices $\mc A = \{\mb A_1, \hdots, \mb A_k\}$ is the set of all the linear combinations of $\mc A$, denoted with $\text{span}(\mc A)$. $\mc A$ is said to span a space $\mc V$ if $\text{span}(\mc A) = \mc V$
\item \textit{Basis} for a linear space $\mc V$ is a set of linearly independent matrices that spans $\mc V$.
\item \textit{Dimension.} The number of matrices in a basis for a lin. space $\mc V$ is called the dimension. 
	\begin{itemize}
	\item $\dim (\mc R^{m\times n}) = mn$
	\item $\dim (\mc U) \le mn$ for any $\mc U \subset \mc R^{m\times n}$
	\end{itemize}
\item \textit{Column rank and row rank.} The column (or row) rank of a matrix is the dimension of its column (or row) space.
\item \textit{Rank} is the column or row rank of an $m\times n$ matrix $\mb A$; the two latter ranks are the same anyway (T4.4.1). 
	\begin{itemize}
	\item $\rank (\mb A)\le m$ and $\rank(\mb A)\le n$ (L4.3.3)
	\item For any $n\times p$ mat $\mb F$, $\rank(\mb{AF}) \le \rank (\mb A)$ and  $\rank(\mb AF) \le \rank (\mb F)$ (C4.4.5)
	\item $\rank(\mb{A,B}) \le \rank(\mb A) + \rank(\mb B)$  and $\rank\left(\begin{matrix} \mb A \\ \mb C \end{matrix} \right) \le \rank(\mb A) + \rank(\mb B)$ (L4.5.7)
	\item $\rank(\mb A +\mb B) \le \rank(\mb{A,B})$  (L4.5.8)
	\item $\rank(\mb A +\mb B) \le \rank(\mb A) + \rank(\mb B)$  (C4.5.9)
	\item $\rank\left( \begin{matrix}
	\mb A & \mb 0 \\ \mb 0 & \mb B
	\end{matrix}\right) = \rank(\mb A) + \rank(\mb B)$ (L4.5.11)
	\item Repeated application of the above leads to $\rank\left[\diag(\mb{A}_1, \hdots \mb A_k)\right] = \rank(\mb A_1) + \hdots \rank(\mb A_k)$
	\end{itemize}
\item \textit{Nonsingular} matrix is an $m \times m$ matrix $\mb A$ with rank $m$.
\end{itemize}


\section{Important Lemmas/Theorems/Equations}
\begin{itemize}
\item \textbf{Regarding the column/row spaces}.
	\begin{itemize}
	\item \textbf{L4.2.2}. $\mcc B \subset \mcc A \iff \exists \mb F $ such that $\mb{B=AF}$. Similarly, $\mcr C \subset \mcr A \iff \exists \mb L$ such that $\mb{C} = \mb{LA}$.
	\item \textbf{T4.4.6}. Given $\mb{A,B}$. If $\mathcal{C}(\mb B) \subset \mathcal{C}(\mb A)$ and $\rank(\mb B) = \rank(\mb A)$, then $\mathcal{C}(\mb B)=\mathcal{C}(\mb A)$. Similar for row space.
	\end{itemize}
\item \textbf{Regarding the ranks and singularity/nonsingularity.}
	\begin{itemize}
	\item \textbf{T4.4.1} The row rank of any matrix equals its column rank.
	\item \textbf{T4.4.2}. Let $\mb A$ be an $m\times n$ matrix of row rank $r$ and column rank $c$. Then, there exists an $m\times c$ mat $\mb B$ and a $c\times n$ mat $\mb L$ s.t. $\mb{A=BL}$.
	\item \textbf{T4.4.4} Let $\mb{A,B,C}$ be $m\times n$, $m \times p$ and $q\times n$. Then, 	$\mcc B \subset \mcc A \implies \rank(\mb B) \le \rank(\mb C)$ and $\mcr C \subset \mcr A \implies \rank(\mb C) \le \rank(\mb A) $.
	\item Let $\mb A$ be $m\times n $ of rank $r$. Then, $\exists$ $m\times r$ $\mb{B}$ and $r\times n$ $\mb{T}$ s.t. $\mb{A = BT}$. Moreover, $\rank(\mb B) = \rank(\mb T) = r.$
	\item \textbf{T4.4.10}. Any matrix of rank $r$ contains an $r\times r$ nonsingular submatrix.
	\item \textbf{L4.4.11}. Any symmetric matrix of rank $r$ contains an $r\times r$ nonsingular principal submatrix.
	\end{itemize}
\item \textbf{Regarding partitioned matrices and sums of matrices.} For any $\mb{A,B,C}$ with respective sizes $m\times n, m\times p, q\times n$,
	\begin{itemize}
	\item \textbf{C4.5.2}: $\rank(\mb A) \le \rank(\mb{A,B})$; $\rank(\mb B) \le \rank(\mb{A,B})$; $\rank(\mb A) \le \rank\left(\begin{matrix} \mb A \\ \mb C\end{matrix}\right)$ and $\rank(\mb C) \le \rank\left(\begin{matrix} \mb A \\ \mb C\end{matrix}\right)$
	\item \textbf{L4.5.4}: $\mcc{A,B} = \mcc{A, B-AL}$. Similar result applies to row spaces.
	\end{itemize}
\end{itemize}

\begin{itemize}
\item Lemma 4.2.2 is easily the most important result in the chapter. Many results in the chapter can be proved directly through this lemma.
\item Corollary 4.2.3 also very useful.
\item Theorem 4.3.2
\item Corollary 4.3.3
\item Theorem 4.3.4
\item Lemma 4.3.5
\item Theorem 4.3.7
\item Theorem 4.4.2
\item The restatements of column/row space results in terms of rank results in Section 4.4.b
\item Theorem 4.4.8
\item Lemma 4.5.1
\item Lemma 4.5.4
\item Lemma 4.5.7
\item Corollary 4.5.9
\item Theorem 4.4.10
\end{itemize}


\section{Redo}
\begin{itemize}
\item All listed above
\item Theorem 4.4.4
\item Theorem 4.4.9
\item Theorem 4.4.10
\item Lemma 4.5.11
\item Theorem 4.4.1
\item Exercise 4
\item Exercise 5
\item Exercise 6
\item Exercise 7
\item Exercise 9
\item Exercise 10
\item Exercise 11a
\end{itemize}

\section{Important concepts}
\begin{itemize}
\item Linear Space
\item Row/Column space
\item Basis
\item Linear Space
\item Rank
\item Spanning set
\item Dimension
\end{itemize}


\chapter{Trace of a (Square) Matrix}

The chapter gives some basic identities about traces, and then a very important equivalence for finding whether a matrix is null, $\mb{0}$.

\section*{Notes}
\begin{itemize}
\item Basic trace identities and some properties (Section 5.2 and 5.3)
	\begin{itemize}
	\item $\tr(k\mb{A}) = k \tr(\mb{A})$
	\item $\tr(\mb{A+B}) = \tr(\mb{A})+\tr(\mb{B})$
	\item $\tr(\mb{A'A})\geq 0$
	\item $\tr(\mb{A}') = \tr(\mb{A})$
	\item $\tr(\mb{AB}) = \tr(\mb{B'A'}) = \tr(\mb{BA}) =  \tr(\mb{A'B'})$
	\item The last leads to allowing any cyclical rotations: $\tr(\mb{ABC}) = \tr(\mb{CAB}) = \tr(\mb{BCA})$\\
	(It is easy to remember that only cyclicals are allowed\----the matrices have to be multipliable)
	\item \textbf{L5.3.1}: $\mb{A} = \mb{0} \iff \tr(\mb{A'A})=0$ \\
	\textbf{C5.3.2}: $\mb{A=0} \iff \mb{A'A=0}$ \\
	\textbf{C5.3.3}: $\mb{AB=AC} \iff \mb{A'AB=A'AC}$ and (transposed) $\mb{BA'=CA'} \iff \mb{BA'A=A'AC}$ 
	\end{itemize}
\end{itemize}

\section*{Redo}
\begin{itemize}
\item Prove Corollary 5.3.2
\item Prove Corollary 5.3.3
\item Exercise 3
\end{itemize}

\chapter{Geometrical Considerations}
Geometry is very important because it allows for an intuitive understanding of many linear algebra concepts. There are two opinions about those who teach and write about linear statistical models. Those who advocate the ``geometrical approach'' emphasize its elegance and intuitiveness, and those who advocate a more ``algebraic approach'' highlight its rigour and the fact that it's more suggestive of computational approaches.


\section*{Important concepts}
\begin{itemize}
\item Inner (or dot) Product (the ``usual'' one): $\mb{x}\cdot\mb{y} = \mb{x'y}$ for vecs $\mb{A'\cdot B} = \tr(\mb{AB})$ for mats
\item Norm: $||\mb x|| = (\mb{x'\cdot x})^{1/2}$
\item Angle: $\cos \theta = \frac{\mb{x \cdot y}}{||\mb{x}|| ||\mb{y}||}$ (similar for matrices)
\item Distance: $||\mb{A-B}||$
\item Orthogonality
\end{itemize}

\section*{Notes}
\begin{itemize}
\item Schwarz Inequality: $|\mb{A\cdot B}|\leq ||\mb{A}||\,||\mb{B}||$
\item Gram-Schmidt Orthogonalization: $x_{ij}=\frac{\mb{A}_j\cdot \mb{B}_i}{\mb{B}_i \cdot \mb{B}_i}$
\item Any linear space has an orthonormal basis (T6.4.3): Follows from T4.3.9 and the fact that we can apply Gram-Schmidt Orthogonalization to any set of bases 
\item QR decomposition for (full-column matrix): $\mb{A=BX=(BD)(EX)=QR}$, where cols of $\mb Q$ are orthonormal and $\mb{R}$ is upper triangular; cols of $\mb B$ are only orthogonal and $\mb{D,E}$ are diagonal matrices such that (clearly) $\mb D^{-1} = \mb E$ (See Exercise 5 for generalization of QR decomposition to column rank deficient matrices.)
\item The four necessary properties for qualifying as inner product:
	\begin{enumerate}
	\item $\mb{A \cdot B = B \cdot A}$
	\item $\mb{A \cdot A}\ge 0$  with equality holding iff $\mb{A}=0$
	\item $(k\mb{A})\cdot \mb{B} = k(\mb{A\cdot B})$
	\item $(\mb{A+B})\cdot \mb{C} = \mb{(A \cdot C) + (B \cdot C)}$
	\end{enumerate}
\end{itemize}

\section*{Redo}
\begin{itemize}
\item Schwarz Inequality
\item Exercise 1
\item Exercise 5
\end{itemize}

\chapter{Consistency and Compatibility}
This chapter is about the two important concepts in the title that relate to whether a linear system has solution(s). Finding the solution(s) is the topic of Chapter 11.

\section*{Important Concepts}
\begin{itemize}
\item Linear system (in $\mb{x}$) is: $\mb{Ax=B}$. $\mb A$ is the \textit{coefficient matrix}, $B$ is the \textit{right side} and any $x$ that satisfies the equality is a solution. 
\item A linear system is called \textit{homogeneous} if $\mb{B=0}$ and \textit{non-homogeneous} otherwise.
\item A system is called \textit{consistent} if it has one or more solutions.
\end{itemize}

\section*{Notes}
\begin{itemize}
\item \textbf{T7.2.1} gives four necessary and sufficient conditions for consistency. All four can be derived from the first condition: $\mathcal{C}(\mb{B}) \subset \mathcal{C}(\mb{A})$ (and then using L4.2.2, L4.5.1 and C4.5.2).
\item \textbf{T7.2.2}: A system is \textit{consistent} if $\mb{A}$ has full row rank.
\item A linear system is \textit{compatible} if $\mb{k'B=0}$ for every $\mb k'$ such that $\mb{k'A=0}$
\item \textbf{T7.3.1} is the the chapter's main theorem that connects the two main concepts: A linear system is compatible iff it is consistent
\item \textbf{T7.4.1} connects the chapter with the Trace chapter; specifically with C5.3.3. This leads to important equivalences between $\mb{A'A}$ and $\mb A$ in terms of column/row spaces and ranks (see Section 7.4.b).
\item \textbf{L7.4.5}. For any $\mb{A}$, $\mathcal{C}(\mb{A'A})=\mathcal{C}(\mb{A}')$, $\mathcal{R}(\mb{A'A})=\mathcal{R}(\mb{A})$ and $\rank(\mb{A'A})=\rank(\mb{A})$.
\end{itemize}


\section*{Redo}
\begin{itemize}
\item Prove T7.2.1 and T7.2.2
\item Prove T7.3.1
\item Prove T7.4.1
\item Prove all T's/C's in Section 7.4.b: this will help with consolidating parts of Chapter 4.
\item Prove T7.4.8
\item Exercise 1b
\end{itemize}




\chapter{Inverse Matrices}
The chapter introduces the (left, right) inverses and their basic properties. Orthogonal matrices are also introduced. There is a detailed treatment on the use of partitioned matrices for computing (i) the rank and (ii) the inverse. Specifically, if one of the diagonal blocks of a $2\times 2$ partitioning is non-singular, then computing the rank and inverse becomes easy; the larger the non-singular block the easier to compute the rank/inverse. We can use permutation matrices to make the non-singular block as large as possible and thus facilitate computations (see example in \#103).


\section*{Basic identities}
\begin{itemize}
\item The following assume that $\mb{A,B}$ are non-singular $n\times n$ matrices
	\begin{itemize}
	\item $(k\mb A)^{-1}=(1/k)\mb A^{-1}$
	\item $(\mb{AB})^{-1}=\mb{B}^{-1} \mb{A}^{-1}$ 
	\item $(\mb A^{-1})' = (\mb A')^{-1}$
	\item $\rank(\mb{AB}) = n$
	\end{itemize}
\item The following assume that $\mb Q, \mb Q_i$ are \textit{orthogonal}
	\begin{itemize}
	\item $\mb{Q}\mb{Q}'=\mb{Q}'\mb{Q}=\mb I$
	\item $\mb{Q}^{-1}=\mb{Q}'$
	\item $\mb{Q}_1\mb{Q}_2\hdots\mb{Q}_k$ is orthogonal
	\end{itemize}
\end{itemize}

\section*{Important Concepts}
\begin{itemize}
\item Left inverse (LI), right inverse (RI), inverse (\aka left \textit{and} right inverse)
\item Orthogonal matrix: A matrix is orthogonal if its columns form an \textit{ortho\textbf{normal}} set.
\item Permutation matrices
\item Schur complement of a paritioned matrix $\begin{bmatrix}
\mb W & \mb V \\ \mb U & \mb T
\end{bmatrix}$ where $\mb T$ is non-singular is $\mb{Q=W-VT^{-1}U}$. It is used, among others, when computing inverses (T8.5.11)
\end{itemize}


\section*{Notes}
\begin{itemize}
\item \textbf{L8.1.1} is used very widely: (i) $\mb A$ has RI $\iff$ $\mb A$ has full row rank, and (ii) $\mb A$ has LI $\iff$ $\mb A$ has full col rank. (Remember: \textbf{R}ight inverse if full \textbf{r}ow rank.)
\item \textbf{L8.3.2} If $\mb A$ is of full column rank, then $\mathcal{R}(\mb{AB})=\mathcal{R}(\mb B)$ and $\rank(\mb{AB})=\rank(\mb B)$. If $\mb{B}$ is of full row rank, then $\mathcal{C}(\mb{AB})=\mathcal{C}(\mb A)$ and $\rank(\mb{AB})=\rank(\mb{A})$.
\item \textbf{C8.3.3} If $\mb A$ non-singular, then $\rank(\mb{AB})=\rank(\mb B)$ (i.e. rank limited by $\mb B$). 
\item Helmert matrix computes an orthogonal matrix from one vector whose none element is $0$.
\item Permutation matrices can become very convenient for computing rank (see L8.5.1 and S8.5.f)
\item Block-triangular matrices are convenient for computing inverses (see L8.5.4, T8.5.11)
	\begin{itemize}
	\item The easiest is with block-diagonal matrices $\begin{bmatrix}
	\mb{T} & \mb 0 \\ \mb 0 & \mb W
	\end{bmatrix}^{-1} = \begin{bmatrix}
	\mb{T}^{-1} & \mb 0 \\ \mb 0 & \mb W^{-1}
	\end{bmatrix}$
	\item Then with unit block triangular matrices, L8.5.2, $\begin{bmatrix}
	\mb{I} & \mb 0 \\ \mb V & \mb I
	\end{bmatrix}^{-1} = \begin{bmatrix}
	\mb{I} & \mb 0 \\ -\mb V & \mb I
	\end{bmatrix}$ (sim. for upper up. matrix)
	\item Next, L8.5.4, $\begin{bmatrix}
\mb W & \mb V \\ \mb 0 & \mb T
\end{bmatrix} = \begin{bmatrix}
\mb W^{-1} & \mb -\mb{W^{-1}VT^{-1}} \\ \mb 0 & \mb T^{-1}
\end{bmatrix}$  (sim. for upper low. matrix)
	\item Generalization (S8.5.d): The lemmas above lead to generalizations (through induction) and recursive algorithms for computing the inverses of matrices partitioned to larger than $2\times 2$ submatrices
	\end{itemize}
\item \textbf{T8.5.10}: If $\mb T$ is $m\times m$ and nonsingular, then $\rank\left(\begin{matrix}
	\mb T & \mb U \\ \mb V & \mb W
	\end{matrix}\right) = m + \rank(\mb{W-VT^{-1}U})$
\item \textbf{T8.5.11}: Inverse of non-block-triangular part. m. $\begin{bmatrix}
\mb W & \mb V \\ \mb U & \mb T
\end{bmatrix}$, given that $\mb T$ is non-singular, is computed using the results for block-triangular matrices and the Schur complement, $\mb{Q}$ (see def of $\mb{Q}$ above):
	\begin{equation}
	\begin{bmatrix}
	\mb{T} &  \mb{U} \\
	\mb{V} & \mb{W}
	\end{bmatrix}^{-1} = 
	\begin{bmatrix}
	\mb{T}^{-1} + \mb{T}^{-1}\mb{UQ}^{-1}\mb{VT}^{-1} & -\mb{T}^{-1}\mb{UQ}^{-1} \\
	-\mb{Q}^{-1}\mb{VT}^{-1} 	&  	\mb{Q}^{-1}
	\end{bmatrix}
	\end{equation}
\item The same theorem is used to establish the non-singularity of the part. m.
\item Summary on how to compute (i) the rank or (ii) the inverse of a part. m. $\begin{bmatrix}
\mb{A}_{11} & \mb{A}_{12} \\ \mb{A}_{21} & \mb{A}_{22}
\end{bmatrix}$, given that either $\mb{A}_{11}$ or $\mb{A}_{22}$ is non-singular (see \#101).
	\begin{itemize}
	\item Rank: (1) Invert $\mb{A}_{11}$ or $\mb{A}_{22}$, (2) form the Schur complement, (3) find the rank of the Schur complement
	\item Inverse: (1) Compute $\mb{A}^{-1}_{11}$ or $\mb{A}^{-1}_{22}$, (2) compute the Schur complement, (3) use formula (8.5.16) or (8.5.17).
	\end{itemize}
\end{itemize}

\section*{Redo}
\begin{itemize}
\item Prove L8.1.1
\item Prove L8.4.1
\item Prove L8.5.3
\item Prove L8.5.4
\item Prove T8.5.5
\end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Generalized Inverses}

This chapter is about the finding inverses for matrices that are not square or nonsingular. Those so-called generalized inverses apparently become ubiquitous in statistical analysis and possibly other disciplines. The results here are used heavily in Chapter 11 where solutions to linear systems are discussed.


\section*{Important Concepts}
\begin{itemize}
\item A \textit{generalized inverse} (GI) of an $m\times n$ matrix $\mb A$ is any $n\times m$ matrix $\mb G$ such that $\mb{AGA=A}$. A GI of $\mb A$ is typically denoted as $\mb A^-$
\end{itemize}

\section*{Basic Identities and Properties}
\begin{itemize}
\item $(1/k)\mb{A}^-$ is a GI of $k\mb A$ (L9.3.1)
\item $(\mb A^-)'$ is a GI of $\mb A'$ (L.9.3.3)
\item If $\mb A$ is symmetric, then $(\mb{A}^-)'$ is a GI of $\mb A$ (a weaker prop compared to nonsingular $(\mb A^{-1})' = \mb A^{-1} $)
\item $\rank(\mb{A^-A}) = \rank(\mb{AA^-}) = \rank(\mb A)$ (C9.3.8)
\item \textbf{L9.3.5}: $\mathcal{C}(\mb B)\subset \mathcal{C}( \mb A) \iff \mb{B=AA^-B}$ and $\mathcal{R}(\mb C)\subset \mathcal{R}( \mb A) \iff \mb{C=CA^-A}$ \\ (A weaker prop compared to nonsingular $\mb{AA^{-1}=I}$ or, equivalently, $\mb{AA^{-1}-I=0}$)
	\begin{itemize}
	\item This lemma immediately relates to the consistency of a linear system:
	\item \textbf{L9.5.1}: A linear system $\mb{AX=B}$ is consistent iff $\mb{AA^-B=B}$
	\end{itemize}
\end{itemize}

\section*{Notes}
\begin{itemize}
\item Existence of GI:
	\begin{itemize}
	\item \textbf{L9.1.4}: Every matrix has at least one GI
	\item \textbf{L9.1.1}: A nonsingular matrix has exactly one GI (\ie its inverse)
	\item A matrix that is not nonsingular has infinite numbers of matrices (see last para of \#114)
	\end{itemize}
\item On computing a GI of a matrix: The theorems in Sec 9.2.a give a way to compute a GI through partitioning
\begin{itemize}
	\item \textbf{T9.2.1}: Let $\mb A$ be $m\times n$ with rank $r$ and partition it, $\mb A=\begin{bmatrix} \mb A_{11} & \mb A_{12} \\ \mb A_{21} & \mb A_{22} \end{bmatrix}$. Suppose $\mb A_{11}$ is nonsingular. Then, $\mb G$ is a GI of $\mb A$ iff 
	$\mb{G}=\begin{bmatrix}
	\mb A^{-1}_{11}-\mb{X}\mb A_{21} \mb A^{-1}_{11}-\mb A^{-1}_{11} \mb A_{12} \mb Y - \mb A^{-1}_{11} \mb A_{12} \mb{ZA}_{21}\mb A^{-1}_{11} & \mb X \\
	\mb Y & \mb Z
	\end{bmatrix}$ for some $\mb{X, Y}$ and $\mb Z$ (of appropriate dimensions)
	\item The theorem above becomes particularly powerful when combined with T9.2.3
	\item \textbf{L9.2.2}: Let $\mb{A}$ be $m\times n$ partitioned as $\mb{A} = \begin{bmatrix} \mb{A}_{11} & \mb A_{12} \\ \mb{A}_{21} & \mb{A}_{22} \end{bmatrix}$ such that $\rank(\mb{A}_{11})=r$. Then, $\mb{A}_{22}=\mb{A}_{21}\mb{A}_11^{-1}\mb{A}_{12}$.
	\item \textbf{T9.2.3}: Let $\mb A$ be $m\times n$ with rank $r$. Let $\mb B$ be obtained from $\mb A$ by permuting its rows/columns, \ie $\mb{B=PAQ}$, so that $\mb{B}_{11}$, the principal leading $r\times r$ submatrix of $\mb{B}$, is of rank $r$. Then, $\mb G$ is GI of $\mb B$ iff 
	$\mb{G}=\mb Q \begin{bmatrix}
	\mb B^{-1}_{11}-\mb{X}\mb B_{21} \mb B^{-1}_{11}-\mb B^{-1}_{11} \mb B_{12} \mb Y - \mb B^{-1}_{11} \mb B_{12} \mb{ZB}_{21}\mb B^{-1}_{11} & \mb X \\
	\mb Y & \mb Z
	\end{bmatrix}\mb P$
	\item The theorems above can be used to define a 5-step procedure to compute a GI of $\mb A$: 1) find the rank of $\mb A$, $r$; 2) locate the $r$ independent rows and columns of $\mb A$; 3) Form a submatrix of $\mb A$, $\mb B_{11}$, by striking out all except those $r$ rows and columns; 4) Compute the inverse of $\mb B_{11}$; 5) Use T9.2.1 by setting $\mb{X,Y,Z}$ to zero.
	\end{itemize}
	\item Obtaining \textit{every} GI in terms of a particular GI: There us an elegant theorem that provides two formulae for this purpose
	\begin{itemize}
		\item \textbf{T9.2.7}: Let $\mb A$ be $m\times n$ and $\mb G$ a particular inverse. Then, a matrix $\mb G^*$ is a GI of $\mb A$ iff \\
	$\mb G^* = \mb{G+Z-GAZAG}$ for some $\mb Z$. Also, $\mb G^*$ is a GI iff \\
	$\mb{G^*=G+(I-GA)T+S(I-AG)}$ for some matrices $\mb T$ and $\mb S$.
		\item By letting $\mb Z$ or $\mb{T, S}$ vary over all matrices, one can compute all GI's from one $\mb G$.
	\end{itemize}
	\item Obtaining a matrix invariant to the choice of GI
	\begin{itemize}
	\item \textbf{T9.4.1}: 	Let $\mb{A,B,C}$ be matrices such that $\mathcal{R}(\mb B) \subset \mathcal{R}(\mb A)$ and $\mathcal{C}(\mb C) \subset \mathcal{C}(\mb B)$, then $\mb{BA^-C}$ is invariant to the choice of $\mb A^-$. (Nearly-converse holds too.)
	\end{itemize}
	\item The Schur treatment for GIs on Partitioned Matrices
	\begin{itemize}
	\item A treatment similar to that of computing the inverse (see Section 8.5) applies to GIs
	\item The Schur complement is now defined as $\mb{Q=W-VT^{-1}U}$. 
	\item \textbf{T9.6.1}: Computing the rank (v similar to T8.5.10). Suppose $\mathcal{C}(\mb U) \subset \mathcal{C}(\mb T)$ and $\mathcal{R}(\mb V) \subset \mathcal{C}(\mb T)$. Then, $\rank\left(\begin{matrix}
	\mb T & \mb U \\ \mb V & \mb W
	\end{matrix}\right) = \rank(\mb T) + \rank(\mb Q)$. Moreover, the GI of the partitioned matrix can be computed in terms of the GI of $\mb T^{-}, \mb Q^{-}, \mb V, \mb U$ (\#121). The bottom right of the GI is $\mb Q^-$
	\item \textbf{T9.6.4}: The bottom right of \textit{any} GI of the above partitioned matrix is a GI of $\mb Q$.
	\end{itemize}
\item Comparison of the 5-step algorithm (above) and the Schur way: 5-step requires finding a basis that spans $\mb A$, whereas Schur-way requires $\mathcal{C}(\mb U) \subset \mathcal{C}(\mb T)$ and $\mathcal{R}(\mb V) \subset \mathcal{C}(\mb T)$.
\end{itemize}


\section*{Important Lemmas and Theorems}
\begin{itemize}
\item \textbf{L9.3.5}: See about $\mb{B=AA^-B}$ -- see definition above in Basic Identities and Properties
\item \textbf{T9.2.7}: On computing all GIs from a particular GI (above)
\item \textbf{T9.4.1}: Obtaining an matrix invariant to the choice of GI
\end{itemize}


\section*{Redo}
\begin{itemize}
\item Prove T9.1.2
\item Prove T9.2.7
\item Prove T9.4.1
\item Prove T9.6.4
\item Solve Exercise 7
\item Solve Exercise 12a
\end{itemize}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Idempotent Matrices}
Apparently the class of idempotent matrices is very important in statistics and other disciplines. They do have some very interesting and non-obvious properties (\eg L10.1.1, L10.1.2 or T10.2.1)

\section*{Definition and Some Properties}
\begin{itemize}
\item $\mb A$ is said to be idempotent if $\mb A^2 = A$
\item The only $n\times n$ idempotent matrix is $\mb I_n$ (L10.1.1)
\item $\mb A'$ is idempotent $\iff$ $\mb A$ is idempotent (L10.1.2)
\item $\mb A$ is idempotent $\iff$ $\mb{I-A}$ is idempotent (L10.1.2)
\end{itemize}

\section*{Notes}
\begin{itemize}
\item One of the most non-intuitive relations is through the following theorem (and its corollary):
	\begin{itemize}
	\item \textbf{T10.2.1}: For any square matrix $\mb A$ such that $\mb A^2=k \mb A$, $\tr(\mb A) = k\, \rank(\mb A)$
	\item \textbf{C10.2.2}: For any idempotent matrix $\mb A$, $\rank(\mb A) = \tr(\mb A)$ (I guess I find this non-intuitive because the rank \---an integer\--- is computed with $\tr$ which does not in general yield an integer!)
	\item \textbf{T10.2.7}: $\mb B$ is GI of $\mb A$ iff $\mb{BA}$ is idempotent and $\rank(\mb {BA}) = \rank(\mb A)$ (This also is non-intuitive b/c idempotent matrices seem like they are rare but yet so easy to obtain from a matrix and its inverse!)
	\item \textbf{L10.2.4}: For any idempotent matrix, $\rank(\mb{I-A})=\tr(\mb{I-A})=n-\rank(\mb{A})$.
	\end{itemize}
\end{itemize}


\section*{Redo}
\begin{itemize}
\item Prove T10.2.1
\item Prove T10.2.7
\item Solve Exercise 3
\end{itemize}












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Linear Systems: Solutions}



\section*{Notes}
\begin{itemize}
\item Relationships between $\mathcal{N}(\mb{A})$ and $\mathcal{C}(\mb{A})$ and $\mathcal{C}(\mb{X})$
	\begin{itemize}
	\item $\mathcal{N}(\mb{A}) \subset \mathcal{C}(\mb{I-A})$
	\item $\mathcal{N}(\mb{I-A}) \subset \mathcal{C}(\mb{A})$ (p140)
	\item \textbf{C11.2.2}: $\mathcal{N}(\mb{A})=\mathcal{C}(\mb{I-A^-A})$ 
	\item (1) If $\mb{AX=0}$, then $\mathcal{C}(\mb{X})\subset \mathcal{N}(\mb{A})$. (2) If also $\rank(\mb{X})=n-\rank(\mb{A})$, then $\mathcal{C}(\mb{X})=\mathcal{N}(\mb{A})$
	\item \textbf{T11.7.1} Let $\mb{A}$ be $n\times n$.  Then, $\mathcal{N}(\mb{A})=\mathcal{C}(\mb{I-A}) \iff \mb{A}$ is idempotent.
	\end{itemize}
\item General forms of solutions (useful for finding/checking alternative solutions etc.)
	\begin{itemize}
	\item \textbf{T11.2.1} (Homogeneous): $\mb{X}^*$ is solution to homogeneous LS $\mb{AX=0} \iff \mb{X}^{*}=(\mb{I-A^- A})\mb{Y}$ for some $\mb{Y}$ 
	\item \textbf{T11.2.3} (Nonhom.): $\mb{X}_0$ is solution to $\mb{AX=B}$. $\mb{X}^{*}$ is sol. $\iff \mb{X}^*=\mb{X}_0+\mb{Z}^*$ for some $\mb{Z}^*$ sol. to $\mb{AZ=0}$
	\item \textbf{T11.2.4} (Nonhom.): $\mb{X}^{*}$ is solution to $\mb{AX=B} \iff \mb{X}^*=\mb{A^-B}+(\mb{I-A^-A})\mb{Y}$ for some $\mb{Y}$
	\item \textbf{T11.5.1}: Let $\rank(\mb{A})=n$, $\rank(\mb{B})=p$. Then, $\mb{X}^*$ is solution to $\mb{AX=B} \iff \mb{X}^*=\mb{A^-B}$ for some GI $\mb{A^-}$
	\end{itemize}
\item Number of solutions
	\begin{itemize}
	\item \textbf{L11.3.1} (homogeneous, vec.): $\mb{A}$ is $m\times n$. Then, $\dim\left[\mathcal{N}(\mb{A})\right]=n-\rank(\mb{A})$. Note: system is $\mb{Ax=0}$
	\item \textbf{L11.3.2} (homogeneous, mat.) The dimension of solution space for $\mb{AX=0}$ (in $n\times p$ matrix $\mb{X}$) is $p[n-\rank(\mb{A})]$
	\item Summary based on p142-143:
		\begin{itemize}
		\item (homog.) If $\mb{A}$ has full col. rank, $\mb{AX}=\mb{0}$ has \textit{one} solution (\ie the null mat. $\mb{0}$)
		\item (homog.) If $\mb{A}$ is col. rank deficient, then $\mb{AX}=0$ has infinite solutions
		\item (nonhom.) Since there is one-to-one correspondence between solutions to $\mb{AX=0}$ and sol.s to $\mb{AX=B}$, the two above apply to nonhomogeneous case too given that the nonhom. system is consistent
		\end{itemize}
	\end{itemize}
\item Equivalence of linear systems: When are the solution sets of two linear systems the same?
	\begin{itemize}
	\item \textbf{L11.6.1.} If $\mb{C}$ is of full col. rank, then the systems $\mb{AX=B}$ and $\mb{CAX=CB}$ are equivalent
	\item \textbf{L11.6.2.} For any $\mb{A}$, the linear systems $\mb{A'AX=A'AF}$ and $\mb{AX=AF}$ are equivalent
	\item \textbf{Exercise 6.} If $\rank(\mb{CD})=\rank(\mb{D})$, then system $\mb{CDAX}=\mb{CDB}$ is equivalent to the system $\mb{DAX}=\mb{DB}$
	\item \textbf{Exercise 7.} If $\rank[\mb{C(A,B)}]=\rank(\mb{A,B})$, then the systems $\mb{AX=B}$ and $\mb{CAX=CB}$ are equivalent
	\end{itemize}
\item Can we derive a vector/matrix $\mb{K'X}$ invariant to the choice of solution $\mb{X}$?
	\begin{itemize}
	\item \textbf{T11.10.1}. Let $\mb{K}$ be $n\times q$. Then, $\mb{K'X}$ is invariant to solution $\mb{X} \iff$ $\mathcal{R}(\mb{K}') \subset \mathcal{R}(\mb{A})$
	\item \textbf{T11.10.3}. Let $\mb{X,Y}$ be solutions to $\mb{AX=B}$ and $\mb{A'Y=K}$. Then, $\mb{K'X}_0=\mb{Y'}_0\mb{B}$ for any solutions $\mb{X}_0,\mb{Y}_0$.
	\end{itemize}
\item Approaches to solve linear systems
	\begin{itemize}
	\item If $\mb{A}$ is upper block-triangular such that the diagonal blocks $\mb{A}_{ii}$ are non-singular, then back-substitution (Section 11.8)
	\item If $\mb{A}$ is lower block-triangular such that the diagonal blocks $\mb{A}_{ii}$ are non-singular, then forward elimination
	\item Find a decomposition $\mb{A=KT}$ such that solving becomes easier. More on Ch. 14 and 21 (p.149)
	\item \textbf{T11.11.1}. Absorption: see Section p.152 and 4-step summary in p. 153
	\end{itemize}
\end{itemize}





















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Projections and Projection Matrices}


\section*{Definitions and some properties}

\begin{itemize}
\item \textit{Orthogonality}: If matrix $\mb{Y}$ is orthogonal to every matrix in a subspace $\mathcal{U}$, then $\mb{Y}$ is orthogonal to $\mathcal{U}$: $\mb{Y} \perp \mathcal{U}$.
\item \textit{Projection}: Let $\mb{Y} \in \mathcal{V}$, $\mb{Z}\in\mathcal{V}$ and $\mathcal{U} \subset \mathcal{V}$. $\mb{Z}$ is the projection of $\mb{Y}$ on $\mathcal{U}$ if $(\mb{Y-Z}) \perp \mathcal{U}$\\\hspace*{1.75cm} For any $\mb{Y} \in \mathcal{U}$, $\mb{Y}$ itself is the projection of $\mb{Y}$ on $\mathcal{U}$
\item The \textit{orthogonal complement} $\mathcal{U}^\perp$ of $\mathcal{U} \subset \mathcal{V}$ is the set of all matrices in $\mathcal{V}$ that are orthogonal to $\mathcal{U}$ 
	\begin{itemize}
	\item $\mathcal{U} = (\mathcal{U}^\perp)^\perp$ (T12.5.4)
	\item $\mathcal{W}\subset \mathcal{U} \iff \mathcal{U}^\perp \subset \mathcal{W}^\perp$ (C12.5.6)
	\item $\dim(\mathcal{V})=\dim(\mathcal{U})+\dim(\mathcal{U}^\perp)$ for $\mathcal{U}\subset \mathcal{U}^\perp$ (T12.5.12)
	\end{itemize}
	\item \textit{Projection matrix}: $\mb{P_{X}}:=\mb{X(X'X)^-X'}$. That is, $\mb{P}_{\mb{X}}\mb{y}$ is the projection of any $\mb{y} \in \mathcal{R}^n$ on $\mathcal{C}(\mb{X})$
	\begin{itemize}
	\item $\mb{P}_\mb{X}\mb{X}=\mb{X}$ (T12.3.4)
	\item $\mb{P}_\mb{X}' = \mb{P}_\mb{X}$ (\ie symmetric, T12.3.4)
	\item $\mb{P}_\mb{X}^2 = \mb{P}_\mb{X}$ (\ie idempotent, T12.3.4)
	\item $(\mb{I-P}_\mb{X})$ is also idempotent and symmetric
	\item $(\mb{I-P}_\mb{X})$ is the projection matrix for $\mathcal{C}^\perp(\mb{X})$ (C12.5.10)
	\end{itemize}
\end{itemize}

\section*{Notes}
\begin{itemize}
\item Orthogonality conditions
	\begin{itemize}
	\item \textbf{L12.1.1}. $\mb{Y}\perp\mathcal{U}=\text{sp}(\mb{X}_1,\hdots,\mb{X}_k) \iff \mb{Y\cdot X}_i$ for $i=1,...,k$. $\mathcal{U\perp V}=\text{sp}(\mb{Z}_1,\hdots,\mb{Z}_t) \iff \mb{X}_i \perp \mb{Z}_j \,\,\forall i,j$.
	\item \textbf{C12.1.2}. $\mb{y}\, (\in \mathcal{R}^m) \perp \mathcal{C}(\mb{X}) \iff \mb{X}'\mb{y}=0$
	\item \textbf{T12.1.3}. Let $\mb{Y} \in \mathcal{V}$, $\mathcal{U}\subset \mathcal{V}$ and $\text{dim}(\mathcal{U})=r$. Then, there is \textit{unique} $\mb{Z} \in \mathcal{U}$ s.t. $(\mb{Y-Z})\perp \mathcal{U}$. Let $\{\mb{X}_1,\hdots,\mb{X}_r\}$ be an orthonormal basis for $\mathcal{U}$. Then, $\mb{Z}=c_1\mb{X}_1+\hdots+c_r\mb{X}_r$ where $c_i=\mb{X\cdot Y}_i$. Moreover, $\mb{Z=Y} \iff \mb{Y}\in\mathcal{U}$. 
	\end{itemize}
\item Projection of column vector and Projection Matrix
	\begin{itemize}
	\item \textbf{T12.2.1}. $\mb{z}$ is the projection of $\mb{y}\in\mathcal{R}^n$ on $\mathcal{U} \subset \mathcal{R}^n \implies \mb{z}=\mb{Xb}^*$ for any solution $\mb{b}^*$ to $\mb{X}'\mb{Xb}=\mb{X}'\mb{y}$ \\\hspace*{1.455cm} Also (C12.2.2), $\mb{z}=\mb{X}(\mb{X}'\mb{X})^-\mb{X}'\mb{y}$ 
	\item \textbf{T12.3.1} (uniqueness). Let $\mathcal{U} \subset \mathcal{R}^n$. There is \textit{unique} $n\times n$ matrix $\mb{A}$ s.t. $\mb{Ay}$ is the projection of any $\mb{y} \in \mathcal{R}^n$ on $\mathcal{U}$. For any $\mb{X}$ s.t. $\mathcal{C}(\mb{X})=\mathcal{U}$, $\mb{A}=\mb{P}_\mb{X}$. (see def. of $\mb{P}_\mb{X}$ above)
	\item \textbf{T12.3.4} (other outcomes of this theorem are properties above). $\mb{P}_\mb{X}=\mb{X}\mb{B}^*$ for any solution to $\mb{X}'\mb{XB}=\mb{X}'$
	\item \textbf{T12.5.8} (orth. compl). Let $\mb{Y}\in\mathcal{V},\, \mathcal{U}\subset\mathcal{V}$. Projection of $\mb{Y}$ on $\mathcal{U}^\perp$ is $\mb{Y-Z}$ where $\mb{Z}$ is proj. of $\mb{Y}$ on $\mathcal{U}$
	\item \textbf{C12.5.10}. $(\mb{I-P}_\mb{X})$ is the projection matrix for $\mathcal{C}^\perp(\mb{X})$
	\item \textbf{T12.5.11}. Let $\mb{Y} \in \mathcal{V},\,\mathcal{U}\subset\mathcal{V}$. $\exists$ \textit{unique} $\mb{Z}\in \mathcal{U},\,\mb{W}\in\mathcal{U}^\perp$ s.t. $\mb{Y}=\mb{Z+W}$. Also $\mb{Z}$ ($\mb{W}$) is proj. of $\mb{Y}$ on $\mathcal{U}$ ($\mathcal{U}^\perp$).
	\end{itemize}
\item Least squares connection
	\begin{itemize}
	\item \textbf{T12.4.1}. Let $\mb{Y} \in \mathcal{V}$ and $\mathcal{U}\subset V$. Then, for $\mb{W}\in\mathcal{U}$, $||\mb{Y-W}||$ is minimized by $\mb{W}=\mb{Z}$ where $\mb{Z}$ is $\mb{Y}$'s projection on $\mathcal{U}$. Moreover, $||\mb{Y-Z}||^2=\mb{Y\cdot(Y-Z)}$.
	\item \textbf{T12.4.2} (Residue). For a column vector projection $\mb{z}$, the residue $||\mb{y}-\mb{z}||^2$ is $\mathbf{y}'(\mb{I-\mb{P}_\mb{X}})\mb{y}$
	\end{itemize}
\end{itemize}




















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Determinant}


\section{Definitions and some properties}

\begin{itemize}
\item \textit{Determinant} of $\mb{A}$: $\det(\mb{A})=|\mb{A}| = \sum\limits_{\mb{j}\in \mathcal{J}} (-1)^{\phi_n(j_1,\hdots,j_n)} a_{1j_1}\hdots a_{nj_n}$ where $\mb{j}=(j_1,\hdots,j_n)$ and $\mathcal{J}$ is the set that contains all permutations of first $n$ positive integers, and $\phi(\cdot)$ is, \eg $\phi_5(3,7,2,1,4) = 2+3+1+0=6$ (p178)
	\begin{itemize}
	\item $|\mb{A}'|=|\mb{A}|$ (L13.2.1)
	\item If $\mb{B}$ is same with $\mb{A}$ except that one of its rows/columns is multiplied with $k$, then $|\mb{B}|=k|\mb{A}|$ (L13.2.1)
	\item $|k\mb{A}|=k^n |\mb{A}|$ (C13.2.4)
	\item $|-\mb{A}|=(-1)^n |\mb{A}|$
	\item If $\mb{B}$ is formed from $\mb{A}$ by interchanging 2 of its columns or rows, then $|\mb{B}|=-|\mb{A}|$ (T13.2.6)
	\item If $\mb{B}$ is $n\times p$ and $\mb{C}$ is $n\times q$, then $|\mb{B,C}|=(-1)^{pq}|\mb{B,C}|$ (T13.2.7)
	\item $\mb{|AB|=|A||B|}$, and more generally $|\mb{A}_{11}\mb{A}_{22}\hdots \mb{A}_{rr}|=|\mb{A}_{11}||\mb{A}_{22}|\hdots |\mb{A}_{rr}|$ (T13.3.4)
	\item $|\mb{A}^k|=|\mb{A}|^k$
	\item $|\mb{A'A}|=|\mb{A}|^2$
	\item $\mb{A}$ is nonsingular $\iff |\mb{A}|\neq 0$ (T13.3.7)
	\item If $\mb{A}$ is nonsingular, then $|\mb{A}^{-1}|=1/|\mb{A}|$
\end{itemize}
\item Cofactor, minor and adjoint
	\begin{itemize}
	\item \textit{Minor}. Let $\mb{A}_{ij}$ be the submatrix of $\mb{A}$ obtained by striking out its $i$th row and $j$th column. Then, $|\mb{A}_{ij}|$ is called the minor of the element $a_{ij}$ of $\mb{A}$.
	\item \textit{Cofactor}, $\alpha_{ij}$ is the signed minor: $\alpha_{ij}=(-1)^{i+j} |\mb{A}_{ij}|$	
	\item The \textit{adjoint} of $\mb{A}$ is the (transposed) matrix of cofactors: $\adj(\mb{A})=\begin{bmatrix}
	\alpha_{11} & \hdots & \alpha_{n1} \\ \vdots & \ddots & \vdots \\ \alpha_{1n} & \hdots & \alpha_{nn} \end{bmatrix}$
	\end{itemize}
\item Important properties of adjoint
	\begin{itemize}
	\item $\mb{A} \,\adj(\mb{A})=|\mb{A}|\mb{I}_n$ (T13.5.3)
	\item If $\mb{A}$ is nonsingular, then $\mb{A}^{-1}=(1/|\mb{A}|)\,\adj(\mb{A})$
	\item $\adj(\mb{AB})=\adj(\mb{B})\adj(\mb{A})$ (Exercise 14)
	\end{itemize}
\end{itemize}

\section{Notes}
\begin{itemize}
\item Computation in special cases
	\begin{itemize}
	\item If $\mb{A}$ is (upper or lower) triangular, then $|\mb{A}|=a_{11}a_{22}\hdots a_{nn}$
	\item \textbf{T13.1.1}. $\left|\begin{matrix} \mb{T}  & \mb{0} \\\mb{V} & \mb{W} \end{matrix}\right|=\mb{|T||W|}$. Repeated application $\implies$ For any block triangular $\mb{A}$, $|\mb{A}|=|\mb{A}_{11}||\mb{A}_{22}|\hdots |\mb{A}_{rr}|.$
	\item \textbf{C13.3.2}. $\left|\begin{matrix} \mb{0}  & \mb{T} \\\mb{W} & \mb{V} \end{matrix}\right|=\left|\begin{matrix} \mb{V}  & \mb{W} \\\mb{T} & \mb{0} \end{matrix}\right|=(-1)^{mn}\mb{|T||W|}$ 
	\item If $\mb{T}$ is nonsingular, then $\left|\begin{matrix} \mb{T}  & \mb{U} \\\mb{V} & \mb{W} \end{matrix}\right|=\mb{|T||W-VT^{-1}U|}$	\end{itemize}
\item Some cases where determinants are the same
	\begin{itemize}
	\item \textbf{T13.2.10}. If $\mb{B}$ is formed from $\mb{A}$ by adding to any one row or column of $\mb{A}$, scalar multiples of one or more other rows, then $|\mb{B}|=|\mb{A}|$.
	\item \textbf{T13.2.11}. For any upper/lower \textit{unit} triangular $\mb{T}$, $\mb{|AT|=|TA|=|A|}$
	\end{itemize}
\item Some cases where determinant is $0$ or $\pm 1$
	\begin{itemize}
	\item \textbf{L13.2.9}. If a row or column of $\mb{A}$ is multiple of another row or column, then $|\mb{A}|=0$
	\item \textbf{T13.3.7}. $|\mb{A}|=0 \iff \mb{A}$ is nonsingular (see also properties above)
	\item \textbf{C13.3.6} For any orthogonal matrix $\mb{P}$, $|\mb{P}|=\pm 1$
	\end{itemize}
\end{itemize}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Linear, Bilinear and Quadratic Forms}

\section{Definitions and some properties}
\begin{itemize}
\item Linear, bilinear, quadratic forms
	\begin{itemize}
	\item Linear form:  $\mb{ax}$, linear function: $f(\mb{x})$. $\mb{a}$: coefficient vector of linear form
	\item Bilinear form:  $\mb{x'Ay}$, bilinear function: $f(\mb{x,y})$. $\mb{A}$: matrix of the linear form
	\item Symmetric bilinear form is when $\mb{x'Ay} = \mb{y'Ax}$ for all $\mb{x,y}$.  
	\item Quadratic form: $\mb{x'Ax}$. $\mb{A}$: matrix of the quadratic form.
	\item An $n\times n$ matrix is skew-symmetrix if $\mb{A}'=-\mb{A}$ (which implies $a_{ii}=0$ for $i=1,\hdots,n$).
	\end{itemize}
\item Nonnegative definite quadratic forms
	\begin{itemize}
	\item A quadratic form $\mb{x'Ax}$ (or its matrix $\mb{A}$) is called \textit{nonnegative definite} if $\mb{x'Ax}\ge 0$ for all $\mb{x} \in \mathcal{R}^n$
	\item A quadratic form $\mb{x'Ax}$ (or its matrix $\mb{A}$) is called \textit{positive definite} if $\mb{x'Ax}> 0$ for all $\mb{x} \in \mathcal{R}^n$
	\item A nonnegative quadratic form $\mb{x'Ax}$ (or its matrix $\mb{A}$) is called \textit{positive semidefinite} if there exist nonnull $\mb{x}\in \mathcal{R}^n$ such that $\mb{x'Ax}=0$.
	\item A quadratic form  $\mb{x'Ax}$ (or its matrix $\mb{A}$) is called \textit{nonpositive definite}, \textit{negative definite} or \textit{negative semidefinite} if $-\mb{x'Ax}$ is nonnegative definite, positive definite or positive semidefinite.
	\item A quadratic form that is neither nonnegative nor nonpositive definite is called \textit{indefinite}.
	\end{itemize}
\item Some properties of nonnegative matrices/forms
	\begin{itemize}
	\item If $\mb{A}$ is nonnegative and $k$ is a scalar such that $k>0$, then $k\mb{A}$ is also nonnegative (L14.2.3).
	\item If $\mb{A},\mb{B}$ are nonnegative, then $\mb{A+B}$ is nonnegative. If one of them is positive, then $\mb{A+B}$ is positive. (L14.2.4)
	\item Any positive definite matrix is nonsingular (L14.2.8).
	\item The inverse of a positive definite (semidefinite nonsingular) matrix is positive definite (semidefinite). (C13.2.11)
	\item Every symmetric idempotent matrix is nonnegative definite (L14.2.7).
	\item A symmetric nonnegative definite $\mb{A}$ is positive definite $\iff$ it $\mb{A}$ is nonsingular (C14.3.12).
\end{itemize}
\item Properties regarding traces of nonnegative matrices
\begin{itemize}
	\item $\tr(\mb{A})>0$ for any positive definite $\mb{A}$. (T14.7.1)
	\item $\tr(\mb{A})\ge 0$ for any nonnegative matrix, with equality holding iff $\mb{A}$ is skew-symmetric. (T14.7.2)
	\item For any positive $\mb{A}$ and nonnull symmetric nonnegative $\mb{B}$, $\tr(\mb{AB})>0$. (T14.7.4)
	\item For any positive $\mb{A}$ and symmetric nonnegative $\mb{B}$, $\tr(\mb{AB})\ge 0$ with equality holding iff $\mb{B}=\mb{0}$. (C14.7.5)
	\item For any nonnegative $\mb{A}$ and symmetric nonneg. $\mb{B}$, $\tr(\mb{AB})\ge 0$; equality holds iff $\mb{BAB}$ is skew-symm. (T14.7.6)
	\end{itemize}
\item Properties of determinants of nonnegative matrices
\begin{itemize} 
\item For any symmetric positive $\mb{A}$, $|\mb{A}|>0$; for any symmetric positive semidefinite $\mb{A}$, $|\mb{A}|=0$ (T14.9.1)
\item For nonnegative $\mb{A}$, $|\mb{A}|\ge 0$. For positive definite or nonsingular nonsymmetric positive semidefinite $\mb A$, $|\mb{A}|>0$. For symmetric positive semidefinite or singular nonsymmetric positive semidefinite $\mb A$, $|\mb{A}|=0$ (T14.9.3, C14.9.4)
\end{itemize}
\item Inner products, orthogonality, projections, orthogonal complements
	\begin{itemize}
	\item \textit{Inner product} bw $\mb{x,y}$ wrt a spd. (symmetric and positive definite) matrix $\mb{W}$ is $\mb{x'Wy}$. (see also below)
	\item \textit{Orthogonality}. $\mb{x}\perp_{\mb{W}}\mb{y} \iff \mb{x'Wy}=0 $ for spd. $\mb{W}$.
	\item $\mb{x}\perp_{\mb{W}}\mathcal{U} \iff  \mb{x'Wy}=0 \,\, \forall \mb{y} \in \mathcal{U}$ 
	\item $\mathcal{U}\perp_{\mb{W}}\mathcal{V} \iff  \mb{x'Wy}=0 \,\, \forall \mb{x} \in \mathcal{U}, \mb{y} \in \mathcal{V}$ 
	\item \textit{Projection matrix} wrt spd. $\mb{W}$ is $\mb{P_{X,W}=X(X'WX)^-X'W}$
	\item \textit{Projection} of $\mb{y}\in\mathcal{R}^n$ to $\mathcal{U} \subset \mathcal{R}^n$ wrt spd. $\mb{W}$ is $\mb{z}\in\mathcal{U}$ such that $\mb{y-z}\perp_{\mb{W}}\mathcal{U}$.
	\item \textit{Generalized or weighted least squares problem} is minimizing $\mb{(y-u)'W(y-u)}$ for $\mb{u}$ wrt spd. $\mb{W}$.
	\item \textit{Orthogonal complement} of $\mathcal{C}(\mb{X})$ is $\mathcal{C}^\perp_\mb{W}(\mb{X})=\{\mb{y}\in\mathcal{R}^n : \mb{X'Wy}=0\}=\mathcal{N}(X'W)=\mathcal{C}(\mb{I-P_{X,W}})$
	\item \textit{Gram matrix} (or \textit{Gramian}). Let $\mb{X}_1, \hdots, \mb{X}_r \in \mathcal{V}$. Gramian is the matrix whose $ij$th element is $\mb{X}_{i} \cdot \mb{X}_{j}$ (\ie inner product).
	\end{itemize}
\end{itemize}


\section{Decompositions of matrices and quadratic forms}
This section presents some fundamental decompositions. There is a specific focus on the decomposition of symmetric matrices and positive or nonnegative definite matrices, but there are generic decompositions as well.\\

\noindent\textbf{Any square matrices}. Let $\mb{A}$ be a square matrix.
\begin{itemize}
\item $\exists$ nonsingular $\mb{P}$ and $n$ scalars $d_1, \hdots, d_n$ such that $\mb{x'Ax}=\sum_{i=1}^n d_i y_i^2$ for $\mb{y=Px}$ (C14.3.6)
\item $\mb{LDU}$ decomposition ($\mb{L}$ $(\mb{U})$: lower (upper) unit triangular, $\mb{D}$: diagonal). Let $\mb{A}_i$ be the $i$th order leading principal submatrix of $\mb{A}$, and partition it as $$\mb{A}_i=\begin{bmatrix}
\mb{A}_{i-1} & \mb{a}_i \\ \mb{b}_i' & a_{ii}
\end{bmatrix}.$$ $\mb{A}$ can be expressed as $\mb{A=LDU}$ if for $i=2,\hdots,n$, $\mb{a}_i \in \mathcal{C}(\mb{A}_{i-1})$ and $\mb{b}'_i \in \mathcal{R}(\mb{A}_{i-1})$. (T14.5.4)
\end{itemize}


\noindent\textbf{For symmetric matrices}. Let $\mb{A}$ be a symmetric matrix. 
\begin{itemize}
\item $\exists$ nonsingular $\mb{Q}$ s.t. $\mb{Q'AQ}$ is a diagonal matrix. (T14.3.4)
\item $\exists$ nonsingular $\mb{P}$ and diagonal $\mb{D}$  s.t. $\mb{A}=\mb{P'DP}$. (C14.3.5)
\item $\mb{A}$ has $\mb{U'DU}$ decomposition if $\mb{a}_i \in \mathcal{C}(\mb{A}_{i-1})$ (see LDU decomposition above for notation) (T14.5.4) 
\end{itemize}

\noindent\textbf{For symmetric nonnegative matrices}. Let $\mb{A}$ be an $n\times n$ symmetric nonnegative matrix.
\begin{itemize}
\item $\exists$  $\mb{P}$ s.t. $\mb{A}=\mb{P'P}$. (T14.3.7, C14.3.8 and C14.3.10)
\item $\mb A$ has a U'DU decomposition (with nonnegative diagonal elements). (C14.5.15)
\item Let $r=\rank(\mb{A})$. There is unique upper triangular $\mb{T}$ with $n-r$ null rows s.t. $\mb{A=T'T}$.
\item Let $\mb{A}=\begin{bmatrix}
\mb{T} & \mb U \\ \mb V & \mb W
\end{bmatrix}$. Then, $\exists$ $\mb{R,S}$ such that $\mb{T}=\mb{R'R}$, $\mb{U=R'S}$, $\mb{V=S'R}$, $\mb{W=S'S}$. 
\end{itemize}


\noindent\textbf{For positive definite matrices}. Let $\mb{A}$ be a positive definite matrix.\begin{itemize}
\item $\mb{A}$ has a unique LDU decomposition (T14.5.9).
\end{itemize}

\noindent\textbf{For positive and symmetric matrices}. Let $\mb{A}$ be a positive definite and symmetric matrix.\begin{itemize}
\item $\exists$ nonsingular $\mb{P}$ such that $\mb{A=P'P}$ (C14.3.13).
\item $\mb{A}$ has a unique U'DU decomposition (C14.5.10).
\item \textit{Cholesky decomposition}. $\exists$ unique upper triangular $\mb{T}$ s.t. $\mb{A}=\mb{T'T}$. (T14.5.11)
\end{itemize}







\section{Notes}
\begin{itemize}
\item Equivalence/symmetry of bilinear/quadratic forms
	\begin{itemize}
	\item A bilinear form $\mb{x'Ay}$ is symmetric $\iff$ $\mb{A}$ is a symmetric matrix (p209).
	\item \textbf{L14.1.1}. Two quadratic forms $\mb{x'Ax}$  $\mb{x'Bx}$ are identically equal $\iff$ $\mb{A+A'=B+B'}$ 
	\item \textbf{C14.1.2} For any quadratic form $\mb{x'Ax}$, there is a \textit{unique} symmetric matrix $\mb{B}$ such as $\mb{x'Ax=x'Bx}$, namely $\mb{B}=(1/2)\mb{(A+A')}$
 	\end{itemize}
\item On nonnegative and positive definiteness
	\begin{itemize}
	\item \textbf{T14.2.9}. Let $\mb{A}$ be $n\times n$ and $\mb{P}$ any $n\times m$. (1) If $\mb{A}$ is nonnegative, then $\mb{P'AP}$ is nonnegative. (2) If $\mb{A}$ nonnegative and $\rank(\mb{P})<m$, then $\mb{P'AP}$ is positive semidefinite. (3) If $\mb{A}$ is positive, then $\mb{P'AP}$ is positive.
	\item \textbf{C14.2.10}. If $\mb{P}$ is nonsingular and $\mb{A}$ pos. definite (semidefinite), then $\mb{P'AP}$ is pos. definite (semidefinite).
	\item \textbf{C14.2.12}. Any principal submatrix of a positive definite (semidefinite) matrix is positive (nonnegative) definite.
	\item \textbf{C14.2.13}. The diagonal elements of a positive (nonnegative) definite matrix are positive (nonnegative).
	\item \textbf{C14.2.14}. Let $\textbf{P}$ be any $n\times m$. If $\rank(\mb{P})=m$, $\mb{P'P}$ is positive definite, otherwise, it's positive semidefinite. 
	\item \textbf{C14.2.14}. Let, $\textbf{P}$ be nonsingular and $\mb{D}$ diagonal. Then, (1) $\mb{P'DP}$ is nonnegative (positive) $\iff$ $\mb{D}$'s elements are nonnegative (positive). (2) $\mb{P'DP}$ is positive definite if $\mb D$'s elements are nonnegative and include some zeros.
	\end{itemize}
\item Decomposition of symmetric matrices
	\begin{itemize}
	\item \textbf{T14.3.4}. For any $n\times n$ symmetric $\mb{A}$, $\exists$ nonsingular $\mb{Q}$ such that $\mb{Q'AQ}$ is a diagonal matrix.
	\item \textbf{T14.3.4}. For any $n\times n$ symmetric $\mb{A}$, $\exists$ nonsingular $\mb{P}$ and diagonal $\mb{D}$ such that $\mb{A}=\mb{P'DP}$.
	\item \textbf{C14.3.6}. For any $n\times n$ $\mb{A}$, $\exists$ nonsingular $\mb{P}$ and scalars $d_1, \hdots, d_n$ s.t. the quadratic form $\mb{x'Ax}$ is expressible as a linear combination $\sum_{i=1}^n d_i y_i^2$ of the squares of the elements $y_1, \hdots, y_n$ of the transformed vector $\mb{y=Px}$
	\end{itemize}
\item Decomposition of symmetric nonnegative matrices
	\begin{itemize}
	\item \textbf{T14.3.7}. A non-null $n\times n$ $\mb{A}$ is symmetric nonnegative of rank $r$ $\iff$ $\exists$ $r\times n$ $\mb{A}$ s.t. $\mb{A}=\mb{P'P}$.
	\item \textbf{C14.3.9} (and \textbf{C14.3.13}). Let $\rank(\mb{A+A'}) = r$. The quadratic form $\mb{x'Ax}$ is nonnegative (or positive assuming $r=n$) $\iff$ $\exists$ $r\times n$ $\mb{P}$ s.t. $\mb{x'Ax}=\mb{Px}'\mb{Px}\, \forall \mb{x}$. 
	\item \textbf{C14.3.13}. An $n\times n$  symmetric $\mb A$ is positive definite $\iff$ $\exists$ nonsingular $\mb{P}$ s.t. $\mb{A=P'P}$.
	\item \textbf{L14.4.1}. Every symmetric nonnegative matrix has a symmetric positive definite generalized inverse.
	\end{itemize}
\item Some results regarding the uniqueness of LDU decomposition
	\begin{itemize}
	\item (\textbf{T14.5.6}) \textit{If} $\mb{A}$ has an LDU decomposition and if $\mb{A}$'s $n-1$ order principal submatrix is nonsingular, then the LDU decomposition is unique.
	\item (\textbf{C14.5.7}) $\mb{A}$ has unique LDU decomposition if its $i$th order principal submatrix is nonsingular for $i=1,\hdots, n-1$
	\item (\textbf{L14.5.8}) If a symmetric $\mb A$ has a unique LDU decomposition, then it's actually an U'DU decomposition.
	\end{itemize}
\item Obtaining generalized inverse from LDU or U'DU decomposition.
	\begin{itemize}
	\item (p236). If $\mb{A}$ has LDU (U'DU) decomposition, then $\mb G=\mb{U}^{-1}\mb{D}^{-}\mb{L}^{-1}$ [$\mb G=\mb{U}^{-1}\mb{D}^{-}(\mb{U}^{-1})'$] is a GI.
	\item (p236). Assume $\mb A$ has Cholesky dec. $\mb{A=T'T}$ and let $T_1$ be the submatrix of $\mb{T}$ obtained by striking out its null rows, and let $R$ be the right inverse of $\mb T_1$. Then, $\mb{RR'}$ is a GI of $\mb{A}$.
	\end{itemize}
\item On block-diagonal and partitioned matrices
	\begin{itemize}
	\item \textbf{C14.8.2}. If $\mb{A}=\begin{bmatrix}
\mb{T} & \mb U \\ \mb V & \mb W
\end{bmatrix}$ is sym. nonneg., then, $\mathcal{C}(\mb U) \subset \mathcal{C}(\mb{T})$, $\mathcal{R}(\mb V) \subset \mathcal{R}(\mb{T})$, $\mathcal{C}(\mb V) \subset \mathcal{C}(\mb{W})$, $\mathcal{R}(\mb U) \subset \mathcal{R}(\mb{W})$.
	\item \textbf{L14.8.3}. Let $\mb{A}=\diag(\mb{A}_1, \hdots, \mb{A}_k)$. Then, $\mb{A}$ is nonneg (positive) $\iff$ $\mb{A}_1,\hdots,\mb{A}_k$ are nonneg (positive). 
	\item \textbf{T14.8.4}. Let $\mb{A}=\begin{bmatrix}
\mb{T} & \mb U \\ \mb V & \mb W
\end{bmatrix}$. (1) $\mb{A}$ is positive definite $\implies$ Schur complements $\mb{W-V\mb{T}^{-1}\mb{U}}$ and $\mb{T-U\mb{W}^{-1}\mb{V}}$ are positive. (2) $\mb{A}$ is symmetric positive semidefinite $\implies$ Schur complements $\mb{W-V\mb{T}^{-}\mb{U}}$ and $\mb{T-U\mb{W}^{-}\mb{V}}$ are nonnegative. (3) $\mb{A}$ is positive semidefinite and $\mathcal{C}(\mb U) \subset \mathcal{C}(\mb{T})$ or $\mathcal{R}(\mb V) \subset \mathcal{R}(\mb{T})$ $\implies$ Schur complement $\mb{W-V\mb{T}^{-}\mb{U}}$ is nonnegative. (Similar applies to Schur complement of relative to $\mb{W}^-$.)
	\item \textbf{T14.8.5}. Partial converse of T14.8.4.
	\end{itemize}
\item Necessary and/or sufficient conditions for positive/nonnegative definiteness of symmetric matrices
	\begin{itemize}
	\item \textbf{T14.9.5}. Let $\mb{A}$ be symmetric. $\mb A$ is positive definite $\iff$ the determinants of all of its leading principal submatrices are positive.
	\item \textbf{T14.9.7}. Let $\mb{A}$ be symmetic.  $\mb A$ is positive definite $\iff$ (1) is nonsingular (2) the determinants of all its principal submatrices are nonnegative.
	\item \textbf{L14.9.10}. Let $\mb{A}$ be symmetric of rank $r$ with a non-singular $r\times r$ principal submatrix $\mb{A}_*$. $\mb{A}$ is nonnegative $\iff$ $\mb{A}_*$ is positive definite.
	\item \textbf{T14.9.1}. Let $\mb{A}$ be symmetric. $\mb{A}$ is nonnegative definite $\iff$ Every principal submatrix of $\mb{A}$ is nonnegative.
	\end{itemize}
\item Geomatrical considerations.
	\begin{itemize}
	\item p253. For any symmetric positive $\mb{A}$, $\mb{x'Ay}$ is an inner product for $\mathcal{R}^{n\times 1}$ as it satisfies all 4 properties (in p59).
	\item p253. Any inner product for $\mathcal{R}^{n\times 1}$ is expressible as a bilinear form.
	\end{itemize}
\item Some connections with linear spaces and row/colummn spaces
	\begin{itemize}
	\item \textbf{T14.11.1}. Let $\mb{W}$ be $m\times m$ symmetric nonnegative. Then, for any $m\times n$ $\mb{A}$ and $m\times p$ $\mb{B}$, the linear system $\mb{A'WAX=A'WB}$ is consistent.
	\item \textbf{L14.11.2}. For any $m\times n$ $\mb{A}$ and any $m\times m$ symmetric nonnegative $\mb{W}$, $\mathcal{C}(\mb{A'WA})=\mathcal{C}(\mb{A'W})$, $\mathcal{R}(\mb{A'WA})=\mathcal{C}(\mb{WA})$ and $\rank(\mb{A'WA})=\rank(\mb{WA})$.
  	\end{itemize}
\item Projection, Inner product, Generalized Least Squares
	\begin{itemize}
	\item \textbf{T14.2.8}. Let $\mathcal{U}\subset \mathcal{R}^n$, $\mb{X}$ be s.t. $\mathcal{C}(\mb{X})=\mathcal{U}$, $\mb{W}$ be any spd. Then, $\exists$ A s.t. $\mb{Ay}$ is the projection of any $\mb{y}\in\mathcal{R}^n$ on $\mathcal{U}$. Moreover, $\mb{A}=\mb{P_{X,W}}$. 
	\item \textbf{T14.12.11}. Properties of the projection matrix wrt spd $\mb{W}$ (more on p261)
		\begin{itemize}
		\item $\mb{P_{X,W}X=X}$ and also $\mb{WP_{X,W}X=WX}$
		\item $\mb{WP_{X,W}}$ and $\mb{P_{X,W}W^{-1}}$ are symmetric
		\item $\mb{P_{X,W}}$ and $(\mb{I}-\mb{P_{X,W}})$ are idempotent
		\item $\mathcal{C}(\mb{P_{X,W}})=\mathcal{C}(\mb{X})$ and $\mathcal{R}(\mb{P_{X,W}})=\mathcal{R}(\mb{X'W})$
		\item $\mb{P'_{X,W}WP_{X,W}}=\mb{WP_{X,W}}$
		\end{itemize}
	\item \textbf{T14.12.16}. A matrix $\mb{A}$ is a projection matrix wrt spd $\mb{W} \iff \mb{A'WA=WA}$. 
	\item \textbf{T14.12.18}. Any of the following are necessary and sufficient conditions for $\mb{P_{X,V}}=\mb{P_{X,W}}$
		\begin{enumerate}
		\item $\mb{X'VP_{X,W}}=\mb{X'V}$ 
		\item $\mb{P'_{X,W}VP_{X,W}} = \mb{VP_{X,W}}$
		\item $\mb{VP_{X,W}}$ is symmetric
		\item $\exists$ matrix $\mb{Q}$ s.t. $\mb{VX=WXQ}$ or, equivalently, $\mathcal{C}(\mb{VX})\subset\mathcal{C}(\mb{WX})$
		\end{enumerate}
	\item \textbf{T14.12.23}. If $\mb{z}$ is a projection on orthogonal complement of $\mb{y}$ on $\mathcal{C}(\mb{X})^\perp_{\mb{W}}$, then $\mb{z}=\mb{y-Xb^*}$ for any solution to $\mb{X'WXb=X'Wy}$. In particular, $\mb{z=y-\mb{P}_{X,W}y}=(\mb{I-P_{X,W}})\mb{y}$
	\end{itemize}

\end{itemize}











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Matrix Differentiation}

\section{Notations}

Differentiation of \textit{scalar-valued} functions: Let $f(\mb{x})$ be a scalar-valued function of an $m$-dimensional vector $\mb{x}$.
\begin{itemize}
\item $\partial f(\mb{x})/\partial x_j$ or $D_jf(\mb{x})$ are the first order partial derivatives of $f$ wrt $x_j$.
\item $\partial f(\mb{x})/\mb{x}'$ or $\mb{D}f({\mb{x}})$ are row vectors $\partial f(\mb{x})\mb{x}' = \mb{D}f({\mb{x}})=(\partial f(\mb{x})/\partial x_1, \partial f(\mb{x})/\partial x_2, \hdots, \partial f(\mb{x})/\partial x_m)$
\item $\partial f(\mb{x})/\mb{x} = [\partial f(\mb{x})/\mb{x}']'$ 
\end{itemize}
%
%
Differentiation of \textit{vector of functions}: Let $\mb{f}$ be a column vector with $p$ functions $\mb{f}=(f_1, \hdots, f_p)'$.
\begin{itemize}
\item $D_j\mb{f(c)}$ is the column vector $D_j\mb{f(c)}=[D_jf_1(\mb{c}), \hdots, D_jf_p(\mb{c})]'$.
\end{itemize}
%
%
Differentiation of matrix of functions: Let $\mb{F}$ be a $p\times q$ matrix of $pq$ functions,  $\mb{F}=\{f_{st}\}$.
\begin{itemize}
\item $\partial \mb{F(x)}/x_j$ is the $p\times q$ matrix whose $st$th element $\partial f_{st}\mb{(x)}/x_j$.
\end{itemize}


\section{Definitions}
\begin{itemize}
\item \textit{Neighborhood} of an $m$-dimensional vector $\mb{c}$ with the usual norm for $\mathcal{R}^{m\times 1}$ is $B_r(\mb{c})=\{\mb{x}\in \mathcal{R}^{m\times 1} : ||\mb{x-c}||<r\}$.
\item \textit{Interior point}. If there is a neighborhood of $\mb{x}$ that is contained entirely in a set $S$, then $\mb{x}$ is an interior point of $S$.
\end{itemize}
%
Let $f(\mb{x})$ be a scalar-valued function of an $m$-dimensional vector $\mb{x}$.
\begin{itemize}
\item \textit{Open set} is a set whose all points are interior points.
\item \textit{Continuity}: A function $f(\mb{x})$ is continuous at an interior point $\mb{c}$ if $\lim_{\mb{x}\to\mb{c}} f(\mb{x})=f(\mb{c})$.
\item \textit{Continuously differentiable function} is a function whose all first-order derivatives exists and are continuous (p288).
\item \textit{Continuously differentiable matrix} is an $\mb{F}$ whose all $pq$ elements (\ie functions) are continuously differentiable.
\item \textit{Gradient vector} of $f$ is $[\mb{D}f(\mb{x})]' = \partial f(\mb{x})/\mb{x}$
\item \textit{Hessian}, notated with $\mb{H}f$, is the $m\times m$ matrix whose $ij$th element is $\partial^2 f(\mb{x})/\partial x_i \partial x_j$
\end{itemize}
%
Let $\mb{f}$ be a column vector with $p$ functions $\mb{f}=(f_1, \hdots, f_p)'$.
\begin{itemize}
\item \textit{Jacobian matrix} is the $p\times m$ matrix $\mb{Df(c)}=[D_1\mb{f}(c), \hdots, D_m\mb{f}(c)]$  (see above for $D_j\mb{f}(c)$).
\item \textit{Jacobian} is $\det[\mb{Df(c)}]$
\item \textit{Gradient vector} is the $m\times p$ matrix $[\mb{Df(c)}]'$
\end{itemize}
Some basic results:
\begin{itemize}
\item $\frac{\partial \mb{F}'}{x_j}=\left(\frac{\partial \mb{F}}{x_j}\right)'$
\end{itemize}


\section{Results on matrix/vector differentiation}

\subsection*{On linear and quadratic forms}
\begin{itemize}
\item Let $\mathbf{a}=(a_1,\hdots,a_m)$ be constant (or fn of $\mb{x}$ that is invariant wrt $x_j$), and $\mb{A}$ be an $m\times m$ constant matrix (or matrix of functions invariant wrt $x_j$). Then:
\begin{itemize}\textsf{•}
\item $\frac{\partial  \mb{a'x}}{\partial x_j}=a_j$ (see p294). The core idea is to see that $\frac{\partial x_i}{\partial x_j}=\begin{cases}1 &\text{ if }i=j\\0& \text{ else}\end{cases}$
\item $\frac{\partial  \mb{a'x}}{\partial \mathbf{x}}=\mb{a}$ or $\frac{\partial  \mb{a'x}}{\partial \mathbf{x}'}=\mb{a}'$
\item $\frac{\partial \mb{x'Ax}}{\partial x_j} = \sum\limits_{i=1}^m a_{ij}x_i+\sum\limits_{k=1}^{m} a_{jk}x_k$ (see \#295)\\ 
\item $\frac{\partial \mb{x'Ax}}{\partial \mb{x}} = (\mathbf{A+A'})\mathbf{x}$ \---- unless $\mb{A}$ is symmetric, in which case $\frac{\partial \mb{x'Ax}}{\partial \mb{x}} = 2\mathbf{Ax}$
\item $\frac{\partial^2 \mb{x'Ax}}{\partial x_s x_j} = a_{sj}+a_{js}$ (see \#295)
\item $\frac{\partial^2 \mb{x'Ax}}{\partial \mb{x}\partial\mb{x}'} = (\mathbf{A+A'})$ \---- unless $\mb{A}$ is symmetric, in which case  $\frac{\partial^2 \mb{x'Ax}}{\partial \mb{x}\partial\mb{x}'} = 2\mathbf{A}$
\end{itemize}
\end{itemize}
%
\subsection*{Differentiation of vector/matrix forms}
\begin{itemize}
\item Let $\mb{x}=(x_1,\hdots,x_m)$ and $\mb{X}$ a matrix.
\begin{itemize}
\item $\frac{\partial \mathbf{x}}{\partial x_i} = \mb{u}_j$
\item if $\mb{X}$ matrix of `independent' variables $x_{ij}$:\\
$\frac{\partial \mathbf{X}}{\partial x_{ij}} = 	\mathbf{u}_i\mb{u}_j'$
\item if $\mb{X}$ symmetric matrix: \\
$\frac{\partial \mb{X}}{\partial x_{ij}} = \mb{u}_i\mb{u}_j'+\mb{u}_j	\mb{u}_i'$
%
\end{itemize}
\end{itemize}
\subsection*{Differentiation of matrix of functions}
\begin{itemize}
\item Let $\mb{F, G}$ be matrices of functions
\begin{itemize}
\item $\frac{\partial \mb{FG}}{\partial x_j} = \mb{F}\frac{\partial \mb{G}}{\partial x_j}+\frac{\partial \mb{F}}{\partial x_j}\mb{G}$
\item $\frac{\partial \mb{FGH}}{\partial x_j} = \mb{FG}\frac{\partial \mb{H}}{\partial x_j}+\mb{F}\frac{\partial \mb{G}}{\partial x_j}\mb{H}+\mb{FG}\frac{\partial \mb{H}}{\partial x_j}$
\item if $g$ is fn of $\mathbf{x}: \frac{\partial g\mb{F}}{\partial x_i} = \frac{\partial g}{\partial x_i}\mb{F}+g\frac{\partial \mb{F}}{\partial x_i}$
\end{itemize}
\end{itemize}
%
%
\subsection*{Trace and differentiation}
\begin{itemize}
\item Let $\mb{F}$ be a matrix of functions of $\mb{x}$, $\mb{A}$ matrix of constants and $\mb{X}$ a matrix of variables $\mb{X}=\{x_{ij}\}$ 
	\begin{itemize}
	\item  $\frac{\partial \tr(\mb{F})}{\partial x_j} = \tr\left(\frac{\partial \mb{F}}{\partial x_j}\right)$ 
	\item if $\mb{X}$ is matrix of `independent' variables: $\frac{\partial \mathbf{X}}{\partial x_{ij}} = 	\mathbf{u}_i\mb{u}_j'$
	\item if $\mb{X}$ symmetric matrix: $\frac{\partial \mb{X}}{\partial x_{ij}} = \mb{u}_i\mb{u}_j'+\mb{u}_j	\mb{u}_i'$	
	\end{itemize}
\end{itemize}

\subsection*{Derivatives of determinants}
\begin{itemize}
\item $\mb{X}, \mb F, \mb A$ defined just as above
	\begin{itemize}
	\item $\frac{\p\det(\mb{X})}{\p x_{ij}} = \xi_{ij}$ where $\xi_{ij}$ is the cofactor of $x_{ij}$
	\item If $\mb X$ is matrix of `independent' variables
		\begin{itemize}	
		\item $\frac{\p\det(\mb{X})}{\p \mb{X}} = [\adj(\mb{X})]'$
		\item $\frac{\p \log\det \mb X}{\p \mb X} = (\mb{X}^{-1})'$
		\end{itemize}
	\item If $\mb X$ is symmetric matrix
		\begin{itemize}
		\item $\frac{\p\det(\mb{X})}{\p \mb{X}} = 2 \adj(\mb{X}) - \diag(\xi_{11}, \hdots, {\xi_{mm}})$
		\item $\frac{\p \log\det \mb X}{\p \mb X} = 2\mb{X}^{-1} - \diag(y_{11}, \hdots, y_{mm}) $		
		\end{itemize}

	\item $\frac{\p\det(\mb{F})}{\p x_j} = \tr\left[\adj(\mb{F})\frac{\p\mb F}{\p x_j}\right]' = $ (if $\mb F$ nonsingular) $ = |\mb F| \tr\left( \mb{F}^{-1} \frac{p \mb F}{\mb x_j} \right) $
	\item $\frac{\p \log\det \mb F}{\p x_j} = \tr\left(\mb F^{-1}\frac{\p \mb F}{\p x_j}\right)$
	\end{itemize}
\end{itemize}

\subsection*{Derivatives of inverses and generalized inverses}
\begin{itemize}
\item $\mb F $ defined as above
	\begin{itemize}
	\item If $\mb F$ nonsingular: $\frac{\p \mb{F}^{-1}}{\p x_j} = \mb F^{-1} \frac{\p \mb F}{\p x_j} \mb F^{-1} $
	\item For any $\mb F$, $\exists$ a generalized inverse $\mb {G}$ such that $\frac{\p \mb{G}}{\p x_j} = \mb G \frac{\p \mb F}{\p x_j} \mb G $ (T15.10.1)
	\item For any $\mb F$ and any of its GIs $\mb G$, $\mb F \frac{\p \mb{G}}{\p x_j} \mb F  = \mb F \mb G \frac{\p \mb F}{\p x_j} \mb G \mb F $ (L15.10.5)
	\end{itemize}

\end{itemize}


%Basic rules/results on matrix/vector differentiation.





\section{Notes}
 \begin{itemize}
	\item \textbf{T15.7.1} (The Chain Rule). Let $\mb{h} = \{h_i\}$ be an $n\times 1$ vector of functions of variables $\mb{x}=(x_1,\hdots,x_m)$. Let $g$ be a scalar-valued function of a vector of $\mb{y}=(y_1,\hdots,y_n)$. Define $f(\mb{x}) = g[\mb{h(x)}]$. Then, the $j$th partial derivative of $f$: $$D_j f(\mb{c}) = \sum_{i=1}^n D_i g[\mb{h}(c)]D_j h_i(\mb{c}) = \mb{D}g[\mb{h(c)}]D_j\mb{h(c)}$$\\
	%
	%
	Alternative notation: $$\frac{\partial f}{\partial x_j} = \sum_{i=1}^n \frac{\partial g}{\partial y_i }\frac{\partial h_i}{\partial  x_j} = \frac{\partial g}{\partial \mb{y}'}\frac{\partial \mb{h}}{\partial x_j}.$$\\
	%
	%
	Differentiation wrt. vector: $$\frac{\partial f}{\partial \mb{x}'} = \sum_{i=i}^n \frac{\partial g}{\partial y_i }\frac{\partial h_i}{\partial  \mb{x}'} = \frac{\partial g}{\partial \mb{y}'}\frac{\partial \mb{h}}{\partial \mb{x}'}.$$\\
	%
	%
	Differentiation of vector of functions $\mb f(\mb{x}) = \mb g[\mb{h(x)}]$ wrt. single variable: 
	$$\frac{\partial \mb{f}}{\partial x_j} = \sum_{i=1}^n \frac{\partial \mb g}{\partial y_i }\frac{\partial h_i}{\partial  x_j} = \frac{\partial \mb g}{\partial \mb{y}'}\frac{\partial \mb{h}}{\partial x_j}.$$\\
	%
	%
	Differentiation of vector of functions wrt. vector: 
	$$\frac{\partial \mb{f}}{\partial \mb{x}'} = \sum_{i=1}^n \frac{\partial \mb g}{\partial y_i }\frac{\partial h_i}{\partial  \mb{x}'} = \frac{\partial \mb g}{\partial \mb{y}'}\frac{\partial \mb{h}}{\partial \mb{x}'}.$$\\
	%
	%
	Differentiation of an $n\times r$ matrix $\mb H = \{h_{ij}\}$ of functions $f(\mb x) = g[\mb H(\mb x)]$ wrt. single variable: 
	$$\frac{\partial \mb{f}}{\partial x_j} = \sum_{i=1}^n \sum_{s=1}^r \frac{\partial \mb g}{\partial y_i }\frac{\partial h_{is}}{\partial  x_j} = \tr\left[ \left( \frac{\partial g}{\partial \mb Y} \right)' \frac{\partial \mb{H}}{\p x_j} \right]$$
\end{itemize}


\section{Some multiple integrals}
Let $\mb W$ be $n\times n$ symmetric positive definite matrix, $\mb A$ an $n\times n$ matrix and $\mb{k,c}$ be $n\times 1$ vectors. From C14.3.13 (see above), we know that $\exists$ nonsingular $\mb P$ s.t. $\mb{W=P'P}$. Using the variable transformation $\mb{z=P(x-c)}$, the following integrals are evaluated as:
%
$$\int_{\mathcal{R}^n}\exp[-(1/2)\mb{(x-c)'W(x-c)}]d\mb{x} = (2\pi)^{n/2} |\mb{W}|^{-1/2}$$
%
$$\int_{\mathcal{R}^n}(\mb{k'x})\exp [-(1/2)\mb{(x-c)'W(x-c)}] d\mb{x} = (\mb{k'c})(2\pi)^{n/2} |\mb{W}|^{-1/2}$$
$$\int_{\mathcal{R}^n}\mb{(x-c)'W(x-c)} \exp [-(1/2)\mb{(x-c)'W(x-c)}]  d\mb{x}  = (2\pi)^{n/2} |\mb{W}|^{-1/2}\tr(\mb{AW^{-1}}) $$




































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%x
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Redo:}
\subsection*{Chapter 4}
\begin{itemize}
\item Theorem 4.4.10
\end{itemize}
\subsection*{Chapter 11}
\begin{itemize}
\item Exercise 3
\end{itemize}
\subsection*{Chapter 13}
\begin{itemize}
\item Exercise 12.a
\item Re-prove T13.3.4
\end{itemize}



\clearpage
\chapter*{Towards a cheatsheet}
\section*{Non-singularity}
\begin{itemize}
\item Any positive definite matrix is nonsingular (L14.2.8)
\item 
\end{itemize}



\clearpage
\chapter*{Quizzes}
\begin{itemize}
\item How to check if a column vector $\mb{b}$ is in the column space (or range) of a matrix $\mb{A}$?\\
1) You can compare the ranks of $\mb A$ and $\mb{\tilde{A}} = [\mb{A}  & \mb b]$
2) You can do a ``full'' QR factorization on $[\mb{A}  & \mb b]$ and check if the last column generated a new vector for the orthogonal set of vectors extracted in the previous steps of the QR.
\end{itemize}






\bibliographystyle{IEEEtran}
\bibliography{../bibliography}



\end{document}









